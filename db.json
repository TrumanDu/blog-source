{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/wechatpay.jpg","path":"images/wechatpay.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/next-boot.js","path":"js/src/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/muse.js","path":"js/src/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.all-contributorsrc","hash":"43eb0149c78e464c695f0dd758bb8c59353182b3","modified":1553950396242},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1553950396243},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1553950396245},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1553950396247},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1553950396248},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1553950396278},{"_id":"themes/next/.gitignore","hash":"2edf061c2944a33635765dba3d95d3a882cd814b","modified":1561133738660},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1553950396275},{"_id":"themes/next/.travis.yml","hash":"fb9ac54e875f6ea16d5c83db497f6bd70ae83198","modified":1553950396277},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1553950396286},{"_id":"themes/next/bower.json","hash":"9869a845e7a8bf51558350d7620ec9e17be89265","modified":1553950396284},{"_id":"themes/next/_config.yml","hash":"8468d1e3e5918437307a8f382c0cda2e18b58de5","modified":1555155559068},{"_id":"themes/next/README.md","hash":"c84241a56ca078763324fc629fcebfd6e92875fd","modified":1553950396280},{"_id":"themes/next/gulpfile.coffee","hash":"67eaf2515100971f6195b60eeebbfe5e8de895ab","modified":1553950396325},{"_id":"themes/next/package.json","hash":"5912233efcabf8c6d7dcd2c2036c77b6631b5677","modified":1553950396507},{"_id":"source/_posts/DockerFile编写.md","hash":"18c2d997c7296640f7a388cb7eb9c6badb052c78","modified":1477314522646},{"_id":"source/_posts/ES-Kibana-Packetbeat使用总结.md","hash":"084fc4e072b6602929e6a239583f423caa24bb66","modified":1468847857862},{"_id":"source/_posts/Eclipse-Maven-Update-时JDK版本变更问题.md","hash":"bd2da96ccfe432748e753e84d4e242f09baac1c0","modified":1477315338234},{"_id":"source/_posts/Elasticsearch-For-HDFS.md","hash":"e95dbfe6d19b807eb80df63fe9a29388b603c957","modified":1532260669410},{"_id":"source/_posts/Elasticsearch-Mapping-Best-Practice.md","hash":"dbd752bf48b5ff7a0e9cc0ab4893c3b0d4967ba5","modified":1551536844862},{"_id":"source/_posts/Elasticsearch-store探索.md","hash":"b4e0fa23a8797eed9b67425c0294113033e56cd9","modified":1551536706053},{"_id":"source/_posts/Elasticsearch使用备注.md","hash":"1e38fc1135848d956f969331261543f6c8f90f3d","modified":1477315160031},{"_id":"source/_posts/Elasticsearch集群原理探索.md","hash":"580a404e97e83c00b8d52df45b75e6704ff5c09b","modified":1532256138589},{"_id":"source/_posts/Git命令总结.md","hash":"d880e91a63c719b151e34c4d0aefda43ba4c85aa","modified":1516016195326},{"_id":"source/_posts/Git-remote-同步远程仓库.md","hash":"30bbfb5116db55e48bc0031b6e92da023c073201","modified":1516016347253},{"_id":"source/_posts/HBase常用的操作二-java-API.md","hash":"0c5e73f13f21f3fa2d25ecaa060b69a54f20af51","modified":1465907549603},{"_id":"source/_posts/HBase常用的操作一-shell.md","hash":"0ef748093497ae0ffb362746f7ebe91aa374b4d0","modified":1465907375700},{"_id":"source/_posts/Hbase-使用教程.md","hash":"aed820c9ac995e038007680f9553ef65652e0e86","modified":1464789328024},{"_id":"source/_posts/HBase系统架构与存储格式.md","hash":"6222fc8824d4300d6d26e272fa20060ace72fd7a","modified":1465903463041},{"_id":"source/_posts/Hbase工程问题汇总.md","hash":"19422d49488cd0c36de5899d0881b2bb9c2bf93b","modified":1459953032359},{"_id":"source/_posts/Hbase数据迁移到Hive中.md","hash":"3cec82b3efda19f791dc39d32e18d049da336892","modified":1458998582596},{"_id":"source/_posts/ICE飞冰权限管理.md","hash":"23722dc1576354aa6a1e9ef08d2903b2f6391c98","modified":1555155444228},{"_id":"source/_posts/Hbase数据迁移到Hive中问题汇总.md","hash":"d8e17dddd52cbde0826e1d28c3d00bfdddda0db3","modified":1458996275766},{"_id":"source/_posts/JVM基本结构-内存分配.md","hash":"7b3053018dfa1132012be610a9e165fc67bdba82","modified":1551536911777},{"_id":"source/_posts/Java-多线程实现方式.md","hash":"e500b9c4eb36a5332ecd42a5c0563b3abc72fa4f","modified":1471783656144},{"_id":"source/_posts/Kafka-Blance-Topic.md","hash":"2cb87affb2b37be6bbe78d4fbfd6db8dfdf6d788","modified":1488718213340},{"_id":"source/_posts/KSQL-Tutorials-and-Examples.md","hash":"cee572ed6e034f0df2f2681a72791394283a28fe","modified":1561132054477},{"_id":"source/_posts/Kafka-消息生产及消费原理.md","hash":"a63761d8b3215c844b1c07f1b13dc0d316c26af6","modified":1562580115197},{"_id":"source/_posts/Kafka原理及设计理论总结.md","hash":"85ff66cec8ac217b03418040fb95dae1dd5635fc","modified":1488717458108},{"_id":"source/_posts/Kafka面试题与答案全套整理.md","hash":"e913b8f80228ba9e1370d6fe1cd5ca7db1fdba33","modified":1561132030384},{"_id":"source/_posts/Kibana之sentinl-slack使用教程.md","hash":"79c56fee8acb68f09ee41cf7d4e731a4a3566de1","modified":1505622419510},{"_id":"source/_posts/MapReduce笔记.md","hash":"2bdf00382bd2765fc4b43c9e458e6f4424f9536d","modified":1477314419490},{"_id":"source/_posts/Mapredure迁移数据到Hbase.md","hash":"33a665a0956a519d1eff48981841273b8afebfdb","modified":1566477519869},{"_id":"source/_posts/Markdown-语法学习.md","hash":"64bcf2e9ccf79944a17393577354c01003048ad6","modified":1457786888575},{"_id":"source/_posts/OutOfMemoryError-unable-to-create-native-thread问题追究.md","hash":"cf20109187085984ea7bba815e775149c3f55014","modified":1532260950763},{"_id":"source/_posts/React学习总结.md","hash":"063ee33bdf585e5ee80fb5241a86ba341d2e7fcf","modified":1554642760224},{"_id":"source/_posts/React问题笔记.md","hash":"da586b20f3ada615a8539d4fca003a2a864c22e1","modified":1539598049541},{"_id":"source/_posts/RediSearch-探索.md","hash":"1e86617ddd90271df600d06eef7883c184b5de96","modified":1500474081737},{"_id":"source/_posts/Redis-Cluster-Resharding.md","hash":"1c89c17a43d204e993a0608cf29e0caeeff112eb","modified":1566477106151},{"_id":"source/_posts/Redis-Cluster-批量操作实现.md","hash":"3f97e3497f125f58c4080d7ff41cc91546b4e342","modified":1465094055669},{"_id":"source/_posts/Redis-Cluster慢查询修复总结.md","hash":"b4b8c67e0557fa2f0e371c8e39c038bf5650fe3a","modified":1472385100298},{"_id":"source/_posts/Redis-Info信息详情.md","hash":"ca78d599f6e28fa33adcbd7d350fab7b7d9b8d23","modified":1464790853498},{"_id":"source/_posts/Redis-运维shell-工具.md","hash":"888722711d95a1be838a9845574ecd48025e7b9a","modified":1500474556998},{"_id":"source/_posts/Redis4-0-新特性尝鲜.md","hash":"50b92f0d806bcae1ae119f47fc8d3ae58c1bcd16","modified":1566477106166},{"_id":"source/_posts/RedisCluster-集群配置文件损坏修复.md","hash":"a7735e5b8880029697396010d0474ee68c776a90","modified":1496323152526},{"_id":"source/_posts/RedisCluster搭建.md","hash":"a76cabbaa87f9a5b34d968ceff6d41b287353c08","modified":1468848209181},{"_id":"source/_posts/Redis维护总结.md","hash":"4975685a51ed3ead6e63f9ffd27cdf014f4e6a4c","modified":1471783687395},{"_id":"source/_posts/RedisCluster构建批量操作探讨.md","hash":"95deb91de7ca871b2a25187642a12963668386cc","modified":1472384207953},{"_id":"source/_posts/RedisCluster监控系统.md","hash":"2bb37242add531dbad5347fec4832f366362570c","modified":1477316236486},{"_id":"source/_posts/Sentinl-（Kibana-Alert-Report-App-for-Elasticsearch.md","hash":"43b43cb121b179dfcee493e227848fc618d835c9","modified":1505623449235},{"_id":"source/_posts/Shell学习.md","hash":"5a459c584b827b56e910c497ef3ec3d83e1ec474","modified":1477314459727},{"_id":"source/_posts/Spring-Cloud简介.md","hash":"39395cc976867052e3733a0f20ab2b82f53e64a4","modified":1532259312627},{"_id":"source/_posts/Thymeleaf布局.md","hash":"d5cb5686a2c4a79e40008bfa00e11aad3002c740","modified":1532259304963},{"_id":"source/_posts/beat开发环境搭建.md","hash":"61e5125730d8ad239de14823b07ec6b279d52104","modified":1539596373132},{"_id":"source/_posts/docker-registry创建.md","hash":"195fcaa64234ecbb8bf900de3c5b31a804e6be51","modified":1477315656912},{"_id":"source/_posts/docker命令.md","hash":"b04594b77b823ca16152bcbe53a643e0da60bd1c","modified":1566477106166},{"_id":"source/_posts/eclipse-marketplace无法安装解决.md","hash":"2593c3a5f25c81683a72bc347b963bb0283840f7","modified":1516015361032},{"_id":"source/_posts/eureka获取注册服务.md","hash":"cda135da133b9a2bf4d0f69e8375a02ad4430860","modified":1532259225275},{"_id":"source/_posts/hbase单机搭建教程.md","hash":"f60723b38404b1e9473f24c52eb3257ef5904ba4","modified":1566477473050},{"_id":"source/_posts/git-reset、git-checkout和git-revert.md","hash":"5ae8975af65c02c920b1ffa59a1f44b705e35fee","modified":1532260496012},{"_id":"source/_posts/java8系列专栏之Date-Time-API.md","hash":"26e152b6383282c9d2b47d4f4feaf205d954bebb","modified":1556449387955},{"_id":"source/_posts/java8系列专栏之Lambda与Stream-API.md","hash":"e85ecb259d47134ba59ccc460684d809d6a6b417","modified":1556540082299},{"_id":"source/_posts/java8系列专栏之Optinal.md","hash":"0ee3e4e5342a463f197b6157329fce24373a3c60","modified":1556452450165},{"_id":"source/_posts/java8系列专栏之字符串.md","hash":"2a3d8c69845a0ab2488baad7aa962c09186a1edd","modified":1556449620989},{"_id":"source/_posts/jvm-在docker中内存占用问题探索.md","hash":"d9d4b462f0a8fad22978b0c6b0f200d71dd9b979","modified":1532259279306},{"_id":"source/_posts/kafka-consumer-源码分析（一）Consumer处理流程.md","hash":"ccf9e2e9442fd409596fa8acd86a38963af228c9","modified":1561647804178},{"_id":"source/_posts/kafka-consumer-源码分析（三）Consumer消费再均衡原理探究.md","hash":"529b1d464b872e9240c72d4157f24de96ed218f7","modified":1561897594832},{"_id":"source/_posts/kafka-consumer-源码分析（二）分区分配策略.md","hash":"e16db2b6bdeea75ad1746ef51cf52c16d746e213","modified":1561648321195},{"_id":"source/_posts/kafka-patition计算.md","hash":"55526ef94df6b07257401b0adefe6fd71bd8739b","modified":1488718286092},{"_id":"source/_posts/kafka-producer源码分析.md","hash":"cb9038a1cc79407a3eeac2f6680939b295cae971","modified":1561121485364},{"_id":"source/_posts/kafka-源码-idea-开发环境搭建.md","hash":"05375eb0b918d2bcaab68f952f142675c8c5da08","modified":1560520627368},{"_id":"source/_posts/kafka-逻辑架构设计.md","hash":"f996ab80252800942cec26c7696113ae8abca29d","modified":1562574616949},{"_id":"source/_posts/kafka使用教程.md","hash":"364e2bfbdd31e89d142e608056a0630eb4b27db5","modified":1566477106166},{"_id":"source/_posts/kafka幂等性和事务使用及实现原理.md","hash":"f8500abd32c193a2b562a87d59c1b21de6972f6e","modified":1562584421927},{"_id":"source/_posts/linux-shell问题记录.md","hash":"a6f5bc0a6f29f8f5779974414f33de7e590bc450","modified":1503307659222},{"_id":"source/_posts/linux-问题记录.md","hash":"03638cddb9b62eb12503596ff5ec0b0a62a9861d","modified":1566477497124},{"_id":"source/_posts/linux性能优化笔记之CPU篇.md","hash":"9c6f5a7143f4a286e023ef5975d42874580de4c5","modified":1551536781871},{"_id":"source/_posts/linux环境jdk安装及配置.md","hash":"1ac4f18307154d848f13a8b213c546f131ecd568","modified":1460722605616},{"_id":"source/_posts/linux运维问题.md","hash":"56a0ba452d447ab78642777976bc90dd83d68065","modified":1471783522581},{"_id":"source/_posts/logstash使用教程.md","hash":"01a5317ea51fa94a18c4081488cacdbfe4976450","modified":1477315594415},{"_id":"source/_posts/logstash插件开发.md","hash":"c84642cd45ed6f7c003eb4bc7e2aba67a0d082ee","modified":1477315437352},{"_id":"source/_posts/logstash插件使用.md","hash":"8b10f93ecfd1201faec4991c6ce7fbe9f9369bd9","modified":1477315533992},{"_id":"source/_posts/maven打包案例.md","hash":"1b212c40b074b821ac62b404c7ee040b6df3ea6d","modified":1566478144139},{"_id":"source/_posts/metricbeat-新增kafka-metrices-教程.md","hash":"6017a4d1c12ca6860891b276712fbfc329dcc899","modified":1539596571549},{"_id":"source/_posts/nginx-教程之-docker-化安装.md","hash":"74c53d41eb5b84a0eabf394a0e04ebb12625ef0a","modified":1516015649622},{"_id":"source/_posts/nginx-教程之-nginx-配置学习.md","hash":"688d49ef29dfd1acd3a10c57c24e658d530257a6","modified":1516015700194},{"_id":"source/_posts/node-js模块学习总结.md","hash":"98460d9fbe576fd0e60709cb6091008515a1f7ce","modified":1516015503298},{"_id":"source/_posts/node-js简介.md","hash":"797a975b308c61dc199192370fc79d51c2da3d63","modified":1503307446720},{"_id":"source/_posts/nodejs邮件发送.md","hash":"58573f172465856032a775c7aa0fa656c22157fc","modified":1505623772411},{"_id":"source/_posts/node-js读取json文件.md","hash":"2518c21c1a3b1640e211b41383ba4021e2911a8d","modified":1505623861460},{"_id":"source/_posts/oozie搭建及examples使用教程.md","hash":"707386d2df9718be3977fca260de4dd7c03fc61e","modified":1496324286996},{"_id":"source/_posts/spring学习（一）.md","hash":"ee1ec792522033cf6094b013bf9babf390024e49","modified":1468118701084},{"_id":"source/_posts/storm集群搭建.md","hash":"5b09f1770e5fd2e113381f359c68536a06128d8a","modified":1566478339842},{"_id":"source/_posts/x-pack之graph研究.md","hash":"de3c604eb1dd53d35613cb5f996a8970de7779bb","modified":1566477106166},{"_id":"source/_posts/zookeeper运维经验.md","hash":"ff4593dab4b31ba4122869fcf30aa09a59707ece","modified":1488718040403},{"_id":"source/_posts/不回忆就再也想不起来.md","hash":"a6d318779d23c37016c84bf0d9007b24495272ff","modified":1539599651916},{"_id":"source/_posts/使用maven创建自定义archetype.md","hash":"25d7ab63442b5f868a8365429cf248d4be70f69b","modified":1472383828533},{"_id":"source/_posts/利用winscp与putty构建自动化部署.md","hash":"4597de7ca70997d061379ca0384c6a4f094050a2","modified":1463144003974},{"_id":"source/_posts/如何优雅的下线kafkaleader.md","hash":"a77e156e870e6527938764e6cb6d3164bc6162bf","modified":1551706743354},{"_id":"source/_posts/在express站点中使用ejs模板引擎.md","hash":"f06cf9248257ff57e40b9a86357b734703654e49","modified":1505622494012},{"_id":"source/_posts/利用外部表读取orc文件.md","hash":"f31b2b44ff8169026dac1ea2bdb3767f9890a4bb","modified":1471783597976},{"_id":"source/_posts/如何优雅的分析redis中的内存数据.md","hash":"6a8a6f62b832654d22c4d491946ad683bd3a4789","modified":1552480811950},{"_id":"source/_posts/如何监控kafka消费Lag情况.md","hash":"5fa2ad4e19bbbf814f74e7c3874d9c4f4edc5613","modified":1566477106182},{"_id":"source/_posts/如何精准化爬取github-repository给star用户信息.md","hash":"f3cf88224abea08bd8aad5425ad7eb311f7c4ac6","modified":1552480983040},{"_id":"source/_posts/开源项目学习记录.md","hash":"63a90e1d7e1722a337d1a3ca134814d7fa8a152d","modified":1468848950040},{"_id":"source/_posts/浅析零拷贝技术.md","hash":"c9d173f9a0dc888d2151280dcc1ae30f3f0d5b64","modified":1560520209701},{"_id":"source/_posts/百度地图调用问题解决方案.md","hash":"0036d795f77626c760f0e6a8ff7a65adad7f25a2","modified":1532260294153},{"_id":"source/_posts/算法篇之排序.md","hash":"0e36b7b6119e9223c79f6f0e3360696158400a8c","modified":1542203543153},{"_id":"source/categories/index.md","hash":"9b67de182c99b575486f4f9643c02bc5b9610523","modified":1457793682309},{"_id":"source/about/index.md","hash":"7255aba436efc54dce351c65411e023d19f1b557","modified":1554726262892},{"_id":"source/tags/index.md","hash":"172b1402f2b1b59faea485deb82533ff5db2981d","modified":1457788046913},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1553950396289},{"_id":"themes/next/docs/AUTHORS.md","hash":"51a0a13da55ff3d596970b2f9ab4531c6b2211f2","modified":1553950396292},{"_id":"themes/next/docs/DATA-FILES.md","hash":"9a1895c0a0db705c4c48f512e86917f9af1ec3fb","modified":1553950396294},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"1dada3c3404445a00367882b8f97cdf092b7943d","modified":1553950396291},{"_id":"themes/next/docs/INSTALLATION.md","hash":"b74ef6fedf76cdb156e2265759ee0a789ddd49cc","modified":1553950396296},{"_id":"themes/next/docs/MATH.md","hash":"0540cd9c961b07931af9f38a83bc9a0f90cd5291","modified":1553950396300},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"721a1aa9feed1b580ab99af8e69ed22699121e88","modified":1553950396297},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1553950396299},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"c9f2ed8e15c137b1885d9ca8b7197d9f457971e9","modified":1553950396302},{"_id":"themes/next/languages/de.yml","hash":"79b3221344da335743b5ef5a82efa9338d64feb0","modified":1553950396327},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1553950396329},{"_id":"themes/next/languages/fr.yml","hash":"0393558717065293bdf732866471cebb0c884f6a","modified":1553950396330},{"_id":"themes/next/languages/en.yml","hash":"d66b8b48840443a4f9c72c7696a21e292f685a47","modified":1553950396329},{"_id":"themes/next/languages/id.yml","hash":"f3302a4dfdc9be38a52d6e081411574b1ea01671","modified":1553950396332},{"_id":"themes/next/languages/ja.yml","hash":"3f25eca504ee5a519987b4402731f1bb7f5191c9","modified":1553950396336},{"_id":"themes/next/languages/it.yml","hash":"31eb878b53d60ff47e3e534cdd7a839c8801ac6e","modified":1553950396333},{"_id":"themes/next/languages/ko.yml","hash":"75f2fe142f76bf623e34ed3570598226f55f2b8b","modified":1553950396336},{"_id":"themes/next/languages/nl.yml","hash":"08f16ce395dacc88847fc30dc6b985ce22fb8948","modified":1553950396338},{"_id":"themes/next/languages/pt-BR.yml","hash":"c7de8b77f44e75be4f04423088a1c891537aa601","modified":1553950396339},{"_id":"themes/next/languages/ru.yml","hash":"720b92a9ec075b68737d296b1f29ad8e01151c85","modified":1553950396343},{"_id":"themes/next/languages/tr.yml","hash":"6d2f53d3687a7a46c67c78ab47908accd8812add","modified":1553950396344},{"_id":"themes/next/languages/pt.yml","hash":"ca5072c967e5eb1178ffed91827459eda6e4e6e2","modified":1553950396341},{"_id":"themes/next/languages/uk.yml","hash":"6320439c6e9ff81e5b8f8129ca16e9a744b37032","modified":1553950396345},{"_id":"themes/next/languages/vi.yml","hash":"e2f0dd7f020a36aa6b73ed4d00dcc4259a7e5e9d","modified":1553950396347},{"_id":"themes/next/languages/zh-HK.yml","hash":"c22113c4a6c748c18093dae56da5a9e8c5b963cd","modified":1553950396354},{"_id":"themes/next/languages/zh-TW.yml","hash":"dbf4dd87716babb2db4f5332fae9ec190a6f636a","modified":1553950396356},{"_id":"themes/next/languages/zh-CN.yml","hash":"069f15da910d6f9756be448167c07ea5aa5dc346","modified":1553950396348},{"_id":"themes/next/layout/_layout.swig","hash":"ba786b1baba49021928e2e508da53f2fd1369b3f","modified":1553950396363},{"_id":"themes/next/layout/archive.swig","hash":"61bc56e77e653684fc834f63dcbdadf18687c748","modified":1553950396497},{"_id":"themes/next/layout/category.swig","hash":"ad0ac6a1ff341f8eab9570e7fb443962948c5f9d","modified":1553950396499},{"_id":"themes/next/layout/index.swig","hash":"bdcc9f57adef49706b16b107791cacecbc23c1dc","modified":1553950396500},{"_id":"themes/next/layout/page.swig","hash":"5d06ee8f477ffc39932d0251aa792ffcaf8faf14","modified":1553950396502},{"_id":"themes/next/layout/tag.swig","hash":"283519d4d5b67814412863a3e0212bac18bcc5a0","modified":1553950396506},{"_id":"themes/next/layout/post.swig","hash":"af74e97d57cf00cde6f8dbd4364f27910915454e","modified":1553950396503},{"_id":"themes/next/layout/schedule.swig","hash":"e79f43df0e9a6cf48bbf00882de48c5a58080247","modified":1553950396504},{"_id":"themes/next/scripts/merge-configs.js","hash":"5f96f63e86825fd7028c2522e4111103e261a758","modified":1553950396516},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1553950396518},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1553950396808},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1553950396810},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1553950396811},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396724},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"a51de08657f5946f4028b11373280ddc04639525","modified":1553950396304},{"_id":"themes/next/docs/ru/README.md","hash":"4dd2fccccdc7080554f1460690c79b68eb66057d","modified":1553950396307},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"7b2963daac19b0c14f98ebef375d5fbce8fc3f44","modified":1553950396306},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"1a4e41adcf5831057f3f7b3025ed4a5ef7c442b4","modified":1553950396309},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"e771c5b745608c6fb5ae2fa1c06c61b3699627ec","modified":1553950396311},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"750c9a7eee2ebb5c3e967b9233529a2d074bf4af","modified":1553950396314},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"018a259694f4a8c7c384e1f323531442cba5fbf3","modified":1553950396313},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"67f4a987e7db0ab1ce1ea4c311f2961df07b6681","modified":1553950396316},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"baca12cc24be082f1db28c7f283493569666321c","modified":1553950396317},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"b17fc344ff61603f83387c0f9b2b2189aae81d50","modified":1553950396319},{"_id":"themes/next/docs/zh-CN/README.md","hash":"5e98b411da7360dbd84a94185dbb3330f24739c3","modified":1553950396322},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"734b371a0dd910eb9fe087f50c95ce35340bb832","modified":1553950396320},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"2095d1214a4e519a1d31b67b41c89080fa3285d3","modified":1553950396323},{"_id":"themes/next/layout/_custom/head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1553950396359},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1553950396361},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1553950396362},{"_id":"themes/next/layout/_macro/post.swig","hash":"31ba947998f0c962b04ae7f42f9d3db934209a79","modified":1553950396371},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"891ab67815969dd8736cb22fbbb3f791b8fff4e4","modified":1553950396370},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"91017f58f83d9505ce99109fffdc51c032bf017e","modified":1553950396373},{"_id":"themes/next/layout/_partials/comments.swig","hash":"d0b9e841d55c974d02f43823a06a2627f8e46431","modified":1553950396375},{"_id":"themes/next/layout/_partials/footer.swig","hash":"6d56acdcdc12ebca9c1d90f8a2b52ad17aafca6e","modified":1553950396377},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"1ad13269b43b900356f3bdab7947d6a86f035a2c","modified":1553950396378},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"914155d5d758306cff405beefd4a07973fd8fc77","modified":1553950396395},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"dee345054d564dd56f74bb143942d3edd1cb8150","modified":1553950396396},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"c31d54154eed347f603009d2d65f7bf8d9a6885a","modified":1553950396417},{"_id":"themes/next/layout/_scripts/exturl.swig","hash":"9e00cb9b3fdfe2e2c4877a874d0d3ecb7fd0f3ee","modified":1553950396418},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"28b1cd4c065fcd214a1d6dd06f54bb62c3519aad","modified":1553950396420},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"efb3404a3303622f3be60944d9d1926972c5c248","modified":1553950396421},{"_id":"themes/next/layout/_scripts/scroll-cookie.swig","hash":"f58463133bf8cfef5ff07f686b834ff8cbbe492f","modified":1553950396431},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4130f995f0c4f81a44266194ecae9df96fad174c","modified":1553950396432},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"87bcb495f7ddd81cc3fe2c2a886e51c08053019b","modified":1553950396456},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"4b93dc7ac0573c402aabcb5c933bbcb893b07c51","modified":1553950396457},{"_id":"themes/next/layout/_third-party/chatra.swig","hash":"87182367d7954457cb2498bbfa9445c03c2d619e","modified":1553950396458},{"_id":"themes/next/layout/_third-party/copy-code.swig","hash":"07fc0ae1a30c5aa9269d6efdaec598164b1d191c","modified":1553950396474},{"_id":"themes/next/layout/_third-party/mermaid.swig","hash":"80dfc0879866e6512cb67590a3b2d8741a66f980","modified":1553950396482},{"_id":"themes/next/layout/_third-party/pangu.swig","hash":"76f5933925670044ec65b454295ba7e0a8439986","modified":1553950396484},{"_id":"themes/next/layout/_third-party/pdf.swig","hash":"4ae61c7efb16e962385bfe522a38c4d29cdcccbe","modified":1553950396485},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"7db4ad4a8dd5420dad2f6890f5299945df0af970","modified":1553950396483},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"7cc1294a5fbedf3502688248a433c358339e5ae0","modified":1553950396487},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"c476dc3693a9dd0be2d136a45b0d7fdef55d4d92","modified":1553950396488},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"882cd0b68c493af1b6d945660f9c21085e006ffc","modified":1553950396489},{"_id":"themes/next/layout/_third-party/tidio.swig","hash":"b44010cd577e4d063c3406772938c4b117ec7b7b","modified":1553950396495},{"_id":"themes/next/scripts/filters/exturl.js","hash":"b19c7c1021e57367b3b3bbf5678381017ed5667d","modified":1553950396511},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cdb6152582313268d970ffeef99b4a8a7850f034","modified":1553950396513},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"a40ce6bc852bb4bff8b9f984fa064741dd151e96","modified":1553950396515},{"_id":"themes/next/scripts/tags/button.js","hash":"95a520f6529424a03c7ead6dbfd5e626d672febb","modified":1553950396520},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"4519ab8e6898f2ee90d05cde060375462b937a7d","modified":1553950396522},{"_id":"themes/next/scripts/tags/exturl.js","hash":"20014a7cd234beb1c3b0facdd9dddef58e8eb85a","modified":1553950396524},{"_id":"themes/next/scripts/tags/full-image.js","hash":"a6b2264215c555c553b2c5db85fa90678798d0d5","modified":1553950396525},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"ab4a82a7246265717556c7a42f897430340b88cf","modified":1553950396528},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1553950396530},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"23d839333422375e85d44e476f554faf49973a3c","modified":1553950396527},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1553950396531},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1553950396533},{"_id":"themes/next/scripts/tags/pdf.js","hash":"ab995f0fc60d60f637220e2651111b775b8a06de","modified":1553950396534},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1553950396537},{"_id":"themes/next/scripts/tags/tabs.js","hash":"72a5adbd8f300bee1d0c289367598ca06b2bed17","modified":1553950396536},{"_id":"themes/next/source/css/main.styl","hash":"5e7d28bc539e84f8b03e68df82292f7fc0f2d023","modified":1553950396722},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1553950396728},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1553950396726},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1553950396729},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1553950396731},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1553950396734},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1553950396732},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1553950396735},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1553950396737},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1553950396738},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1553950396740},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1553950396741},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1553950396743},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1553950396745},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1553950396744},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1553950396751},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1553950396749},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1553950396747},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1553950396748},{"_id":"themes/next/source/images/wechatpay.jpg","hash":"3abed4f67e5acb1301615bd65a916ae5364d443d","modified":1503318510941},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396667},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396668},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396672},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396717},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1553950396720},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"25aea3d764b952f3f6d28ab86d7212d138e892df","modified":1553950396368},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"4eb8e222dc337211efb0d3bbdb5e29af3e6ecdb8","modified":1553950396367},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"b57bf9c865bed0f22157176a8085de168a1aef77","modified":1553950396380},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"466e69a4b4fbdc57f33e60a16b0d87e494385e21","modified":1553950396383},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"fd079a414ca0f42f4cddd00247a9d5a5f58c4d8e","modified":1553950396382},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"c909f6e96373c151dea325bcddfdd8c9522421b6","modified":1553950396387},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"03f669356bbaa70144b743f3312178e1981ac3a8","modified":1553950396385},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"39c4ad0e36b7c1260da98ba345f7bd72a2ac0f2e","modified":1553950396388},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"e015c7d9b84062b60b15b36be3ef11929dd10943","modified":1553950396390},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"daa6e5b7dbc409d6bf8a031d5413d8229e9c0995","modified":1553950396392},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"f46699a9daa5fef599733cbab35cb75cf7a05444","modified":1553950396394},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"be6683db6a269d83bb0441d7cf74db63a240fa8a","modified":1553950396398},{"_id":"themes/next/layout/_partials/post/reward.swig","hash":"f62b801c7999da67b4bdca9c5e373b9b5ed039dc","modified":1553950396402},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f331ad02beea8990066d32ad6ec9f859672c3615","modified":1553950396400},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"fb7727e8ec63a58238a7206bf70eb273c8879993","modified":1553950396403},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c609097b95eb6127c2784f47f2230e6e6efc0be2","modified":1553950396411},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"647e8677d1ccfb3f7918dd3ea2ff7078504a845d","modified":1553950396414},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"54b43d406cf37932e7b60f46814e864d31b1842c","modified":1553950396413},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"f14e9e8c27af82f1bfe794e252dec0d7e521f503","modified":1553950396407},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1553950396408},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1553950396405},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"31245e09ce0465b994cebd94223a531585c4eab4","modified":1553950396409},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"e0f0a753d4920ffb37ddbc8270515654a0b9b92a","modified":1553950396423},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a62c93f19429f159bcf0c2e533ffc619aa399755","modified":1553950396425},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"3c548934b97cc426544947f7a2ae35c270b5e33f","modified":1553950396427},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"84018384d00e4a584d613589adae6674a3060a36","modified":1553950396428},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"23c6d15aa2a305f9d29caee1b60cfae84d32fa09","modified":1553950396430},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"66d562b3778dbc839f7c00103bd0099c5d61602a","modified":1553950396436},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"83dd7df11b100bae38c9faab9a478f92149a0315","modified":1553950396437},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"73576c9683d9ad9b124916dc6c660607fe7cc1fa","modified":1553950396439},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"2e1de38f44af00209129d4051b7ae307cb11ad68","modified":1553950396440},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"335005a9f8b36349f0ad0a7beeba6969c55fc7f7","modified":1553950396442},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"53202062267391353d49f269e7eb74eb87d30921","modified":1553950396443},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"54d7993ae773573ee103c22802b7e98b193e1a3a","modified":1553950396446},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"8ab040fccba41675bc835973515530af8a51f8bd","modified":1553950396444},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"39928f358dd13d9fc1a4641800e57be157ecd815","modified":1553950396449},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"623e73bedef067ac24a398ef27c8197295da872d","modified":1553950396447},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"71fb01bcad43bc9410ab19190373b9f7e59215b5","modified":1553950396450},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"d18c87d7839e7407e39acd2998bcc9e0b34611b0","modified":1553950396451},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a22d1ea29a5ffe46199ab7d108a291a05af8d5b6","modified":1553950396453},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"4cff8bf5c42c62f7f0ac1f0d70f839dae39ba77a","modified":1553950396454},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"d685df1516cb138d7a83bac5d7878a1e0fa8bc04","modified":1553950396464},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"bc3fc9d053b3d1fc0cd3918bf9a629a6f38f6414","modified":1553950396460},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"8b4a94dd80b3bac7c5390c8a7fd377b88c2cb78e","modified":1553950396465},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"4e86e1ace90a70bb8862f5e6de9dbe7bfc046bee","modified":1553950396467},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"9a4923d2aa5182531ea7a7fb9abe824450026208","modified":1553950396469},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"88b9613263380c8a114780dff9ba94ba62b113f9","modified":1553957239738},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1a5d94f5779a2ce13abc886dd78e0617f89c34b9","modified":1553950396471},{"_id":"themes/next/layout/_third-party/comments/utteranc.swig","hash":"cfc72fbbf49d54d21b89ac0fad2f189af0e43ddf","modified":1553957909484},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"1b72c755101c9dfb85da13df9a0abccf37cd1dd2","modified":1553950396473},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"43a20fa0e9ae2f4254f04813f9c619dd36b49ae5","modified":1553950396477},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"ea1c136f960667a0a13b334db497b9b19c41f629","modified":1553950396479},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"767ba29f258db5d2e5baf875a6f36ac1d44df6a3","modified":1553950396480},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"89e41d4c298d8d70b4d1c833c7e599d089f2b3d4","modified":1553950396491},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"d45ca53af17d1d83fd27f8ed0917a72f0060e1a9","modified":1553950396494},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"078bd2d5815eb23e8c5f74467dc0042babea00ae","modified":1553950396493},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1553950396663},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1553950396666},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"1aabac9e37a8f4451c86d09037b3a1f8b30eaf5e","modified":1553950396670},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"c5513b49daceeac0909ccfc8c9feb27ac4d0ac85","modified":1553950396671},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"e9b0752f08398709e787546a246baca12b4c557f","modified":1553950396714},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1553950396716},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"da7049f3d9a157abe0ecc62611edcf43605ba84d","modified":1553950396718},{"_id":"themes/next/source/css/_variables/base.styl","hash":"e37aab667be94576f6145b61a78cfe87836c68b6","modified":1553950396719},{"_id":"themes/next/source/js/src/affix.js","hash":"ad343aa406fd8181b5f310434817ce98fc2219e3","modified":1553950396754},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"84906eeae57bd06744dd20160b93eacf658f97e2","modified":1553950396756},{"_id":"themes/next/source/js/src/exturl.js","hash":"c48aa4b3c0e578a807fd3661e6cd4f3890777437","modified":1553950396757},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"f11e84def0352b7dd6393f1b83e55a40ab468686","modified":1553950396759},{"_id":"themes/next/source/js/src/motion.js","hash":"d0a6d9dbcc57159e54bbb1f683b86632ae0b78f0","modified":1553950396761},{"_id":"themes/next/source/js/src/next-boot.js","hash":"696a0c2cf158001576d56b48195ec8e39e835b47","modified":1553950396762},{"_id":"themes/next/source/js/src/post-details.js","hash":"7d309b771e86c7e22ce11cc25625481ef7d5985c","modified":1553950396763},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"c4867626afab749404daf321367f9b6b8e223f69","modified":1553950396769},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"68d3690152c89e7adb08bb35ec28dbda2bd93686","modified":1553950396770},{"_id":"themes/next/source/js/src/utils.js","hash":"6a07990fe4374f8485b7dfa5797d029d8c8a024d","modified":1553950396772},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1553950396775},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1553950396777},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1553950396778},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1553950396779},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1553950396781},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1553950396803},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1553950396805},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1553950396797},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1553950396806},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"1a4ac0d119f2126ef8951897338706edce112235","modified":1553950396651},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"13dfba1fc57ef39e7f2bbe15fe73bca1e47880a9","modified":1553950396543},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"9fd526db0527c71243f05e18086f937dc67b1c3e","modified":1553950396545},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"698e4d6d894dd3db14fca5695b84bafcc4b1e4aa","modified":1553950396544},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1553950396547},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"997058180065d986e05df72992cc2cbfd7febd7e","modified":1553950396548},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"a4c6ee546a94fd69e5b7a1e4c054ab8cacb73d2a","modified":1553950396578},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"afdd21533db18d846e1a2663b1199761b1bd2c1e","modified":1553950396605},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"cfa64bd8ee2ff9f943673e339d69341e76fbf031","modified":1553950396604},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"40144394fdfe05d400f39f6763f66f75479a2e34","modified":1553950396654},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"35c6fd7eab3779bd9e38b7ba8825ab0c67a1be7a","modified":1553950396655},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"97d39280d8f48ae250bb7d0982b37b066e0461ff","modified":1553950396657},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"fec36a14080104b5862e9f021eab117d87c5f7c5","modified":1553950396658},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"12d23b0a50d12b687886ae8f1ff2073e7313b914","modified":1553950396661},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1553950396660},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"a609ff811f2b2764f5470236fe2fb1f3aa6ccba5","modified":1553950396675},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"8da8416213127595dfc4d2b358639194647e7bd3","modified":1553950396679},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"d0e9065b0dbbc01811259f0597d1790268b4881b","modified":1553950396677},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"40f266e24af4dedc9497056ab18ebcfda38dd47d","modified":1553950396681},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1553950396680},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"06d9d00257abd28414ec0b746f866bf9911cf5ec","modified":1553950396683},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"25f05ed8da68d034dce7f06e0f20f6cd55841070","modified":1553950396686},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1553950396684},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"157e6915dcf5990566e463acffa71043b2651c07","modified":1553950396695},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"234b44cfd03f9c9e3e179ff5fd698ac876341913","modified":1553950396697},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"04706657af638f2746ae59520e6fc78577c7682c","modified":1553950396693},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"6aee54cd5a20181e18596565356bd54c66e33823","modified":1553950396698},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1553950396700},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"b9619c9827f969ca2e2f5878552362a7b858918f","modified":1553950396704},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"e73d6da74c5755442e831d8fd7d922c5b32bd892","modified":1553950396706},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0b3001909f3446843b226030524ea8498d4d8997","modified":1553950396708},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"5b5e0a02a7bf63de9efcd33a4e482939cce5822d","modified":1553950396709},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"adb7379f3b9001840eb38b260434e89365771a81","modified":1553950396712},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"0d6f0df798449b710e1e5dbd43d470089b2a3c95","modified":1553950396711},{"_id":"themes/next/source/js/src/schemes/muse.js","hash":"ccc0c5cd4ec6f8159c98990ad83f11a5c0b0234c","modified":1553950396765},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"3eea56cc9ce47bb4760930c4c69cebf847a7fbb2","modified":1553950396767},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1553950396783},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1553950396785},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1553950396787},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1553950396795},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"2df409df04fdb52d7234876a9f6e502edd4e3929","modified":1553950396550},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"a8f4d4b86acaa34c99111b2dde5d0779cc7e0de6","modified":1553950396553},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"c9cfb4b99e1ec8ec9cf075cb761b8f7fa5fe63fd","modified":1553950396554},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1553950396556},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"33200f60bd6a8bbfc66dd49a239bcc75c2f564c1","modified":1553950396557},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"b8647d6140141b0a160607f6353e4d4594cca92e","modified":1553950396559},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"3a0efe849587b34f20d4e260028dc799215b0bb3","modified":1553950396560},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"9c1a082e6c1f96187a099c3f4cb5424c0c9fd06e","modified":1553950396564},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"e5a5f8747fdf2ca960e4e73c081b8952afd62224","modified":1553950396568},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"024e8ff40ca881c6fbf45712897e22f58a3811ab","modified":1553950396562},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1553950396565},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fa1cea6fcc3f552d57cc7d28380a304859139bf6","modified":1553950396569},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1553950396571},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"9a8fb61bd2d184de9d206e62ba8961d1845c5669","modified":1553950396572},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"c27527cdeb9e3a9f447f7238f442a5dc33fde4e6","modified":1553950396575},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1553950396574},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"c97c819a65f6967485184399397601e5133deda6","modified":1553950396576},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1553950396580},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"f3b0d259e991ac86454ae5eac6bc94dc8691d8c9","modified":1553950396582},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"fc94dd09b4245143b452d6cf2fc4c12134d99d6d","modified":1553950396583},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a73346f999b31355075cd58637946a8950cf6f7e","modified":1553950396584},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"f14cefc99309934d4103a3aa785e1258d858813f","modified":1553950396586},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d77f85d3af2d7090d84b28ab01c6a49f92eec647","modified":1553950396589},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"b6a241626783d2ac115d683fd59ec283af68e5bb","modified":1553950396588},{"_id":"themes/next/source/css/_common/components/post/post-reading_progress.styl","hash":"4aad8e36178faaa71a767af0084d578df4c09f73","modified":1553950396592},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ccd0b1309acff0c676fdcc848a8ae2d05f0369ab","modified":1553950396593},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"13d365ee626c01f17ec664b3f54f51d8b9ee7cf4","modified":1553950396590},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1553950396595},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1553950396596},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"539fc0880b2e035e8316d5d4b423703195c1b7ba","modified":1553950396597},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"c8009fd9598a661b7d23158b5121b6ac266939e9","modified":1553950396599},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"981795aad232c8bd3f52a0ed8720db696d18a234","modified":1553950396600},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"615fca7dff197a2ca3df674cf963ce70b8525985","modified":1553950396602},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"a5484d8436b2b7862faf6e7309a9e7b88cdd0027","modified":1553950396608},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"ab1776c5dc537beabb0ab81a0f04e08bebad070b","modified":1553950396610},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"03a4e75e963e3e7cc393d588b1495a88d52e0e40","modified":1553950396611},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"e58bb8b7127aa21e8260493a425ec00fcb25d338","modified":1553950396614},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"b36eea093bd4b32056b5de6f370ff57e50b25a49","modified":1553950396613},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"9204c79c05d620ecd5d411cdf11e27441b6281dc","modified":1553950396617},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"7e2ba73073daaea0a18c3d67ff137dd683af7011","modified":1553950396616},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"7af79cbbe4013f549799013b9d2146f61eafc85e","modified":1553950396618},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"3cb387fa70017f3c24a1a1884461d29deda54585","modified":1553950396620},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"ed3a2960ebce7396d1893bb8e08c99c7d9259140","modified":1553950396621},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"2d58ad90f148e845bc7023751a7a13260600f8d6","modified":1553950396626},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"fde59300ec38868676ff5ed495b9dc9b02d07ffc","modified":1553950396624},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1553950396627},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"b43421291bf85b589e8d0ec853e238d36ab80631","modified":1553950396628},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"020fac447d7a17c03e2802f0f724ae0738088354","modified":1553950396630},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"d705cbd78503d24f20efbb79c896c5cb440c07e6","modified":1553950396631},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"4305813408a1cd6aba764a7769b94b081d383d4f","modified":1553950396636},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"da7a21f5a2f7dcf4c5a4788d7670159ca4132b65","modified":1553950396634},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"54c5398c7bf5b8bd9f38a9ece1dd82a9255f9a30","modified":1553950396633},{"_id":"themes/next/source/css/_common/components/third-party/copy-code.styl","hash":"d9c244b1c3a09a7fccd3c3f732e6fb112a8cd565","modified":1553950396638},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"2fbe52f955da41c7a14eb09918bf86a252e4504f","modified":1553950396641},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"a01484e350ad5fc9b1fdfbfafb2ddd9687ad4d20","modified":1553950396639},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"2a1008f1044b450b806adc166754ba9513e68375","modified":1553950396642},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"ed8a12982c0497eeb9d7642781abeb801428f83d","modified":1553950396644},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"6880467b4f6d7b057fb8291aa10966429a0a3bff","modified":1553950396645},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"35dc9f3990fadff3ea038d4e8ac75923219886ed","modified":1553950396646},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"7cf42f96ba6b249c75e00dad251ebacf7de61e6c","modified":1553950396649},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"9801977a23268e36c5deefd270423f6f1a0c3bb2","modified":1553950396648},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1553950396688},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1553950396691},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1553950396701},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1553950396793},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1553950396801},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1553950396791},{"_id":"public/categories/index.html","hash":"a61f8daa51c9768e9b954ece7b62b53d8d116a62","modified":1566477631488},{"_id":"public/about/index.html","hash":"ee78181698212b00861d1781862211234fdd1ec7","modified":1566477631509},{"_id":"public/tags/index.html","hash":"b93eec5976296fcd2eae3efc4a25e59c0287504c","modified":1566477631509},{"_id":"public/2019/06/14/浅析零拷贝技术/index.html","hash":"1717f96f4bff9599d00dbec2bdb0ebe2052fc14d","modified":1566477631509},{"_id":"public/2019/06/14/kafka-producer源码分析/index.html","hash":"a3217a566216654d71f24de87b06cf639cd63938","modified":1566477631509},{"_id":"public/2019/04/29/java8系列专栏之Lambda与Stream-API/index.html","hash":"5948e96fc9ea65c1470868c249c1ca4a3762ee42","modified":1566477631509},{"_id":"public/2019/06/14/kafka-源码-idea-开发环境搭建/index.html","hash":"bc111324104aeb30e463e8395e7730509ccf9fcf","modified":1566477631509},{"_id":"public/2019/04/28/java8系列专栏之Optinal/index.html","hash":"2ec9b909f5f1c057c0de3fed99cdbfc21414cf44","modified":1566477631509},{"_id":"public/2019/04/13/如何监控kafka消费Lag情况/index.html","hash":"85d024eb7f3cfb31941fefd46daac87671dcd761","modified":1566477631510},{"_id":"public/2019/04/13/Kafka面试题与答案全套整理/index.html","hash":"9d5f9127862b479b2655b1d98dbb015391232808","modified":1566477631510},{"_id":"public/2019/04/28/java8系列专栏之字符串/index.html","hash":"6018775581d89a58739364f523b833f51225ed4b","modified":1566477631510},{"_id":"public/2019/04/28/java8系列专栏之Date-Time-API/index.html","hash":"355522f8e13541f988c3091af5617a8621ee9c1c","modified":1566477631510},{"_id":"public/2019/04/07/React学习总结/index.html","hash":"811a8b72107996f0cdde5105e749514f12470040","modified":1566477631510},{"_id":"public/2019/04/07/ICE飞冰权限管理/index.html","hash":"95ee163c30fe9ab931e33e3087a5e925cac27c5a","modified":1566477631510},{"_id":"public/2019/03/15/KSQL-Tutorials-and-Examples/index.html","hash":"5cb42d2e4e256afa26f9277a514f7ac6e818a544","modified":1566477631510},{"_id":"public/2019/03/04/如何优雅的分析redis中的内存数据/index.html","hash":"a1057c875730a6fbc51f07e3b87fffb21138bd28","modified":1566477631510},{"_id":"public/2019/03/13/如何精准化爬取github-repository给star用户信息/index.html","hash":"d57cf89e74df681f3629ab22756f8ee8a6469b7a","modified":1566477631511},{"_id":"public/2019/03/04/如何优雅的下线kafkaleader/index.html","hash":"10cb09c6715c1cd297a69a82f0acedf939751fca","modified":1566477631511},{"_id":"public/2019/03/02/JVM基本结构-内存分配/index.html","hash":"4f9e50d939129aa2dac788cef0cec98583f92af1","modified":1566477631511},{"_id":"public/2019/03/02/Elasticsearch-Mapping-Best-Practice/index.html","hash":"eef6cf50e07398f7c261f85f81d662fe856e6397","modified":1566477631511},{"_id":"public/2019/03/02/Elasticsearch-store探索/index.html","hash":"c456ad3af4427d2c0de0ed0efad2a7154fd4f3af","modified":1566477631511},{"_id":"public/2019/03/02/linux性能优化笔记之CPU篇/index.html","hash":"fda3ff625af4a8a4d56f3d39d71bc7d8937ef11a","modified":1566477631511},{"_id":"public/2018/10/29/算法篇之排序/index.html","hash":"cf60964cbbccb24d6a450b927f551ed0b12f50c0","modified":1566477631511},{"_id":"public/2018/10/15/不回忆就再也想不起来/index.html","hash":"dda046f4e669358c595f7c23f23cb09112f59aa1","modified":1566477631511},{"_id":"public/2018/10/15/React问题笔记/index.html","hash":"9c515572ae2568a2820e58dc8f94943a9afc03ff","modified":1566477631511},{"_id":"public/2018/10/15/metricbeat-新增kafka-metrices-教程/index.html","hash":"de0a9f37135ce5a688207821abc3b37c84f3c93c","modified":1566477631511},{"_id":"public/2018/07/22/Thymeleaf布局/index.html","hash":"0611fde4009222494cc39c9989d7f74e1232aae8","modified":1566477631512},{"_id":"public/2018/10/15/beat开发环境搭建/index.html","hash":"d9e6780114f5cc2f2a15ce288c0cfed828dfdfbd","modified":1566477631512},{"_id":"public/2018/07/22/Elasticsearch-For-HDFS/index.html","hash":"9f15b11d1d55e14d30aaa0e568086b018f956a87","modified":1566477631512},{"_id":"public/2018/07/22/OutOfMemoryError-unable-to-create-native-thread问题追究/index.html","hash":"e0d308540d96d6ad9edbe0b145743fa1c8e0c08e","modified":1566477631512},{"_id":"public/2018/07/22/x-pack之graph研究/index.html","hash":"7fc97bc3d12198d44f1b60433504a9bad8e63410","modified":1566477631512},{"_id":"public/2018/07/22/eureka获取注册服务/index.html","hash":"5c63bf4eb9693a3277920d135f2c0e79e4a6f309","modified":1566477631513},{"_id":"public/2018/07/22/git-reset、git-checkout和git-revert/index.html","hash":"ff6a9daf37afdc1a6679f375d1cb34ea88d2d62e","modified":1566477631513},{"_id":"public/2018/07/22/Spring-Cloud简介/index.html","hash":"d81a16e3c67710f2a5e1b842d646d869fb9a56cd","modified":1566477631513},{"_id":"public/2018/07/22/Elasticsearch集群原理探索/index.html","hash":"1cfe60a4ad02632660c4bb51eaf5bc4f7d11bb96","modified":1566477631513},{"_id":"public/2018/07/22/jvm-在docker中内存占用问题探索/index.html","hash":"92541c0cd4d7e875727c63a5d4bfcf63153659c6","modified":1566477631513},{"_id":"public/2018/07/22/百度地图调用问题解决方案/index.html","hash":"9772120ad6f91f47a4efd3f5de1eb75e2c7899ed","modified":1566477631513},{"_id":"public/2018/01/15/nginx-教程之-nginx-配置学习/index.html","hash":"9f3975867b9dcf4d879081a7db65a13627581866","modified":1566477631513},{"_id":"public/2018/01/15/nginx-教程之-docker-化安装/index.html","hash":"a4ec71119412f79e7ed6c638b2281f4cb46b35eb","modified":1566477631513},{"_id":"public/2018/01/15/eclipse-marketplace无法安装解决/index.html","hash":"ffb4c1756a3c3bf533dff386fb13ad88c578b9a7","modified":1566477631513},{"_id":"public/2018/01/15/node-js模块学习总结/index.html","hash":"a36399de7b0f8e543cfb26e78c8100e0c8b0aa8c","modified":1566477631514},{"_id":"public/2018/01/15/Git-remote-同步远程仓库/index.html","hash":"97d86fe2c812629899237671ad6e7db721ef3777","modified":1566477631514},{"_id":"public/2018/01/15/Git命令总结/index.html","hash":"e5231e16aed34a24b2c19025aab0c8b3167b6b5f","modified":1566477631514},{"_id":"public/2017/09/17/在express站点中使用ejs模板引擎/index.html","hash":"b0875fac56c3691723b05678d5587236ad92f233","modified":1566477631514},{"_id":"public/2017/09/17/node-js读取json文件/index.html","hash":"43c06a9158f5abdfd921d20ceab7bddc70520033","modified":1566477631514},{"_id":"public/2017/09/17/Kibana之sentinl-slack使用教程/index.html","hash":"6e4b4396e87ec75186f811799bcf5185239727fd","modified":1566477631514},{"_id":"public/2017/09/17/nodejs邮件发送/index.html","hash":"92378d491c7104ce68da86ad817a08f7919004da","modified":1566477631514},{"_id":"public/2017/09/17/Sentinl-（Kibana-Alert-Report-App-for-Elasticsearch/index.html","hash":"aa8e7efe7974f6835bf94de18b497377f609bccd","modified":1566477631514},{"_id":"public/2017/08/21/node-js简介/index.html","hash":"e20a1d458c384b31ec0fe21942e3b5d375861a0c","modified":1566477631515},{"_id":"public/2017/08/21/linux-shell问题记录/index.html","hash":"e233163a06413f4d064f2adac53526375b102d7f","modified":1566477631515},{"_id":"public/2017/07/19/Redis4-0-新特性尝鲜/index.html","hash":"86da6ba2b8e4f071557de0c947cc54317eca17bc","modified":1566477631515},{"_id":"public/2017/07/19/Redis-运维shell-工具/index.html","hash":"3e0fb312e021563e9d5c155d541cc13de8ae1df9","modified":1566477631515},{"_id":"public/2017/07/19/RediSearch-探索/index.html","hash":"2b2b8a97d2fd134e6711d8994f2467352e001b82","modified":1566477631515},{"_id":"public/2017/06/01/oozie搭建及examples使用教程/index.html","hash":"1585166cc111a5827c49cf1f558fc4b6835aba72","modified":1566477631515},{"_id":"public/2017/06/01/RedisCluster-集群配置文件损坏修复/index.html","hash":"c26eb82b367c81e18565facd72408795db9f043b","modified":1566477631515},{"_id":"public/2017/03/05/kafka-patition计算/index.html","hash":"61f7daa030ff08de94dfe60c425ac929d329a906","modified":1566477631515},{"_id":"public/2017/03/05/Kafka-Blance-Topic/index.html","hash":"8f31d8a2995bedef923def2973ba32e360793648","modified":1566477631515},{"_id":"public/2017/03/05/zookeeper运维经验/index.html","hash":"626f3a762fdd710624b109573b875d4c74c3f392","modified":1566477631515},{"_id":"public/2017/03/05/kafka使用教程/index.html","hash":"728cb41e29152ffd679bb6d8a3b964d0c31a82f5","modified":1566477631515},{"_id":"public/2017/03/05/Kafka原理及设计理论总结/index.html","hash":"a1b536206e83f42fbd54acac2ac991b45af340ca","modified":1566477631516},{"_id":"public/2016/10/24/RedisCluster监控系统/index.html","hash":"d0d097389f71645dfc3e03c68ebf0d079ad10e35","modified":1566477631516},{"_id":"public/2016/10/24/docker-registry创建/index.html","hash":"7534a3c9cde77ac828cc5b80b9a0ed706434d773","modified":1566477631516},{"_id":"public/2016/10/24/logstash使用教程/index.html","hash":"fc95bf74795e3d2d7cfe43538e09ba8f1ab6b211","modified":1566477631516},{"_id":"public/2016/10/24/logstash插件使用/index.html","hash":"78f020210c09d9f6f8ba36ef640b983ea2f527a6","modified":1566477631516},{"_id":"public/2016/10/24/logstash插件开发/index.html","hash":"8b4e3dfe2b3bba3bd1b4c055c205bcc6d5cc72b7","modified":1566477631516},{"_id":"public/2016/10/24/maven打包案例/index.html","hash":"ed03962ecd93909278e6b2bed0954aa865d51841","modified":1566478451445},{"_id":"public/2016/10/24/Eclipse-Maven-Update-时JDK版本变更问题/index.html","hash":"008d74b7793136378329e686df2027128fba4257","modified":1566477631516},{"_id":"public/2016/10/24/Elasticsearch使用备注/index.html","hash":"5f7dba7be5af8ed69932f86f85b98201cf52ab46","modified":1566477631516},{"_id":"public/2016/10/24/docker命令/index.html","hash":"b88bfdb804ae6d5e6f0d7a28d2ae83e6db9cf281","modified":1566477631516},{"_id":"public/2016/10/24/Mapredure迁移数据到Hbase/index.html","hash":"ff9f4a111bdae46b1ffbbcf927097669511115ca","modified":1566477631516},{"_id":"public/2016/10/24/DockerFile编写/index.html","hash":"4c94696e5811b41e7ac2b479138bc24d343307cc","modified":1566477631517},{"_id":"public/2016/10/24/MapReduce笔记/index.html","hash":"551134bbaffe67ff2b3c89ddd3bc162e328ff126","modified":1566477631517},{"_id":"public/2016/08/28/使用maven创建自定义archetype/index.html","hash":"1470bc053d05135ad0d4adc200795df550a6040e","modified":1566477631517},{"_id":"public/2016/08/28/Redis-Cluster慢查询修复总结/index.html","hash":"29661326580f1001e50c786d037813a6b29d2dcb","modified":1566477631517},{"_id":"public/2016/08/21/Redis-Cluster-Resharding/index.html","hash":"dc06cb8d9e73dfc79b2cacbe12e8e853e2270991","modified":1566477631517},{"_id":"public/2016/08/21/Java-多线程实现方式/index.html","hash":"b2732aa8d14a0aba86d20372b2b4e2e3b978bacd","modified":1566477631517},{"_id":"public/2016/08/21/利用外部表读取orc文件/index.html","hash":"552c4a2634bd405a0b596ca5830d1e2790275aa7","modified":1566477631517},{"_id":"public/2016/07/18/linux运维问题/index.html","hash":"848a616cf83a17990a9662f9e8b190d5c0c7433d","modified":1566477631517},{"_id":"public/2016/07/10/开源项目学习记录/index.html","hash":"5f1ed8153c9100c646cf8e39381e9a9d8beb9ab9","modified":1566477631517},{"_id":"public/2016/07/18/ES-Kibana-Packetbeat使用总结/index.html","hash":"76dcbc37400fac3a4a5f0fd71af83752079be29c","modified":1566477631517},{"_id":"public/2016/07/18/RedisCluster搭建/index.html","hash":"5f5bf4790dbb6eaa4c241cb45133575e312b2436","modified":1566477631517},{"_id":"public/2016/07/10/spring学习（一）/index.html","hash":"90db52c270b30364c45283089b4e59ed15badb42","modified":1566477631518},{"_id":"public/2016/07/10/Shell学习/index.html","hash":"8e2638573cc613aa5dff93cdc37af8643cf18e79","modified":1566477631518},{"_id":"public/2016/06/14/HBase常用的操作二-java-API/index.html","hash":"e1a6f0778518b00849fc1f33a053e778211cbf08","modified":1566477631518},{"_id":"public/2016/06/14/HBase常用的操作一-shell/index.html","hash":"5372b4c388e6a8c4b55ba3dbef13ea8f2d475ffa","modified":1566477631518},{"_id":"public/2016/06/14/HBase系统架构与存储格式/index.html","hash":"d2575c6ea96a3c53393dae61587f3e3adebbbdb8","modified":1566477631518},{"_id":"public/2016/06/05/Redis-Cluster-批量操作实现/index.html","hash":"13f9dbe8711c3cdacaf08d8ec63632d30c739b13","modified":1566477631518},{"_id":"public/2016/06/01/Redis-Info信息详情/index.html","hash":"5041f837fe73060019327eba72ffe93be751225a","modified":1566477631518},{"_id":"public/2016/06/01/Redis维护总结/index.html","hash":"31c23c725dd3c7622227f49369df3c91ea98ad0c","modified":1566477631518},{"_id":"public/2016/06/01/Hbase-使用教程/index.html","hash":"722098b0b8b42c32ba39e28584dcdcfc391460ed","modified":1566477631518},{"_id":"public/2016/05/13/利用winscp与putty构建自动化部署/index.html","hash":"018d16c4a65db72b1ac3ef55bde5fcfe41366fd8","modified":1566477631519},{"_id":"public/2016/05/09/RedisCluster构建批量操作探讨/index.html","hash":"182ae8935d4b79c4bd9b4e51646f0e8b9d974d75","modified":1566477631519},{"_id":"public/2016/04/15/storm集群搭建/index.html","hash":"7b18b822da9b120d463ff838f8e803faad7db0f2","modified":1566478451446},{"_id":"public/2016/04/15/linux环境jdk安装及配置/index.html","hash":"73a7de28c7c09480e7950bf8b5ae64837cfdec31","modified":1566477631519},{"_id":"public/2016/04/06/Hbase工程问题汇总/index.html","hash":"bb29cacd3bfc7ac9ea1a3f561114381ec4eb0166","modified":1566477631519},{"_id":"public/2016/04/13/linux-问题记录/index.html","hash":"79c8dc6c64ebfb5c08efbd8b31ed1ea9d9b4cd50","modified":1566477631519},{"_id":"public/2016/03/26/Hbase数据迁移到Hive中/index.html","hash":"205ccdfaec3664f6df080cde31d07e32c7cc6a3b","modified":1566477631519},{"_id":"public/2016/03/19/hbase单机搭建教程/index.html","hash":"93e8ef6458b65db7b35f595b8727b1a129d3a3b3","modified":1566477631519},{"_id":"public/2016/03/26/Hbase数据迁移到Hive中问题汇总/index.html","hash":"d771ea645348c2453f220e0c1bec3a67b7caa281","modified":1566477631519},{"_id":"public/2016/03/12/Markdown-语法学习/index.html","hash":"5231349fa700c3d58f173dfaa1a24130c595819a","modified":1566477631519},{"_id":"public/categories/docker/index.html","hash":"a826a0166b2586575204bb017c0690879daeef78","modified":1566477631520},{"_id":"public/categories/elasticsearch/index.html","hash":"338a0bba4b087f366d307787aca603d69aa5a6eb","modified":1566477631520},{"_id":"public/categories/java/index.html","hash":"8edca0f82ad0472472616aedebd9cf9d7acb9463","modified":1566477631520},{"_id":"public/categories/elasticsearch/page/2/index.html","hash":"f2176af4cd31eda0189a9a6645f50e93664b639e","modified":1566477631520},{"_id":"public/categories/git/index.html","hash":"77f5f1b0ef02e0ba532790f34c513a5800a54fef","modified":1566477631520},{"_id":"public/categories/hbase/index.html","hash":"bd14b89351c86bdb08ec373763c49ccaeadea2d3","modified":1566477631520},{"_id":"public/categories/kafka/index.html","hash":"aa1a073712f2e8ff80188d66167ec3ddcfbd1c1a","modified":1566477631520},{"_id":"public/categories/前端技术/index.html","hash":"0579ca98f01cedfd0e848d40169dfccf2adaa479","modified":1566477631520},{"_id":"public/categories/hadoop/index.html","hash":"1bac3a73b1c06c4de0eaf0ca5709a2c3f5161f2b","modified":1566477631520},{"_id":"public/categories/linux/index.html","hash":"ff8b3c856a0554f43ea3471d596e89290a1b1ced","modified":1566477631520},{"_id":"public/categories/React/index.html","hash":"6a66c0ba4e1ff81acd906c009abce60cd0e133cf","modified":1566477631520},{"_id":"public/categories/redis/index.html","hash":"3af4aa6f738c8eec06eae9cf328d3a248ec04034","modified":1566477631520},{"_id":"public/categories/redis/page/2/index.html","hash":"1c5d701302e2a409f6ed9372b4a82a1ec454d731","modified":1566477631520},{"_id":"public/categories/shell/index.html","hash":"aa63d9584d860a559a24d696c8909aadd6c3d6a9","modified":1566477631520},{"_id":"public/categories/linux/java/index.html","hash":"a61d4ebc23ffa029122ad77abe4453ea76214470","modified":1566477631521},{"_id":"public/2019/07/08/kafka幂等性和事务使用及实现原理/index.html","hash":"08b6a7f912c9b9fa75a33ef549c517e09ff9b25f","modified":1566477631732},{"_id":"public/2019/07/08/Kafka-消息生产及消费原理/index.html","hash":"be629612127dc228ab3aacc14bc2a6ae1a2661a4","modified":1566477631733},{"_id":"public/2019/07/08/kafka-逻辑架构设计/index.html","hash":"fb427c4f7aa2e6cb501ab1857f927d8deb07d475","modified":1566477631733},{"_id":"public/2019/06/30/kafka-consumer-源码分析（三）Consumer消费再均衡原理探究/index.html","hash":"75795a376c8340bfb90c3f398d4858f46f6ccd08","modified":1566477631733},{"_id":"public/2019/06/27/kafka-consumer-源码分析（一）Consumer处理流程/index.html","hash":"510b8383cbdf64cca64658b1ee55f647fa15059f","modified":1566477631733},{"_id":"public/2019/06/27/kafka-consumer-源码分析（二）分区分配策略/index.html","hash":"98e5dfcbf644dad3f9f5be2a77925fcf7f947dba","modified":1566477631733},{"_id":"public/categories/kafka/page/2/index.html","hash":"202502475b25092e8a41cb9d2229715ddf47f481","modified":1566477631733},{"_id":"public/categories/thymeleaf/index.html","hash":"5594f6a8d89c4b3b21c4b71b42e93878fcde3c93","modified":1566477631733},{"_id":"public/categories/开发工具/index.html","hash":"33d8b2940c24f87fc6767cc31c639a645cf3ca72","modified":1566477631734},{"_id":"public/categories/springcloud/index.html","hash":"8720005758228758b18af131ef8543c820acc036","modified":1566477631734},{"_id":"public/categories/thymeleaf/java/index.html","hash":"22d955be28aaf265d678422b354e168d09381c3f","modified":1566477631734},{"_id":"public/categories/java/docker/index.html","hash":"e9e081dbf9b3426a5322b87d8b5cecd16eeff70f","modified":1566477631734},{"_id":"public/categories/thymeleaf/java/模板/index.html","hash":"c7c9762fb4c9f65c33170a4c3be0b286320e1a39","modified":1566477631734},{"_id":"public/categories/springcloud/java/index.html","hash":"2f8c046c6a308e8d137f4b08a4b9ff2a71c277ff","modified":1566477631734},{"_id":"public/categories/nginx/index.html","hash":"2053bcf4323c0f778f9cad974df7e88f561d122a","modified":1566477631734},{"_id":"public/categories/springcloud/java/分布式框架/index.html","hash":"26f17fa1a93f27548b7fc532cd84380f05fe005a","modified":1566477631734},{"_id":"public/categories/node-js/index.html","hash":"bc788910704b506466a3cbc205b90f6de1f1bf72","modified":1566477631734},{"_id":"public/categories/storm/index.html","hash":"c0d30775e195ba2ac57ccdca560e291da2144b28","modified":1566477631734},{"_id":"public/categories/oozie/index.html","hash":"f1a18407b315fba4368363f9aa118c1cd0d8b976","modified":1566477631734},{"_id":"public/categories/spring/index.html","hash":"9fa8e1e5038dcaa5e59012214f946267f937123c","modified":1566477631734},{"_id":"public/categories/zookeeper/index.html","hash":"14d53e07db8d3b36096ddf37e4a437935ef5483f","modified":1566477631734},{"_id":"public/categories/心情/index.html","hash":"6c566743ab768265d92ef3198f6bf0746e9f087e","modified":1566477631734},{"_id":"public/categories/hive/index.html","hash":"ba65f15c95333359edc45acb5f25618ae3e97f0d","modified":1566477631734},{"_id":"public/categories/批处理/index.html","hash":"1aa38c345051ee0e6cf2202f1c0812c379224e2a","modified":1566477631734},{"_id":"public/categories/python/index.html","hash":"2fe0f330e12772b8498a6ccb56db1cb3660d3773","modified":1566477631734},{"_id":"public/categories/算法/index.html","hash":"b951779f2a40c6ec8d2e4cc7acffa7276456bb85","modified":1566477631734},{"_id":"public/categories/记录/index.html","hash":"6bacb20aed2ca00a9d30f117f1b2c3e2923dc255","modified":1566477631734},{"_id":"public/categories/ksql/index.html","hash":"0a86018181d7600a8e173f3cc4f1c5c649a44bde","modified":1566477631734},{"_id":"public/index.html","hash":"c9e7096ebf7dcb145561235edb3392843078d1f3","modified":1566477631735},{"_id":"public/page/2/index.html","hash":"02e46ba9a360435fdb27c5d398d8b681d53a4f92","modified":1566477631735},{"_id":"public/page/3/index.html","hash":"0ba951765fa39e30f3156e41f7e30d399da677b8","modified":1566477631735},{"_id":"public/page/4/index.html","hash":"c976b1e427f42f4d44043f8a6d4b23813dc1237e","modified":1566477631735},{"_id":"public/page/5/index.html","hash":"b767435c436c0c4f57971db56bfe8aa3c8dabfaf","modified":1566477631735},{"_id":"public/page/6/index.html","hash":"9e2c40d1767c56892f813440772edc48f08a366d","modified":1566477631735},{"_id":"public/page/7/index.html","hash":"49116369a2594e40706137b951e2989d30d64118","modified":1566478451446},{"_id":"public/page/8/index.html","hash":"c70611d3930bae820e5bc89d455f745553a1682d","modified":1566477631735},{"_id":"public/page/9/index.html","hash":"dec5345fb5030f1db9c9646d504491eb59959256","modified":1566477631735},{"_id":"public/archives/index.html","hash":"bf960aa1a48057ddc1a3e9eb0e4fec3019906504","modified":1566477631735},{"_id":"public/archives/page/2/index.html","hash":"1102ca6046ec6acbd9d61d38e74273db7b82b54e","modified":1566477631735},{"_id":"public/page/10/index.html","hash":"07070935f11a5e6aefbff95d7fd6a4350f2f6a52","modified":1566478451447},{"_id":"public/archives/page/4/index.html","hash":"edbfbb368558f121b4bb7973fb588d09fd17e6b7","modified":1566477631735},{"_id":"public/archives/page/3/index.html","hash":"f47e0165ca45dd413387a890915bde4fcbe23167","modified":1566477631735},{"_id":"public/archives/page/5/index.html","hash":"1457e2013800b36f8adfe040963389f335e0e7ed","modified":1566477631736},{"_id":"public/archives/page/6/index.html","hash":"8e56f992437f7c5b36274023f376e36313cd5966","modified":1566477631736},{"_id":"public/archives/page/7/index.html","hash":"757f365fde0c29cc2c25e9d5ed7fc19590cb24ad","modified":1566477631736},{"_id":"public/archives/page/8/index.html","hash":"41d182693be3e610f4a8b5e63d5dc90ca067e525","modified":1566477631736},{"_id":"public/archives/page/9/index.html","hash":"4336d1748401a312dbb2834f1dd88abc694d5522","modified":1566477631736},{"_id":"public/archives/page/10/index.html","hash":"cb0d4eba1017397fdae14431509a7844c13faf5b","modified":1566477631736},{"_id":"public/archives/2016/index.html","hash":"d2f10c31d2562cc9b3631c99412aaedc951c82ab","modified":1566477631736},{"_id":"public/archives/2016/page/2/index.html","hash":"18cd3aaa5083ee5f9b4194ea0a5f39b3244aad28","modified":1566477631737},{"_id":"public/archives/2016/page/3/index.html","hash":"02af9d094d9b317c9316236302309faa1e6e4c3a","modified":1566477631737},{"_id":"public/archives/2016/03/index.html","hash":"b030d7991df34484f696d2f1804bcfcd061a96b8","modified":1566477631737},{"_id":"public/archives/2016/page/4/index.html","hash":"7a215f4bf84f591e1270831fd5cc19df80ac1343","modified":1566477631738},{"_id":"public/archives/2016/04/index.html","hash":"a8bed2344f9b63b9607a1f356a587c62f84f2e8e","modified":1566477631738},{"_id":"public/archives/2016/05/index.html","hash":"b10c495d90aac60a34deeb09372ac2afb571bea5","modified":1566477631738},{"_id":"public/archives/2016/06/index.html","hash":"c0fa9d03330d772411528b9fd6614cd56facedfd","modified":1566477631738},{"_id":"public/archives/2016/07/index.html","hash":"1a36d78ce0c5f1ae29fdfdc69d6727de28696f30","modified":1566477631738},{"_id":"public/archives/2016/10/index.html","hash":"33a33fb6c5e31b567b1077c7893d3abec43f9f7e","modified":1566477631738},{"_id":"public/archives/2016/08/index.html","hash":"68168b69c76b3df7ec6f61d1d3caa7574f954c06","modified":1566477631738},{"_id":"public/archives/2016/10/page/2/index.html","hash":"187a4b027113a17371c0aa07dd4818f4bcc59e4f","modified":1566477631738},{"_id":"public/archives/2017/index.html","hash":"047508ea71d1fd080b91fe0c2813912a5ec71452","modified":1566477631738},{"_id":"public/archives/2017/page/2/index.html","hash":"6a100a486cc3fcdd0278df8de26084ef49c2e7f6","modified":1566477631738},{"_id":"public/archives/2017/03/index.html","hash":"0c32d1867026b69a251290871e9fee0018b01eda","modified":1566477631738},{"_id":"public/archives/2017/06/index.html","hash":"fc373b51f0bb4daf65a54cc89f3e16bdc1715146","modified":1566477631738},{"_id":"public/archives/2017/07/index.html","hash":"35e50babe0a424d98189c02fddee520466d97a1f","modified":1566477631738},{"_id":"public/archives/2017/08/index.html","hash":"5b0a5caa9938bad03a6b7ddf8455b2c760b5857c","modified":1566477631739},{"_id":"public/archives/2017/09/index.html","hash":"bfa46a6c5ced7ed7a59e520f93f9dc8c9cd55d53","modified":1566477631739},{"_id":"public/archives/2018/01/index.html","hash":"2b8d90c7653a673b380bd854a6fc18668133456a","modified":1566477631739},{"_id":"public/archives/2018/page/2/index.html","hash":"a5e2b9e9c5f7f5b4fadace579ac1ee0054aa6ae2","modified":1566477631739},{"_id":"public/archives/2018/index.html","hash":"e7657aa518f29be41fd86fe67829ae65047196b9","modified":1566477631739},{"_id":"public/archives/2018/page/3/index.html","hash":"471fef2155336c1109217c39db23d490c5a529ee","modified":1566477631739},{"_id":"public/archives/2018/10/index.html","hash":"fb99b09d286050a8ead5bf3592f5127a49753645","modified":1566477631739},{"_id":"public/archives/2018/07/index.html","hash":"5799bf6ae4a4e4295f310477403600d3dfafdef2","modified":1566477631739},{"_id":"public/archives/2019/index.html","hash":"c750b6ae30e0cfd935af0e33e4dac47bd928d8c0","modified":1566477631739},{"_id":"public/archives/2019/page/2/index.html","hash":"2563cbd443f7c6a10561b3bf86bb0d4d0c03c25d","modified":1566477631739},{"_id":"public/archives/2019/03/index.html","hash":"a11846a21201caedacddef7ed69946d7aa41fd02","modified":1566477631739},{"_id":"public/archives/2019/04/index.html","hash":"52d72be3e4dff6d6e9eabcf855b016da118129e0","modified":1566477631739},{"_id":"public/archives/2019/06/index.html","hash":"4a4a0d33dab9541e3c3de3475893f6f9ba7e70c0","modified":1566477631740},{"_id":"public/tags/虚拟化/index.html","hash":"12bcc89542e521c8f74c110961d2f15ce9120390","modified":1566477631740},{"_id":"public/tags/research/index.html","hash":"721b70f8eca60b82a4868e74f3997bf46d3e0aaa","modified":1566477631740},{"_id":"public/tags/research/page/2/index.html","hash":"e36e0c633e0be8399ac0c545ec2a08a7124ee28c","modified":1566477631740},{"_id":"public/tags/开发笔记/index.html","hash":"6c6fb684df38f3d087ac7ac061a27416b008ac7a","modified":1566477631740},{"_id":"public/tags/开发笔记/page/2/index.html","hash":"f6393a0f7c42a20ed0e6e24294808d1bfa321cd3","modified":1566477631740},{"_id":"public/tags/经验/index.html","hash":"d6296a62d8bf4e284d7fc94f68c7010b77e6e0b2","modified":1566477631740},{"_id":"public/tags/命令/index.html","hash":"04bae1fa8303789f58f570cbadc0d688fbf6fee8","modified":1566477631740},{"_id":"public/tags/大数据/index.html","hash":"bcfd4dcb2847ad99ed024cc5b360be155a374ad5","modified":1566477631740},{"_id":"public/tags/大数据/page/2/index.html","hash":"bfc307088eb0ad7c8eb21315231725a6eec6160c","modified":1566477631740},{"_id":"public/tags/笔记/index.html","hash":"ffb7e7954849e0ade44701acc9cceee505b2617a","modified":1566477631740},{"_id":"public/tags/编程/index.html","hash":"0550ff4fd458ecfbc97fdbdd4db6c93a246ec248","modified":1566477631741},{"_id":"public/tags/大数据/page/3/index.html","hash":"ab69bab06b9fa2e0c80616fa9e48d65d4c73edce","modified":1566477631741},{"_id":"public/tags/教程/index.html","hash":"508603c0af3b6854e869cd797c7673c47c3fedb7","modified":1566477631741},{"_id":"public/tags/kafka/index.html","hash":"b46147973e0c89a7d38e407ab4c43150748a0cc4","modified":1566477631741},{"_id":"public/tags/专栏/index.html","hash":"2e46819a0289f6105db8e634b574f708eab950d8","modified":1566477631741},{"_id":"public/tags/写作技巧/index.html","hash":"a64ebe8589e54c6bbfb68f784e4ad357fd617dea","modified":1566477631741},{"_id":"public/tags/问题答疑/index.html","hash":"008b111de0ebe4928a804f8716a8dc43c9342d76","modified":1566477631741},{"_id":"public/tags/nosql/index.html","hash":"6ce95ed4ad29c8e30e6ad285215fb1f826f2ea09","modified":1566477631741},{"_id":"public/tags/运维/index.html","hash":"f71d5bb8a88b439002e50bfc4bd6d565e8c38386","modified":1566477631741},{"_id":"public/tags/部署与维护/index.html","hash":"5b2b473ba89a1e6f619b841f973ca8ae89a3c44d","modified":1566477631741},{"_id":"public/tags/散心/index.html","hash":"60875767d76ae9e998e03d4f72138e95f99970f7","modified":1566477631741},{"_id":"public/tags/爬虫/index.html","hash":"66317e68abbbd45bdbd05905b895f9518b16a879","modified":1566477631741},{"_id":"public/tags/基础理论知识/index.html","hash":"c08821229e0a5f60c8fdf53f43788020d45743a4","modified":1566477631741},{"_id":"public/tags/算法/index.html","hash":"f94299a083caf60a5950c9507fa2b25dc1cd43de","modified":1566477631741},{"_id":"public/tags/实时分析/index.html","hash":"9b99ea27e4294dea9dec957d19df031d6198f524","modified":1566477631742},{"_id":"public/tags/技术分享/index.html","hash":"8d2c017ff20d94eb71dfc6348f2cc35e37fddcde","modified":1566477631742},{"_id":"public/page/11/index.html","hash":"4b82a0eae16d892d260462ba0be73d9a254bcda4","modified":1566477631744},{"_id":"public/archives/page/11/index.html","hash":"5c4a2c3f73c48f0ccff2bc0239a15332cb5a5dd5","modified":1566477631744},{"_id":"public/archives/2019/page/3/index.html","hash":"0118a18731b7b41d4dda0573c162d1fa7a73ac31","modified":1566477631744},{"_id":"public/archives/2019/07/index.html","hash":"42d958b4003bc94f9bcf21338f7a58a2ad5717a0","modified":1566477631744},{"_id":"public/tags/专刊/index.html","hash":"1d26cae839e6313bdc8dbb50710d111c5c64c17c","modified":1566477631744},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1566477631744},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1566477631744},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1566477631744},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1566477631745},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1566477631745},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1566477631745},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1566477631745},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1566477631745},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1566477631745},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1566477631745},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1566477631745},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1566477631745},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1566477631745},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1566477631745},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1566477631745},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1566477631745},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1566477631745},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1566477631745},{"_id":"public/images/wechatpay.jpg","hash":"3abed4f67e5acb1301615bd65a916ae5364d443d","modified":1566477631745},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1566477631745},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1566477631746},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1566477632684},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1566477632684},{"_id":"public/js/src/affix.js","hash":"a2aab233d99297435a5274bf512c3c753fe08e80","modified":1566477632690},{"_id":"public/js/src/exturl.js","hash":"54825acc8de4793feac415be227b965428f4e97d","modified":1566477632690},{"_id":"public/js/src/algolia-search.js","hash":"1f7f10c579e7703d0f6acb8b73f3d78a07d0c623","modified":1566477632690},{"_id":"public/js/src/js.cookie.js","hash":"e0afce539f1fb81d59e3c6f0a68d736e2fb45d93","modified":1566477632690},{"_id":"public/js/src/scrollspy.js","hash":"fa3c92968bcdbcb8d95a1729f7659d9753cbd077","modified":1566477632690},{"_id":"public/js/src/next-boot.js","hash":"e0615efab5f81ba0fd39c0527eac31144deac7ce","modified":1566477632690},{"_id":"public/js/src/post-details.js","hash":"0dde5e6d4547587662a3256317a9d5d1db507692","modified":1566477632690},{"_id":"public/js/src/scroll-cookie.js","hash":"d07b3776708d4ae79ed2037c4c7391d5c9b06b19","modified":1566477632690},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1566477632690},{"_id":"public/js/src/schemes/muse.js","hash":"e9bfa6b343b67625f58757efce46ccdaac8f308c","modified":1566477632690},{"_id":"public/js/src/schemes/pisces.js","hash":"9eb63cba0327d3d11b6cbfcbe40b88e97a8378a3","modified":1566477632690},{"_id":"public/css/main.css","hash":"f624c06f589a5819e6c1ff96b36f7921e62fdd83","modified":1566477632690},{"_id":"public/js/src/motion.js","hash":"a16bc0b701646bf6653484675f4d5dc0f892d184","modified":1566477632691},{"_id":"public/js/src/utils.js","hash":"703375f367acfbd0596733c34437d1b2681abf72","modified":1566477632691},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1566477632691},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1566477632734},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1566477632734},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1566477632764},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1566477632765},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1566477632765},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1566477632792},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1566477632795}],"Category":[{"name":"docker","_id":"cjzmobv4k00028ceekmdai5yg"},{"name":"elasticsearch","_id":"cjzmobv4r00078ceeifczslqh"},{"name":"java","_id":"cjzmobv4w000d8cee7ttihvfu"},{"name":"git","_id":"cjzmobv5c00108ceecpk18hws"},{"name":"hbase","_id":"cjzmobv5k001d8ceeux24pe1l"},{"name":"kafka","_id":"cjzmobvaz00288ceegrgrrmou"},{"name":"前端技术","_id":"cjzmobvb6002h8ceem3ffbtq3"},{"name":"hadoop","_id":"cjzmobvbf002u8ceeal8co9kh"},{"name":"React","_id":"cjzmobvbm00348ceekbjt2kj4"},{"name":"linux","_id":"cjzmobvbr003a8cee4fkycz6h"},{"name":"redis","_id":"cjzmobvc1003o8ceekw8pz4y3"},{"name":"java","parent":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvcg00498ceevf8vm30b"},{"name":"shell","_id":"cjzmobvcz00538ceesyazglay"},{"name":"thymeleaf","_id":"cjzmobvd2005a8ceecwj0xwgb"},{"name":"开发工具","_id":"cjzmobvd6005i8ceebdi2f2ne"},{"name":"springcloud","_id":"cjzmobvdb005p8ceejeces10c"},{"name":"java","parent":"cjzmobvd2005a8ceecwj0xwgb","_id":"cjzmobvdg005y8ceelt5mqmgx"},{"name":"docker","parent":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvdk00658ceet8e9kjqx"},{"name":"java","parent":"cjzmobvdb005p8ceejeces10c","_id":"cjzmobvdp006e8ceexdcqc98b"},{"name":"模板","parent":"cjzmobvdg005y8ceelt5mqmgx","_id":"cjzmobvdt006l8ceeeiiz6pcr"},{"name":"分布式框架","parent":"cjzmobvdp006e8ceexdcqc98b","_id":"cjzmobve0006u8ceexotx8m69"},{"name":"nginx","_id":"cjzmobveb007c8ceeziikyu7h"},{"name":"node.js","_id":"cjzmobvek007r8ceeyd8q7n04"},{"name":"oozie","_id":"cjzmobvex008e8cee0lj4vm4h"},{"name":"spring","_id":"cjzmobvf7008u8ceeyf3o0n7p"},{"name":"storm","_id":"cjzmobvfb00928ceecckaipki"},{"name":"zookeeper","_id":"cjzmobvfh009a8ceenakuz4cn"},{"name":"心情","_id":"cjzmobvfk009h8ceea1qvd1i1"},{"name":"批处理","_id":"cjzmobvfm009n8cee9u5lxk94"},{"name":"hive","_id":"cjzmobvfo009s8ceekj9v0t3o"},{"name":"python","_id":"cjzmobvfq009x8cee32jj65k3"},{"name":"记录","_id":"cjzmobvfs00a18ceeewafz21x"},{"name":"算法","_id":"cjzmobvft00a48ceevm1xso0r"},{"name":"ksql","_id":"cjzmobvgb00a98ceetrbm7d88"}],"Data":[],"Page":[{"title":"categories","date":"2016-03-12T13:09:25.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-03-12 21:09:25\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-03-12T14:41:22.309Z","path":"categories/index.html","layout":"page","_id":"cjzmobvau00258ceeh7xgdy4z","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"个人简介","date":"2016-03-12T13:11:34.000Z","_content":"\n专注于大数据领域（redis,kafka,elastic stack），内心怀储互联网之梦。相信代码改变世界，程序设计人生。\n\n期待更多志同道合的朋友交流。\n\n\n# 联系方式\n\n- Email：1753177225@qq.com\n- QQ：1753177225\n\n---\n\n# 个人信息\n\n - Truman/男/1990\n - 本科/西安邮电大学\n - 工作年限：5年\n - 技术博客：http://trumandu.github.io\n - Github:  http://github.com/TrumanDu\n - 期望职位：Java高级程序员，架构师\n - 期望薪资：税前月薪16k~20k，特别喜欢的公司可例外\n - 期望城市：西安\n\n---\n\n# 工作经历\n\n## newegg公司 （ 2015年11月 ~ 至今 ）\n\n### RCT项目 \n- **项目描述**：RCT 是一个通过解析rdb文件对redis内存结构分析的一站式平台。 支持对非集群/集群rdb文件分析、Slowlog查询与监控、ClientList查询与监控。\n- **项目特色**：采用分布式架构，通过解析redis rdb文件，多角度多场景分析redis中内存数据结构，提供友好的可视化报表以及邮件通知等功能。\n- **项目职责**: 主要负责架构设计，平台代码开发，项目管理等工作。\n- **项目业绩**: 目前已在产线上使用，多次发挥重要作用，为公司节省了宝贵内存资源。\n\n\n### SyncBigdataPlatform项目 \n- **项目描述**：SyncBigdataPlatform 是依赖大数据存储技术，提供高性能读写，jumplocation,以及跨location数据同步服务平台。DB层采用Hbase,Cassandra,Cache层采用Redis，数据同步层采用Kafka。提供restful接口，以供业务调用。\n- **项目特色**：支持自定义业务扩展，支持热部署，支持配置信息动态调整，支持可视化配置信息更改，支持多数据中心高速数据同步，支持jumplocation操作，支持业务数据格式与数据库scheme自定义设计\n- **项目职责**: 主要负责架构设计，平台代码开发，项目管理等工作。\n- **项目业绩**: 目前平台上线多个主要服务，可以支持重点核心业务系统，上线一年来，稳定运行无误，获得公司优秀项目荣誉。\n\n### 基础服务搭建与运维\n- **项目职责**: 搭建RedisCluster,Elasticsearch,KafkaCluster集群，对集群进行日常维护，处理线上问题，开发周边组件，便于更好的维护集群。\n- **项目业绩**: 为newegg线上服务提供可靠稳定的cache,消息，检索服务。\n\n### 服务Docker化\n- **项目描述**：针对公司kafka/redis/elasticsearch/opentsdb等基础服务docker化，其次针对业务项目进行docker化部署\n- **项目职责**: 针对不同需求场景设计部署架构，编写shell脚本，Dockfile文件。\n- **项目业绩**: 将我们项目组所有服务实现全部docker化，降低服务维护难度，极大的解放劳动力。\n\n### RedisClientAPI项目\n- **项目描述**：该项目提供一个使用Redis的rest/thrift API,客户端可以跨多种编程语言，更高效，更便捷的使用Redis的功能。\n- **项目职责**: 负责该项目的设计及开发工作\n- **项目业绩**: 提供高效API服务，承受上亿次调用，未出现任何事故。实现了RedisCluster批量操作，慢查询追踪等功能。项目难点是对于批量操作的实现方案改造。\n\n### MonitorPlatform项目 \n- **项目描述**：该项目可以用来检测机器指标，基础服务指标，业务指标等数据，通过可视化技术快捷高效的监控相关服务。\n- **项目职责**: 主要负责该项目的设计与开发工作，该项目利用ES stack技术，使用metricbeat收集机器指标信息，使用grafana做可视化。\n- **项目业绩**: 实现我们团队服务的无死角化监控，打造具体我们team特色的监控平台。\n\n### 其他项目\nredis cluster监控，KafkaTools,EC Dashboard，kibana 插件(email_table,indices_view，cleaner,curator)\n\n## huatek公司 （ 2014年1月 ~ 2015年10月 ）\n\n### 基于Activiti工作流引擎开发\n- **项目描述**：公司发展拙见壮大，出差请假管理混乱，公司技术支持管理需要，同时业务组系统需要相关工作流引擎支持，在此背景下，公司专门成立人员开发工作流引擎。此系统可以可视化设计流程，快速便捷的适应不同业务场景需要。目前已经上线三套流程。该系统包含以下几个模块：自定义字段、自定义表单、流程设计、用户组管理、申请任务、待办任务、任务列表等模块。\n- **项目职责**: \n负责工作流引擎表单设计，流程权限设计与开发，流程数据的汇总与导出，各个模块数据的查询等。\n- **项目业绩**: \n顺利实现公司办公平台出差请假流程，技术支持流程。为固定资产业务系统提供购置流程。\n\n\n### 李宁订单优化管理系统\n- **项目描述**：由于目前SAP系统中根据合同开单时，需要大量人工计算，为了提高订单配货发单效率，以此为基础为李宁发货员工开发一套针对订单拆单、配货，其中包含一部分有规律有逻辑的计算法则，达到订单拆分的目的系统.该系统包含以下几个模块：信息管理、流程审批、分货管理、订单管理、报表信息。\n- **项目职责**: 按照计划表参与开发该系统信息管理、流程审批、分货管理、订单管理、报表信息模块，接口模块。\n- **项目业绩**: 按照项目需求，完成目前所涉及的所有开发任务。\n\n---\n\n# 开源项目和作品\n\n## 开源项目\n\n - [RCT](https://github.com/xaecbd/RCT) redis 内存分析工具，获得业界关注。\n - [autocomplate-shell](https://github.com/TrumanDu/autocomplate-shell)：VS Code 编写shell插件，下载量7k+\n - [indices_view](https://github.com/TrumanDu/indices_view)：查看ES indices kibana 插件已被[elastic](https://www.elastic.co/guide/en/kibana/current/known-plugins.html)官网收录，[码云周刊第68期](https://blog.gitee.com/2018/04/16/weekly068/)推荐 ）\n - [cleaner](https://github.com/TrumanDu/cleaner)：设置es index ttl 插件,已被[elastic](https://www.elastic.co/guide/en/kibana/current/known-plugins.html)官网收录\n\n## 技术文章\n- [技术博客](http://trumandu.github.io)\n- [RedisCluster构建批量操作探讨](http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/)\n- [Kibana Plugin Development Tutorial](https://kibana.gitbook.io/kibana-plugin-development-tutorial/) \n\n# 技能清单\n\n以下均为我熟练使用的技能\n\n- 语言： Java/Bash/Node.js\n- 前端框架：Bootstrap/Jquery/HTML5/React\n- RPC框架：Thrift\n- Web框架：Spring/Express\n- ORM框架：Mybatis\n- 微服务架构：Springboot\n- 项目管理：Maven\n- 关系型数据库：MySQL/SQLite\n- NOSQL：Redis/Hbase/Cassandra/Elasticsearch\n- 消息队列：Kafka\n- 实时计算：KSQL\n- 虚拟化：Docker\n- 大数据相关：Hadoop/Hive/Zookeeper/Storm/Oozie\n- 监控相关：Opentsdb/Elastic stack(es/kibana/logstash)\n- 系统：Linux/Window\n- 版本管理：SVN/GIT\n\n---\n","source":"about/index.md","raw":"---\ntitle: 个人简介\ndate: 2016-03-12 21:11:34\n---\n\n专注于大数据领域（redis,kafka,elastic stack），内心怀储互联网之梦。相信代码改变世界，程序设计人生。\n\n期待更多志同道合的朋友交流。\n\n\n# 联系方式\n\n- Email：1753177225@qq.com\n- QQ：1753177225\n\n---\n\n# 个人信息\n\n - Truman/男/1990\n - 本科/西安邮电大学\n - 工作年限：5年\n - 技术博客：http://trumandu.github.io\n - Github:  http://github.com/TrumanDu\n - 期望职位：Java高级程序员，架构师\n - 期望薪资：税前月薪16k~20k，特别喜欢的公司可例外\n - 期望城市：西安\n\n---\n\n# 工作经历\n\n## newegg公司 （ 2015年11月 ~ 至今 ）\n\n### RCT项目 \n- **项目描述**：RCT 是一个通过解析rdb文件对redis内存结构分析的一站式平台。 支持对非集群/集群rdb文件分析、Slowlog查询与监控、ClientList查询与监控。\n- **项目特色**：采用分布式架构，通过解析redis rdb文件，多角度多场景分析redis中内存数据结构，提供友好的可视化报表以及邮件通知等功能。\n- **项目职责**: 主要负责架构设计，平台代码开发，项目管理等工作。\n- **项目业绩**: 目前已在产线上使用，多次发挥重要作用，为公司节省了宝贵内存资源。\n\n\n### SyncBigdataPlatform项目 \n- **项目描述**：SyncBigdataPlatform 是依赖大数据存储技术，提供高性能读写，jumplocation,以及跨location数据同步服务平台。DB层采用Hbase,Cassandra,Cache层采用Redis，数据同步层采用Kafka。提供restful接口，以供业务调用。\n- **项目特色**：支持自定义业务扩展，支持热部署，支持配置信息动态调整，支持可视化配置信息更改，支持多数据中心高速数据同步，支持jumplocation操作，支持业务数据格式与数据库scheme自定义设计\n- **项目职责**: 主要负责架构设计，平台代码开发，项目管理等工作。\n- **项目业绩**: 目前平台上线多个主要服务，可以支持重点核心业务系统，上线一年来，稳定运行无误，获得公司优秀项目荣誉。\n\n### 基础服务搭建与运维\n- **项目职责**: 搭建RedisCluster,Elasticsearch,KafkaCluster集群，对集群进行日常维护，处理线上问题，开发周边组件，便于更好的维护集群。\n- **项目业绩**: 为newegg线上服务提供可靠稳定的cache,消息，检索服务。\n\n### 服务Docker化\n- **项目描述**：针对公司kafka/redis/elasticsearch/opentsdb等基础服务docker化，其次针对业务项目进行docker化部署\n- **项目职责**: 针对不同需求场景设计部署架构，编写shell脚本，Dockfile文件。\n- **项目业绩**: 将我们项目组所有服务实现全部docker化，降低服务维护难度，极大的解放劳动力。\n\n### RedisClientAPI项目\n- **项目描述**：该项目提供一个使用Redis的rest/thrift API,客户端可以跨多种编程语言，更高效，更便捷的使用Redis的功能。\n- **项目职责**: 负责该项目的设计及开发工作\n- **项目业绩**: 提供高效API服务，承受上亿次调用，未出现任何事故。实现了RedisCluster批量操作，慢查询追踪等功能。项目难点是对于批量操作的实现方案改造。\n\n### MonitorPlatform项目 \n- **项目描述**：该项目可以用来检测机器指标，基础服务指标，业务指标等数据，通过可视化技术快捷高效的监控相关服务。\n- **项目职责**: 主要负责该项目的设计与开发工作，该项目利用ES stack技术，使用metricbeat收集机器指标信息，使用grafana做可视化。\n- **项目业绩**: 实现我们团队服务的无死角化监控，打造具体我们team特色的监控平台。\n\n### 其他项目\nredis cluster监控，KafkaTools,EC Dashboard，kibana 插件(email_table,indices_view，cleaner,curator)\n\n## huatek公司 （ 2014年1月 ~ 2015年10月 ）\n\n### 基于Activiti工作流引擎开发\n- **项目描述**：公司发展拙见壮大，出差请假管理混乱，公司技术支持管理需要，同时业务组系统需要相关工作流引擎支持，在此背景下，公司专门成立人员开发工作流引擎。此系统可以可视化设计流程，快速便捷的适应不同业务场景需要。目前已经上线三套流程。该系统包含以下几个模块：自定义字段、自定义表单、流程设计、用户组管理、申请任务、待办任务、任务列表等模块。\n- **项目职责**: \n负责工作流引擎表单设计，流程权限设计与开发，流程数据的汇总与导出，各个模块数据的查询等。\n- **项目业绩**: \n顺利实现公司办公平台出差请假流程，技术支持流程。为固定资产业务系统提供购置流程。\n\n\n### 李宁订单优化管理系统\n- **项目描述**：由于目前SAP系统中根据合同开单时，需要大量人工计算，为了提高订单配货发单效率，以此为基础为李宁发货员工开发一套针对订单拆单、配货，其中包含一部分有规律有逻辑的计算法则，达到订单拆分的目的系统.该系统包含以下几个模块：信息管理、流程审批、分货管理、订单管理、报表信息。\n- **项目职责**: 按照计划表参与开发该系统信息管理、流程审批、分货管理、订单管理、报表信息模块，接口模块。\n- **项目业绩**: 按照项目需求，完成目前所涉及的所有开发任务。\n\n---\n\n# 开源项目和作品\n\n## 开源项目\n\n - [RCT](https://github.com/xaecbd/RCT) redis 内存分析工具，获得业界关注。\n - [autocomplate-shell](https://github.com/TrumanDu/autocomplate-shell)：VS Code 编写shell插件，下载量7k+\n - [indices_view](https://github.com/TrumanDu/indices_view)：查看ES indices kibana 插件已被[elastic](https://www.elastic.co/guide/en/kibana/current/known-plugins.html)官网收录，[码云周刊第68期](https://blog.gitee.com/2018/04/16/weekly068/)推荐 ）\n - [cleaner](https://github.com/TrumanDu/cleaner)：设置es index ttl 插件,已被[elastic](https://www.elastic.co/guide/en/kibana/current/known-plugins.html)官网收录\n\n## 技术文章\n- [技术博客](http://trumandu.github.io)\n- [RedisCluster构建批量操作探讨](http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/)\n- [Kibana Plugin Development Tutorial](https://kibana.gitbook.io/kibana-plugin-development-tutorial/) \n\n# 技能清单\n\n以下均为我熟练使用的技能\n\n- 语言： Java/Bash/Node.js\n- 前端框架：Bootstrap/Jquery/HTML5/React\n- RPC框架：Thrift\n- Web框架：Spring/Express\n- ORM框架：Mybatis\n- 微服务架构：Springboot\n- 项目管理：Maven\n- 关系型数据库：MySQL/SQLite\n- NOSQL：Redis/Hbase/Cassandra/Elasticsearch\n- 消息队列：Kafka\n- 实时计算：KSQL\n- 虚拟化：Docker\n- 大数据相关：Hadoop/Hive/Zookeeper/Storm/Oozie\n- 监控相关：Opentsdb/Elastic stack(es/kibana/logstash)\n- 系统：Linux/Window\n- 版本管理：SVN/GIT\n\n---\n","updated":"2019-04-08T12:24:22.892Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjzmobvax00278ceebh90cjb1","content":"<p>专注于大数据领域（redis,kafka,elastic stack），内心怀储互联网之梦。相信代码改变世界，程序设计人生。</p>\n<p>期待更多志同道合的朋友交流。</p>\n<h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1><ul>\n<li>Email：<a href=\"mailto:1753177225@qq.com\" target=\"_blank\" rel=\"noopener\">1753177225@qq.com</a></li>\n<li>QQ：1753177225</li>\n</ul>\n<hr>\n<h1 id=\"个人信息\"><a href=\"#个人信息\" class=\"headerlink\" title=\"个人信息\"></a>个人信息</h1><ul>\n<li>Truman/男/1990</li>\n<li>本科/西安邮电大学</li>\n<li>工作年限：5年</li>\n<li>技术博客：<a href=\"http://trumandu.github.io\">http://trumandu.github.io</a></li>\n<li>Github:  <a href=\"http://github.com/TrumanDu\" target=\"_blank\" rel=\"noopener\">http://github.com/TrumanDu</a></li>\n<li>期望职位：Java高级程序员，架构师</li>\n<li>期望薪资：税前月薪16k~20k，特别喜欢的公司可例外</li>\n<li>期望城市：西安</li>\n</ul>\n<hr>\n<h1 id=\"工作经历\"><a href=\"#工作经历\" class=\"headerlink\" title=\"工作经历\"></a>工作经历</h1><h2 id=\"newegg公司-（-2015年11月-至今-）\"><a href=\"#newegg公司-（-2015年11月-至今-）\" class=\"headerlink\" title=\"newegg公司 （ 2015年11月 ~ 至今 ）\"></a>newegg公司 （ 2015年11月 ~ 至今 ）</h2><h3 id=\"RCT项目\"><a href=\"#RCT项目\" class=\"headerlink\" title=\"RCT项目\"></a>RCT项目</h3><ul>\n<li><strong>项目描述</strong>：RCT 是一个通过解析rdb文件对redis内存结构分析的一站式平台。 支持对非集群/集群rdb文件分析、Slowlog查询与监控、ClientList查询与监控。</li>\n<li><strong>项目特色</strong>：采用分布式架构，通过解析redis rdb文件，多角度多场景分析redis中内存数据结构，提供友好的可视化报表以及邮件通知等功能。</li>\n<li><strong>项目职责</strong>: 主要负责架构设计，平台代码开发，项目管理等工作。</li>\n<li><strong>项目业绩</strong>: 目前已在产线上使用，多次发挥重要作用，为公司节省了宝贵内存资源。</li>\n</ul>\n<h3 id=\"SyncBigdataPlatform项目\"><a href=\"#SyncBigdataPlatform项目\" class=\"headerlink\" title=\"SyncBigdataPlatform项目\"></a>SyncBigdataPlatform项目</h3><ul>\n<li><strong>项目描述</strong>：SyncBigdataPlatform 是依赖大数据存储技术，提供高性能读写，jumplocation,以及跨location数据同步服务平台。DB层采用Hbase,Cassandra,Cache层采用Redis，数据同步层采用Kafka。提供restful接口，以供业务调用。</li>\n<li><strong>项目特色</strong>：支持自定义业务扩展，支持热部署，支持配置信息动态调整，支持可视化配置信息更改，支持多数据中心高速数据同步，支持jumplocation操作，支持业务数据格式与数据库scheme自定义设计</li>\n<li><strong>项目职责</strong>: 主要负责架构设计，平台代码开发，项目管理等工作。</li>\n<li><strong>项目业绩</strong>: 目前平台上线多个主要服务，可以支持重点核心业务系统，上线一年来，稳定运行无误，获得公司优秀项目荣誉。</li>\n</ul>\n<h3 id=\"基础服务搭建与运维\"><a href=\"#基础服务搭建与运维\" class=\"headerlink\" title=\"基础服务搭建与运维\"></a>基础服务搭建与运维</h3><ul>\n<li><strong>项目职责</strong>: 搭建RedisCluster,Elasticsearch,KafkaCluster集群，对集群进行日常维护，处理线上问题，开发周边组件，便于更好的维护集群。</li>\n<li><strong>项目业绩</strong>: 为newegg线上服务提供可靠稳定的cache,消息，检索服务。</li>\n</ul>\n<h3 id=\"服务Docker化\"><a href=\"#服务Docker化\" class=\"headerlink\" title=\"服务Docker化\"></a>服务Docker化</h3><ul>\n<li><strong>项目描述</strong>：针对公司kafka/redis/elasticsearch/opentsdb等基础服务docker化，其次针对业务项目进行docker化部署</li>\n<li><strong>项目职责</strong>: 针对不同需求场景设计部署架构，编写shell脚本，Dockfile文件。</li>\n<li><strong>项目业绩</strong>: 将我们项目组所有服务实现全部docker化，降低服务维护难度，极大的解放劳动力。</li>\n</ul>\n<h3 id=\"RedisClientAPI项目\"><a href=\"#RedisClientAPI项目\" class=\"headerlink\" title=\"RedisClientAPI项目\"></a>RedisClientAPI项目</h3><ul>\n<li><strong>项目描述</strong>：该项目提供一个使用Redis的rest/thrift API,客户端可以跨多种编程语言，更高效，更便捷的使用Redis的功能。</li>\n<li><strong>项目职责</strong>: 负责该项目的设计及开发工作</li>\n<li><strong>项目业绩</strong>: 提供高效API服务，承受上亿次调用，未出现任何事故。实现了RedisCluster批量操作，慢查询追踪等功能。项目难点是对于批量操作的实现方案改造。</li>\n</ul>\n<h3 id=\"MonitorPlatform项目\"><a href=\"#MonitorPlatform项目\" class=\"headerlink\" title=\"MonitorPlatform项目\"></a>MonitorPlatform项目</h3><ul>\n<li><strong>项目描述</strong>：该项目可以用来检测机器指标，基础服务指标，业务指标等数据，通过可视化技术快捷高效的监控相关服务。</li>\n<li><strong>项目职责</strong>: 主要负责该项目的设计与开发工作，该项目利用ES stack技术，使用metricbeat收集机器指标信息，使用grafana做可视化。</li>\n<li><strong>项目业绩</strong>: 实现我们团队服务的无死角化监控，打造具体我们team特色的监控平台。</li>\n</ul>\n<h3 id=\"其他项目\"><a href=\"#其他项目\" class=\"headerlink\" title=\"其他项目\"></a>其他项目</h3><p>redis cluster监控，KafkaTools,EC Dashboard，kibana 插件(email_table,indices_view，cleaner,curator)</p>\n<h2 id=\"huatek公司-（-2014年1月-2015年10月-）\"><a href=\"#huatek公司-（-2014年1月-2015年10月-）\" class=\"headerlink\" title=\"huatek公司 （ 2014年1月 ~ 2015年10月 ）\"></a>huatek公司 （ 2014年1月 ~ 2015年10月 ）</h2><h3 id=\"基于Activiti工作流引擎开发\"><a href=\"#基于Activiti工作流引擎开发\" class=\"headerlink\" title=\"基于Activiti工作流引擎开发\"></a>基于Activiti工作流引擎开发</h3><ul>\n<li><strong>项目描述</strong>：公司发展拙见壮大，出差请假管理混乱，公司技术支持管理需要，同时业务组系统需要相关工作流引擎支持，在此背景下，公司专门成立人员开发工作流引擎。此系统可以可视化设计流程，快速便捷的适应不同业务场景需要。目前已经上线三套流程。该系统包含以下几个模块：自定义字段、自定义表单、流程设计、用户组管理、申请任务、待办任务、任务列表等模块。</li>\n<li><strong>项目职责</strong>:<br>负责工作流引擎表单设计，流程权限设计与开发，流程数据的汇总与导出，各个模块数据的查询等。</li>\n<li><strong>项目业绩</strong>:<br>顺利实现公司办公平台出差请假流程，技术支持流程。为固定资产业务系统提供购置流程。</li>\n</ul>\n<h3 id=\"李宁订单优化管理系统\"><a href=\"#李宁订单优化管理系统\" class=\"headerlink\" title=\"李宁订单优化管理系统\"></a>李宁订单优化管理系统</h3><ul>\n<li><strong>项目描述</strong>：由于目前SAP系统中根据合同开单时，需要大量人工计算，为了提高订单配货发单效率，以此为基础为李宁发货员工开发一套针对订单拆单、配货，其中包含一部分有规律有逻辑的计算法则，达到订单拆分的目的系统.该系统包含以下几个模块：信息管理、流程审批、分货管理、订单管理、报表信息。</li>\n<li><strong>项目职责</strong>: 按照计划表参与开发该系统信息管理、流程审批、分货管理、订单管理、报表信息模块，接口模块。</li>\n<li><strong>项目业绩</strong>: 按照项目需求，完成目前所涉及的所有开发任务。</li>\n</ul>\n<hr>\n<h1 id=\"开源项目和作品\"><a href=\"#开源项目和作品\" class=\"headerlink\" title=\"开源项目和作品\"></a>开源项目和作品</h1><h2 id=\"开源项目\"><a href=\"#开源项目\" class=\"headerlink\" title=\"开源项目\"></a>开源项目</h2><ul>\n<li><a href=\"https://github.com/xaecbd/RCT\" target=\"_blank\" rel=\"noopener\">RCT</a> redis 内存分析工具，获得业界关注。</li>\n<li><a href=\"https://github.com/TrumanDu/autocomplate-shell\" target=\"_blank\" rel=\"noopener\">autocomplate-shell</a>：VS Code 编写shell插件，下载量7k+</li>\n<li><a href=\"https://github.com/TrumanDu/indices_view\" target=\"_blank\" rel=\"noopener\">indices_view</a>：查看ES indices kibana 插件已被<a href=\"https://www.elastic.co/guide/en/kibana/current/known-plugins.html\" target=\"_blank\" rel=\"noopener\">elastic</a>官网收录，<a href=\"https://blog.gitee.com/2018/04/16/weekly068/\" target=\"_blank\" rel=\"noopener\">码云周刊第68期</a>推荐 ）</li>\n<li><a href=\"https://github.com/TrumanDu/cleaner\" target=\"_blank\" rel=\"noopener\">cleaner</a>：设置es index ttl 插件,已被<a href=\"https://www.elastic.co/guide/en/kibana/current/known-plugins.html\" target=\"_blank\" rel=\"noopener\">elastic</a>官网收录</li>\n</ul>\n<h2 id=\"技术文章\"><a href=\"#技术文章\" class=\"headerlink\" title=\"技术文章\"></a>技术文章</h2><ul>\n<li><a href=\"http://trumandu.github.io\">技术博客</a></li>\n<li><a href=\"http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/\">RedisCluster构建批量操作探讨</a></li>\n<li><a href=\"https://kibana.gitbook.io/kibana-plugin-development-tutorial/\" target=\"_blank\" rel=\"noopener\">Kibana Plugin Development Tutorial</a> </li>\n</ul>\n<h1 id=\"技能清单\"><a href=\"#技能清单\" class=\"headerlink\" title=\"技能清单\"></a>技能清单</h1><p>以下均为我熟练使用的技能</p>\n<ul>\n<li>语言： Java/Bash/Node.js</li>\n<li>前端框架：Bootstrap/Jquery/HTML5/React</li>\n<li>RPC框架：Thrift</li>\n<li>Web框架：Spring/Express</li>\n<li>ORM框架：Mybatis</li>\n<li>微服务架构：Springboot</li>\n<li>项目管理：Maven</li>\n<li>关系型数据库：MySQL/SQLite</li>\n<li>NOSQL：Redis/Hbase/Cassandra/Elasticsearch</li>\n<li>消息队列：Kafka</li>\n<li>实时计算：KSQL</li>\n<li>虚拟化：Docker</li>\n<li>大数据相关：Hadoop/Hive/Zookeeper/Storm/Oozie</li>\n<li>监控相关：Opentsdb/Elastic stack(es/kibana/logstash)</li>\n<li>系统：Linux/Window</li>\n<li>版本管理：SVN/GIT</li>\n</ul>\n<hr>\n","site":{"data":{}},"excerpt":"","more":"<p>专注于大数据领域（redis,kafka,elastic stack），内心怀储互联网之梦。相信代码改变世界，程序设计人生。</p>\n<p>期待更多志同道合的朋友交流。</p>\n<h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1><ul>\n<li>Email：<a href=\"mailto:1753177225@qq.com\" target=\"_blank\" rel=\"noopener\">1753177225@qq.com</a></li>\n<li>QQ：1753177225</li>\n</ul>\n<hr>\n<h1 id=\"个人信息\"><a href=\"#个人信息\" class=\"headerlink\" title=\"个人信息\"></a>个人信息</h1><ul>\n<li>Truman/男/1990</li>\n<li>本科/西安邮电大学</li>\n<li>工作年限：5年</li>\n<li>技术博客：<a href=\"http://trumandu.github.io\">http://trumandu.github.io</a></li>\n<li>Github:  <a href=\"http://github.com/TrumanDu\" target=\"_blank\" rel=\"noopener\">http://github.com/TrumanDu</a></li>\n<li>期望职位：Java高级程序员，架构师</li>\n<li>期望薪资：税前月薪16k~20k，特别喜欢的公司可例外</li>\n<li>期望城市：西安</li>\n</ul>\n<hr>\n<h1 id=\"工作经历\"><a href=\"#工作经历\" class=\"headerlink\" title=\"工作经历\"></a>工作经历</h1><h2 id=\"newegg公司-（-2015年11月-至今-）\"><a href=\"#newegg公司-（-2015年11月-至今-）\" class=\"headerlink\" title=\"newegg公司 （ 2015年11月 ~ 至今 ）\"></a>newegg公司 （ 2015年11月 ~ 至今 ）</h2><h3 id=\"RCT项目\"><a href=\"#RCT项目\" class=\"headerlink\" title=\"RCT项目\"></a>RCT项目</h3><ul>\n<li><strong>项目描述</strong>：RCT 是一个通过解析rdb文件对redis内存结构分析的一站式平台。 支持对非集群/集群rdb文件分析、Slowlog查询与监控、ClientList查询与监控。</li>\n<li><strong>项目特色</strong>：采用分布式架构，通过解析redis rdb文件，多角度多场景分析redis中内存数据结构，提供友好的可视化报表以及邮件通知等功能。</li>\n<li><strong>项目职责</strong>: 主要负责架构设计，平台代码开发，项目管理等工作。</li>\n<li><strong>项目业绩</strong>: 目前已在产线上使用，多次发挥重要作用，为公司节省了宝贵内存资源。</li>\n</ul>\n<h3 id=\"SyncBigdataPlatform项目\"><a href=\"#SyncBigdataPlatform项目\" class=\"headerlink\" title=\"SyncBigdataPlatform项目\"></a>SyncBigdataPlatform项目</h3><ul>\n<li><strong>项目描述</strong>：SyncBigdataPlatform 是依赖大数据存储技术，提供高性能读写，jumplocation,以及跨location数据同步服务平台。DB层采用Hbase,Cassandra,Cache层采用Redis，数据同步层采用Kafka。提供restful接口，以供业务调用。</li>\n<li><strong>项目特色</strong>：支持自定义业务扩展，支持热部署，支持配置信息动态调整，支持可视化配置信息更改，支持多数据中心高速数据同步，支持jumplocation操作，支持业务数据格式与数据库scheme自定义设计</li>\n<li><strong>项目职责</strong>: 主要负责架构设计，平台代码开发，项目管理等工作。</li>\n<li><strong>项目业绩</strong>: 目前平台上线多个主要服务，可以支持重点核心业务系统，上线一年来，稳定运行无误，获得公司优秀项目荣誉。</li>\n</ul>\n<h3 id=\"基础服务搭建与运维\"><a href=\"#基础服务搭建与运维\" class=\"headerlink\" title=\"基础服务搭建与运维\"></a>基础服务搭建与运维</h3><ul>\n<li><strong>项目职责</strong>: 搭建RedisCluster,Elasticsearch,KafkaCluster集群，对集群进行日常维护，处理线上问题，开发周边组件，便于更好的维护集群。</li>\n<li><strong>项目业绩</strong>: 为newegg线上服务提供可靠稳定的cache,消息，检索服务。</li>\n</ul>\n<h3 id=\"服务Docker化\"><a href=\"#服务Docker化\" class=\"headerlink\" title=\"服务Docker化\"></a>服务Docker化</h3><ul>\n<li><strong>项目描述</strong>：针对公司kafka/redis/elasticsearch/opentsdb等基础服务docker化，其次针对业务项目进行docker化部署</li>\n<li><strong>项目职责</strong>: 针对不同需求场景设计部署架构，编写shell脚本，Dockfile文件。</li>\n<li><strong>项目业绩</strong>: 将我们项目组所有服务实现全部docker化，降低服务维护难度，极大的解放劳动力。</li>\n</ul>\n<h3 id=\"RedisClientAPI项目\"><a href=\"#RedisClientAPI项目\" class=\"headerlink\" title=\"RedisClientAPI项目\"></a>RedisClientAPI项目</h3><ul>\n<li><strong>项目描述</strong>：该项目提供一个使用Redis的rest/thrift API,客户端可以跨多种编程语言，更高效，更便捷的使用Redis的功能。</li>\n<li><strong>项目职责</strong>: 负责该项目的设计及开发工作</li>\n<li><strong>项目业绩</strong>: 提供高效API服务，承受上亿次调用，未出现任何事故。实现了RedisCluster批量操作，慢查询追踪等功能。项目难点是对于批量操作的实现方案改造。</li>\n</ul>\n<h3 id=\"MonitorPlatform项目\"><a href=\"#MonitorPlatform项目\" class=\"headerlink\" title=\"MonitorPlatform项目\"></a>MonitorPlatform项目</h3><ul>\n<li><strong>项目描述</strong>：该项目可以用来检测机器指标，基础服务指标，业务指标等数据，通过可视化技术快捷高效的监控相关服务。</li>\n<li><strong>项目职责</strong>: 主要负责该项目的设计与开发工作，该项目利用ES stack技术，使用metricbeat收集机器指标信息，使用grafana做可视化。</li>\n<li><strong>项目业绩</strong>: 实现我们团队服务的无死角化监控，打造具体我们team特色的监控平台。</li>\n</ul>\n<h3 id=\"其他项目\"><a href=\"#其他项目\" class=\"headerlink\" title=\"其他项目\"></a>其他项目</h3><p>redis cluster监控，KafkaTools,EC Dashboard，kibana 插件(email_table,indices_view，cleaner,curator)</p>\n<h2 id=\"huatek公司-（-2014年1月-2015年10月-）\"><a href=\"#huatek公司-（-2014年1月-2015年10月-）\" class=\"headerlink\" title=\"huatek公司 （ 2014年1月 ~ 2015年10月 ）\"></a>huatek公司 （ 2014年1月 ~ 2015年10月 ）</h2><h3 id=\"基于Activiti工作流引擎开发\"><a href=\"#基于Activiti工作流引擎开发\" class=\"headerlink\" title=\"基于Activiti工作流引擎开发\"></a>基于Activiti工作流引擎开发</h3><ul>\n<li><strong>项目描述</strong>：公司发展拙见壮大，出差请假管理混乱，公司技术支持管理需要，同时业务组系统需要相关工作流引擎支持，在此背景下，公司专门成立人员开发工作流引擎。此系统可以可视化设计流程，快速便捷的适应不同业务场景需要。目前已经上线三套流程。该系统包含以下几个模块：自定义字段、自定义表单、流程设计、用户组管理、申请任务、待办任务、任务列表等模块。</li>\n<li><strong>项目职责</strong>:<br>负责工作流引擎表单设计，流程权限设计与开发，流程数据的汇总与导出，各个模块数据的查询等。</li>\n<li><strong>项目业绩</strong>:<br>顺利实现公司办公平台出差请假流程，技术支持流程。为固定资产业务系统提供购置流程。</li>\n</ul>\n<h3 id=\"李宁订单优化管理系统\"><a href=\"#李宁订单优化管理系统\" class=\"headerlink\" title=\"李宁订单优化管理系统\"></a>李宁订单优化管理系统</h3><ul>\n<li><strong>项目描述</strong>：由于目前SAP系统中根据合同开单时，需要大量人工计算，为了提高订单配货发单效率，以此为基础为李宁发货员工开发一套针对订单拆单、配货，其中包含一部分有规律有逻辑的计算法则，达到订单拆分的目的系统.该系统包含以下几个模块：信息管理、流程审批、分货管理、订单管理、报表信息。</li>\n<li><strong>项目职责</strong>: 按照计划表参与开发该系统信息管理、流程审批、分货管理、订单管理、报表信息模块，接口模块。</li>\n<li><strong>项目业绩</strong>: 按照项目需求，完成目前所涉及的所有开发任务。</li>\n</ul>\n<hr>\n<h1 id=\"开源项目和作品\"><a href=\"#开源项目和作品\" class=\"headerlink\" title=\"开源项目和作品\"></a>开源项目和作品</h1><h2 id=\"开源项目\"><a href=\"#开源项目\" class=\"headerlink\" title=\"开源项目\"></a>开源项目</h2><ul>\n<li><a href=\"https://github.com/xaecbd/RCT\" target=\"_blank\" rel=\"noopener\">RCT</a> redis 内存分析工具，获得业界关注。</li>\n<li><a href=\"https://github.com/TrumanDu/autocomplate-shell\" target=\"_blank\" rel=\"noopener\">autocomplate-shell</a>：VS Code 编写shell插件，下载量7k+</li>\n<li><a href=\"https://github.com/TrumanDu/indices_view\" target=\"_blank\" rel=\"noopener\">indices_view</a>：查看ES indices kibana 插件已被<a href=\"https://www.elastic.co/guide/en/kibana/current/known-plugins.html\" target=\"_blank\" rel=\"noopener\">elastic</a>官网收录，<a href=\"https://blog.gitee.com/2018/04/16/weekly068/\" target=\"_blank\" rel=\"noopener\">码云周刊第68期</a>推荐 ）</li>\n<li><a href=\"https://github.com/TrumanDu/cleaner\" target=\"_blank\" rel=\"noopener\">cleaner</a>：设置es index ttl 插件,已被<a href=\"https://www.elastic.co/guide/en/kibana/current/known-plugins.html\" target=\"_blank\" rel=\"noopener\">elastic</a>官网收录</li>\n</ul>\n<h2 id=\"技术文章\"><a href=\"#技术文章\" class=\"headerlink\" title=\"技术文章\"></a>技术文章</h2><ul>\n<li><a href=\"http://trumandu.github.io\">技术博客</a></li>\n<li><a href=\"http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/\">RedisCluster构建批量操作探讨</a></li>\n<li><a href=\"https://kibana.gitbook.io/kibana-plugin-development-tutorial/\" target=\"_blank\" rel=\"noopener\">Kibana Plugin Development Tutorial</a> </li>\n</ul>\n<h1 id=\"技能清单\"><a href=\"#技能清单\" class=\"headerlink\" title=\"技能清单\"></a>技能清单</h1><p>以下均为我熟练使用的技能</p>\n<ul>\n<li>语言： Java/Bash/Node.js</li>\n<li>前端框架：Bootstrap/Jquery/HTML5/React</li>\n<li>RPC框架：Thrift</li>\n<li>Web框架：Spring/Express</li>\n<li>ORM框架：Mybatis</li>\n<li>微服务架构：Springboot</li>\n<li>项目管理：Maven</li>\n<li>关系型数据库：MySQL/SQLite</li>\n<li>NOSQL：Redis/Hbase/Cassandra/Elasticsearch</li>\n<li>消息队列：Kafka</li>\n<li>实时计算：KSQL</li>\n<li>虚拟化：Docker</li>\n<li>大数据相关：Hadoop/Hive/Zookeeper/Storm/Oozie</li>\n<li>监控相关：Opentsdb/Elastic stack(es/kibana/logstash)</li>\n<li>系统：Linux/Window</li>\n<li>版本管理：SVN/GIT</li>\n</ul>\n<hr>\n"},{"title":"tags","date":"2016-03-12T13:04:59.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-03-12 21:04:59\ntype: \"tags\"\n---\n","updated":"2016-03-12T13:07:26.913Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjzmobvb0002b8cee3ujarb1u","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"DockerFile编写","date":"2016-10-24T13:07:56.000Z","_content":"所有指令都是大写\n1. ADD,COPY\n\n两个都是将本地文件复制到镜像中，区别是ADD可以指定绝对路径的文件，言外之意是可以上传除当前目录之外的文件。而COPY只能上传当前目录的文件。\n\n这两条命令复制文件夹的话，只会讲子目录复制到指定目录下。例如\n\nADD  redis3.0.4 /opt/app/redis/\n\n只会将redis3.0.4下文件复制到redis目录下，不包含redis3.0.4目录。COPY同理\n2. CMD,ENTRYPOINT\n\n两个都是容器启动时运行的命令，区别是CMD可以被覆盖，而ENTRYPOINT不会。ENTRYPOINT只能是最后一个生效。","source":"_posts/DockerFile编写.md","raw":"---\ntitle: DockerFile编写\ndate: 2016-10-24 21:07:56\ntags: 虚拟化\ncategories:\n- docker\n---\n所有指令都是大写\n1. ADD,COPY\n\n两个都是将本地文件复制到镜像中，区别是ADD可以指定绝对路径的文件，言外之意是可以上传除当前目录之外的文件。而COPY只能上传当前目录的文件。\n\n这两条命令复制文件夹的话，只会讲子目录复制到指定目录下。例如\n\nADD  redis3.0.4 /opt/app/redis/\n\n只会将redis3.0.4下文件复制到redis目录下，不包含redis3.0.4目录。COPY同理\n2. CMD,ENTRYPOINT\n\n两个都是容器启动时运行的命令，区别是CMD可以被覆盖，而ENTRYPOINT不会。ENTRYPOINT只能是最后一个生效。","slug":"DockerFile编写","published":1,"updated":"2016-10-24T13:08:42.646Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4d00008cee6m6qnwe4","content":"<p>所有指令都是大写</p>\n<ol>\n<li>ADD,COPY</li>\n</ol>\n<p>两个都是将本地文件复制到镜像中，区别是ADD可以指定绝对路径的文件，言外之意是可以上传除当前目录之外的文件。而COPY只能上传当前目录的文件。</p>\n<p>这两条命令复制文件夹的话，只会讲子目录复制到指定目录下。例如</p>\n<p>ADD  redis3.0.4 /opt/app/redis/</p>\n<p>只会将redis3.0.4下文件复制到redis目录下，不包含redis3.0.4目录。COPY同理</p>\n<ol start=\"2\">\n<li>CMD,ENTRYPOINT</li>\n</ol>\n<p>两个都是容器启动时运行的命令，区别是CMD可以被覆盖，而ENTRYPOINT不会。ENTRYPOINT只能是最后一个生效。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>所有指令都是大写</p>\n<ol>\n<li>ADD,COPY</li>\n</ol>\n<p>两个都是将本地文件复制到镜像中，区别是ADD可以指定绝对路径的文件，言外之意是可以上传除当前目录之外的文件。而COPY只能上传当前目录的文件。</p>\n<p>这两条命令复制文件夹的话，只会讲子目录复制到指定目录下。例如</p>\n<p>ADD  redis3.0.4 /opt/app/redis/</p>\n<p>只会将redis3.0.4下文件复制到redis目录下，不包含redis3.0.4目录。COPY同理</p>\n<ol start=\"2\">\n<li>CMD,ENTRYPOINT</li>\n</ol>\n<p>两个都是容器启动时运行的命令，区别是CMD可以被覆盖，而ENTRYPOINT不会。ENTRYPOINT只能是最后一个生效。</p>\n"},{"title":"ES+Kibana+Packetbeat使用总结","date":"2016-07-18T13:15:32.000Z","_content":"# 简介\nElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshark。\n\nPacketbeat 是一个实时的网络数据包分析器，通过结合ES可以构建一个应用监控和性能分析系统。Packetbeat 可以将监控数据发送到es，或者redis,logstash中。目前支持以下协议：\n- ICMP (v4 and v6)\n- DNS\n- HTTP\n- Mysql\n- PostgreSQL\n- Redis\n- Thrift-RPC\n- MongoDB\n- Memcache\n\n# 搭建\n## 搭建Es\n1. 下载\n```\nwget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.4/elasticsearch-2.3.4.tar.gz\n```\n2. 解压\n```\ntar -xvf elasticsearch-2.3.4.tar.gz\n```\n3. 配置 ES\n默认配置即可正常使用，但配置config/elasticsearch.yml使用效果更好。\n配置如下：\n```\n cluster.name: my-application\n node.name: node-1\n node.master: true\n node.data: true \n path.logs: /data/truman/elasticsearch-2.3.4/logs\n bootstrap.mlockall: true\n\n network.host: 192.168.1.42\n\n http.port: 9200\n\n discovery.zen.ping.multicast.enabled: false\n discovery.zen.ping.unicast.hosts: [\"192.168.1.42\"]\n\n discovery.zen.fd.ping_interval: 1s\n discovery.zen.fd.ping_timeout: 30s\n discovery.zen.fd.ping_retries: 5\n\n action.auto_create_index: true\n action.disable_delete_all_indices: true\n indices.memory.index_buffer_size: 30%\n indices.memory.min_shard_index_buffer_size: 12mb\n indices.memory.min_index_buffer_size: 96mb\n action.write_consistency: one\n index.number_of_shards: 3\n index.number_of_replicas: 1\n threadpool:\n  bulk:\n   type: fixed\n   queue_size: 300\n\n index.translog.flush_threshold_period: 10m\n```\n4. 启动\n在使用root账户启动脚本可能会报错，在次修改一下elasticsearch启动脚本，增加以下内容：\n```\nES_JAVA_OPTS=\"-Des.insecure.allow.root=true\"\n```\n然后后台启动脚本\n```\nnohup bin/elasticsearch &\n```\n访问 http://192.168.1.42:9200/即可查看是否启动成功。\n5. 安装head plugin\nhead 插件能够可视化操作index与数据,在http://192.168.1.42:9200/_plugin/head/上进行操作\n\n安装\n```\nbin/plugin install mobz/elasticsearch-head\n```\n移除\n```\nbin/plugin remove head\n```\n## 安装kibana\n1. 下载\n```\nwget https://download.elastic.co/kibana/kibana/kibana-4.5.2-linux-x64.tar.gz\n```\n2. 启动\n```\ntar -xvf kibana-4.5.2-linux-x64.tar.gz\ncd kibana-4.5.2-linux-x64\nbin/kibana\n```\n在浏览器中访问http://192.168.1.42:5601查看是否成功。\n## 安装Packetbeat\n1. 下载\n```\nwget https://download.elastic.co/beats/packetbeat/packetbeat-1.2.3-x86_64.tar.gz\n```\n2. 解压\n```\ntar -xvf packetbeat-1.2.3-x86_64.tar.gz\n```\n3. 配置 Packetbeat\n修改es输出即可,默认是localhost,将该内容修改为es所在的主机ip，即可使用默认配置运行了。\n```\noutput:\n  ### Elasticsearch as output\n  elasticsearch:\n    # Array of hosts to connect to.\n     hosts: [\"192.168.1.42:9200\"]\n     template:\n\n      # Template name. By default the template name is packetbeat.\n      #name: \"packetbeat\"\n\n      # Path to template file\n      path: \"packetbeat.template.json\"\n\n      # Overwrite existing template\n      #overwrite: false\n```\n修改完成后，可以通过以下命令检验修改配置是否正确(该shell需要root权限)\n```\nsudo ./packetbeat -configtest -e\n```\n4. 加载索引模板（index template）到es\n- 加载\n```\ncurl -XPUT 'http://192.168.1.42:9200/_template/packetbeat' -d@/etc/packetbeat/packetbeat.template.json\n```\n加载成功后，访问以下网址，看packetbeat索引模板是否加上。\n```\nhttp://192.168.1.42:9200/_template/packetbeat\n```\n- 删除模板文件\n```\ncurl -XDELETE 'http://192.168.1.42:9200/_template/packetbeat'\n```\n5. 启动\n```\nsudo nohup ./packetbeat &\n```\n6. 测试\n模拟简单http请求\n```\ncurl http://www.aibibang.com > /dev/null\n```\n查询数据\n```\ncurl -XGET 'http://192.168.1.42:9200/packetbeat-*/_search?pretty'\n```\n查询出数据，即可证明安装成功！\n\n## 经验总结\n1. 索引未创建成功\n\n在搭建过程中，可能由于索引模板配置错误，导致索引未创建成功。但还存在一种情况，即没有数据采集进去。采集到数据以后才会根据索引模板创建索引\n2. thrift 监控配置\n\n目前对thrift监控仅支持binary协议，详细配置如下\n```\nthrift:\n    # Configure the ports where to listen for Thrift-RPC traffic. You can disable\n    # the Thrift-RPC protocol by commenting out the list of ports.\n    ports: [9090]\n    transport_type: framed\n    protocol_type: binary\n    string_max_size: 200\n    collection_max_size: 20\n    capture_reply: true\n    obfuscate_strings: true\n    drop_after_n_struct_fields: 100\n```\n3. 修改采集字段\n~~使用过程中，希望采集自定义字段，当前版本未发现~~。但在最新版本5.0.0-alpha4中可通过修改yml配置文件删除掉不需要采集的字段。\n在5.0.0-alpha4中配置如下：\n```\nfilters:\n - drop_fields:\n     fields: [\"request\", \"query\",\"server\",\"proc\",\"client_server\"]\n```\n其实老的版本在index template中也是可以设置的。通过设置_source,例如：\n```\n\"_source\": {\n        \"excludes\": [\n          \"request\", \n\t\t  \"query\",\n\t\t  \"server\",\n\t\t  \"proc\",\n\t\t  \"params\",\n\t\t  \"beat.hostname\",\n\t\t  \"client_server\"\n        ]\n      },\n```\n","source":"_posts/ES-Kibana-Packetbeat使用总结.md","raw":"---\ntitle: ES+Kibana+Packetbeat使用总结\ndate: 2016-07-18 21:15:32\ntags: research\ncategories:\n- elasticsearch\n---\n# 简介\nElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshark。\n\nPacketbeat 是一个实时的网络数据包分析器，通过结合ES可以构建一个应用监控和性能分析系统。Packetbeat 可以将监控数据发送到es，或者redis,logstash中。目前支持以下协议：\n- ICMP (v4 and v6)\n- DNS\n- HTTP\n- Mysql\n- PostgreSQL\n- Redis\n- Thrift-RPC\n- MongoDB\n- Memcache\n\n# 搭建\n## 搭建Es\n1. 下载\n```\nwget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.4/elasticsearch-2.3.4.tar.gz\n```\n2. 解压\n```\ntar -xvf elasticsearch-2.3.4.tar.gz\n```\n3. 配置 ES\n默认配置即可正常使用，但配置config/elasticsearch.yml使用效果更好。\n配置如下：\n```\n cluster.name: my-application\n node.name: node-1\n node.master: true\n node.data: true \n path.logs: /data/truman/elasticsearch-2.3.4/logs\n bootstrap.mlockall: true\n\n network.host: 192.168.1.42\n\n http.port: 9200\n\n discovery.zen.ping.multicast.enabled: false\n discovery.zen.ping.unicast.hosts: [\"192.168.1.42\"]\n\n discovery.zen.fd.ping_interval: 1s\n discovery.zen.fd.ping_timeout: 30s\n discovery.zen.fd.ping_retries: 5\n\n action.auto_create_index: true\n action.disable_delete_all_indices: true\n indices.memory.index_buffer_size: 30%\n indices.memory.min_shard_index_buffer_size: 12mb\n indices.memory.min_index_buffer_size: 96mb\n action.write_consistency: one\n index.number_of_shards: 3\n index.number_of_replicas: 1\n threadpool:\n  bulk:\n   type: fixed\n   queue_size: 300\n\n index.translog.flush_threshold_period: 10m\n```\n4. 启动\n在使用root账户启动脚本可能会报错，在次修改一下elasticsearch启动脚本，增加以下内容：\n```\nES_JAVA_OPTS=\"-Des.insecure.allow.root=true\"\n```\n然后后台启动脚本\n```\nnohup bin/elasticsearch &\n```\n访问 http://192.168.1.42:9200/即可查看是否启动成功。\n5. 安装head plugin\nhead 插件能够可视化操作index与数据,在http://192.168.1.42:9200/_plugin/head/上进行操作\n\n安装\n```\nbin/plugin install mobz/elasticsearch-head\n```\n移除\n```\nbin/plugin remove head\n```\n## 安装kibana\n1. 下载\n```\nwget https://download.elastic.co/kibana/kibana/kibana-4.5.2-linux-x64.tar.gz\n```\n2. 启动\n```\ntar -xvf kibana-4.5.2-linux-x64.tar.gz\ncd kibana-4.5.2-linux-x64\nbin/kibana\n```\n在浏览器中访问http://192.168.1.42:5601查看是否成功。\n## 安装Packetbeat\n1. 下载\n```\nwget https://download.elastic.co/beats/packetbeat/packetbeat-1.2.3-x86_64.tar.gz\n```\n2. 解压\n```\ntar -xvf packetbeat-1.2.3-x86_64.tar.gz\n```\n3. 配置 Packetbeat\n修改es输出即可,默认是localhost,将该内容修改为es所在的主机ip，即可使用默认配置运行了。\n```\noutput:\n  ### Elasticsearch as output\n  elasticsearch:\n    # Array of hosts to connect to.\n     hosts: [\"192.168.1.42:9200\"]\n     template:\n\n      # Template name. By default the template name is packetbeat.\n      #name: \"packetbeat\"\n\n      # Path to template file\n      path: \"packetbeat.template.json\"\n\n      # Overwrite existing template\n      #overwrite: false\n```\n修改完成后，可以通过以下命令检验修改配置是否正确(该shell需要root权限)\n```\nsudo ./packetbeat -configtest -e\n```\n4. 加载索引模板（index template）到es\n- 加载\n```\ncurl -XPUT 'http://192.168.1.42:9200/_template/packetbeat' -d@/etc/packetbeat/packetbeat.template.json\n```\n加载成功后，访问以下网址，看packetbeat索引模板是否加上。\n```\nhttp://192.168.1.42:9200/_template/packetbeat\n```\n- 删除模板文件\n```\ncurl -XDELETE 'http://192.168.1.42:9200/_template/packetbeat'\n```\n5. 启动\n```\nsudo nohup ./packetbeat &\n```\n6. 测试\n模拟简单http请求\n```\ncurl http://www.aibibang.com > /dev/null\n```\n查询数据\n```\ncurl -XGET 'http://192.168.1.42:9200/packetbeat-*/_search?pretty'\n```\n查询出数据，即可证明安装成功！\n\n## 经验总结\n1. 索引未创建成功\n\n在搭建过程中，可能由于索引模板配置错误，导致索引未创建成功。但还存在一种情况，即没有数据采集进去。采集到数据以后才会根据索引模板创建索引\n2. thrift 监控配置\n\n目前对thrift监控仅支持binary协议，详细配置如下\n```\nthrift:\n    # Configure the ports where to listen for Thrift-RPC traffic. You can disable\n    # the Thrift-RPC protocol by commenting out the list of ports.\n    ports: [9090]\n    transport_type: framed\n    protocol_type: binary\n    string_max_size: 200\n    collection_max_size: 20\n    capture_reply: true\n    obfuscate_strings: true\n    drop_after_n_struct_fields: 100\n```\n3. 修改采集字段\n~~使用过程中，希望采集自定义字段，当前版本未发现~~。但在最新版本5.0.0-alpha4中可通过修改yml配置文件删除掉不需要采集的字段。\n在5.0.0-alpha4中配置如下：\n```\nfilters:\n - drop_fields:\n     fields: [\"request\", \"query\",\"server\",\"proc\",\"client_server\"]\n```\n其实老的版本在index template中也是可以设置的。通过设置_source,例如：\n```\n\"_source\": {\n        \"excludes\": [\n          \"request\", \n\t\t  \"query\",\n\t\t  \"server\",\n\t\t  \"proc\",\n\t\t  \"params\",\n\t\t  \"beat.hostname\",\n\t\t  \"client_server\"\n        ]\n      },\n```\n","slug":"ES-Kibana-Packetbeat使用总结","published":1,"updated":"2016-07-18T13:17:37.862Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4i00018ceeacz9tw2i","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshark。</p>\n<p>Packetbeat 是一个实时的网络数据包分析器，通过结合ES可以构建一个应用监控和性能分析系统。Packetbeat 可以将监控数据发送到es，或者redis,logstash中。目前支持以下协议：</p>\n<ul>\n<li>ICMP (v4 and v6)</li>\n<li>DNS</li>\n<li>HTTP</li>\n<li>Mysql</li>\n<li>PostgreSQL</li>\n<li>Redis</li>\n<li>Thrift-RPC</li>\n<li>MongoDB</li>\n<li>Memcache</li>\n</ul>\n<h1 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h1><h2 id=\"搭建Es\"><a href=\"#搭建Es\" class=\"headerlink\" title=\"搭建Es\"></a>搭建Es</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.4/elasticsearch-2.3.4.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf elasticsearch-2.3.4.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置 ES<br>默认配置即可正常使用，但配置config/elasticsearch.yml使用效果更好。<br>配置如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.name: my-application</span><br><span class=\"line\">node.name: node-1</span><br><span class=\"line\">node.master: true</span><br><span class=\"line\">node.data: true </span><br><span class=\"line\">path.logs: /data/truman/elasticsearch-2.3.4/logs</span><br><span class=\"line\">bootstrap.mlockall: true</span><br><span class=\"line\"></span><br><span class=\"line\">network.host: 192.168.1.42</span><br><span class=\"line\"></span><br><span class=\"line\">http.port: 9200</span><br><span class=\"line\"></span><br><span class=\"line\">discovery.zen.ping.multicast.enabled: false</span><br><span class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;192.168.1.42&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">discovery.zen.fd.ping_interval: 1s</span><br><span class=\"line\">discovery.zen.fd.ping_timeout: 30s</span><br><span class=\"line\">discovery.zen.fd.ping_retries: 5</span><br><span class=\"line\"></span><br><span class=\"line\">action.auto_create_index: true</span><br><span class=\"line\">action.disable_delete_all_indices: true</span><br><span class=\"line\">indices.memory.index_buffer_size: 30%</span><br><span class=\"line\">indices.memory.min_shard_index_buffer_size: 12mb</span><br><span class=\"line\">indices.memory.min_index_buffer_size: 96mb</span><br><span class=\"line\">action.write_consistency: one</span><br><span class=\"line\">index.number_of_shards: 3</span><br><span class=\"line\">index.number_of_replicas: 1</span><br><span class=\"line\">threadpool:</span><br><span class=\"line\"> bulk:</span><br><span class=\"line\">  type: fixed</span><br><span class=\"line\">  queue_size: 300</span><br><span class=\"line\"></span><br><span class=\"line\">index.translog.flush_threshold_period: 10m</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动<br>在使用root账户启动脚本可能会报错，在次修改一下elasticsearch启动脚本，增加以下内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ES_JAVA_OPTS=&quot;-Des.insecure.allow.root=true&quot;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>然后后台启动脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup bin/elasticsearch &amp;</span><br></pre></td></tr></table></figure></p>\n<p>访问 <a href=\"http://192.168.1.42:9200/即可查看是否启动成功。\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:9200/即可查看是否启动成功。</a></p>\n<ol start=\"5\">\n<li>安装head plugin<br>head 插件能够可视化操作index与数据,在<a href=\"http://192.168.1.42:9200/_plugin/head/上进行操作\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:9200/_plugin/head/上进行操作</a></li>\n</ol>\n<p>安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/plugin install mobz/elasticsearch-head</span><br></pre></td></tr></table></figure></p>\n<p>移除<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/plugin remove head</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装kibana\"><a href=\"#安装kibana\" class=\"headerlink\" title=\"安装kibana\"></a>安装kibana</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/kibana/kibana/kibana-4.5.2-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf kibana-4.5.2-linux-x64.tar.gz</span><br><span class=\"line\">cd kibana-4.5.2-linux-x64</span><br><span class=\"line\">bin/kibana</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>在浏览器中访问<a href=\"http://192.168.1.42:5601查看是否成功。\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:5601查看是否成功。</a></p>\n<h2 id=\"安装Packetbeat\"><a href=\"#安装Packetbeat\" class=\"headerlink\" title=\"安装Packetbeat\"></a>安装Packetbeat</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/beats/packetbeat/packetbeat-1.2.3-x86_64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf packetbeat-1.2.3-x86_64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置 Packetbeat<br>修改es输出即可,默认是localhost,将该内容修改为es所在的主机ip，即可使用默认配置运行了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output:</span><br><span class=\"line\">  ### Elasticsearch as output</span><br><span class=\"line\">  elasticsearch:</span><br><span class=\"line\">    # Array of hosts to connect to.</span><br><span class=\"line\">     hosts: [&quot;192.168.1.42:9200&quot;]</span><br><span class=\"line\">     template:</span><br><span class=\"line\"></span><br><span class=\"line\">      # Template name. By default the template name is packetbeat.</span><br><span class=\"line\">      #name: &quot;packetbeat&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">      # Path to template file</span><br><span class=\"line\">      path: &quot;packetbeat.template.json&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">      # Overwrite existing template</span><br><span class=\"line\">      #overwrite: false</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>修改完成后，可以通过以下命令检验修改配置是否正确(该shell需要root权限)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./packetbeat -configtest -e</span><br></pre></td></tr></table></figure></p>\n<ol start=\"4\">\n<li>加载索引模板（index template）到es</li>\n</ol>\n<ul>\n<li>加载<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPUT &apos;http://192.168.1.42:9200/_template/packetbeat&apos; -d@/etc/packetbeat/packetbeat.template.json</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>加载成功后，访问以下网址，看packetbeat索引模板是否加上。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://192.168.1.42:9200/_template/packetbeat</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>删除模板文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://192.168.1.42:9200/_template/packetbeat&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol start=\"5\">\n<li><p>启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nohup ./packetbeat &amp;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>测试<br>模拟简单http请求</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http://www.aibibang.com &gt; /dev/null</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>查询数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XGET &apos;http://192.168.1.42:9200/packetbeat-*/_search?pretty&apos;</span><br></pre></td></tr></table></figure></p>\n<p>查询出数据，即可证明安装成功！</p>\n<h2 id=\"经验总结\"><a href=\"#经验总结\" class=\"headerlink\" title=\"经验总结\"></a>经验总结</h2><ol>\n<li>索引未创建成功</li>\n</ol>\n<p>在搭建过程中，可能由于索引模板配置错误，导致索引未创建成功。但还存在一种情况，即没有数据采集进去。采集到数据以后才会根据索引模板创建索引</p>\n<ol start=\"2\">\n<li>thrift 监控配置</li>\n</ol>\n<p>目前对thrift监控仅支持binary协议，详细配置如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift:</span><br><span class=\"line\">    # Configure the ports where to listen for Thrift-RPC traffic. You can disable</span><br><span class=\"line\">    # the Thrift-RPC protocol by commenting out the list of ports.</span><br><span class=\"line\">    ports: [9090]</span><br><span class=\"line\">    transport_type: framed</span><br><span class=\"line\">    protocol_type: binary</span><br><span class=\"line\">    string_max_size: 200</span><br><span class=\"line\">    collection_max_size: 20</span><br><span class=\"line\">    capture_reply: true</span><br><span class=\"line\">    obfuscate_strings: true</span><br><span class=\"line\">    drop_after_n_struct_fields: 100</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>修改采集字段<br><del>使用过程中，希望采集自定义字段，当前版本未发现</del>。但在最新版本5.0.0-alpha4中可通过修改yml配置文件删除掉不需要采集的字段。<br>在5.0.0-alpha4中配置如下：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filters:</span><br><span class=\"line\"> - drop_fields:</span><br><span class=\"line\">     fields: [&quot;request&quot;, &quot;query&quot;,&quot;server&quot;,&quot;proc&quot;,&quot;client_server&quot;]</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其实老的版本在index template中也是可以设置的。通过设置_source,例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;_source&quot;: &#123;</span><br><span class=\"line\">        &quot;excludes&quot;: [</span><br><span class=\"line\">          &quot;request&quot;, </span><br><span class=\"line\">\t\t  &quot;query&quot;,</span><br><span class=\"line\">\t\t  &quot;server&quot;,</span><br><span class=\"line\">\t\t  &quot;proc&quot;,</span><br><span class=\"line\">\t\t  &quot;params&quot;,</span><br><span class=\"line\">\t\t  &quot;beat.hostname&quot;,</span><br><span class=\"line\">\t\t  &quot;client_server&quot;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">      &#125;,</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshark。</p>\n<p>Packetbeat 是一个实时的网络数据包分析器，通过结合ES可以构建一个应用监控和性能分析系统。Packetbeat 可以将监控数据发送到es，或者redis,logstash中。目前支持以下协议：</p>\n<ul>\n<li>ICMP (v4 and v6)</li>\n<li>DNS</li>\n<li>HTTP</li>\n<li>Mysql</li>\n<li>PostgreSQL</li>\n<li>Redis</li>\n<li>Thrift-RPC</li>\n<li>MongoDB</li>\n<li>Memcache</li>\n</ul>\n<h1 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h1><h2 id=\"搭建Es\"><a href=\"#搭建Es\" class=\"headerlink\" title=\"搭建Es\"></a>搭建Es</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.4/elasticsearch-2.3.4.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf elasticsearch-2.3.4.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置 ES<br>默认配置即可正常使用，但配置config/elasticsearch.yml使用效果更好。<br>配置如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.name: my-application</span><br><span class=\"line\">node.name: node-1</span><br><span class=\"line\">node.master: true</span><br><span class=\"line\">node.data: true </span><br><span class=\"line\">path.logs: /data/truman/elasticsearch-2.3.4/logs</span><br><span class=\"line\">bootstrap.mlockall: true</span><br><span class=\"line\"></span><br><span class=\"line\">network.host: 192.168.1.42</span><br><span class=\"line\"></span><br><span class=\"line\">http.port: 9200</span><br><span class=\"line\"></span><br><span class=\"line\">discovery.zen.ping.multicast.enabled: false</span><br><span class=\"line\">discovery.zen.ping.unicast.hosts: [&quot;192.168.1.42&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">discovery.zen.fd.ping_interval: 1s</span><br><span class=\"line\">discovery.zen.fd.ping_timeout: 30s</span><br><span class=\"line\">discovery.zen.fd.ping_retries: 5</span><br><span class=\"line\"></span><br><span class=\"line\">action.auto_create_index: true</span><br><span class=\"line\">action.disable_delete_all_indices: true</span><br><span class=\"line\">indices.memory.index_buffer_size: 30%</span><br><span class=\"line\">indices.memory.min_shard_index_buffer_size: 12mb</span><br><span class=\"line\">indices.memory.min_index_buffer_size: 96mb</span><br><span class=\"line\">action.write_consistency: one</span><br><span class=\"line\">index.number_of_shards: 3</span><br><span class=\"line\">index.number_of_replicas: 1</span><br><span class=\"line\">threadpool:</span><br><span class=\"line\"> bulk:</span><br><span class=\"line\">  type: fixed</span><br><span class=\"line\">  queue_size: 300</span><br><span class=\"line\"></span><br><span class=\"line\">index.translog.flush_threshold_period: 10m</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动<br>在使用root账户启动脚本可能会报错，在次修改一下elasticsearch启动脚本，增加以下内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ES_JAVA_OPTS=&quot;-Des.insecure.allow.root=true&quot;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>然后后台启动脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup bin/elasticsearch &amp;</span><br></pre></td></tr></table></figure></p>\n<p>访问 <a href=\"http://192.168.1.42:9200/即可查看是否启动成功。\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:9200/即可查看是否启动成功。</a></p>\n<ol start=\"5\">\n<li>安装head plugin<br>head 插件能够可视化操作index与数据,在<a href=\"http://192.168.1.42:9200/_plugin/head/上进行操作\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:9200/_plugin/head/上进行操作</a></li>\n</ol>\n<p>安装<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/plugin install mobz/elasticsearch-head</span><br></pre></td></tr></table></figure></p>\n<p>移除<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/plugin remove head</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"安装kibana\"><a href=\"#安装kibana\" class=\"headerlink\" title=\"安装kibana\"></a>安装kibana</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/kibana/kibana/kibana-4.5.2-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf kibana-4.5.2-linux-x64.tar.gz</span><br><span class=\"line\">cd kibana-4.5.2-linux-x64</span><br><span class=\"line\">bin/kibana</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>在浏览器中访问<a href=\"http://192.168.1.42:5601查看是否成功。\" target=\"_blank\" rel=\"noopener\">http://192.168.1.42:5601查看是否成功。</a></p>\n<h2 id=\"安装Packetbeat\"><a href=\"#安装Packetbeat\" class=\"headerlink\" title=\"安装Packetbeat\"></a>安装Packetbeat</h2><ol>\n<li><p>下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://download.elastic.co/beats/packetbeat/packetbeat-1.2.3-x86_64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>解压</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvf packetbeat-1.2.3-x86_64.tar.gz</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置 Packetbeat<br>修改es输出即可,默认是localhost,将该内容修改为es所在的主机ip，即可使用默认配置运行了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output:</span><br><span class=\"line\">  ### Elasticsearch as output</span><br><span class=\"line\">  elasticsearch:</span><br><span class=\"line\">    # Array of hosts to connect to.</span><br><span class=\"line\">     hosts: [&quot;192.168.1.42:9200&quot;]</span><br><span class=\"line\">     template:</span><br><span class=\"line\"></span><br><span class=\"line\">      # Template name. By default the template name is packetbeat.</span><br><span class=\"line\">      #name: &quot;packetbeat&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">      # Path to template file</span><br><span class=\"line\">      path: &quot;packetbeat.template.json&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">      # Overwrite existing template</span><br><span class=\"line\">      #overwrite: false</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>修改完成后，可以通过以下命令检验修改配置是否正确(该shell需要root权限)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo ./packetbeat -configtest -e</span><br></pre></td></tr></table></figure></p>\n<ol start=\"4\">\n<li>加载索引模板（index template）到es</li>\n</ol>\n<ul>\n<li>加载<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPUT &apos;http://192.168.1.42:9200/_template/packetbeat&apos; -d@/etc/packetbeat/packetbeat.template.json</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>加载成功后，访问以下网址，看packetbeat索引模板是否加上。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://192.168.1.42:9200/_template/packetbeat</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>删除模板文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://192.168.1.42:9200/_template/packetbeat&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol start=\"5\">\n<li><p>启动</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nohup ./packetbeat &amp;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>测试<br>模拟简单http请求</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http://www.aibibang.com &gt; /dev/null</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>查询数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XGET &apos;http://192.168.1.42:9200/packetbeat-*/_search?pretty&apos;</span><br></pre></td></tr></table></figure></p>\n<p>查询出数据，即可证明安装成功！</p>\n<h2 id=\"经验总结\"><a href=\"#经验总结\" class=\"headerlink\" title=\"经验总结\"></a>经验总结</h2><ol>\n<li>索引未创建成功</li>\n</ol>\n<p>在搭建过程中，可能由于索引模板配置错误，导致索引未创建成功。但还存在一种情况，即没有数据采集进去。采集到数据以后才会根据索引模板创建索引</p>\n<ol start=\"2\">\n<li>thrift 监控配置</li>\n</ol>\n<p>目前对thrift监控仅支持binary协议，详细配置如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">thrift:</span><br><span class=\"line\">    # Configure the ports where to listen for Thrift-RPC traffic. You can disable</span><br><span class=\"line\">    # the Thrift-RPC protocol by commenting out the list of ports.</span><br><span class=\"line\">    ports: [9090]</span><br><span class=\"line\">    transport_type: framed</span><br><span class=\"line\">    protocol_type: binary</span><br><span class=\"line\">    string_max_size: 200</span><br><span class=\"line\">    collection_max_size: 20</span><br><span class=\"line\">    capture_reply: true</span><br><span class=\"line\">    obfuscate_strings: true</span><br><span class=\"line\">    drop_after_n_struct_fields: 100</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>修改采集字段<br><del>使用过程中，希望采集自定义字段，当前版本未发现</del>。但在最新版本5.0.0-alpha4中可通过修改yml配置文件删除掉不需要采集的字段。<br>在5.0.0-alpha4中配置如下：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filters:</span><br><span class=\"line\"> - drop_fields:</span><br><span class=\"line\">     fields: [&quot;request&quot;, &quot;query&quot;,&quot;server&quot;,&quot;proc&quot;,&quot;client_server&quot;]</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其实老的版本在index template中也是可以设置的。通过设置_source,例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;_source&quot;: &#123;</span><br><span class=\"line\">        &quot;excludes&quot;: [</span><br><span class=\"line\">          &quot;request&quot;, </span><br><span class=\"line\">\t\t  &quot;query&quot;,</span><br><span class=\"line\">\t\t  &quot;server&quot;,</span><br><span class=\"line\">\t\t  &quot;proc&quot;,</span><br><span class=\"line\">\t\t  &quot;params&quot;,</span><br><span class=\"line\">\t\t  &quot;beat.hostname&quot;,</span><br><span class=\"line\">\t\t  &quot;client_server&quot;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">      &#125;,</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Eclipse Maven Update 时JDK版本变更问题","date":"2016-10-24T13:21:38.000Z","_content":"1.新建一个Maven项目JDK版本和系统版本不对应，\n\n2.右键Maven项目->Maven->Update ProjectJDK版本改变了，\n\n3.操作系统的JDK重装了新的版本，这是引起前面两个现象的主要原因。\n\n修改方法（假如系统jdk版本是1.8）：\n\n方法一：在pom.xml文件中指定jdk的版本:\n```\n<build>  \n        <plugins>  \n            <plugin>  \n                <groupId>org.apache.maven.plugins</groupId>  \n                <artifactId>maven-compiler-plugin</artifactId>  \n                <version>3.1</version>  \n                <configuration>  \n                    <source>1.8</source>  \n                    <target>1.8</target>  \n                </configuration>  \n            </plugin>  \n        </plugins>  \n    </build>\n```    \n方法二：修改settings.xml，找到profiles节点，在里面添加\n```\n<profile>    \n    <id>jdk-1.8</id>    \n     <activation>    \n          <activeByDefault>true</activeByDefault>    \n          <jdk>1.8</jdk>    \n      </activation>    \n<properties>    \n<maven.compiler.source>1.8</maven.compiler.source>    \n<maven.compiler.target>1.8</maven.compiler.target>    \n<maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>    \n</properties>    \n</profile> \n```\n然后在update项目就可以保持和系统jdk版本一致。推荐使用第二种方法，减少对每个项目配置。","source":"_posts/Eclipse-Maven-Update-时JDK版本变更问题.md","raw":"---\ntitle: Eclipse Maven Update 时JDK版本变更问题\ndate: 2016-10-24 21:21:38\ntags: 开发笔记\ncategories:\n- java\n---\n1.新建一个Maven项目JDK版本和系统版本不对应，\n\n2.右键Maven项目->Maven->Update ProjectJDK版本改变了，\n\n3.操作系统的JDK重装了新的版本，这是引起前面两个现象的主要原因。\n\n修改方法（假如系统jdk版本是1.8）：\n\n方法一：在pom.xml文件中指定jdk的版本:\n```\n<build>  \n        <plugins>  \n            <plugin>  \n                <groupId>org.apache.maven.plugins</groupId>  \n                <artifactId>maven-compiler-plugin</artifactId>  \n                <version>3.1</version>  \n                <configuration>  \n                    <source>1.8</source>  \n                    <target>1.8</target>  \n                </configuration>  \n            </plugin>  \n        </plugins>  \n    </build>\n```    \n方法二：修改settings.xml，找到profiles节点，在里面添加\n```\n<profile>    \n    <id>jdk-1.8</id>    \n     <activation>    \n          <activeByDefault>true</activeByDefault>    \n          <jdk>1.8</jdk>    \n      </activation>    \n<properties>    \n<maven.compiler.source>1.8</maven.compiler.source>    \n<maven.compiler.target>1.8</maven.compiler.target>    \n<maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>    \n</properties>    \n</profile> \n```\n然后在update项目就可以保持和系统jdk版本一致。推荐使用第二种方法，减少对每个项目配置。","slug":"Eclipse-Maven-Update-时JDK版本变更问题","published":1,"updated":"2016-10-24T13:22:18.234Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4n00048ceeqw0lbrcy","content":"<p>1.新建一个Maven项目JDK版本和系统版本不对应，</p>\n<p>2.右键Maven项目-&gt;Maven-&gt;Update ProjectJDK版本改变了，</p>\n<p>3.操作系统的JDK重装了新的版本，这是引起前面两个现象的主要原因。</p>\n<p>修改方法（假如系统jdk版本是1.8）：</p>\n<p>方法一：在pom.xml文件中指定jdk的版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;  </span><br><span class=\"line\">        &lt;plugins&gt;  </span><br><span class=\"line\">            &lt;plugin&gt;  </span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  </span><br><span class=\"line\">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  </span><br><span class=\"line\">                &lt;version&gt;3.1&lt;/version&gt;  </span><br><span class=\"line\">                &lt;configuration&gt;  </span><br><span class=\"line\">                    &lt;source&gt;1.8&lt;/source&gt;  </span><br><span class=\"line\">                    &lt;target&gt;1.8&lt;/target&gt;  </span><br><span class=\"line\">                &lt;/configuration&gt;  </span><br><span class=\"line\">            &lt;/plugin&gt;  </span><br><span class=\"line\">        &lt;/plugins&gt;  </span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\">```    </span><br><span class=\"line\">方法二：修改settings.xml，找到profiles节点，在里面添加</span><br></pre></td></tr></table></figure></p>\n<p><profile><br>    <id>jdk-1.8</id><br>     <activation><br>          <activebydefault>true</activebydefault><br>          <jdk>1.8</jdk><br>      </activation>    </profile></p>\n<p><properties>    </properties></p>\n<p>&lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    </p>\n<p>&lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    </p>\n<p>&lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt;<br><br><br><code>`</code><br>然后在update项目就可以保持和系统jdk版本一致。推荐使用第二种方法，减少对每个项目配置。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1.新建一个Maven项目JDK版本和系统版本不对应，</p>\n<p>2.右键Maven项目-&gt;Maven-&gt;Update ProjectJDK版本改变了，</p>\n<p>3.操作系统的JDK重装了新的版本，这是引起前面两个现象的主要原因。</p>\n<p>修改方法（假如系统jdk版本是1.8）：</p>\n<p>方法一：在pom.xml文件中指定jdk的版本:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;  </span><br><span class=\"line\">        &lt;plugins&gt;  </span><br><span class=\"line\">            &lt;plugin&gt;  </span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  </span><br><span class=\"line\">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;  </span><br><span class=\"line\">                &lt;version&gt;3.1&lt;/version&gt;  </span><br><span class=\"line\">                &lt;configuration&gt;  </span><br><span class=\"line\">                    &lt;source&gt;1.8&lt;/source&gt;  </span><br><span class=\"line\">                    &lt;target&gt;1.8&lt;/target&gt;  </span><br><span class=\"line\">                &lt;/configuration&gt;  </span><br><span class=\"line\">            &lt;/plugin&gt;  </span><br><span class=\"line\">        &lt;/plugins&gt;  </span><br><span class=\"line\">    &lt;/build&gt;</span><br><span class=\"line\">```    </span><br><span class=\"line\">方法二：修改settings.xml，找到profiles节点，在里面添加</span><br></pre></td></tr></table></figure></p>\n<p><profile><br>    <id>jdk-1.8</id><br>     <activation><br>          <activebydefault>true</activebydefault><br>          <jdk>1.8</jdk><br>      </activation>    </profile></p>\n<p><properties>    </properties></p>\n<p>&lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    </p>\n<p>&lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    </p>\n<p>&lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt;<br><br><br><code>`</code><br>然后在update项目就可以保持和系统jdk版本一致。推荐使用第二种方法，减少对每个项目配置。</p>\n"},{"title":"Elasticsearch For HDFS","date":"2018-07-22T10:51:22.000Z","_content":"# Elasticsearch For HDFS\n\n## 1.前言\n本文主要是探讨repository-hdfs插件，该插件的目的是可以将es的数据备份到HDFS中，并且可以从该文件恢复数据。\n***\n## 2.安装\n\n1. 安装\n   ```\n   sudo bin/elasticsearch-plugin install repository-hdfs\n   ```\n   这里的安装是**在线安装**，当然还可以离线安装。例如：\n   ```\n   sudo bin/elasticsearch-plugin install file:///home/hadoop/elk/repository-hdfs.zip\n   ```\n   \n2. 卸载\n   ```\n   sudo bin/elasticsearch-plugin remove repository-hdfs\n   ```\n***\n## 3.使用\n### 3.1创建仓库\n- 创建仓库\n```\nPUT _snapshot/my_hdfs_repository\n{\n  \"type\": \"hdfs\",\n  \"settings\": {\n    \"uri\": \"hdfs://namenode:8020/\",\n    \"path\": \"elasticsearch/respositories/my_hdfs_repository\"\n  }\n}\n```\n\n属性|描述\n---|---\n`uri`| hdfs url 地址. 例如: \"hdfs://<host>:<port>/\". (Required)\n`path`|数据备份与加载地址. 例如: \"path/to/file\". (Required)\n`load_defaults`|是否加载默认的Hadoop配置。 (Enabled by default)\n`conf.<key>`|要添加到Hadoop配置的内联配置参数。 （可选）插件只能识别hadoop[core](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml)和[hdfs]((http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml))配置文件中面向客户端的属性。\n`compress`|是否压缩元数据. (Disabled by default)\n`chunk_size`|覆盖块大小. (Disabled by default)\n`security.principal`|\n\n- 查看所有的仓库\n```\nGET /_snapshot/_all\n```\n\n### 3.2备份数据\n - 快照所有index\n   ```\n   PUT /_snapshot/my_hdfs_repository/snapshot_all?wait_for_completion=false\n   ```\n - 快照指定index\n   ```\n   PUT /_snapshot/my_hdfs_repository/snapshot_1?wait_for_completion=false\n    {\n      \"indices\": \"index_1,index_2\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": false\n    }\n   ```\n   其中indices可以指定多个index,例如：index_1,index_2\n - 查看快照进度\n   ```\n   GET /_snapshot/my_hdfs_repository/snapshot_all/_status\n   ```\n - 取消快照任务（删除快照）\n   ```\n   DELETE /_snapshot/my_hdfs_repository/snapshot_all\n   ```\n   这个可以删除一个存在的快照或者取消一个正在进行的快照。\n\n### 3.3恢复数据\n - 恢复指定快照中所有index\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_all/_restore\n   ```\n - 恢复指定快照中的index\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_all/_restore\n    {\n      \"indices\": \"metricbeat-6.3.0-2018.07.18\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": true,\n      \"rename_pattern\": \"metricbeat-(.+)\",\n      \"rename_replacement\": \"allrestored_metricbeat-$1\"\n    }\n   ```\n   在这里可以设置恢复后新的index的名称，也可以和之前的保持一致（前提是集群中不存在该index）\n - 查看恢复进度\n   ```\n   GET index1,index2/_recovery?human\n   GET _cat/recovery?v\n   ```\n - 恢复任务设置\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_2/_restore\n    {\n      \"indices\": \"ec_gatewaymsg_jpf\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": true,\n      \"index_settings\": {\n        \"index.number_of_replicas\": 0\n      },\n      \"rename_pattern\": \"ec_gatewaymsg_(.+)\",\n      \"rename_replacement\": \"restore_$1\"\n    }\n   ```\n   在恢复过程，可以修改部分index 的设置，但是分片数除外。\n***\n## 4.参考\n1. [repository-hdfs](https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-hdfs.html#repository-hdfs)\n2. [modules-snapshots](https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-snapshots.html)","source":"_posts/Elasticsearch-For-HDFS.md","raw":"---\ntitle: Elasticsearch For HDFS\ndate: 2018-07-22 18:51:22\ntags: research\ncategories:\n- elasticsearch\n---\n# Elasticsearch For HDFS\n\n## 1.前言\n本文主要是探讨repository-hdfs插件，该插件的目的是可以将es的数据备份到HDFS中，并且可以从该文件恢复数据。\n***\n## 2.安装\n\n1. 安装\n   ```\n   sudo bin/elasticsearch-plugin install repository-hdfs\n   ```\n   这里的安装是**在线安装**，当然还可以离线安装。例如：\n   ```\n   sudo bin/elasticsearch-plugin install file:///home/hadoop/elk/repository-hdfs.zip\n   ```\n   \n2. 卸载\n   ```\n   sudo bin/elasticsearch-plugin remove repository-hdfs\n   ```\n***\n## 3.使用\n### 3.1创建仓库\n- 创建仓库\n```\nPUT _snapshot/my_hdfs_repository\n{\n  \"type\": \"hdfs\",\n  \"settings\": {\n    \"uri\": \"hdfs://namenode:8020/\",\n    \"path\": \"elasticsearch/respositories/my_hdfs_repository\"\n  }\n}\n```\n\n属性|描述\n---|---\n`uri`| hdfs url 地址. 例如: \"hdfs://<host>:<port>/\". (Required)\n`path`|数据备份与加载地址. 例如: \"path/to/file\". (Required)\n`load_defaults`|是否加载默认的Hadoop配置。 (Enabled by default)\n`conf.<key>`|要添加到Hadoop配置的内联配置参数。 （可选）插件只能识别hadoop[core](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml)和[hdfs]((http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml))配置文件中面向客户端的属性。\n`compress`|是否压缩元数据. (Disabled by default)\n`chunk_size`|覆盖块大小. (Disabled by default)\n`security.principal`|\n\n- 查看所有的仓库\n```\nGET /_snapshot/_all\n```\n\n### 3.2备份数据\n - 快照所有index\n   ```\n   PUT /_snapshot/my_hdfs_repository/snapshot_all?wait_for_completion=false\n   ```\n - 快照指定index\n   ```\n   PUT /_snapshot/my_hdfs_repository/snapshot_1?wait_for_completion=false\n    {\n      \"indices\": \"index_1,index_2\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": false\n    }\n   ```\n   其中indices可以指定多个index,例如：index_1,index_2\n - 查看快照进度\n   ```\n   GET /_snapshot/my_hdfs_repository/snapshot_all/_status\n   ```\n - 取消快照任务（删除快照）\n   ```\n   DELETE /_snapshot/my_hdfs_repository/snapshot_all\n   ```\n   这个可以删除一个存在的快照或者取消一个正在进行的快照。\n\n### 3.3恢复数据\n - 恢复指定快照中所有index\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_all/_restore\n   ```\n - 恢复指定快照中的index\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_all/_restore\n    {\n      \"indices\": \"metricbeat-6.3.0-2018.07.18\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": true,\n      \"rename_pattern\": \"metricbeat-(.+)\",\n      \"rename_replacement\": \"allrestored_metricbeat-$1\"\n    }\n   ```\n   在这里可以设置恢复后新的index的名称，也可以和之前的保持一致（前提是集群中不存在该index）\n - 查看恢复进度\n   ```\n   GET index1,index2/_recovery?human\n   GET _cat/recovery?v\n   ```\n - 恢复任务设置\n   ```\n   POST /_snapshot/my_hdfs_repository/snapshot_2/_restore\n    {\n      \"indices\": \"ec_gatewaymsg_jpf\",\n      \"ignore_unavailable\": true,\n      \"include_global_state\": true,\n      \"index_settings\": {\n        \"index.number_of_replicas\": 0\n      },\n      \"rename_pattern\": \"ec_gatewaymsg_(.+)\",\n      \"rename_replacement\": \"restore_$1\"\n    }\n   ```\n   在恢复过程，可以修改部分index 的设置，但是分片数除外。\n***\n## 4.参考\n1. [repository-hdfs](https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-hdfs.html#repository-hdfs)\n2. [modules-snapshots](https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-snapshots.html)","slug":"Elasticsearch-For-HDFS","published":1,"updated":"2018-07-22T11:57:49.410Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4o00058ceeks8s3o4d","content":"<h1 id=\"Elasticsearch-For-HDFS\"><a href=\"#Elasticsearch-For-HDFS\" class=\"headerlink\" title=\"Elasticsearch For HDFS\"></a>Elasticsearch For HDFS</h1><h2 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1.前言\"></a>1.前言</h2><p>本文主要是探讨repository-hdfs插件，该插件的目的是可以将es的数据备份到HDFS中，并且可以从该文件恢复数据。</p>\n<hr>\n<h2 id=\"2-安装\"><a href=\"#2-安装\" class=\"headerlink\" title=\"2.安装\"></a>2.安装</h2><ol>\n<li><p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin install repository-hdfs</span><br></pre></td></tr></table></figure>\n<p>这里的安装是<strong>在线安装</strong>，当然还可以离线安装。例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin install file:///home/hadoop/elk/repository-hdfs.zip</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol start=\"2\">\n<li>卸载<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin remove repository-hdfs</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h2 id=\"3-使用\"><a href=\"#3-使用\" class=\"headerlink\" title=\"3.使用\"></a>3.使用</h2><h3 id=\"3-1创建仓库\"><a href=\"#3-1创建仓库\" class=\"headerlink\" title=\"3.1创建仓库\"></a>3.1创建仓库</h3><ul>\n<li>创建仓库<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _snapshot/my_hdfs_repository</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;type&quot;: &quot;hdfs&quot;,</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;uri&quot;: &quot;hdfs://namenode:8020/&quot;,</span><br><span class=\"line\">    &quot;path&quot;: &quot;elasticsearch/respositories/my_hdfs_repository&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>uri</code></td>\n<td>hdfs url 地址. 例如: “hdfs://<host>:<port>/“. (Required)</port></host></td>\n</tr>\n<tr>\n<td><code>path</code></td>\n<td>数据备份与加载地址. 例如: “path/to/file”. (Required)</td>\n</tr>\n<tr>\n<td><code>load_defaults</code></td>\n<td>是否加载默认的Hadoop配置。 (Enabled by default)</td>\n</tr>\n<tr>\n<td><code>conf.&lt;key&gt;</code></td>\n<td>要添加到Hadoop配置的内联配置参数。 （可选）插件只能识别hadoop<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml\" target=\"_blank\" rel=\"noopener\">core</a>和<a href=\"(http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\">hdfs</a>)配置文件中面向客户端的属性。</td>\n</tr>\n<tr>\n<td><code>compress</code></td>\n<td>是否压缩元数据. (Disabled by default)</td>\n</tr>\n<tr>\n<td><code>chunk_size</code></td>\n<td>覆盖块大小. (Disabled by default)</td>\n</tr>\n<tr>\n<td><code>security.principal</code></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查看所有的仓库<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /_snapshot/_all</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"3-2备份数据\"><a href=\"#3-2备份数据\" class=\"headerlink\" title=\"3.2备份数据\"></a>3.2备份数据</h3><ul>\n<li><p>快照所有index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT /_snapshot/my_hdfs_repository/snapshot_all?wait_for_completion=false</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>快照指定index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT /_snapshot/my_hdfs_repository/snapshot_1?wait_for_completion=false</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;index_1,index_2&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: false</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中indices可以指定多个index,例如：index_1,index_2</p>\n</li>\n<li><p>查看快照进度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /_snapshot/my_hdfs_repository/snapshot_all/_status</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>取消快照任务（删除快照）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DELETE /_snapshot/my_hdfs_repository/snapshot_all</span><br></pre></td></tr></table></figure>\n<p>这个可以删除一个存在的快照或者取消一个正在进行的快照。</p>\n</li>\n</ul>\n<h3 id=\"3-3恢复数据\"><a href=\"#3-3恢复数据\" class=\"headerlink\" title=\"3.3恢复数据\"></a>3.3恢复数据</h3><ul>\n<li><p>恢复指定快照中所有index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_all/_restore</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>恢复指定快照中的index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_all/_restore</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;metricbeat-6.3.0-2018.07.18&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: true,</span><br><span class=\"line\">   &quot;rename_pattern&quot;: &quot;metricbeat-(.+)&quot;,</span><br><span class=\"line\">   &quot;rename_replacement&quot;: &quot;allrestored_metricbeat-$1&quot;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里可以设置恢复后新的index的名称，也可以和之前的保持一致（前提是集群中不存在该index）</p>\n</li>\n<li><p>查看恢复进度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET index1,index2/_recovery?human</span><br><span class=\"line\">GET _cat/recovery?v</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>恢复任务设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_2/_restore</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;ec_gatewaymsg_jpf&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: true,</span><br><span class=\"line\">   &quot;index_settings&quot;: &#123;</span><br><span class=\"line\">     &quot;index.number_of_replicas&quot;: 0</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;rename_pattern&quot;: &quot;ec_gatewaymsg_(.+)&quot;,</span><br><span class=\"line\">   &quot;rename_replacement&quot;: &quot;restore_$1&quot;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>在恢复过程，可以修改部分index 的设置，但是分片数除外。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-参考\"><a href=\"#4-参考\" class=\"headerlink\" title=\"4.参考\"></a>4.参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-hdfs.html#repository-hdfs\" target=\"_blank\" rel=\"noopener\">repository-hdfs</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-snapshots.html\" target=\"_blank\" rel=\"noopener\">modules-snapshots</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Elasticsearch-For-HDFS\"><a href=\"#Elasticsearch-For-HDFS\" class=\"headerlink\" title=\"Elasticsearch For HDFS\"></a>Elasticsearch For HDFS</h1><h2 id=\"1-前言\"><a href=\"#1-前言\" class=\"headerlink\" title=\"1.前言\"></a>1.前言</h2><p>本文主要是探讨repository-hdfs插件，该插件的目的是可以将es的数据备份到HDFS中，并且可以从该文件恢复数据。</p>\n<hr>\n<h2 id=\"2-安装\"><a href=\"#2-安装\" class=\"headerlink\" title=\"2.安装\"></a>2.安装</h2><ol>\n<li><p>安装</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin install repository-hdfs</span><br></pre></td></tr></table></figure>\n<p>这里的安装是<strong>在线安装</strong>，当然还可以离线安装。例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin install file:///home/hadoop/elk/repository-hdfs.zip</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol start=\"2\">\n<li>卸载<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo bin/elasticsearch-plugin remove repository-hdfs</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<h2 id=\"3-使用\"><a href=\"#3-使用\" class=\"headerlink\" title=\"3.使用\"></a>3.使用</h2><h3 id=\"3-1创建仓库\"><a href=\"#3-1创建仓库\" class=\"headerlink\" title=\"3.1创建仓库\"></a>3.1创建仓库</h3><ul>\n<li>创建仓库<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _snapshot/my_hdfs_repository</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;type&quot;: &quot;hdfs&quot;,</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;uri&quot;: &quot;hdfs://namenode:8020/&quot;,</span><br><span class=\"line\">    &quot;path&quot;: &quot;elasticsearch/respositories/my_hdfs_repository&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>uri</code></td>\n<td>hdfs url 地址. 例如: “hdfs://<host>:<port>/“. (Required)</port></host></td>\n</tr>\n<tr>\n<td><code>path</code></td>\n<td>数据备份与加载地址. 例如: “path/to/file”. (Required)</td>\n</tr>\n<tr>\n<td><code>load_defaults</code></td>\n<td>是否加载默认的Hadoop配置。 (Enabled by default)</td>\n</tr>\n<tr>\n<td><code>conf.&lt;key&gt;</code></td>\n<td>要添加到Hadoop配置的内联配置参数。 （可选）插件只能识别hadoop<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml\" target=\"_blank\" rel=\"noopener\">core</a>和<a href=\"(http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\">hdfs</a>)配置文件中面向客户端的属性。</td>\n</tr>\n<tr>\n<td><code>compress</code></td>\n<td>是否压缩元数据. (Disabled by default)</td>\n</tr>\n<tr>\n<td><code>chunk_size</code></td>\n<td>覆盖块大小. (Disabled by default)</td>\n</tr>\n<tr>\n<td><code>security.principal</code></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>查看所有的仓库<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /_snapshot/_all</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"3-2备份数据\"><a href=\"#3-2备份数据\" class=\"headerlink\" title=\"3.2备份数据\"></a>3.2备份数据</h3><ul>\n<li><p>快照所有index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT /_snapshot/my_hdfs_repository/snapshot_all?wait_for_completion=false</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>快照指定index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT /_snapshot/my_hdfs_repository/snapshot_1?wait_for_completion=false</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;index_1,index_2&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: false</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中indices可以指定多个index,例如：index_1,index_2</p>\n</li>\n<li><p>查看快照进度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET /_snapshot/my_hdfs_repository/snapshot_all/_status</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>取消快照任务（删除快照）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DELETE /_snapshot/my_hdfs_repository/snapshot_all</span><br></pre></td></tr></table></figure>\n<p>这个可以删除一个存在的快照或者取消一个正在进行的快照。</p>\n</li>\n</ul>\n<h3 id=\"3-3恢复数据\"><a href=\"#3-3恢复数据\" class=\"headerlink\" title=\"3.3恢复数据\"></a>3.3恢复数据</h3><ul>\n<li><p>恢复指定快照中所有index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_all/_restore</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>恢复指定快照中的index</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_all/_restore</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;metricbeat-6.3.0-2018.07.18&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: true,</span><br><span class=\"line\">   &quot;rename_pattern&quot;: &quot;metricbeat-(.+)&quot;,</span><br><span class=\"line\">   &quot;rename_replacement&quot;: &quot;allrestored_metricbeat-$1&quot;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里可以设置恢复后新的index的名称，也可以和之前的保持一致（前提是集群中不存在该index）</p>\n</li>\n<li><p>查看恢复进度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET index1,index2/_recovery?human</span><br><span class=\"line\">GET _cat/recovery?v</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>恢复任务设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST /_snapshot/my_hdfs_repository/snapshot_2/_restore</span><br><span class=\"line\"> &#123;</span><br><span class=\"line\">   &quot;indices&quot;: &quot;ec_gatewaymsg_jpf&quot;,</span><br><span class=\"line\">   &quot;ignore_unavailable&quot;: true,</span><br><span class=\"line\">   &quot;include_global_state&quot;: true,</span><br><span class=\"line\">   &quot;index_settings&quot;: &#123;</span><br><span class=\"line\">     &quot;index.number_of_replicas&quot;: 0</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;rename_pattern&quot;: &quot;ec_gatewaymsg_(.+)&quot;,</span><br><span class=\"line\">   &quot;rename_replacement&quot;: &quot;restore_$1&quot;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>在恢复过程，可以修改部分index 的设置，但是分片数除外。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"4-参考\"><a href=\"#4-参考\" class=\"headerlink\" title=\"4.参考\"></a>4.参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-hdfs.html#repository-hdfs\" target=\"_blank\" rel=\"noopener\">repository-hdfs</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.3/modules-snapshots.html\" target=\"_blank\" rel=\"noopener\">modules-snapshots</a></li>\n</ol>\n"},{"title":"Elasticsearch Mapping Best Practice","date":"2019-03-02T14:26:48.000Z","_content":"## Mapping各字段的选型流程\n\n![image](https://github.com/TrumanDu/pic_repository/blob/master/Mapping%E5%90%84%E5%AD%97%E6%AE%B5%E7%9A%84%E9%80%89%E5%9E%8B%E6%B5%81%E7%A8%8B.png?raw=true)\n\n## 规则\nelasticsearch 为了更好的让大家开箱即用，默认启用了大量不必要的设置，为了降低空间使用，提升查询效率，请按以下规则构建最佳的Mapping\n\n### 1. 禁用不需要的功能\n#### 不用查询，禁用index\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"integer\",\n          \"index\": false\n        }\n      }\n    }\n  }\n}\n```\n\n#### 不关心评分，禁用该功能\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"text\",\n          \"norms\": false\n        }\n      }\n    }\n  }\n}\n```\n#### 不需要短语查询，禁用index positions\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"text\",\n          \"index_options\": \"freqs\"\n        }\n      }\n    }\n  }\n}\n```\n\n### 2. 不要使用默认的Mapping\n默认Mapping的字段类型是系统自动识别的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。\n\n根据业务需要选择合适的类型，有利于节省空间和提升精度。\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"dynamic_templates\": [\n        {\n          \"strings\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### 3. 考虑identifiers映射为keyword\n\n某些数据是数字的事实并不意味着它应该始终映射为数字字段。 Elasticsearch索引数字的方式可以优化范围查询，而keyword字段在term查询时更好。\n通常，存储诸如ISBN的标识符或标识来自另一数据库的记录的任何数字的字段很少用于范围查询或聚合。这就是为什么他们可能会受益于被映射为keyword而不是integer或long。\n\n## 参考\n1. [Tune for disk usage](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-disk-usage.html#tune-for-disk-usage)\n2. [让 Elasticsearch 飞起来：性能优化实践干货](https://toutiao.io/posts/lkqldx/preview)","source":"_posts/Elasticsearch-Mapping-Best-Practice.md","raw":"---\ntitle: Elasticsearch Mapping Best Practice\ndate: 2019-03-02 22:26:48\ntags: 经验\ncategories:\n- elasticsearch\n---\n## Mapping各字段的选型流程\n\n![image](https://github.com/TrumanDu/pic_repository/blob/master/Mapping%E5%90%84%E5%AD%97%E6%AE%B5%E7%9A%84%E9%80%89%E5%9E%8B%E6%B5%81%E7%A8%8B.png?raw=true)\n\n## 规则\nelasticsearch 为了更好的让大家开箱即用，默认启用了大量不必要的设置，为了降低空间使用，提升查询效率，请按以下规则构建最佳的Mapping\n\n### 1. 禁用不需要的功能\n#### 不用查询，禁用index\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"integer\",\n          \"index\": false\n        }\n      }\n    }\n  }\n}\n```\n\n#### 不关心评分，禁用该功能\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"text\",\n          \"norms\": false\n        }\n      }\n    }\n  }\n}\n```\n#### 不需要短语查询，禁用index positions\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"text\",\n          \"index_options\": \"freqs\"\n        }\n      }\n    }\n  }\n}\n```\n\n### 2. 不要使用默认的Mapping\n默认Mapping的字段类型是系统自动识别的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。\n\n根据业务需要选择合适的类型，有利于节省空间和提升精度。\n```\nPUT index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"dynamic_templates\": [\n        {\n          \"strings\": {\n            \"match_mapping_type\": \"string\",\n            \"mapping\": {\n              \"type\": \"keyword\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n### 3. 考虑identifiers映射为keyword\n\n某些数据是数字的事实并不意味着它应该始终映射为数字字段。 Elasticsearch索引数字的方式可以优化范围查询，而keyword字段在term查询时更好。\n通常，存储诸如ISBN的标识符或标识来自另一数据库的记录的任何数字的字段很少用于范围查询或聚合。这就是为什么他们可能会受益于被映射为keyword而不是integer或long。\n\n## 参考\n1. [Tune for disk usage](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-disk-usage.html#tune-for-disk-usage)\n2. [让 Elasticsearch 飞起来：性能优化实践干货](https://toutiao.io/posts/lkqldx/preview)","slug":"Elasticsearch-Mapping-Best-Practice","published":1,"updated":"2019-03-02T14:27:24.862Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4q00068ceeiv809oxt","content":"<h2 id=\"Mapping各字段的选型流程\"><a href=\"#Mapping各字段的选型流程\" class=\"headerlink\" title=\"Mapping各字段的选型流程\"></a>Mapping各字段的选型流程</h2><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/Mapping%E5%90%84%E5%AD%97%E6%AE%B5%E7%9A%84%E9%80%89%E5%9E%8B%E6%B5%81%E7%A8%8B.png?raw=true\" alt=\"image\"></p>\n<h2 id=\"规则\"><a href=\"#规则\" class=\"headerlink\" title=\"规则\"></a>规则</h2><p>elasticsearch 为了更好的让大家开箱即用，默认启用了大量不必要的设置，为了降低空间使用，提升查询效率，请按以下规则构建最佳的Mapping</p>\n<h3 id=\"1-禁用不需要的功能\"><a href=\"#1-禁用不需要的功能\" class=\"headerlink\" title=\"1. 禁用不需要的功能\"></a>1. 禁用不需要的功能</h3><h4 id=\"不用查询，禁用index\"><a href=\"#不用查询，禁用index\" class=\"headerlink\" title=\"不用查询，禁用index\"></a>不用查询，禁用index</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;integer&quot;,</span><br><span class=\"line\">          &quot;index&quot;: false</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"不关心评分，禁用该功能\"><a href=\"#不关心评分，禁用该功能\" class=\"headerlink\" title=\"不关心评分，禁用该功能\"></a>不关心评分，禁用该功能</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;norms&quot;: false</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"不需要短语查询，禁用index-positions\"><a href=\"#不需要短语查询，禁用index-positions\" class=\"headerlink\" title=\"不需要短语查询，禁用index positions\"></a>不需要短语查询，禁用index positions</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;index_options&quot;: &quot;freqs&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-不要使用默认的Mapping\"><a href=\"#2-不要使用默认的Mapping\" class=\"headerlink\" title=\"2. 不要使用默认的Mapping\"></a>2. 不要使用默认的Mapping</h3><p>默认Mapping的字段类型是系统自动识别的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。</p>\n<p>根据业务需要选择合适的类型，有利于节省空间和提升精度。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;dynamic_templates&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;strings&quot;: &#123;</span><br><span class=\"line\">            &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class=\"line\">            &quot;mapping&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-考虑identifiers映射为keyword\"><a href=\"#3-考虑identifiers映射为keyword\" class=\"headerlink\" title=\"3. 考虑identifiers映射为keyword\"></a>3. 考虑identifiers映射为keyword</h3><p>某些数据是数字的事实并不意味着它应该始终映射为数字字段。 Elasticsearch索引数字的方式可以优化范围查询，而keyword字段在term查询时更好。<br>通常，存储诸如ISBN的标识符或标识来自另一数据库的记录的任何数字的字段很少用于范围查询或聚合。这就是为什么他们可能会受益于被映射为keyword而不是integer或long。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-disk-usage.html#tune-for-disk-usage\" target=\"_blank\" rel=\"noopener\">Tune for disk usage</a></li>\n<li><a href=\"https://toutiao.io/posts/lkqldx/preview\" target=\"_blank\" rel=\"noopener\">让 Elasticsearch 飞起来：性能优化实践干货</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Mapping各字段的选型流程\"><a href=\"#Mapping各字段的选型流程\" class=\"headerlink\" title=\"Mapping各字段的选型流程\"></a>Mapping各字段的选型流程</h2><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/Mapping%E5%90%84%E5%AD%97%E6%AE%B5%E7%9A%84%E9%80%89%E5%9E%8B%E6%B5%81%E7%A8%8B.png?raw=true\" alt=\"image\"></p>\n<h2 id=\"规则\"><a href=\"#规则\" class=\"headerlink\" title=\"规则\"></a>规则</h2><p>elasticsearch 为了更好的让大家开箱即用，默认启用了大量不必要的设置，为了降低空间使用，提升查询效率，请按以下规则构建最佳的Mapping</p>\n<h3 id=\"1-禁用不需要的功能\"><a href=\"#1-禁用不需要的功能\" class=\"headerlink\" title=\"1. 禁用不需要的功能\"></a>1. 禁用不需要的功能</h3><h4 id=\"不用查询，禁用index\"><a href=\"#不用查询，禁用index\" class=\"headerlink\" title=\"不用查询，禁用index\"></a>不用查询，禁用index</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;integer&quot;,</span><br><span class=\"line\">          &quot;index&quot;: false</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"不关心评分，禁用该功能\"><a href=\"#不关心评分，禁用该功能\" class=\"headerlink\" title=\"不关心评分，禁用该功能\"></a>不关心评分，禁用该功能</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;norms&quot;: false</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"不需要短语查询，禁用index-positions\"><a href=\"#不需要短语查询，禁用index-positions\" class=\"headerlink\" title=\"不需要短语查询，禁用index positions\"></a>不需要短语查询，禁用index positions</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;foo&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;index_options&quot;: &quot;freqs&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-不要使用默认的Mapping\"><a href=\"#2-不要使用默认的Mapping\" class=\"headerlink\" title=\"2. 不要使用默认的Mapping\"></a>2. 不要使用默认的Mapping</h3><p>默认Mapping的字段类型是系统自动识别的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。</p>\n<p>根据业务需要选择合适的类型，有利于节省空间和提升精度。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;dynamic_templates&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;strings&quot;: &#123;</span><br><span class=\"line\">            &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class=\"line\">            &quot;mapping&quot;: &#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-考虑identifiers映射为keyword\"><a href=\"#3-考虑identifiers映射为keyword\" class=\"headerlink\" title=\"3. 考虑identifiers映射为keyword\"></a>3. 考虑identifiers映射为keyword</h3><p>某些数据是数字的事实并不意味着它应该始终映射为数字字段。 Elasticsearch索引数字的方式可以优化范围查询，而keyword字段在term查询时更好。<br>通常，存储诸如ISBN的标识符或标识来自另一数据库的记录的任何数字的字段很少用于范围查询或聚合。这就是为什么他们可能会受益于被映射为keyword而不是integer或long。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-disk-usage.html#tune-for-disk-usage\" target=\"_blank\" rel=\"noopener\">Tune for disk usage</a></li>\n<li><a href=\"https://toutiao.io/posts/lkqldx/preview\" target=\"_blank\" rel=\"noopener\">让 Elasticsearch 飞起来：性能优化实践干货</a></li>\n</ol>\n"},{"title":"Elasticsearch使用备注","date":"2016-10-24T13:17:55.000Z","_content":"# Elasticsearch使用备注\n## 简介\nbeats + elasticsearch +logstash + kibana 这套工具集合出自于Elastic公司   https://www.elastic.co/guide/index.html\n工具集功能\n- beats是结合elasticsearch,logstash,kibana进行数据分析展示工具集\n- beats主动获取数据，如：日志数据，文件数据，top数据，网络包数据，数据库数据等\n- logstash（可选）用于日志分析，然后将分析后的数据存储到elasticsearch中\n- elasticsearch用于分析、存储beats获取的数据\n - kibana用于展示图形elasticsearch上的数据，如：线图，饼图，表格等\n \n ## 备注\n1. Elasticsearch rest api\n- 查询模板\n```\nhttp://localhost:9200/_template\n```\n- 查询索引\n```\nhttp://localhost:9200/_cat/indices\n```\n- 查重指定索引数据\n```\nhttp://localhost:9200/packetbeat-*/_search?pretty\n```\n2. Kibana地址\n```\nhttp://localhost:5601/\n```\n3. Plugin Head集群可视化管理工具\n需要额外安装\n访问地址如下：\n```\nhttp://localhost:9200/_plugin/head/\n```\n\n## index template管理\n1.删除模板\n```\ncurl -XDELETE 'http://localhost:9200/_template/packetbeat'\n```\n2.上传模板\n```\ncurl -XPUT 'http://localhost:9200/_template/packetbeat' -d@/etc/packetbeat/packetbeat.template.json\n```\n3.删除documents\n```\ncurl -XDELETE 'http://localhost:9200/packetbeat-*'\n```\n","source":"_posts/Elasticsearch使用备注.md","raw":"---\ntitle: Elasticsearch使用备注\ndate: 2016-10-24 21:17:55\ntags: research\ncategories:\n- elasticsearch\n---\n# Elasticsearch使用备注\n## 简介\nbeats + elasticsearch +logstash + kibana 这套工具集合出自于Elastic公司   https://www.elastic.co/guide/index.html\n工具集功能\n- beats是结合elasticsearch,logstash,kibana进行数据分析展示工具集\n- beats主动获取数据，如：日志数据，文件数据，top数据，网络包数据，数据库数据等\n- logstash（可选）用于日志分析，然后将分析后的数据存储到elasticsearch中\n- elasticsearch用于分析、存储beats获取的数据\n - kibana用于展示图形elasticsearch上的数据，如：线图，饼图，表格等\n \n ## 备注\n1. Elasticsearch rest api\n- 查询模板\n```\nhttp://localhost:9200/_template\n```\n- 查询索引\n```\nhttp://localhost:9200/_cat/indices\n```\n- 查重指定索引数据\n```\nhttp://localhost:9200/packetbeat-*/_search?pretty\n```\n2. Kibana地址\n```\nhttp://localhost:5601/\n```\n3. Plugin Head集群可视化管理工具\n需要额外安装\n访问地址如下：\n```\nhttp://localhost:9200/_plugin/head/\n```\n\n## index template管理\n1.删除模板\n```\ncurl -XDELETE 'http://localhost:9200/_template/packetbeat'\n```\n2.上传模板\n```\ncurl -XPUT 'http://localhost:9200/_template/packetbeat' -d@/etc/packetbeat/packetbeat.template.json\n```\n3.删除documents\n```\ncurl -XDELETE 'http://localhost:9200/packetbeat-*'\n```\n","slug":"Elasticsearch使用备注","published":1,"updated":"2016-10-24T13:19:20.031Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4s000a8ceeqej0o8nc","content":"<h1 id=\"Elasticsearch使用备注\"><a href=\"#Elasticsearch使用备注\" class=\"headerlink\" title=\"Elasticsearch使用备注\"></a>Elasticsearch使用备注</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>beats + elasticsearch +logstash + kibana 这套工具集合出自于Elastic公司   <a href=\"https://www.elastic.co/guide/index.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/index.html</a><br>工具集功能</p>\n<ul>\n<li>beats是结合elasticsearch,logstash,kibana进行数据分析展示工具集</li>\n<li>beats主动获取数据，如：日志数据，文件数据，top数据，网络包数据，数据库数据等</li>\n<li>logstash（可选）用于日志分析，然后将分析后的数据存储到elasticsearch中</li>\n<li><p>elasticsearch用于分析、存储beats获取的数据</p>\n<ul>\n<li>kibana用于展示图形elasticsearch上的数据，如：线图，饼图，表格等</li>\n</ul>\n<h2 id=\"备注\"><a href=\"#备注\" class=\"headerlink\" title=\"备注\"></a>备注</h2></li>\n</ul>\n<ol>\n<li>Elasticsearch rest api</li>\n</ol>\n<ul>\n<li><p>查询模板</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_template</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查询索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_cat/indices</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查重指定索引数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/packetbeat-*/_search?pretty</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol start=\"2\">\n<li><p>Kibana地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:5601/</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Plugin Head集群可视化管理工具<br>需要额外安装<br>访问地址如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_plugin/head/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"index-template管理\"><a href=\"#index-template管理\" class=\"headerlink\" title=\"index template管理\"></a>index template管理</h2><p>1.删除模板<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://localhost:9200/_template/packetbeat&apos;</span><br></pre></td></tr></table></figure></p>\n<p>2.上传模板<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPUT &apos;http://localhost:9200/_template/packetbeat&apos; -d@/etc/packetbeat/packetbeat.template.json</span><br></pre></td></tr></table></figure></p>\n<p>3.删除documents<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://localhost:9200/packetbeat-*&apos;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Elasticsearch使用备注\"><a href=\"#Elasticsearch使用备注\" class=\"headerlink\" title=\"Elasticsearch使用备注\"></a>Elasticsearch使用备注</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>beats + elasticsearch +logstash + kibana 这套工具集合出自于Elastic公司   <a href=\"https://www.elastic.co/guide/index.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/index.html</a><br>工具集功能</p>\n<ul>\n<li>beats是结合elasticsearch,logstash,kibana进行数据分析展示工具集</li>\n<li>beats主动获取数据，如：日志数据，文件数据，top数据，网络包数据，数据库数据等</li>\n<li>logstash（可选）用于日志分析，然后将分析后的数据存储到elasticsearch中</li>\n<li><p>elasticsearch用于分析、存储beats获取的数据</p>\n<ul>\n<li>kibana用于展示图形elasticsearch上的数据，如：线图，饼图，表格等</li>\n</ul>\n<h2 id=\"备注\"><a href=\"#备注\" class=\"headerlink\" title=\"备注\"></a>备注</h2></li>\n</ul>\n<ol>\n<li>Elasticsearch rest api</li>\n</ol>\n<ul>\n<li><p>查询模板</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_template</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查询索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_cat/indices</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查重指定索引数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/packetbeat-*/_search?pretty</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ol start=\"2\">\n<li><p>Kibana地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:5601/</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Plugin Head集群可视化管理工具<br>需要额外安装<br>访问地址如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://localhost:9200/_plugin/head/</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"index-template管理\"><a href=\"#index-template管理\" class=\"headerlink\" title=\"index template管理\"></a>index template管理</h2><p>1.删除模板<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://localhost:9200/_template/packetbeat&apos;</span><br></pre></td></tr></table></figure></p>\n<p>2.上传模板<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XPUT &apos;http://localhost:9200/_template/packetbeat&apos; -d@/etc/packetbeat/packetbeat.template.json</span><br></pre></td></tr></table></figure></p>\n<p>3.删除documents<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -XDELETE &apos;http://localhost:9200/packetbeat-*&apos;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Elasticsearch store探索","date":"2019-03-02T14:24:31.000Z","_content":"\n## store是什么？\n\n回答store是什么之前，先说一下正常使用es,我们的字段默认store:false,但是我们还是可以正常查出该数据的，那store:true有什么用呢？\n\n默认情况下，字段值(index:true)以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。\n\n通常这没关系。字段值已经是_source字段的一部分，默认情况下存储该字段。如果您只想检索单个字段或几个字段的值，而不是整个_source，则可以使用[source filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)来实现。\n\n\n\n现在我来说一下store是什么？字面意思是是否存储该数据，如果为true，则会**单独存储该数据。如果_source 没有排除 exclude 掉这个字段，那么应该是会存储多份的。**\n\n如果你要求返回field1（store：true），es会分辨出field1已经被存储了，因此不会从_source中加载，而是从field1的存储块中加载\n\n## 意义\n当数据有title, date,a very large content，而只需要取回title, date，不用从很大的_source字段中抽取。这种场景下性能更高。\n\n## 怎么使用？\n```\nPUT my_index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"title\": {\n          \"type\": \"text\",\n          \"store\": true \n        },\n        \"date\": {\n          \"type\": \"date\",\n          \"store\": true \n        },\n        \"content\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\nPUT my_index/_doc/1\n{\n  \"title\":   \"Some short title\",\n  \"date\":    \"2015-01-01\",\n  \"content\": \"A very long content field...\"\n}\n\nGET my_index/_search\n{\n  \"stored_fields\": [ \"title\", \"date\" ] \n}\n\n```\n\n\n## 相关原理\n\n\n当你将一个field的store属性设置为true，这个会在lucene层面处理。lucene是倒排索引，可以执行快速的全文检索，返回符合检索条 件的文档id列表。在全文索引之外，lucene也提供了存储字段的值的特性，以支持提供id的查询（根据id得到原始信息）。通常我们在lucene层 面存储的field的值是跟随search请求一起返回的（id+field的值）。es并不需要存储你想返回的每一个field的值，因为默认情况下每 一个文档的的完整信息都已经存储了，因此可以跟随查询结构返回你想要的所有field值。 \n\n有一些情况下，显式的存储某些field的值是必须的：当_source被disabled的时候，或者你并不想从source中parser来得到 field的值（即使这个过程是自动的）。\n\n请记住：**从每一个stored field中获取值都需要一次磁盘io，如果想获取多个field的值，就需要多次磁盘io，但是，如果从_source中获取多个field的值，则只 需要一次磁盘io，因为_source只是一个字段而已**。所以在大多数情况下，从_source中获取是快速而高效的。 \n\n\nes中默认的设置_source是enable的，存储整个文档的值。这意味着在执行search操作的时候可以返回整个文档的信息。如果不想返回这个文 档的完整信息，也可以指定要求返回的field，es会自动从_source中抽取出指定field的值返回（比如说highlighting的需求）。\n\n## 注意事项\n哪些情形下需要显式的指定store属性呢？大多数情况并不是必须的。从_source中获取值是快速而且高效的。如果你的文档长度很长，存储 _source或者从_source中获取field的代价很大，\n\n你可以显式的将某些field的store属性设置为true。缺点如上边所说：假设你存 储了10个field，而如果想获取这10个field的值，则需要多次的io，如果从_source中获取则只需要一次，而且_source是被压缩过 的\n\n## 引用\n1. [mapping-store](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-store.html#mapping-store)\n2. [elasticsearch的store属性 vs _source字段](https://www.cnblogs.com/Hai--D/p/5761525.html)\n","source":"_posts/Elasticsearch-store探索.md","raw":"---\ntitle: Elasticsearch store探索\ndate: 2019-03-02 22:24:31\ntags: research\ncategories:\n- elasticsearch\n---\n\n## store是什么？\n\n回答store是什么之前，先说一下正常使用es,我们的字段默认store:false,但是我们还是可以正常查出该数据的，那store:true有什么用呢？\n\n默认情况下，字段值(index:true)以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。\n\n通常这没关系。字段值已经是_source字段的一部分，默认情况下存储该字段。如果您只想检索单个字段或几个字段的值，而不是整个_source，则可以使用[source filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)来实现。\n\n\n\n现在我来说一下store是什么？字面意思是是否存储该数据，如果为true，则会**单独存储该数据。如果_source 没有排除 exclude 掉这个字段，那么应该是会存储多份的。**\n\n如果你要求返回field1（store：true），es会分辨出field1已经被存储了，因此不会从_source中加载，而是从field1的存储块中加载\n\n## 意义\n当数据有title, date,a very large content，而只需要取回title, date，不用从很大的_source字段中抽取。这种场景下性能更高。\n\n## 怎么使用？\n```\nPUT my_index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"title\": {\n          \"type\": \"text\",\n          \"store\": true \n        },\n        \"date\": {\n          \"type\": \"date\",\n          \"store\": true \n        },\n        \"content\": {\n          \"type\": \"text\"\n        }\n      }\n    }\n  }\n}\n\nPUT my_index/_doc/1\n{\n  \"title\":   \"Some short title\",\n  \"date\":    \"2015-01-01\",\n  \"content\": \"A very long content field...\"\n}\n\nGET my_index/_search\n{\n  \"stored_fields\": [ \"title\", \"date\" ] \n}\n\n```\n\n\n## 相关原理\n\n\n当你将一个field的store属性设置为true，这个会在lucene层面处理。lucene是倒排索引，可以执行快速的全文检索，返回符合检索条 件的文档id列表。在全文索引之外，lucene也提供了存储字段的值的特性，以支持提供id的查询（根据id得到原始信息）。通常我们在lucene层 面存储的field的值是跟随search请求一起返回的（id+field的值）。es并不需要存储你想返回的每一个field的值，因为默认情况下每 一个文档的的完整信息都已经存储了，因此可以跟随查询结构返回你想要的所有field值。 \n\n有一些情况下，显式的存储某些field的值是必须的：当_source被disabled的时候，或者你并不想从source中parser来得到 field的值（即使这个过程是自动的）。\n\n请记住：**从每一个stored field中获取值都需要一次磁盘io，如果想获取多个field的值，就需要多次磁盘io，但是，如果从_source中获取多个field的值，则只 需要一次磁盘io，因为_source只是一个字段而已**。所以在大多数情况下，从_source中获取是快速而高效的。 \n\n\nes中默认的设置_source是enable的，存储整个文档的值。这意味着在执行search操作的时候可以返回整个文档的信息。如果不想返回这个文 档的完整信息，也可以指定要求返回的field，es会自动从_source中抽取出指定field的值返回（比如说highlighting的需求）。\n\n## 注意事项\n哪些情形下需要显式的指定store属性呢？大多数情况并不是必须的。从_source中获取值是快速而且高效的。如果你的文档长度很长，存储 _source或者从_source中获取field的代价很大，\n\n你可以显式的将某些field的store属性设置为true。缺点如上边所说：假设你存 储了10个field，而如果想获取这10个field的值，则需要多次的io，如果从_source中获取则只需要一次，而且_source是被压缩过 的\n\n## 引用\n1. [mapping-store](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-store.html#mapping-store)\n2. [elasticsearch的store属性 vs _source字段](https://www.cnblogs.com/Hai--D/p/5761525.html)\n","slug":"Elasticsearch-store探索","published":1,"updated":"2019-03-02T14:25:06.053Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4u000b8cee0eiaze1m","content":"<h2 id=\"store是什么？\"><a href=\"#store是什么？\" class=\"headerlink\" title=\"store是什么？\"></a>store是什么？</h2><p>回答store是什么之前，先说一下正常使用es,我们的字段默认store:false,但是我们还是可以正常查出该数据的，那store:true有什么用呢？</p>\n<p>默认情况下，字段值(index:true)以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。</p>\n<p>通常这没关系。字段值已经是_source字段的一部分，默认情况下存储该字段。如果您只想检索单个字段或几个字段的值，而不是整个_source，则可以使用<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html\" target=\"_blank\" rel=\"noopener\">source filtering</a>来实现。</p>\n<p>现在我来说一下store是什么？字面意思是是否存储该数据，如果为true，则会<strong>单独存储该数据。如果_source 没有排除 exclude 掉这个字段，那么应该是会存储多份的。</strong></p>\n<p>如果你要求返回field1（store：true），es会分辨出field1已经被存储了，因此不会从_source中加载，而是从field1的存储块中加载</p>\n<h2 id=\"意义\"><a href=\"#意义\" class=\"headerlink\" title=\"意义\"></a>意义</h2><p>当数据有title, date,a very large content，而只需要取回title, date，不用从很大的_source字段中抽取。这种场景下性能更高。</p>\n<h2 id=\"怎么使用？\"><a href=\"#怎么使用？\" class=\"headerlink\" title=\"怎么使用？\"></a>怎么使用？</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT my_index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;title&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;store&quot;: true </span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;date&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">          &quot;store&quot;: true </span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;content&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">PUT my_index/_doc/1</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;title&quot;:   &quot;Some short title&quot;,</span><br><span class=\"line\">  &quot;date&quot;:    &quot;2015-01-01&quot;,</span><br><span class=\"line\">  &quot;content&quot;: &quot;A very long content field...&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GET my_index/_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;stored_fields&quot;: [ &quot;title&quot;, &quot;date&quot; ] </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"相关原理\"><a href=\"#相关原理\" class=\"headerlink\" title=\"相关原理\"></a>相关原理</h2><p>当你将一个field的store属性设置为true，这个会在lucene层面处理。lucene是倒排索引，可以执行快速的全文检索，返回符合检索条 件的文档id列表。在全文索引之外，lucene也提供了存储字段的值的特性，以支持提供id的查询（根据id得到原始信息）。通常我们在lucene层 面存储的field的值是跟随search请求一起返回的（id+field的值）。es并不需要存储你想返回的每一个field的值，因为默认情况下每 一个文档的的完整信息都已经存储了，因此可以跟随查询结构返回你想要的所有field值。 </p>\n<p>有一些情况下，显式的存储某些field的值是必须的：当_source被disabled的时候，或者你并不想从source中parser来得到 field的值（即使这个过程是自动的）。</p>\n<p>请记住：<strong>从每一个stored field中获取值都需要一次磁盘io，如果想获取多个field的值，就需要多次磁盘io，但是，如果从_source中获取多个field的值，则只 需要一次磁盘io，因为_source只是一个字段而已</strong>。所以在大多数情况下，从_source中获取是快速而高效的。 </p>\n<p>es中默认的设置_source是enable的，存储整个文档的值。这意味着在执行search操作的时候可以返回整个文档的信息。如果不想返回这个文 档的完整信息，也可以指定要求返回的field，es会自动从_source中抽取出指定field的值返回（比如说highlighting的需求）。</p>\n<h2 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h2><p>哪些情形下需要显式的指定store属性呢？大多数情况并不是必须的。从_source中获取值是快速而且高效的。如果你的文档长度很长，存储 _source或者从_source中获取field的代价很大，</p>\n<p>你可以显式的将某些field的store属性设置为true。缺点如上边所说：假设你存 储了10个field，而如果想获取这10个field的值，则需要多次的io，如果从_source中获取则只需要一次，而且_source是被压缩过 的</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-store.html#mapping-store\" target=\"_blank\" rel=\"noopener\">mapping-store</a></li>\n<li><a href=\"https://www.cnblogs.com/Hai--D/p/5761525.html\" target=\"_blank\" rel=\"noopener\">elasticsearch的store属性 vs _source字段</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"store是什么？\"><a href=\"#store是什么？\" class=\"headerlink\" title=\"store是什么？\"></a>store是什么？</h2><p>回答store是什么之前，先说一下正常使用es,我们的字段默认store:false,但是我们还是可以正常查出该数据的，那store:true有什么用呢？</p>\n<p>默认情况下，字段值(index:true)以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。</p>\n<p>通常这没关系。字段值已经是_source字段的一部分，默认情况下存储该字段。如果您只想检索单个字段或几个字段的值，而不是整个_source，则可以使用<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html\" target=\"_blank\" rel=\"noopener\">source filtering</a>来实现。</p>\n<p>现在我来说一下store是什么？字面意思是是否存储该数据，如果为true，则会<strong>单独存储该数据。如果_source 没有排除 exclude 掉这个字段，那么应该是会存储多份的。</strong></p>\n<p>如果你要求返回field1（store：true），es会分辨出field1已经被存储了，因此不会从_source中加载，而是从field1的存储块中加载</p>\n<h2 id=\"意义\"><a href=\"#意义\" class=\"headerlink\" title=\"意义\"></a>意义</h2><p>当数据有title, date,a very large content，而只需要取回title, date，不用从很大的_source字段中抽取。这种场景下性能更高。</p>\n<h2 id=\"怎么使用？\"><a href=\"#怎么使用？\" class=\"headerlink\" title=\"怎么使用？\"></a>怎么使用？</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT my_index</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;_doc&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;title&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;store&quot;: true </span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;date&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">          &quot;store&quot;: true </span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;content&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">PUT my_index/_doc/1</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;title&quot;:   &quot;Some short title&quot;,</span><br><span class=\"line\">  &quot;date&quot;:    &quot;2015-01-01&quot;,</span><br><span class=\"line\">  &quot;content&quot;: &quot;A very long content field...&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">GET my_index/_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;stored_fields&quot;: [ &quot;title&quot;, &quot;date&quot; ] </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"相关原理\"><a href=\"#相关原理\" class=\"headerlink\" title=\"相关原理\"></a>相关原理</h2><p>当你将一个field的store属性设置为true，这个会在lucene层面处理。lucene是倒排索引，可以执行快速的全文检索，返回符合检索条 件的文档id列表。在全文索引之外，lucene也提供了存储字段的值的特性，以支持提供id的查询（根据id得到原始信息）。通常我们在lucene层 面存储的field的值是跟随search请求一起返回的（id+field的值）。es并不需要存储你想返回的每一个field的值，因为默认情况下每 一个文档的的完整信息都已经存储了，因此可以跟随查询结构返回你想要的所有field值。 </p>\n<p>有一些情况下，显式的存储某些field的值是必须的：当_source被disabled的时候，或者你并不想从source中parser来得到 field的值（即使这个过程是自动的）。</p>\n<p>请记住：<strong>从每一个stored field中获取值都需要一次磁盘io，如果想获取多个field的值，就需要多次磁盘io，但是，如果从_source中获取多个field的值，则只 需要一次磁盘io，因为_source只是一个字段而已</strong>。所以在大多数情况下，从_source中获取是快速而高效的。 </p>\n<p>es中默认的设置_source是enable的，存储整个文档的值。这意味着在执行search操作的时候可以返回整个文档的信息。如果不想返回这个文 档的完整信息，也可以指定要求返回的field，es会自动从_source中抽取出指定field的值返回（比如说highlighting的需求）。</p>\n<h2 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h2><p>哪些情形下需要显式的指定store属性呢？大多数情况并不是必须的。从_source中获取值是快速而且高效的。如果你的文档长度很长，存储 _source或者从_source中获取field的代价很大，</p>\n<p>你可以显式的将某些field的store属性设置为true。缺点如上边所说：假设你存 储了10个field，而如果想获取这10个field的值，则需要多次的io，如果从_source中获取则只需要一次，而且_source是被压缩过 的</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-store.html#mapping-store\" target=\"_blank\" rel=\"noopener\">mapping-store</a></li>\n<li><a href=\"https://www.cnblogs.com/Hai--D/p/5761525.html\" target=\"_blank\" rel=\"noopener\">elasticsearch的store属性 vs _source字段</a></li>\n</ol>\n"},{"title":"Elasticsearch集群原理探索","date":"2018-07-22T10:40:23.000Z","_content":"# Elasticsearch集群原理探索\n\n## 1. Elasticsearch Master 选举过程\n\n## 2. Elasticsearch 文档检索过程\n\n\n## 3. Elasticsearch 分片分配\n\n### 3.1 集群级别\n在es集群，可以通过以下设置，控制分片分配进程。\n\n- **Cluster Level Shard Allocation** \n\n集群级别分片分配 \n\n在集群初始化(重启恢复)，副本分配，再平衡，或者节点加入、离开触发集群分配分片\n\n- **Disk-based Shard Allocation** \n\n依据磁盘分片分配\n\n磁盘因素（默认值85%）达到最低值，将阻止最新分片到该机器上，或者直接移除分片。\n\n达到85%，阻止，达到90，将移除分片到其他机器 ，达到95%对该node上的index 强制只读，如果磁盘够用，撤销只读index block。\n\n主要设置参数如下：\n```\nPUT _cluster/settings\n{\n  \"transient\": {\n    \"cluster.routing.allocation.disk.watermark.low\": \"100gb\",\n    \"cluster.routing.allocation.disk.watermark.high\": \"50gb\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage\": \"10gb\",\n    \"cluster.info.update.interval\": \"1m\"\n  }\n}\n```\n- **Shard Allocation Awareness and Forced Awarenessedit**\n\n分片分配意识\n\n分片分配感知设置允许您告知Elasticsearch您的硬件配置，主要是用来解决避免物理机划分多个虚拟带来不利影响。详细配置见[官网文档](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/allocation-awareness.html)\n\n- **Shard Allocation Filtering**\n\n主要存在以下参数\n\n```\ncluster.routing.allocation.include.{attribute}\n```\n分配分片到包含{attribute}node中\n```\ncluster.routing.allocation.require.{attribute}\n```\n分配分片到必须包含所有{attribute}node中\n```\ncluster.routing.allocation.exclude.{attribute}\n```\n分配分片到不包含{attribute}node中\n\nattribute支持如下属性：\n\n\n属性 | 注释\n---|---\n_name | 匹配节点名称\n_ip| 匹配节点IP\n_host|匹配hostnames\n\n\n\n分片分配过滤\n\n支持用include/exclude过滤器来控制分片的分配。过滤器可以设置在索引级别或者是集群级别\n\n下线一个节点(10.0.0.1)可以按如下操作：\n```\nPUT _cluster/settings\n{\n  \"transient\" : {\n    \"cluster.routing.allocation.exclude._ip\" : \"10.0.0.1\"\n  }\n}\n```\n\n除了逗号作为分隔列出多个值之外，所有属性值都可以用通配符指定\n\n```\nPUT _cluster/settings\n{\n  \"transient\": {\n    \"cluster.routing.allocation.exclude._ip\": \"192.168.2.*\"\n  }\n}\n```\n\n### 3.2 index级别\n\n- **分片分配过滤** 控制哪些分片落在哪些节点\n\n详见集群\n- **延迟分配** 延迟因为一个节点离开分配未分配分片\n\n1. 修改默认设置，默认值是1m\n\n```\nPUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"5m\"\n  }\n}\n\n```\n2. 监控延迟未分配分片\n\n```\nGET _cluster/health\n```\n3. 永久移除一个节点\n\n如果一个节点要永久性移除，需要立即分配分片，仅仅需要将timeout 设置为0\n\n```\nPUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"0\"\n  }\n}\n```\n\n- **单个节点分片总数**  对来自每个节点的相同索引的碎片数量进行硬性限制\n\n以下动态设置允许您为每个节点允许的单个索引指定分片总数的硬性限制\n```\nindex.routing.allocation.total_shards_per_node\n```\n\n\n同样可以设置节点限制，不用关心index情况\n\n```\ncluster.routing.allocation.total_shards_per_node\n```\n\n以上大小都是包括主本与副本数量，两个默认值是无穷大\n\n\n\n## 引用\n1. [Elasticsearch分布式一致性原理剖析(一)-节点篇](https://zhuanlan.zhihu.com/p/34830403)\n2. [elastic](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/modules-cluster.html)","source":"_posts/Elasticsearch集群原理探索.md","raw":"---\ntitle: Elasticsearch集群原理探索\ndate: 2018-07-22 18:40:23\ntags: research\ncategories:\n- elasticsearch\n---\n# Elasticsearch集群原理探索\n\n## 1. Elasticsearch Master 选举过程\n\n## 2. Elasticsearch 文档检索过程\n\n\n## 3. Elasticsearch 分片分配\n\n### 3.1 集群级别\n在es集群，可以通过以下设置，控制分片分配进程。\n\n- **Cluster Level Shard Allocation** \n\n集群级别分片分配 \n\n在集群初始化(重启恢复)，副本分配，再平衡，或者节点加入、离开触发集群分配分片\n\n- **Disk-based Shard Allocation** \n\n依据磁盘分片分配\n\n磁盘因素（默认值85%）达到最低值，将阻止最新分片到该机器上，或者直接移除分片。\n\n达到85%，阻止，达到90，将移除分片到其他机器 ，达到95%对该node上的index 强制只读，如果磁盘够用，撤销只读index block。\n\n主要设置参数如下：\n```\nPUT _cluster/settings\n{\n  \"transient\": {\n    \"cluster.routing.allocation.disk.watermark.low\": \"100gb\",\n    \"cluster.routing.allocation.disk.watermark.high\": \"50gb\",\n    \"cluster.routing.allocation.disk.watermark.flood_stage\": \"10gb\",\n    \"cluster.info.update.interval\": \"1m\"\n  }\n}\n```\n- **Shard Allocation Awareness and Forced Awarenessedit**\n\n分片分配意识\n\n分片分配感知设置允许您告知Elasticsearch您的硬件配置，主要是用来解决避免物理机划分多个虚拟带来不利影响。详细配置见[官网文档](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/allocation-awareness.html)\n\n- **Shard Allocation Filtering**\n\n主要存在以下参数\n\n```\ncluster.routing.allocation.include.{attribute}\n```\n分配分片到包含{attribute}node中\n```\ncluster.routing.allocation.require.{attribute}\n```\n分配分片到必须包含所有{attribute}node中\n```\ncluster.routing.allocation.exclude.{attribute}\n```\n分配分片到不包含{attribute}node中\n\nattribute支持如下属性：\n\n\n属性 | 注释\n---|---\n_name | 匹配节点名称\n_ip| 匹配节点IP\n_host|匹配hostnames\n\n\n\n分片分配过滤\n\n支持用include/exclude过滤器来控制分片的分配。过滤器可以设置在索引级别或者是集群级别\n\n下线一个节点(10.0.0.1)可以按如下操作：\n```\nPUT _cluster/settings\n{\n  \"transient\" : {\n    \"cluster.routing.allocation.exclude._ip\" : \"10.0.0.1\"\n  }\n}\n```\n\n除了逗号作为分隔列出多个值之外，所有属性值都可以用通配符指定\n\n```\nPUT _cluster/settings\n{\n  \"transient\": {\n    \"cluster.routing.allocation.exclude._ip\": \"192.168.2.*\"\n  }\n}\n```\n\n### 3.2 index级别\n\n- **分片分配过滤** 控制哪些分片落在哪些节点\n\n详见集群\n- **延迟分配** 延迟因为一个节点离开分配未分配分片\n\n1. 修改默认设置，默认值是1m\n\n```\nPUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"5m\"\n  }\n}\n\n```\n2. 监控延迟未分配分片\n\n```\nGET _cluster/health\n```\n3. 永久移除一个节点\n\n如果一个节点要永久性移除，需要立即分配分片，仅仅需要将timeout 设置为0\n\n```\nPUT _all/_settings\n{\n  \"settings\": {\n    \"index.unassigned.node_left.delayed_timeout\": \"0\"\n  }\n}\n```\n\n- **单个节点分片总数**  对来自每个节点的相同索引的碎片数量进行硬性限制\n\n以下动态设置允许您为每个节点允许的单个索引指定分片总数的硬性限制\n```\nindex.routing.allocation.total_shards_per_node\n```\n\n\n同样可以设置节点限制，不用关心index情况\n\n```\ncluster.routing.allocation.total_shards_per_node\n```\n\n以上大小都是包括主本与副本数量，两个默认值是无穷大\n\n\n\n## 引用\n1. [Elasticsearch分布式一致性原理剖析(一)-节点篇](https://zhuanlan.zhihu.com/p/34830403)\n2. [elastic](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/modules-cluster.html)","slug":"Elasticsearch集群原理探索","published":1,"updated":"2018-07-22T10:42:18.589Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4x000g8cee3ztn5p5y","content":"<h1 id=\"Elasticsearch集群原理探索\"><a href=\"#Elasticsearch集群原理探索\" class=\"headerlink\" title=\"Elasticsearch集群原理探索\"></a>Elasticsearch集群原理探索</h1><h2 id=\"1-Elasticsearch-Master-选举过程\"><a href=\"#1-Elasticsearch-Master-选举过程\" class=\"headerlink\" title=\"1. Elasticsearch Master 选举过程\"></a>1. Elasticsearch Master 选举过程</h2><h2 id=\"2-Elasticsearch-文档检索过程\"><a href=\"#2-Elasticsearch-文档检索过程\" class=\"headerlink\" title=\"2. Elasticsearch 文档检索过程\"></a>2. Elasticsearch 文档检索过程</h2><h2 id=\"3-Elasticsearch-分片分配\"><a href=\"#3-Elasticsearch-分片分配\" class=\"headerlink\" title=\"3. Elasticsearch 分片分配\"></a>3. Elasticsearch 分片分配</h2><h3 id=\"3-1-集群级别\"><a href=\"#3-1-集群级别\" class=\"headerlink\" title=\"3.1 集群级别\"></a>3.1 集群级别</h3><p>在es集群，可以通过以下设置，控制分片分配进程。</p>\n<ul>\n<li><strong>Cluster Level Shard Allocation</strong> </li>\n</ul>\n<p>集群级别分片分配 </p>\n<p>在集群初始化(重启恢复)，副本分配，再平衡，或者节点加入、离开触发集群分配分片</p>\n<ul>\n<li><strong>Disk-based Shard Allocation</strong> </li>\n</ul>\n<p>依据磁盘分片分配</p>\n<p>磁盘因素（默认值85%）达到最低值，将阻止最新分片到该机器上，或者直接移除分片。</p>\n<p>达到85%，阻止，达到90，将移除分片到其他机器 ，达到95%对该node上的index 强制只读，如果磁盘够用，撤销只读index block。</p>\n<p>主要设置参数如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot;: &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.low&quot;: &quot;100gb&quot;,</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.high&quot;: &quot;50gb&quot;,</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.flood_stage&quot;: &quot;10gb&quot;,</span><br><span class=\"line\">    &quot;cluster.info.update.interval&quot;: &quot;1m&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><strong>Shard Allocation Awareness and Forced Awarenessedit</strong></li>\n</ul>\n<p>分片分配意识</p>\n<p>分片分配感知设置允许您告知Elasticsearch您的硬件配置，主要是用来解决避免物理机划分多个虚拟带来不利影响。详细配置见<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.2/allocation-awareness.html\" target=\"_blank\" rel=\"noopener\">官网文档</a></p>\n<ul>\n<li><strong>Shard Allocation Filtering</strong></li>\n</ul>\n<p>主要存在以下参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.include.&#123;attribute&#125;</span><br></pre></td></tr></table></figure>\n<p>分配分片到包含{attribute}node中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.require.&#123;attribute&#125;</span><br></pre></td></tr></table></figure></p>\n<p>分配分片到必须包含所有{attribute}node中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.exclude.&#123;attribute&#125;</span><br></pre></td></tr></table></figure></p>\n<p>分配分片到不包含{attribute}node中</p>\n<p>attribute支持如下属性：</p>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>注释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>_name</td>\n<td>匹配节点名称</td>\n</tr>\n<tr>\n<td>_ip</td>\n<td>匹配节点IP</td>\n</tr>\n<tr>\n<td>_host</td>\n<td>匹配hostnames</td>\n</tr>\n</tbody>\n</table>\n<p>分片分配过滤</p>\n<p>支持用include/exclude过滤器来控制分片的分配。过滤器可以设置在索引级别或者是集群级别</p>\n<p>下线一个节点(10.0.0.1)可以按如下操作：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot; : &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.exclude._ip&quot; : &quot;10.0.0.1&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>除了逗号作为分隔列出多个值之外，所有属性值都可以用通配符指定</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot;: &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.exclude._ip&quot;: &quot;192.168.2.*&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-index级别\"><a href=\"#3-2-index级别\" class=\"headerlink\" title=\"3.2 index级别\"></a>3.2 index级别</h3><ul>\n<li><strong>分片分配过滤</strong> 控制哪些分片落在哪些节点</li>\n</ul>\n<p>详见集群</p>\n<ul>\n<li><strong>延迟分配</strong> 延迟因为一个节点离开分配未分配分片</li>\n</ul>\n<ol>\n<li>修改默认设置，默认值是1m</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _all/_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>监控延迟未分配分片</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET _cluster/health</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>永久移除一个节点</li>\n</ol>\n<p>如果一个节点要永久性移除，需要立即分配分片，仅仅需要将timeout 设置为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _all/_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;0&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>单个节点分片总数</strong>  对来自每个节点的相同索引的碎片数量进行硬性限制</li>\n</ul>\n<p>以下动态设置允许您为每个节点允许的单个索引指定分片总数的硬性限制<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index.routing.allocation.total_shards_per_node</span><br></pre></td></tr></table></figure></p>\n<p>同样可以设置节点限制，不用关心index情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.total_shards_per_node</span><br></pre></td></tr></table></figure>\n<p>以上大小都是包括主本与副本数量，两个默认值是无穷大</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/34830403\" target=\"_blank\" rel=\"noopener\">Elasticsearch分布式一致性原理剖析(一)-节点篇</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.2/modules-cluster.html\" target=\"_blank\" rel=\"noopener\">elastic</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Elasticsearch集群原理探索\"><a href=\"#Elasticsearch集群原理探索\" class=\"headerlink\" title=\"Elasticsearch集群原理探索\"></a>Elasticsearch集群原理探索</h1><h2 id=\"1-Elasticsearch-Master-选举过程\"><a href=\"#1-Elasticsearch-Master-选举过程\" class=\"headerlink\" title=\"1. Elasticsearch Master 选举过程\"></a>1. Elasticsearch Master 选举过程</h2><h2 id=\"2-Elasticsearch-文档检索过程\"><a href=\"#2-Elasticsearch-文档检索过程\" class=\"headerlink\" title=\"2. Elasticsearch 文档检索过程\"></a>2. Elasticsearch 文档检索过程</h2><h2 id=\"3-Elasticsearch-分片分配\"><a href=\"#3-Elasticsearch-分片分配\" class=\"headerlink\" title=\"3. Elasticsearch 分片分配\"></a>3. Elasticsearch 分片分配</h2><h3 id=\"3-1-集群级别\"><a href=\"#3-1-集群级别\" class=\"headerlink\" title=\"3.1 集群级别\"></a>3.1 集群级别</h3><p>在es集群，可以通过以下设置，控制分片分配进程。</p>\n<ul>\n<li><strong>Cluster Level Shard Allocation</strong> </li>\n</ul>\n<p>集群级别分片分配 </p>\n<p>在集群初始化(重启恢复)，副本分配，再平衡，或者节点加入、离开触发集群分配分片</p>\n<ul>\n<li><strong>Disk-based Shard Allocation</strong> </li>\n</ul>\n<p>依据磁盘分片分配</p>\n<p>磁盘因素（默认值85%）达到最低值，将阻止最新分片到该机器上，或者直接移除分片。</p>\n<p>达到85%，阻止，达到90，将移除分片到其他机器 ，达到95%对该node上的index 强制只读，如果磁盘够用，撤销只读index block。</p>\n<p>主要设置参数如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot;: &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.low&quot;: &quot;100gb&quot;,</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.high&quot;: &quot;50gb&quot;,</span><br><span class=\"line\">    &quot;cluster.routing.allocation.disk.watermark.flood_stage&quot;: &quot;10gb&quot;,</span><br><span class=\"line\">    &quot;cluster.info.update.interval&quot;: &quot;1m&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><strong>Shard Allocation Awareness and Forced Awarenessedit</strong></li>\n</ul>\n<p>分片分配意识</p>\n<p>分片分配感知设置允许您告知Elasticsearch您的硬件配置，主要是用来解决避免物理机划分多个虚拟带来不利影响。详细配置见<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.2/allocation-awareness.html\" target=\"_blank\" rel=\"noopener\">官网文档</a></p>\n<ul>\n<li><strong>Shard Allocation Filtering</strong></li>\n</ul>\n<p>主要存在以下参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.include.&#123;attribute&#125;</span><br></pre></td></tr></table></figure>\n<p>分配分片到包含{attribute}node中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.require.&#123;attribute&#125;</span><br></pre></td></tr></table></figure></p>\n<p>分配分片到必须包含所有{attribute}node中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.exclude.&#123;attribute&#125;</span><br></pre></td></tr></table></figure></p>\n<p>分配分片到不包含{attribute}node中</p>\n<p>attribute支持如下属性：</p>\n<table>\n<thead>\n<tr>\n<th>属性</th>\n<th>注释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>_name</td>\n<td>匹配节点名称</td>\n</tr>\n<tr>\n<td>_ip</td>\n<td>匹配节点IP</td>\n</tr>\n<tr>\n<td>_host</td>\n<td>匹配hostnames</td>\n</tr>\n</tbody>\n</table>\n<p>分片分配过滤</p>\n<p>支持用include/exclude过滤器来控制分片的分配。过滤器可以设置在索引级别或者是集群级别</p>\n<p>下线一个节点(10.0.0.1)可以按如下操作：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot; : &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.exclude._ip&quot; : &quot;10.0.0.1&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>除了逗号作为分隔列出多个值之外，所有属性值都可以用通配符指定</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _cluster/settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;transient&quot;: &#123;</span><br><span class=\"line\">    &quot;cluster.routing.allocation.exclude._ip&quot;: &quot;192.168.2.*&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-index级别\"><a href=\"#3-2-index级别\" class=\"headerlink\" title=\"3.2 index级别\"></a>3.2 index级别</h3><ul>\n<li><strong>分片分配过滤</strong> 控制哪些分片落在哪些节点</li>\n</ul>\n<p>详见集群</p>\n<ul>\n<li><strong>延迟分配</strong> 延迟因为一个节点离开分配未分配分片</li>\n</ul>\n<ol>\n<li>修改默认设置，默认值是1m</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _all/_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>监控延迟未分配分片</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET _cluster/health</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>永久移除一个节点</li>\n</ol>\n<p>如果一个节点要永久性移除，需要立即分配分片，仅仅需要将timeout 设置为0</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT _all/_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;0&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>单个节点分片总数</strong>  对来自每个节点的相同索引的碎片数量进行硬性限制</li>\n</ul>\n<p>以下动态设置允许您为每个节点允许的单个索引指定分片总数的硬性限制<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index.routing.allocation.total_shards_per_node</span><br></pre></td></tr></table></figure></p>\n<p>同样可以设置节点限制，不用关心index情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster.routing.allocation.total_shards_per_node</span><br></pre></td></tr></table></figure>\n<p>以上大小都是包括主本与副本数量，两个默认值是无穷大</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/34830403\" target=\"_blank\" rel=\"noopener\">Elasticsearch分布式一致性原理剖析(一)-节点篇</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.2/modules-cluster.html\" target=\"_blank\" rel=\"noopener\">elastic</a></li>\n</ol>\n"},{"title":"Git命令总结","date":"2018-01-15T11:14:36.000Z","_content":"# Git Commant Summary\n\n1.git checkout\n- 撤销\n\n使用-- filte 可以撤销文件修改到上一次commit或者add状态\n```\ngit checkout -- readme.md\n```\n- 切换分支\n\n直接跟分支名称，可以切换到指定分支上\n```\ngit checkout master\n```\n\n2.git rebase\n\n将指定分支合并到当前分支\n```\ngit rebase origin\n```\n\n3.git remote\n\n添加远程仓库\n```\ngit add origin https://github.com/TrumanDu/nav-dashboad.git\n```\n\n4.git fetch\n\n```\ngit fetch origin\n```\n丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支\n\n5.git reset\n\n```\ngit reset --hard origin/master\n```\n丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支","source":"_posts/Git命令总结.md","raw":"---\ntitle: Git命令总结\ndate: 2018-01-15 19:14:36\ntags: 命令\ncategories:\n- git\n---\n# Git Commant Summary\n\n1.git checkout\n- 撤销\n\n使用-- filte 可以撤销文件修改到上一次commit或者add状态\n```\ngit checkout -- readme.md\n```\n- 切换分支\n\n直接跟分支名称，可以切换到指定分支上\n```\ngit checkout master\n```\n\n2.git rebase\n\n将指定分支合并到当前分支\n```\ngit rebase origin\n```\n\n3.git remote\n\n添加远程仓库\n```\ngit add origin https://github.com/TrumanDu/nav-dashboad.git\n```\n\n4.git fetch\n\n```\ngit fetch origin\n```\n丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支\n\n5.git reset\n\n```\ngit reset --hard origin/master\n```\n丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支","slug":"Git命令总结","published":1,"updated":"2018-01-15T11:36:35.326Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv4z000i8cee8ttfb5ic","content":"<h1 id=\"Git-Commant-Summary\"><a href=\"#Git-Commant-Summary\" class=\"headerlink\" title=\"Git Commant Summary\"></a>Git Commant Summary</h1><p>1.git checkout</p>\n<ul>\n<li>撤销</li>\n</ul>\n<p>使用– filte 可以撤销文件修改到上一次commit或者add状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -- readme.md</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>切换分支</li>\n</ul>\n<p>直接跟分支名称，可以切换到指定分支上<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout master</span><br></pre></td></tr></table></figure></p>\n<p>2.git rebase</p>\n<p>将指定分支合并到当前分支<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase origin</span><br></pre></td></tr></table></figure></p>\n<p>3.git remote</p>\n<p>添加远程仓库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add origin https://github.com/TrumanDu/nav-dashboad.git</span><br></pre></td></tr></table></figure></p>\n<p>4.git fetch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin</span><br></pre></td></tr></table></figure>\n<p>丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支</p>\n<p>5.git reset</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard origin/master</span><br></pre></td></tr></table></figure>\n<p>丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Git-Commant-Summary\"><a href=\"#Git-Commant-Summary\" class=\"headerlink\" title=\"Git Commant Summary\"></a>Git Commant Summary</h1><p>1.git checkout</p>\n<ul>\n<li>撤销</li>\n</ul>\n<p>使用– filte 可以撤销文件修改到上一次commit或者add状态<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -- readme.md</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>切换分支</li>\n</ul>\n<p>直接跟分支名称，可以切换到指定分支上<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout master</span><br></pre></td></tr></table></figure></p>\n<p>2.git rebase</p>\n<p>将指定分支合并到当前分支<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase origin</span><br></pre></td></tr></table></figure></p>\n<p>3.git remote</p>\n<p>添加远程仓库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add origin https://github.com/TrumanDu/nav-dashboad.git</span><br></pre></td></tr></table></figure></p>\n<p>4.git fetch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch origin</span><br></pre></td></tr></table></figure>\n<p>丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支</p>\n<p>5.git reset</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard origin/master</span><br></pre></td></tr></table></figure>\n<p>丢弃本地改动与提交，从指定地方获取最新版代码到本地主分支</p>\n"},{"title":"Git remote 同步远程仓库","date":"2018-01-15T11:18:45.000Z","_content":"# Git remote 同步远程仓库\n在正常开发过程中，我们会fork别人的项目，然后在本地开发，随着时间推移，fork源代码会更新，但是就出现了问题，怎么和源项目保持更新？\n\n## Git更新fork项目代码\n\n1.添加远程仓库\n\n添加：\n```\ngit remote add source_respository_name https://github.com/TrumanDu/nav-dashboad.git\n```\n查看项目所有remote仓库名称：\n```\n$ git remote  \n   origin  \n   source_respository_name \n```\n\n2.同步源仓库信息到本地\n\n```\n$ git remote update source_repository_name \n```\n\n3.将源仓库信息merge到本地分支\n\n```\n$ git checkout branch_name  \n$ git rebase source_repository_name/branch_name \n```\n\n## GitHub开发协同流程\n\nGitHub常用的开发协同流程为：\n\n将别人的仓库fork成自己的origin仓库 → git clone origin仓库到本地 → 本地添加fork源仓库 → 工作前先git remote update下fork源保持代码较新 → coding → push回自己 → github上提出Push Request即可","source":"_posts/Git-remote-同步远程仓库.md","raw":"---\ntitle: Git remote 同步远程仓库\ndate: 2018-01-15 19:18:45\ntags: 开发笔记\ncategories:\n- git\n---\n# Git remote 同步远程仓库\n在正常开发过程中，我们会fork别人的项目，然后在本地开发，随着时间推移，fork源代码会更新，但是就出现了问题，怎么和源项目保持更新？\n\n## Git更新fork项目代码\n\n1.添加远程仓库\n\n添加：\n```\ngit remote add source_respository_name https://github.com/TrumanDu/nav-dashboad.git\n```\n查看项目所有remote仓库名称：\n```\n$ git remote  \n   origin  \n   source_respository_name \n```\n\n2.同步源仓库信息到本地\n\n```\n$ git remote update source_repository_name \n```\n\n3.将源仓库信息merge到本地分支\n\n```\n$ git checkout branch_name  \n$ git rebase source_repository_name/branch_name \n```\n\n## GitHub开发协同流程\n\nGitHub常用的开发协同流程为：\n\n将别人的仓库fork成自己的origin仓库 → git clone origin仓库到本地 → 本地添加fork源仓库 → 工作前先git remote update下fork源保持代码较新 → coding → push回自己 → github上提出Push Request即可","slug":"Git-remote-同步远程仓库","published":1,"updated":"2018-01-15T11:39:07.253Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv53000n8ceepk9bffm8","content":"<h1 id=\"Git-remote-同步远程仓库\"><a href=\"#Git-remote-同步远程仓库\" class=\"headerlink\" title=\"Git remote 同步远程仓库\"></a>Git remote 同步远程仓库</h1><p>在正常开发过程中，我们会fork别人的项目，然后在本地开发，随着时间推移，fork源代码会更新，但是就出现了问题，怎么和源项目保持更新？</p>\n<h2 id=\"Git更新fork项目代码\"><a href=\"#Git更新fork项目代码\" class=\"headerlink\" title=\"Git更新fork项目代码\"></a>Git更新fork项目代码</h2><p>1.添加远程仓库</p>\n<p>添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add source_respository_name https://github.com/TrumanDu/nav-dashboad.git</span><br></pre></td></tr></table></figure></p>\n<p>查看项目所有remote仓库名称：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote  </span><br><span class=\"line\">   origin  </span><br><span class=\"line\">   source_respository_name</span><br></pre></td></tr></table></figure></p>\n<p>2.同步源仓库信息到本地</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote update source_repository_name</span><br></pre></td></tr></table></figure>\n<p>3.将源仓库信息merge到本地分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout branch_name  </span><br><span class=\"line\">$ git rebase source_repository_name/branch_name</span><br></pre></td></tr></table></figure>\n<h2 id=\"GitHub开发协同流程\"><a href=\"#GitHub开发协同流程\" class=\"headerlink\" title=\"GitHub开发协同流程\"></a>GitHub开发协同流程</h2><p>GitHub常用的开发协同流程为：</p>\n<p>将别人的仓库fork成自己的origin仓库 → git clone origin仓库到本地 → 本地添加fork源仓库 → 工作前先git remote update下fork源保持代码较新 → coding → push回自己 → github上提出Push Request即可</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Git-remote-同步远程仓库\"><a href=\"#Git-remote-同步远程仓库\" class=\"headerlink\" title=\"Git remote 同步远程仓库\"></a>Git remote 同步远程仓库</h1><p>在正常开发过程中，我们会fork别人的项目，然后在本地开发，随着时间推移，fork源代码会更新，但是就出现了问题，怎么和源项目保持更新？</p>\n<h2 id=\"Git更新fork项目代码\"><a href=\"#Git更新fork项目代码\" class=\"headerlink\" title=\"Git更新fork项目代码\"></a>Git更新fork项目代码</h2><p>1.添加远程仓库</p>\n<p>添加：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add source_respository_name https://github.com/TrumanDu/nav-dashboad.git</span><br></pre></td></tr></table></figure></p>\n<p>查看项目所有remote仓库名称：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote  </span><br><span class=\"line\">   origin  </span><br><span class=\"line\">   source_respository_name</span><br></pre></td></tr></table></figure></p>\n<p>2.同步源仓库信息到本地</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git remote update source_repository_name</span><br></pre></td></tr></table></figure>\n<p>3.将源仓库信息merge到本地分支</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git checkout branch_name  </span><br><span class=\"line\">$ git rebase source_repository_name/branch_name</span><br></pre></td></tr></table></figure>\n<h2 id=\"GitHub开发协同流程\"><a href=\"#GitHub开发协同流程\" class=\"headerlink\" title=\"GitHub开发协同流程\"></a>GitHub开发协同流程</h2><p>GitHub常用的开发协同流程为：</p>\n<p>将别人的仓库fork成自己的origin仓库 → git clone origin仓库到本地 → 本地添加fork源仓库 → 工作前先git remote update下fork源保持代码较新 → coding → push回自己 → github上提出Push Request即可</p>\n"},{"title":"HBase常用的操作二(java API)","date":"2016-06-14T12:31:20.000Z","_content":"本文接前篇[**HBase常用的操作一(shell)**](http://trumandu.github.io/2016/06/14/HBase%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E4%B8%80-shell/)系列，继续探讨总结操作hbase的方法，重点在于java语言对hbase的操作。\n此次选用最新api。\n\n<!-- more -->\n#### 前言\n在进行已下操作之前，需要首先实例化 Configuration，Admin，Connection。具体代码如下\n```\nprivate static Configuration conf = null;\nprivate static Admin admin = null;\nprivate static Connection connection = null;\nstatic {\n\tconf = HBaseConfiguration.create();\n\tconf.set(\"hbase.zookeeper.quorum\", \"*********\");\n\tconf.set(\"hbase.zookeeper.property.clientPort\", \"2181\");\n\ttry {\n\t\tconnection = ConnectionFactory.createConnection(conf);\n\t\tadmin = connection.getAdmin();\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}\n\n}\n```\n#### 1. DDL操作\n##### 1.1 创建表\n```\n/**\n * 创建表\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static void createTable(String tableName, String[] columnsName) throws IOException {\n\tHTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));\n\t// Adding column families to table descriptor\n\tif (columnsName != null) {\n\t\tfor (String name : columnsName) {\n\t\t\ttableDescriptor.addFamily(new HColumnDescriptor(name));\n\t\t}\n\t}\n\tadmin.createTable(tableDescriptor);\n\tSystem.out.println(\" Table created \");\n}\n```\n##### 1.2 删除表\n```\n/**\n * 删除表\n * @param tableName\n * @throws IOException\n */\npublic static void deleteTable(String tableName) throws IOException {\n\tadmin.disableTable(TableName.valueOf(tableName));\n\tadmin.deleteTable(TableName.valueOf(tableName));\n\n}\n```\n##### 1.3 删除 columns\n```\n/**\n * 删除 columns\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static void deleteColumn(String tableName, String[] columnsName) throws IOException {\n\tif (columnsName != null) {\n\t\tfor (String name : columnsName) {\n\t\t\tadmin.deleteColumn(TableName.valueOf(tableName), name.getBytes());\n\t\t}\n\t}\n}\n```\n##### 1.4 获取所有的表\n```\n/**\n * 获取所有的表\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static HTableDescriptor[] listTable() throws IOException {\n\tHTableDescriptor[] tables = admin.listTables();\n\treturn tables;\n}\n```\n##### 1.5 查看表是否存在\n```\n/**\n * 查看表是否存在\n * @param tableName\n * @return\n * @throws IOException\n */\npublic static boolean tableIsExists(String tableName) throws IOException {\n\treturn admin.tableExists(TableName.valueOf(tableName));\n}\n```\n\n#### 2. DML操作\n##### 2.1 读取数据\n```\n/**\n * 读取数据\n * @param tableName\n * @param row\n * @param family\n * @param column\n * @return\n */\npublic static String getValue(String tableName, byte[] row, byte[] family, byte[] column) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tGet g = new Get(row);\n\t\tResult reasult;\n\t\treasult = table.get(g);\n\t\tbyte[] value = reasult.getValue(family, column);\n\t\tif (value != null) {\n\t\t\treturn new String(value);\n\t\t}\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}finally {\n\t\tif (table != null)\n\t\t\ttry {\n\t\t\t\ttable.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t}\n\n\treturn null;\n}\n```\n##### 2.2 删除数据\n```\n/**\n * 删除数据\n * @param tableName\n * @param row\n * @param family\n * @param column\n */\npublic static void remove(String tableName, byte[] row, byte[] family, byte[] column) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tDelete delete = new Delete(row);\n\t\tif (column != null) {\n\t\t\tdelete.addColumn(family, column);\n\t\t}\n\t\tif (family != null) {\n\t\t\tdelete.addFamily(family);\n\t\t}\n\t\ttable.delete(delete);\n\t\ttable.close();\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}\n}\n```\n##### 2.3 增加数据\n```\n/**\n * 增加数据\n * @param tableName\n * @return\n * @throws IOException\n */\npublic static void addData(String tableName, byte[] row, byte[] family, byte[] column, byte[] data) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tPut p = new Put(row);\n\t\tp.addColumn(family, column, data);\n\t\ttable.put(p);\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t} finally {\n\t\tif (table != null)\n\t\t\ttry {\n\t\t\t\ttable.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t}\n\n}\n```\n#### 3. 其他操作\n##### 3.1 查询集群状态\n```\n/**\n * 获取hbase集群状态\n * @return\n * @throws IOException\n */\npublic static ClusterStatus getClusterStatus() throws IOException {\n\tClusterStatus clusterStatus = admin.getClusterStatus();\n\treturn clusterStatus;\n}\n```","source":"_posts/HBase常用的操作二-java-API.md","raw":"---\ntitle: HBase常用的操作二(java API)\ndate: 2016-06-14 20:31:20\ntags: 大数据\ncategories:\n- hbase\n---\n本文接前篇[**HBase常用的操作一(shell)**](http://trumandu.github.io/2016/06/14/HBase%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E4%B8%80-shell/)系列，继续探讨总结操作hbase的方法，重点在于java语言对hbase的操作。\n此次选用最新api。\n\n<!-- more -->\n#### 前言\n在进行已下操作之前，需要首先实例化 Configuration，Admin，Connection。具体代码如下\n```\nprivate static Configuration conf = null;\nprivate static Admin admin = null;\nprivate static Connection connection = null;\nstatic {\n\tconf = HBaseConfiguration.create();\n\tconf.set(\"hbase.zookeeper.quorum\", \"*********\");\n\tconf.set(\"hbase.zookeeper.property.clientPort\", \"2181\");\n\ttry {\n\t\tconnection = ConnectionFactory.createConnection(conf);\n\t\tadmin = connection.getAdmin();\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}\n\n}\n```\n#### 1. DDL操作\n##### 1.1 创建表\n```\n/**\n * 创建表\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static void createTable(String tableName, String[] columnsName) throws IOException {\n\tHTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));\n\t// Adding column families to table descriptor\n\tif (columnsName != null) {\n\t\tfor (String name : columnsName) {\n\t\t\ttableDescriptor.addFamily(new HColumnDescriptor(name));\n\t\t}\n\t}\n\tadmin.createTable(tableDescriptor);\n\tSystem.out.println(\" Table created \");\n}\n```\n##### 1.2 删除表\n```\n/**\n * 删除表\n * @param tableName\n * @throws IOException\n */\npublic static void deleteTable(String tableName) throws IOException {\n\tadmin.disableTable(TableName.valueOf(tableName));\n\tadmin.deleteTable(TableName.valueOf(tableName));\n\n}\n```\n##### 1.3 删除 columns\n```\n/**\n * 删除 columns\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static void deleteColumn(String tableName, String[] columnsName) throws IOException {\n\tif (columnsName != null) {\n\t\tfor (String name : columnsName) {\n\t\t\tadmin.deleteColumn(TableName.valueOf(tableName), name.getBytes());\n\t\t}\n\t}\n}\n```\n##### 1.4 获取所有的表\n```\n/**\n * 获取所有的表\n * @param tableName\n * @param columnsName\n * @throws IOException\n */\npublic static HTableDescriptor[] listTable() throws IOException {\n\tHTableDescriptor[] tables = admin.listTables();\n\treturn tables;\n}\n```\n##### 1.5 查看表是否存在\n```\n/**\n * 查看表是否存在\n * @param tableName\n * @return\n * @throws IOException\n */\npublic static boolean tableIsExists(String tableName) throws IOException {\n\treturn admin.tableExists(TableName.valueOf(tableName));\n}\n```\n\n#### 2. DML操作\n##### 2.1 读取数据\n```\n/**\n * 读取数据\n * @param tableName\n * @param row\n * @param family\n * @param column\n * @return\n */\npublic static String getValue(String tableName, byte[] row, byte[] family, byte[] column) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tGet g = new Get(row);\n\t\tResult reasult;\n\t\treasult = table.get(g);\n\t\tbyte[] value = reasult.getValue(family, column);\n\t\tif (value != null) {\n\t\t\treturn new String(value);\n\t\t}\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}finally {\n\t\tif (table != null)\n\t\t\ttry {\n\t\t\t\ttable.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t}\n\n\treturn null;\n}\n```\n##### 2.2 删除数据\n```\n/**\n * 删除数据\n * @param tableName\n * @param row\n * @param family\n * @param column\n */\npublic static void remove(String tableName, byte[] row, byte[] family, byte[] column) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tDelete delete = new Delete(row);\n\t\tif (column != null) {\n\t\t\tdelete.addColumn(family, column);\n\t\t}\n\t\tif (family != null) {\n\t\t\tdelete.addFamily(family);\n\t\t}\n\t\ttable.delete(delete);\n\t\ttable.close();\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t}\n}\n```\n##### 2.3 增加数据\n```\n/**\n * 增加数据\n * @param tableName\n * @return\n * @throws IOException\n */\npublic static void addData(String tableName, byte[] row, byte[] family, byte[] column, byte[] data) {\n\tTable table = null;\n\ttry {\n\t\ttable = connection.getTable(TableName.valueOf(tableName));\n\t\tPut p = new Put(row);\n\t\tp.addColumn(family, column, data);\n\t\ttable.put(p);\n\t} catch (IOException e) {\n\t\t// TODO Auto-generated catch block\n\t\te.printStackTrace();\n\t} finally {\n\t\tif (table != null)\n\t\t\ttry {\n\t\t\t\ttable.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t}\n\n}\n```\n#### 3. 其他操作\n##### 3.1 查询集群状态\n```\n/**\n * 获取hbase集群状态\n * @return\n * @throws IOException\n */\npublic static ClusterStatus getClusterStatus() throws IOException {\n\tClusterStatus clusterStatus = admin.getClusterStatus();\n\treturn clusterStatus;\n}\n```","slug":"HBase常用的操作二-java-API","published":1,"updated":"2016-06-14T12:32:29.603Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv55000q8ceem89l20c2","content":"<p>本文接前篇<a href=\"http://trumandu.github.io/2016/06/14/HBase%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E4%B8%80-shell/\"><strong>HBase常用的操作一(shell)</strong></a>系列，继续探讨总结操作hbase的方法，重点在于java语言对hbase的操作。<br>此次选用最新api。</p>\n<a id=\"more\"></a>\n<h4 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h4><p>在进行已下操作之前，需要首先实例化 Configuration，Admin，Connection。具体代码如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static Configuration conf = null;</span><br><span class=\"line\">private static Admin admin = null;</span><br><span class=\"line\">private static Connection connection = null;</span><br><span class=\"line\">static &#123;</span><br><span class=\"line\">\tconf = HBaseConfiguration.create();</span><br><span class=\"line\">\tconf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;*********&quot;);</span><br><span class=\"line\">\tconf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tconnection = ConnectionFactory.createConnection(conf);</span><br><span class=\"line\">\t\tadmin = connection.getAdmin();</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-DDL操作\"><a href=\"#1-DDL操作\" class=\"headerlink\" title=\"1. DDL操作\"></a>1. DDL操作</h4><h5 id=\"1-1-创建表\"><a href=\"#1-1-创建表\" class=\"headerlink\" title=\"1.1 创建表\"></a>1.1 创建表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 创建表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void createTable(String tableName, String[] columnsName) throws IOException &#123;</span><br><span class=\"line\">\tHTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));</span><br><span class=\"line\">\t// Adding column families to table descriptor</span><br><span class=\"line\">\tif (columnsName != null) &#123;</span><br><span class=\"line\">\t\tfor (String name : columnsName) &#123;</span><br><span class=\"line\">\t\t\ttableDescriptor.addFamily(new HColumnDescriptor(name));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tadmin.createTable(tableDescriptor);</span><br><span class=\"line\">\tSystem.out.println(&quot; Table created &quot;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-2-删除表\"><a href=\"#1-2-删除表\" class=\"headerlink\" title=\"1.2 删除表\"></a>1.2 删除表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void deleteTable(String tableName) throws IOException &#123;</span><br><span class=\"line\">\tadmin.disableTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\tadmin.deleteTable(TableName.valueOf(tableName));</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-3-删除-columns\"><a href=\"#1-3-删除-columns\" class=\"headerlink\" title=\"1.3 删除 columns\"></a>1.3 删除 columns</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除 columns</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void deleteColumn(String tableName, String[] columnsName) throws IOException &#123;</span><br><span class=\"line\">\tif (columnsName != null) &#123;</span><br><span class=\"line\">\t\tfor (String name : columnsName) &#123;</span><br><span class=\"line\">\t\t\tadmin.deleteColumn(TableName.valueOf(tableName), name.getBytes());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-4-获取所有的表\"><a href=\"#1-4-获取所有的表\" class=\"headerlink\" title=\"1.4 获取所有的表\"></a>1.4 获取所有的表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 获取所有的表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static HTableDescriptor[] listTable() throws IOException &#123;</span><br><span class=\"line\">\tHTableDescriptor[] tables = admin.listTables();</span><br><span class=\"line\">\treturn tables;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-5-查看表是否存在\"><a href=\"#1-5-查看表是否存在\" class=\"headerlink\" title=\"1.5 查看表是否存在\"></a>1.5 查看表是否存在</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 查看表是否存在</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static boolean tableIsExists(String tableName) throws IOException &#123;</span><br><span class=\"line\">\treturn admin.tableExists(TableName.valueOf(tableName));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-DML操作\"><a href=\"#2-DML操作\" class=\"headerlink\" title=\"2. DML操作\"></a>2. DML操作</h4><h5 id=\"2-1-读取数据\"><a href=\"#2-1-读取数据\" class=\"headerlink\" title=\"2.1 读取数据\"></a>2.1 读取数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 读取数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param row</span><br><span class=\"line\"> * @param family</span><br><span class=\"line\"> * @param column</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static String getValue(String tableName, byte[] row, byte[] family, byte[] column) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tGet g = new Get(row);</span><br><span class=\"line\">\t\tResult reasult;</span><br><span class=\"line\">\t\treasult = table.get(g);</span><br><span class=\"line\">\t\tbyte[] value = reasult.getValue(family, column);</span><br><span class=\"line\">\t\tif (value != null) &#123;</span><br><span class=\"line\">\t\t\treturn new String(value);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;finally &#123;</span><br><span class=\"line\">\t\tif (table != null)</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\ttable.close();</span><br><span class=\"line\">\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn null;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-删除数据\"><a href=\"#2-2-删除数据\" class=\"headerlink\" title=\"2.2 删除数据\"></a>2.2 删除数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param row</span><br><span class=\"line\"> * @param family</span><br><span class=\"line\"> * @param column</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void remove(String tableName, byte[] row, byte[] family, byte[] column) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tDelete delete = new Delete(row);</span><br><span class=\"line\">\t\tif (column != null) &#123;</span><br><span class=\"line\">\t\t\tdelete.addColumn(family, column);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (family != null) &#123;</span><br><span class=\"line\">\t\t\tdelete.addFamily(family);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\ttable.delete(delete);</span><br><span class=\"line\">\t\ttable.close();</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-3-增加数据\"><a href=\"#2-3-增加数据\" class=\"headerlink\" title=\"2.3 增加数据\"></a>2.3 增加数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 增加数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void addData(String tableName, byte[] row, byte[] family, byte[] column, byte[] data) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tPut p = new Put(row);</span><br><span class=\"line\">\t\tp.addColumn(family, column, data);</span><br><span class=\"line\">\t\ttable.put(p);</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; finally &#123;</span><br><span class=\"line\">\t\tif (table != null)</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\ttable.close();</span><br><span class=\"line\">\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-其他操作\"><a href=\"#3-其他操作\" class=\"headerlink\" title=\"3. 其他操作\"></a>3. 其他操作</h4><h5 id=\"3-1-查询集群状态\"><a href=\"#3-1-查询集群状态\" class=\"headerlink\" title=\"3.1 查询集群状态\"></a>3.1 查询集群状态</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 获取hbase集群状态</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static ClusterStatus getClusterStatus() throws IOException &#123;</span><br><span class=\"line\">\tClusterStatus clusterStatus = admin.getClusterStatus();</span><br><span class=\"line\">\treturn clusterStatus;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>本文接前篇<a href=\"http://trumandu.github.io/2016/06/14/HBase%E5%B8%B8%E7%94%A8%E7%9A%84%E6%93%8D%E4%BD%9C%E4%B8%80-shell/\"><strong>HBase常用的操作一(shell)</strong></a>系列，继续探讨总结操作hbase的方法，重点在于java语言对hbase的操作。<br>此次选用最新api。</p>","more":"<h4 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h4><p>在进行已下操作之前，需要首先实例化 Configuration，Admin，Connection。具体代码如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static Configuration conf = null;</span><br><span class=\"line\">private static Admin admin = null;</span><br><span class=\"line\">private static Connection connection = null;</span><br><span class=\"line\">static &#123;</span><br><span class=\"line\">\tconf = HBaseConfiguration.create();</span><br><span class=\"line\">\tconf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;*********&quot;);</span><br><span class=\"line\">\tconf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tconnection = ConnectionFactory.createConnection(conf);</span><br><span class=\"line\">\t\tadmin = connection.getAdmin();</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-DDL操作\"><a href=\"#1-DDL操作\" class=\"headerlink\" title=\"1. DDL操作\"></a>1. DDL操作</h4><h5 id=\"1-1-创建表\"><a href=\"#1-1-创建表\" class=\"headerlink\" title=\"1.1 创建表\"></a>1.1 创建表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 创建表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void createTable(String tableName, String[] columnsName) throws IOException &#123;</span><br><span class=\"line\">\tHTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));</span><br><span class=\"line\">\t// Adding column families to table descriptor</span><br><span class=\"line\">\tif (columnsName != null) &#123;</span><br><span class=\"line\">\t\tfor (String name : columnsName) &#123;</span><br><span class=\"line\">\t\t\ttableDescriptor.addFamily(new HColumnDescriptor(name));</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tadmin.createTable(tableDescriptor);</span><br><span class=\"line\">\tSystem.out.println(&quot; Table created &quot;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-2-删除表\"><a href=\"#1-2-删除表\" class=\"headerlink\" title=\"1.2 删除表\"></a>1.2 删除表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void deleteTable(String tableName) throws IOException &#123;</span><br><span class=\"line\">\tadmin.disableTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\tadmin.deleteTable(TableName.valueOf(tableName));</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-3-删除-columns\"><a href=\"#1-3-删除-columns\" class=\"headerlink\" title=\"1.3 删除 columns\"></a>1.3 删除 columns</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除 columns</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void deleteColumn(String tableName, String[] columnsName) throws IOException &#123;</span><br><span class=\"line\">\tif (columnsName != null) &#123;</span><br><span class=\"line\">\t\tfor (String name : columnsName) &#123;</span><br><span class=\"line\">\t\t\tadmin.deleteColumn(TableName.valueOf(tableName), name.getBytes());</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-4-获取所有的表\"><a href=\"#1-4-获取所有的表\" class=\"headerlink\" title=\"1.4 获取所有的表\"></a>1.4 获取所有的表</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 获取所有的表</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param columnsName</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static HTableDescriptor[] listTable() throws IOException &#123;</span><br><span class=\"line\">\tHTableDescriptor[] tables = admin.listTables();</span><br><span class=\"line\">\treturn tables;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"1-5-查看表是否存在\"><a href=\"#1-5-查看表是否存在\" class=\"headerlink\" title=\"1.5 查看表是否存在\"></a>1.5 查看表是否存在</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 查看表是否存在</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static boolean tableIsExists(String tableName) throws IOException &#123;</span><br><span class=\"line\">\treturn admin.tableExists(TableName.valueOf(tableName));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-DML操作\"><a href=\"#2-DML操作\" class=\"headerlink\" title=\"2. DML操作\"></a>2. DML操作</h4><h5 id=\"2-1-读取数据\"><a href=\"#2-1-读取数据\" class=\"headerlink\" title=\"2.1 读取数据\"></a>2.1 读取数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 读取数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param row</span><br><span class=\"line\"> * @param family</span><br><span class=\"line\"> * @param column</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static String getValue(String tableName, byte[] row, byte[] family, byte[] column) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tGet g = new Get(row);</span><br><span class=\"line\">\t\tResult reasult;</span><br><span class=\"line\">\t\treasult = table.get(g);</span><br><span class=\"line\">\t\tbyte[] value = reasult.getValue(family, column);</span><br><span class=\"line\">\t\tif (value != null) &#123;</span><br><span class=\"line\">\t\t\treturn new String(value);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;finally &#123;</span><br><span class=\"line\">\t\tif (table != null)</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\ttable.close();</span><br><span class=\"line\">\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn null;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-2-删除数据\"><a href=\"#2-2-删除数据\" class=\"headerlink\" title=\"2.2 删除数据\"></a>2.2 删除数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 删除数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @param row</span><br><span class=\"line\"> * @param family</span><br><span class=\"line\"> * @param column</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void remove(String tableName, byte[] row, byte[] family, byte[] column) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tDelete delete = new Delete(row);</span><br><span class=\"line\">\t\tif (column != null) &#123;</span><br><span class=\"line\">\t\t\tdelete.addColumn(family, column);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (family != null) &#123;</span><br><span class=\"line\">\t\t\tdelete.addFamily(family);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\ttable.delete(delete);</span><br><span class=\"line\">\t\ttable.close();</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"2-3-增加数据\"><a href=\"#2-3-增加数据\" class=\"headerlink\" title=\"2.3 增加数据\"></a>2.3 增加数据</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 增加数据</span><br><span class=\"line\"> * @param tableName</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static void addData(String tableName, byte[] row, byte[] family, byte[] column, byte[] data) &#123;</span><br><span class=\"line\">\tTable table = null;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttable = connection.getTable(TableName.valueOf(tableName));</span><br><span class=\"line\">\t\tPut p = new Put(row);</span><br><span class=\"line\">\t\tp.addColumn(family, column, data);</span><br><span class=\"line\">\t\ttable.put(p);</span><br><span class=\"line\">\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; finally &#123;</span><br><span class=\"line\">\t\tif (table != null)</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\ttable.close();</span><br><span class=\"line\">\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-其他操作\"><a href=\"#3-其他操作\" class=\"headerlink\" title=\"3. 其他操作\"></a>3. 其他操作</h4><h5 id=\"3-1-查询集群状态\"><a href=\"#3-1-查询集群状态\" class=\"headerlink\" title=\"3.1 查询集群状态\"></a>3.1 查询集群状态</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\"> * 获取hbase集群状态</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> * @throws IOException</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static ClusterStatus getClusterStatus() throws IOException &#123;</span><br><span class=\"line\">\tClusterStatus clusterStatus = admin.getClusterStatus();</span><br><span class=\"line\">\treturn clusterStatus;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Hbase 命令教程","date":"2016-06-01T13:54:24.000Z","_content":"# Hbase学习\n## Hbase shell操作\n### 一、建表\n1.新建不带namespace表\n```\ncreate 'testTable','t1','t2'\n```\n2.新建带namespace表\n- 首先建一个namespace\n```\ncreate_namespace 'truman'\n```\n- 其次再新建表\n```\ncreate_namespace 'truman：test','t1','t2'\n```\n备注：t1,t2为column family\n```\ncreate'reason:user_test','bs',{NUMREGIONS=>2,SPLITALGO=>'HexStringSplit'}\n\n```\n### 二、查表\n- 列出所有表\n```\nlist\n```\n- 浏览某个表\n``` hbase\nscan 'testTable'\n```\n### 三、数据操作\n- 增加数据\n```\nput 'testTable','row1','t1:name','value1'\nput 'testTable','row2','t1:name','value2'\nput 'testTable','row3','t1:name','value3'\n```\n- 获取数据\n```\nget 'testTable','row1'\n```\n- 删除数据\n```\n\n```\n## 其他\n待完善\n## 参考\n1.https://hbase.apache.org/book.html\n","source":"_posts/Hbase-使用教程.md","raw":"---\ntitle: Hbase 命令教程\ndate: 2016-06-01 21:54:24\ntags: 大数据\ncategories:\n- hbase\n---\n# Hbase学习\n## Hbase shell操作\n### 一、建表\n1.新建不带namespace表\n```\ncreate 'testTable','t1','t2'\n```\n2.新建带namespace表\n- 首先建一个namespace\n```\ncreate_namespace 'truman'\n```\n- 其次再新建表\n```\ncreate_namespace 'truman：test','t1','t2'\n```\n备注：t1,t2为column family\n```\ncreate'reason:user_test','bs',{NUMREGIONS=>2,SPLITALGO=>'HexStringSplit'}\n\n```\n### 二、查表\n- 列出所有表\n```\nlist\n```\n- 浏览某个表\n``` hbase\nscan 'testTable'\n```\n### 三、数据操作\n- 增加数据\n```\nput 'testTable','row1','t1:name','value1'\nput 'testTable','row2','t1:name','value2'\nput 'testTable','row3','t1:name','value3'\n```\n- 获取数据\n```\nget 'testTable','row1'\n```\n- 删除数据\n```\n\n```\n## 其他\n待完善\n## 参考\n1.https://hbase.apache.org/book.html\n","slug":"Hbase-使用教程","published":1,"updated":"2016-06-01T13:55:28.024Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv58000v8ceelyhsla51","content":"<h1 id=\"Hbase学习\"><a href=\"#Hbase学习\" class=\"headerlink\" title=\"Hbase学习\"></a>Hbase学习</h1><h2 id=\"Hbase-shell操作\"><a href=\"#Hbase-shell操作\" class=\"headerlink\" title=\"Hbase shell操作\"></a>Hbase shell操作</h2><h3 id=\"一、建表\"><a href=\"#一、建表\" class=\"headerlink\" title=\"一、建表\"></a>一、建表</h3><p>1.新建不带namespace表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create &apos;testTable&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure></p>\n<p>2.新建带namespace表</p>\n<ul>\n<li><p>首先建一个namespace</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>其次再新建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman：test&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>备注：t1,t2为column family<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create&apos;reason:user_test&apos;,&apos;bs&apos;,&#123;NUMREGIONS=&gt;2,SPLITALGO=&gt;&apos;HexStringSplit&apos;&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"二、查表\"><a href=\"#二、查表\" class=\"headerlink\" title=\"二、查表\"></a>二、查表</h3><ul>\n<li><p>列出所有表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>浏览某个表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scan &apos;testTable&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"三、数据操作\"><a href=\"#三、数据操作\" class=\"headerlink\" title=\"三、数据操作\"></a>三、数据操作</h3><ul>\n<li><p>增加数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">put &apos;testTable&apos;,&apos;row1&apos;,&apos;t1:name&apos;,&apos;value1&apos;</span><br><span class=\"line\">put &apos;testTable&apos;,&apos;row2&apos;,&apos;t1:name&apos;,&apos;value2&apos;</span><br><span class=\"line\">put &apos;testTable&apos;,&apos;row3&apos;,&apos;t1:name&apos;,&apos;value3&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>获取数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get &apos;testTable&apos;,&apos;row1&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>待完善</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">https://hbase.apache.org/book.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Hbase学习\"><a href=\"#Hbase学习\" class=\"headerlink\" title=\"Hbase学习\"></a>Hbase学习</h1><h2 id=\"Hbase-shell操作\"><a href=\"#Hbase-shell操作\" class=\"headerlink\" title=\"Hbase shell操作\"></a>Hbase shell操作</h2><h3 id=\"一、建表\"><a href=\"#一、建表\" class=\"headerlink\" title=\"一、建表\"></a>一、建表</h3><p>1.新建不带namespace表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create &apos;testTable&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure></p>\n<p>2.新建带namespace表</p>\n<ul>\n<li><p>首先建一个namespace</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>其次再新建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman：test&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>备注：t1,t2为column family<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create&apos;reason:user_test&apos;,&apos;bs&apos;,&#123;NUMREGIONS=&gt;2,SPLITALGO=&gt;&apos;HexStringSplit&apos;&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"二、查表\"><a href=\"#二、查表\" class=\"headerlink\" title=\"二、查表\"></a>二、查表</h3><ul>\n<li><p>列出所有表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>浏览某个表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scan &apos;testTable&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"三、数据操作\"><a href=\"#三、数据操作\" class=\"headerlink\" title=\"三、数据操作\"></a>三、数据操作</h3><ul>\n<li><p>增加数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">put &apos;testTable&apos;,&apos;row1&apos;,&apos;t1:name&apos;,&apos;value1&apos;</span><br><span class=\"line\">put &apos;testTable&apos;,&apos;row2&apos;,&apos;t1:name&apos;,&apos;value2&apos;</span><br><span class=\"line\">put &apos;testTable&apos;,&apos;row3&apos;,&apos;t1:name&apos;,&apos;value3&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>获取数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get &apos;testTable&apos;,&apos;row1&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>待完善</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">https://hbase.apache.org/book.html</a></p>\n"},{"title":"Hbase工程问题汇总","date":"2016-04-06T14:11:35.000Z","_content":"1.Hbase web整合报以下错误\nSEVERE: Servlet.service() for servlet [jsp] in context with path [] threw exception [java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;] with root cause \njava.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext; \n**解决方案**\npom文件中排出冲突jar\n```xml\n  <!-- hbase -->\n <dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.0.0-cdh5.4.0</version>\n  <exclusions>\n   <exclusion>\n    <artifactId>jasper-compiler</artifactId>\n    <groupId>tomcat</groupId>\n   </exclusion>\n   <exclusion>\n    <artifactId>jasper-runtime</artifactId>\n    <groupId>tomcat</groupId>\n   </exclusion>\n  </exclusions>\n </dependency>\n```\n***\n2.Hbase jar引入，pom报错误Missing artifact jdk.tools:jdk.tools:jar:1.7\n** 解决方案**\neclipse.ini (before -vmargs!):\n<pre>\n<code>\n-vm\nC:/{your_path_to_jdk170}/jre/bin/server/jvm.dll\n</code>\n</pre>\n出现该错误的原因是eclipse bug ,eclipse打开使用的是jre 不是jdk下面的jre ,因此未找到tools.jar\n***\n3.eclipse java api 访问hbase 失败\n** 解决方案**\n   * zookeeper工作是否正常\n   * 防火墙是否关闭\n\n\n\n","source":"_posts/Hbase工程问题汇总.md","raw":"---\ntitle: Hbase工程问题汇总\ndate: 2016-04-06 22:11:35\ntags: 大数据\ncategories:\n- hbase\n---\n1.Hbase web整合报以下错误\nSEVERE: Servlet.service() for servlet [jsp] in context with path [] threw exception [java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;] with root cause \njava.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext; \n**解决方案**\npom文件中排出冲突jar\n```xml\n  <!-- hbase -->\n <dependency>\n  <groupId>org.apache.hbase</groupId>\n  <artifactId>hbase-client</artifactId>\n  <version>1.0.0-cdh5.4.0</version>\n  <exclusions>\n   <exclusion>\n    <artifactId>jasper-compiler</artifactId>\n    <groupId>tomcat</groupId>\n   </exclusion>\n   <exclusion>\n    <artifactId>jasper-runtime</artifactId>\n    <groupId>tomcat</groupId>\n   </exclusion>\n  </exclusions>\n </dependency>\n```\n***\n2.Hbase jar引入，pom报错误Missing artifact jdk.tools:jdk.tools:jar:1.7\n** 解决方案**\neclipse.ini (before -vmargs!):\n<pre>\n<code>\n-vm\nC:/{your_path_to_jdk170}/jre/bin/server/jvm.dll\n</code>\n</pre>\n出现该错误的原因是eclipse bug ,eclipse打开使用的是jre 不是jdk下面的jre ,因此未找到tools.jar\n***\n3.eclipse java api 访问hbase 失败\n** 解决方案**\n   * zookeeper工作是否正常\n   * 防火墙是否关闭\n\n\n\n","slug":"Hbase工程问题汇总","published":1,"updated":"2016-04-06T14:30:32.359Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv5a000y8ceedhi9e0in","content":"<p>1.Hbase web整合报以下错误<br>SEVERE: Servlet.service() for servlet [jsp] in context with path [] threw exception [java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;] with root cause<br>java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;<br><strong>解决方案</strong><br>pom文件中排出冲突jar<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\">&lt;!-- hbase --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.hbase<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>hbase-client<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.0.0-cdh5.4.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">exclusions</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>jasper-compiler<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>tomcat<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>jasper-runtime<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>tomcat<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;/<span class=\"name\">exclusions</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>2.Hbase jar引入，pom报错误Missing artifact jdk.tools:jdk.tools:jar:1.7<br><strong> 解决方案</strong><br>eclipse.ini (before -vmargs!):</p>\n<p><pre><br><code><br>-vm<br>C:/{your_path_to_jdk170}/jre/bin/server/jvm.dll<br></code><br></pre><br>出现该错误的原因是eclipse bug ,eclipse打开使用的是jre 不是jdk下面的jre ,因此未找到tools.jar</p>\n<hr>\n<p>3.eclipse java api 访问hbase 失败<br><strong> 解决方案</strong></p>\n<ul>\n<li>zookeeper工作是否正常</li>\n<li>防火墙是否关闭</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>1.Hbase web整合报以下错误<br>SEVERE: Servlet.service() for servlet [jsp] in context with path [] threw exception [java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;] with root cause<br>java.lang.AbstractMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;<br><strong>解决方案</strong><br>pom文件中排出冲突jar<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"comment\">&lt;!-- hbase --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">dependency</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>org.apache.hbase<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>hbase-client<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>1.0.0-cdh5.4.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;<span class=\"name\">exclusions</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>jasper-compiler<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>tomcat<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">artifactId</span>&gt;</span>jasper-runtime<span class=\"tag\">&lt;/<span class=\"name\">artifactId</span>&gt;</span></span><br><span class=\"line\">   <span class=\"tag\">&lt;<span class=\"name\">groupId</span>&gt;</span>tomcat<span class=\"tag\">&lt;/<span class=\"name\">groupId</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">exclusion</span>&gt;</span></span><br><span class=\"line\"> <span class=\"tag\">&lt;/<span class=\"name\">exclusions</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>2.Hbase jar引入，pom报错误Missing artifact jdk.tools:jdk.tools:jar:1.7<br><strong> 解决方案</strong><br>eclipse.ini (before -vmargs!):</p>\n<p><pre><br><code><br>-vm<br>C:/{your_path_to_jdk170}/jre/bin/server/jvm.dll<br></code><br></pre><br>出现该错误的原因是eclipse bug ,eclipse打开使用的是jre 不是jdk下面的jre ,因此未找到tools.jar</p>\n<hr>\n<p>3.eclipse java api 访问hbase 失败<br><strong> 解决方案</strong></p>\n<ul>\n<li>zookeeper工作是否正常</li>\n<li>防火墙是否关闭</li>\n</ul>\n"},{"title":"Hbase数据迁移到Hive中","date":"2016-03-26T13:00:28.000Z","_content":"# 背景\n\n&emsp;hadoop体系中许多数据是存放在hbase中，为了便捷实用其中的数据，可以将数据迁移到hive中，通过hive sql 可以迅速便捷的获取并操作相应的数据，同时还可以实用hive sql实现一些mapreduce 操作。具体hive优势详见[**官网**](https://hive.apache.org/)。\n# 目的\n\n将hbase已存在的表数据迁移到hive中\n# 操作步骤\n\n## 版本环境\n\n| soft       | version          |\n| ------------- |:-------------:|\n| hive    | hive-1.1.0-cdh5.4.0 |\n| hbase      | hbase-1.0.0-cdh5.4.0|\n## 准备\n\n首先要确保HIVE_HOME/lib 下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用HBASE_HOME/lib/目录下得jar包：\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到HIVE_HOME/lib 下\n## 操作及测试\n1. hbase现有数据表如下:\n<pre><code>\nhbase(main):002:0> scan 'truman'\nROW                        COLUMN+CELL\n row2                      column=personal:city, timestamp=1458635086205, value=beijing\n row2                      column=personal:name, timestamp=1458635086189, value=reason2\n row2                      column=professional:designation, timestamp=1458635086210, value=test\n1 row(s) in 0.5910 seconds</code></pre>\n2. 新建hive与hbase关联表hbase_table_1\n<pre><code>\n[root@127.0.0.1 hive-1.1.0-cdh5.4.0]$ bin/hive\nLogging initialized using configuration in jar:file:/data/bigdata/hive-1.1.0-cdh5.4.0/lib/hive-common-1.1.0-cdh5.4.0.jar!/hive-log4j.properties\nWARNING: Hive CLI is deprecated and migration to Beeline is recommended.\nhive> CREATE external TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,personal:name\") TBLPROPERTIES (\"hbase.table.name\" = \"truman\");\nOK\nTime taken: 0.857 seconds</code></pre>\n3. 即可在关联表中查询hbase中truman表的数据\n<pre><code>\nhive> show tables;\nOK\nhbase_table_1\nTime taken: 0.272 seconds, Fetched: 1 row(s)\nhive> select * from hbase_table_1;\nOK\nNULL    reason2\nTime taken: 0.625 seconds, Fetched: 1 row(s)</code></pre>\n\n\n# 总结\n&emsp;可以通过关联表实现hbase数据迁移到hive中，同理，hive中数据也可以通过该表迁移到hbase中。\n在新建关联表一定要注意，hbase表存在则使用external。如果不存在则可以不需要该关键字\n# 参考\n\nhttp://www.micmiu.com/bigdata/hive/hive-hbase-integration/","source":"_posts/Hbase数据迁移到Hive中.md","raw":"---\ntitle: Hbase数据迁移到Hive中\ndate: 2016-03-26 21:00:28\ntags: 大数据\ncategories:\n- hbase\n---\n# 背景\n\n&emsp;hadoop体系中许多数据是存放在hbase中，为了便捷实用其中的数据，可以将数据迁移到hive中，通过hive sql 可以迅速便捷的获取并操作相应的数据，同时还可以实用hive sql实现一些mapreduce 操作。具体hive优势详见[**官网**](https://hive.apache.org/)。\n# 目的\n\n将hbase已存在的表数据迁移到hive中\n# 操作步骤\n\n## 版本环境\n\n| soft       | version          |\n| ------------- |:-------------:|\n| hive    | hive-1.1.0-cdh5.4.0 |\n| hbase      | hbase-1.0.0-cdh5.4.0|\n## 准备\n\n首先要确保HIVE_HOME/lib 下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用HBASE_HOME/lib/目录下得jar包：\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到HIVE_HOME/lib 下\n## 操作及测试\n1. hbase现有数据表如下:\n<pre><code>\nhbase(main):002:0> scan 'truman'\nROW                        COLUMN+CELL\n row2                      column=personal:city, timestamp=1458635086205, value=beijing\n row2                      column=personal:name, timestamp=1458635086189, value=reason2\n row2                      column=professional:designation, timestamp=1458635086210, value=test\n1 row(s) in 0.5910 seconds</code></pre>\n2. 新建hive与hbase关联表hbase_table_1\n<pre><code>\n[root@127.0.0.1 hive-1.1.0-cdh5.4.0]$ bin/hive\nLogging initialized using configuration in jar:file:/data/bigdata/hive-1.1.0-cdh5.4.0/lib/hive-common-1.1.0-cdh5.4.0.jar!/hive-log4j.properties\nWARNING: Hive CLI is deprecated and migration to Beeline is recommended.\nhive> CREATE external TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,personal:name\") TBLPROPERTIES (\"hbase.table.name\" = \"truman\");\nOK\nTime taken: 0.857 seconds</code></pre>\n3. 即可在关联表中查询hbase中truman表的数据\n<pre><code>\nhive> show tables;\nOK\nhbase_table_1\nTime taken: 0.272 seconds, Fetched: 1 row(s)\nhive> select * from hbase_table_1;\nOK\nNULL    reason2\nTime taken: 0.625 seconds, Fetched: 1 row(s)</code></pre>\n\n\n# 总结\n&emsp;可以通过关联表实现hbase数据迁移到hive中，同理，hive中数据也可以通过该表迁移到hbase中。\n在新建关联表一定要注意，hbase表存在则使用external。如果不存在则可以不需要该关键字\n# 参考\n\nhttp://www.micmiu.com/bigdata/hive/hive-hbase-integration/","slug":"Hbase数据迁移到Hive中","published":1,"updated":"2016-03-26T13:23:02.596Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv5d00138ceeh1jnkgm2","content":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>&emsp;hadoop体系中许多数据是存放在hbase中，为了便捷实用其中的数据，可以将数据迁移到hive中，通过hive sql 可以迅速便捷的获取并操作相应的数据，同时还可以实用hive sql实现一些mapreduce 操作。具体hive优势详见<a href=\"https://hive.apache.org/\" target=\"_blank\" rel=\"noopener\"><strong>官网</strong></a>。</p>\n<h1 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h1><p>将hbase已存在的表数据迁移到hive中</p>\n<h1 id=\"操作步骤\"><a href=\"#操作步骤\" class=\"headerlink\" title=\"操作步骤\"></a>操作步骤</h1><h2 id=\"版本环境\"><a href=\"#版本环境\" class=\"headerlink\" title=\"版本环境\"></a>版本环境</h2><table>\n<thead>\n<tr>\n<th>soft</th>\n<th style=\"text-align:center\">version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>hive</td>\n<td style=\"text-align:center\">hive-1.1.0-cdh5.4.0</td>\n</tr>\n<tr>\n<td>hbase</td>\n<td style=\"text-align:center\">hbase-1.0.0-cdh5.4.0</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>首先要确保HIVE_HOME/lib 下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用HBASE_HOME/lib/目录下得jar包：</p>\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到HIVE_HOME/lib 下\n## 操作及测试\n1. hbase现有数据表如下:\n<pre><code>\nhbase(main):002:0> scan 'truman'\nROW                        COLUMN+CELL\n row2                      column=personal:city, timestamp=1458635086205, value=beijing\n row2                      column=personal:name, timestamp=1458635086189, value=reason2\n row2                      column=professional:designation, timestamp=1458635086210, value=test\n1 row(s) in 0.5910 seconds</code></pre>\n2. 新建hive与hbase关联表hbase_table_1\n<pre><code>\n[root@127.0.0.1 hive-1.1.0-cdh5.4.0]$ bin/hive\nLogging initialized using configuration in jar:file:/data/bigdata/hive-1.1.0-cdh5.4.0/lib/hive-common-1.1.0-cdh5.4.0.jar!/hive-log4j.properties\nWARNING: Hive CLI is deprecated and migration to Beeline is recommended.\nhive> CREATE external TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,personal:name\") TBLPROPERTIES (\"hbase.table.name\" = \"truman\");\nOK\nTime taken: 0.857 seconds</code></pre>\n3. 即可在关联表中查询hbase中truman表的数据\n<pre><code>\nhive> show tables;\nOK\nhbase_table_1\nTime taken: 0.272 seconds, Fetched: 1 row(s)\nhive> select * from hbase_table_1;\nOK\nNULL    reason2\nTime taken: 0.625 seconds, Fetched: 1 row(s)</code></pre>\n\n\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>&emsp;可以通过关联表实现hbase数据迁移到hive中，同理，hive中数据也可以通过该表迁移到hbase中。<br>在新建关联表一定要注意，hbase表存在则使用external。如果不存在则可以不需要该关键字</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"http://www.micmiu.com/bigdata/hive/hive-hbase-integration/\" target=\"_blank\" rel=\"noopener\">http://www.micmiu.com/bigdata/hive/hive-hbase-integration/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>&emsp;hadoop体系中许多数据是存放在hbase中，为了便捷实用其中的数据，可以将数据迁移到hive中，通过hive sql 可以迅速便捷的获取并操作相应的数据，同时还可以实用hive sql实现一些mapreduce 操作。具体hive优势详见<a href=\"https://hive.apache.org/\" target=\"_blank\" rel=\"noopener\"><strong>官网</strong></a>。</p>\n<h1 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h1><p>将hbase已存在的表数据迁移到hive中</p>\n<h1 id=\"操作步骤\"><a href=\"#操作步骤\" class=\"headerlink\" title=\"操作步骤\"></a>操作步骤</h1><h2 id=\"版本环境\"><a href=\"#版本环境\" class=\"headerlink\" title=\"版本环境\"></a>版本环境</h2><table>\n<thead>\n<tr>\n<th>soft</th>\n<th style=\"text-align:center\">version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>hive</td>\n<td style=\"text-align:center\">hive-1.1.0-cdh5.4.0</td>\n</tr>\n<tr>\n<td>hbase</td>\n<td style=\"text-align:center\">hbase-1.0.0-cdh5.4.0</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>首先要确保HIVE_HOME/lib 下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用HBASE_HOME/lib/目录下得jar包：</p>\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到HIVE_HOME/lib 下\n## 操作及测试\n1. hbase现有数据表如下:\n<pre><code>\nhbase(main):002:0> scan 'truman'\nROW                        COLUMN+CELL\n row2                      column=personal:city, timestamp=1458635086205, value=beijing\n row2                      column=personal:name, timestamp=1458635086189, value=reason2\n row2                      column=professional:designation, timestamp=1458635086210, value=test\n1 row(s) in 0.5910 seconds</code></pre>\n2. 新建hive与hbase关联表hbase_table_1\n<pre><code>\n[root@127.0.0.1 hive-1.1.0-cdh5.4.0]$ bin/hive\nLogging initialized using configuration in jar:file:/data/bigdata/hive-1.1.0-cdh5.4.0/lib/hive-common-1.1.0-cdh5.4.0.jar!/hive-log4j.properties\nWARNING: Hive CLI is deprecated and migration to Beeline is recommended.\nhive> CREATE external TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,personal:name\") TBLPROPERTIES (\"hbase.table.name\" = \"truman\");\nOK\nTime taken: 0.857 seconds</code></pre>\n3. 即可在关联表中查询hbase中truman表的数据\n<pre><code>\nhive> show tables;\nOK\nhbase_table_1\nTime taken: 0.272 seconds, Fetched: 1 row(s)\nhive> select * from hbase_table_1;\nOK\nNULL    reason2\nTime taken: 0.625 seconds, Fetched: 1 row(s)</code></pre>\n\n\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>&emsp;可以通过关联表实现hbase数据迁移到hive中，同理，hive中数据也可以通过该表迁移到hbase中。<br>在新建关联表一定要注意，hbase表存在则使用external。如果不存在则可以不需要该关键字</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"http://www.micmiu.com/bigdata/hive/hive-hbase-integration/\" target=\"_blank\" rel=\"noopener\">http://www.micmiu.com/bigdata/hive/hive-hbase-integration/</a></p>\n"},{"title":"Java 多线程实现方式","date":"2016-08-21T12:46:59.000Z","_content":"# Java 多线程实现方式\n## 新建线程\n### 1.继承Thread，复写run\n实现一个线程最简单的方法\n```\nnew Thread() {\n\tpublic void run() {\n             System.out.println(\"I`am a thread!\");               \n\t};\n}.start();\n```\n### 2.实现runnable,实现run\nrunnable是一个接口类，使用的话，需要实现。\n```\nnew Thread(new Runnable() {\n\tpublic void run() {\n\t\t// TODO Auto-generated method stub\n\t\tSystem.out.println(\"I`am a second thread!\");     \n\t}\n}).start();\n```\n### 3.利用jdk中Executor框架实现\nExecutors类，提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。\n```\nExecutorService executorService = Executors.newSingleThreadExecutor();\nexecutorService.submit(new Runnable() {\n\tpublic void run() {\n\t\tSystem.out.println(\"I`am a thrid thread!\");\n\t}\n});\n```\n\n## 带返回值的多线程\n结合Future，Callable一起使用，可以取得多线程执行的返回值\n### 1.第一种\n```\nExecutorService executorService = Executors.newFixedThreadPool(10);\nList<Future<String>> results = new ArrayList<Future<String>>();\n\tfor( int i=0;i<10000;i++){\n\t\tfinal int j = i;\n\t\tFuture<String> result = executorService.submit(new Callable<String>() {\n\t\t\tpublic String call() throws Exception {\n\t\t\t\t// TODO Auto-generated method stub\n\t\t\t\treturn \"value\"+j;\n\t\t\t}\n\t\t});\n\t\tresults.add(result);\n\t}\nfor(Future<String>result :results){\n\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞\n}\nexecutorService.shutdown();\n```\n### 2.第二种\n```\nExecutorService executorService = Executors.newFixedThreadPool(10);\nList<Callable<String>> tasks = new ArrayList<Callable<String>>();\nfor (int i = 0; i < 10000; i++) {\n\tfinal int j = i;\n\ttasks.add(new Callable<String>() {\n\t\tpublic String call() throws Exception {\n\t\t\t// TODO Auto-generated method stub\n\t\t\treturn \"value\" + j;\n\t\t}\n\t});\n}\n//此时还未执行\nList<Future<String>> results = executorService.invokeAll(tasks);//未执行完成阻塞\nfor (Future<String> result : results) {\n\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞\n}\nexecutorService.shutdown();\n```","source":"_posts/Java-多线程实现方式.md","raw":"---\ntitle: Java 多线程实现方式\ndate: 2016-08-21 20:46:59\ntags: 编程\ncategories:\n- java\n---\n# Java 多线程实现方式\n## 新建线程\n### 1.继承Thread，复写run\n实现一个线程最简单的方法\n```\nnew Thread() {\n\tpublic void run() {\n             System.out.println(\"I`am a thread!\");               \n\t};\n}.start();\n```\n### 2.实现runnable,实现run\nrunnable是一个接口类，使用的话，需要实现。\n```\nnew Thread(new Runnable() {\n\tpublic void run() {\n\t\t// TODO Auto-generated method stub\n\t\tSystem.out.println(\"I`am a second thread!\");     \n\t}\n}).start();\n```\n### 3.利用jdk中Executor框架实现\nExecutors类，提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。\n```\nExecutorService executorService = Executors.newSingleThreadExecutor();\nexecutorService.submit(new Runnable() {\n\tpublic void run() {\n\t\tSystem.out.println(\"I`am a thrid thread!\");\n\t}\n});\n```\n\n## 带返回值的多线程\n结合Future，Callable一起使用，可以取得多线程执行的返回值\n### 1.第一种\n```\nExecutorService executorService = Executors.newFixedThreadPool(10);\nList<Future<String>> results = new ArrayList<Future<String>>();\n\tfor( int i=0;i<10000;i++){\n\t\tfinal int j = i;\n\t\tFuture<String> result = executorService.submit(new Callable<String>() {\n\t\t\tpublic String call() throws Exception {\n\t\t\t\t// TODO Auto-generated method stub\n\t\t\t\treturn \"value\"+j;\n\t\t\t}\n\t\t});\n\t\tresults.add(result);\n\t}\nfor(Future<String>result :results){\n\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞\n}\nexecutorService.shutdown();\n```\n### 2.第二种\n```\nExecutorService executorService = Executors.newFixedThreadPool(10);\nList<Callable<String>> tasks = new ArrayList<Callable<String>>();\nfor (int i = 0; i < 10000; i++) {\n\tfinal int j = i;\n\ttasks.add(new Callable<String>() {\n\t\tpublic String call() throws Exception {\n\t\t\t// TODO Auto-generated method stub\n\t\t\treturn \"value\" + j;\n\t\t}\n\t});\n}\n//此时还未执行\nList<Future<String>> results = executorService.invokeAll(tasks);//未执行完成阻塞\nfor (Future<String> result : results) {\n\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞\n}\nexecutorService.shutdown();\n```","slug":"Java-多线程实现方式","published":1,"updated":"2016-08-21T12:47:36.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv5f00168ceegh2q6auc","content":"<h1 id=\"Java-多线程实现方式\"><a href=\"#Java-多线程实现方式\" class=\"headerlink\" title=\"Java 多线程实现方式\"></a>Java 多线程实现方式</h1><h2 id=\"新建线程\"><a href=\"#新建线程\" class=\"headerlink\" title=\"新建线程\"></a>新建线程</h2><h3 id=\"1-继承Thread，复写run\"><a href=\"#1-继承Thread，复写run\" class=\"headerlink\" title=\"1.继承Thread，复写run\"></a>1.继承Thread，复写run</h3><p>实现一个线程最简单的方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new Thread() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">             System.out.println(&quot;I`am a thread!&quot;);               </span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;.start();</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-实现runnable-实现run\"><a href=\"#2-实现runnable-实现run\" class=\"headerlink\" title=\"2.实现runnable,实现run\"></a>2.实现runnable,实现run</h3><p>runnable是一个接口类，使用的话，需要实现。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new Thread(new Runnable() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\tSystem.out.println(&quot;I`am a second thread!&quot;);     </span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;).start();</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-利用jdk中Executor框架实现\"><a href=\"#3-利用jdk中Executor框架实现\" class=\"headerlink\" title=\"3.利用jdk中Executor框架实现\"></a>3.利用jdk中Executor框架实现</h3><p>Executors类，提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newSingleThreadExecutor();</span><br><span class=\"line\">executorService.submit(new Runnable() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">\t\tSystem.out.println(&quot;I`am a thrid thread!&quot;);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"带返回值的多线程\"><a href=\"#带返回值的多线程\" class=\"headerlink\" title=\"带返回值的多线程\"></a>带返回值的多线程</h2><p>结合Future，Callable一起使用，可以取得多线程执行的返回值</p>\n<h3 id=\"1-第一种\"><a href=\"#1-第一种\" class=\"headerlink\" title=\"1.第一种\"></a>1.第一种</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newFixedThreadPool(10);</span><br><span class=\"line\">List&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;Future&lt;String&gt;&gt;();</span><br><span class=\"line\">\tfor( int i=0;i&lt;10000;i++)&#123;</span><br><span class=\"line\">\t\tfinal int j = i;</span><br><span class=\"line\">\t\tFuture&lt;String&gt; result = executorService.submit(new Callable&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\t\tpublic String call() throws Exception &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\t\t\treturn &quot;value&quot;+j;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\tresults.add(result);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">for(Future&lt;String&gt;result :results)&#123;</span><br><span class=\"line\">\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">executorService.shutdown();</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-第二种\"><a href=\"#2-第二种\" class=\"headerlink\" title=\"2.第二种\"></a>2.第二种</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newFixedThreadPool(10);</span><br><span class=\"line\">List&lt;Callable&lt;String&gt;&gt; tasks = new ArrayList&lt;Callable&lt;String&gt;&gt;();</span><br><span class=\"line\">for (int i = 0; i &lt; 10000; i++) &#123;</span><br><span class=\"line\">\tfinal int j = i;</span><br><span class=\"line\">\ttasks.add(new Callable&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\tpublic String call() throws Exception &#123;</span><br><span class=\"line\">\t\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\t\treturn &quot;value&quot; + j;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">//此时还未执行</span><br><span class=\"line\">List&lt;Future&lt;String&gt;&gt; results = executorService.invokeAll(tasks);//未执行完成阻塞</span><br><span class=\"line\">for (Future&lt;String&gt; result : results) &#123;</span><br><span class=\"line\">\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">executorService.shutdown();</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Java-多线程实现方式\"><a href=\"#Java-多线程实现方式\" class=\"headerlink\" title=\"Java 多线程实现方式\"></a>Java 多线程实现方式</h1><h2 id=\"新建线程\"><a href=\"#新建线程\" class=\"headerlink\" title=\"新建线程\"></a>新建线程</h2><h3 id=\"1-继承Thread，复写run\"><a href=\"#1-继承Thread，复写run\" class=\"headerlink\" title=\"1.继承Thread，复写run\"></a>1.继承Thread，复写run</h3><p>实现一个线程最简单的方法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new Thread() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">             System.out.println(&quot;I`am a thread!&quot;);               </span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;.start();</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-实现runnable-实现run\"><a href=\"#2-实现runnable-实现run\" class=\"headerlink\" title=\"2.实现runnable,实现run\"></a>2.实现runnable,实现run</h3><p>runnable是一个接口类，使用的话，需要实现。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">new Thread(new Runnable() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\tSystem.out.println(&quot;I`am a second thread!&quot;);     </span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;).start();</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-利用jdk中Executor框架实现\"><a href=\"#3-利用jdk中Executor框架实现\" class=\"headerlink\" title=\"3.利用jdk中Executor框架实现\"></a>3.利用jdk中Executor框架实现</h3><p>Executors类，提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newSingleThreadExecutor();</span><br><span class=\"line\">executorService.submit(new Runnable() &#123;</span><br><span class=\"line\">\tpublic void run() &#123;</span><br><span class=\"line\">\t\tSystem.out.println(&quot;I`am a thrid thread!&quot;);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"带返回值的多线程\"><a href=\"#带返回值的多线程\" class=\"headerlink\" title=\"带返回值的多线程\"></a>带返回值的多线程</h2><p>结合Future，Callable一起使用，可以取得多线程执行的返回值</p>\n<h3 id=\"1-第一种\"><a href=\"#1-第一种\" class=\"headerlink\" title=\"1.第一种\"></a>1.第一种</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newFixedThreadPool(10);</span><br><span class=\"line\">List&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;Future&lt;String&gt;&gt;();</span><br><span class=\"line\">\tfor( int i=0;i&lt;10000;i++)&#123;</span><br><span class=\"line\">\t\tfinal int j = i;</span><br><span class=\"line\">\t\tFuture&lt;String&gt; result = executorService.submit(new Callable&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\t\tpublic String call() throws Exception &#123;</span><br><span class=\"line\">\t\t\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\t\t\treturn &quot;value&quot;+j;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\tresults.add(result);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">for(Future&lt;String&gt;result :results)&#123;</span><br><span class=\"line\">\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">executorService.shutdown();</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-第二种\"><a href=\"#2-第二种\" class=\"headerlink\" title=\"2.第二种\"></a>2.第二种</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ExecutorService executorService = Executors.newFixedThreadPool(10);</span><br><span class=\"line\">List&lt;Callable&lt;String&gt;&gt; tasks = new ArrayList&lt;Callable&lt;String&gt;&gt;();</span><br><span class=\"line\">for (int i = 0; i &lt; 10000; i++) &#123;</span><br><span class=\"line\">\tfinal int j = i;</span><br><span class=\"line\">\ttasks.add(new Callable&lt;String&gt;() &#123;</span><br><span class=\"line\">\t\tpublic String call() throws Exception &#123;</span><br><span class=\"line\">\t\t\t// TODO Auto-generated method stub</span><br><span class=\"line\">\t\t\treturn &quot;value&quot; + j;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">//此时还未执行</span><br><span class=\"line\">List&lt;Future&lt;String&gt;&gt; results = executorService.invokeAll(tasks);//未执行完成阻塞</span><br><span class=\"line\">for (Future&lt;String&gt; result : results) &#123;</span><br><span class=\"line\">\tSystem.out.println(result.get());//当线程没有执行完毕会阻塞</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">executorService.shutdown();</span><br></pre></td></tr></table></figure>"},{"title":"Hbase数据迁移到Hive中问题汇总","date":"2016-03-26T12:29:05.000Z","_content":"## 问题1\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apach e/hadoop/hbase/mapreduce/TableInputFormatBase \n\n ** 解决方法：**\n***\n将hbase/lib下\n     hbase-common-1.0.0-cdh5.4.0-tests.jar\n     hbase-common-1.0.0-cdh5.4.0.jar\ncopy到hive/lib下面\n## 问题2\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaExcep tion(message:MetaException(message:java.io.IOException: java.lang.reflect.InvocationTargetExc eption\n ** 解决方法：**\n***\n将hbase/lib下\n     htrace-core-3.0.4.jar\n     htrace-core-3.1.0-incubating.jar \ncopy到hive/lib下面\n## 问题3\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Table truman already exists within HBase; use CREATE EXTERNAL TABLE instead to register it in Hive.)\n ** 解决方法：**\n***\n使用 external\n例如：\n<pre><code>\n create external table hive(id string,name string ,ct string)stored by   'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties (\"hbase.columns.mapping\"= \":key,cf:name,cf:ct\") tblproperties (\"hbase.table.name\" = \"hbase\");\n</code></pre>\n## 问题4 hive中建hbase关联表失败\n** 解决方法：**\n***\n首先要确保<pre><code>HIVE_HOME/lib </code></pre>下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用<pre><code>HBASE_HOME/lib/</code></pre>目录下得jar包：\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到<pre><code>HIVE_HOME/lib </code></pre>下。","source":"_posts/Hbase数据迁移到Hive中问题汇总.md","raw":"---\ntitle: Hbase数据迁移到Hive中问题汇总\ndate: 2016-03-26 20:29:05\ntags: 大数据\ncategories:\n- hbase\n---\n## 问题1\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apach e/hadoop/hbase/mapreduce/TableInputFormatBase \n\n ** 解决方法：**\n***\n将hbase/lib下\n     hbase-common-1.0.0-cdh5.4.0-tests.jar\n     hbase-common-1.0.0-cdh5.4.0.jar\ncopy到hive/lib下面\n## 问题2\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaExcep tion(message:MetaException(message:java.io.IOException: java.lang.reflect.InvocationTargetExc eption\n ** 解决方法：**\n***\n将hbase/lib下\n     htrace-core-3.0.4.jar\n     htrace-core-3.1.0-incubating.jar \ncopy到hive/lib下面\n## 问题3\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Table truman already exists within HBase; use CREATE EXTERNAL TABLE instead to register it in Hive.)\n ** 解决方法：**\n***\n使用 external\n例如：\n<pre><code>\n create external table hive(id string,name string ,ct string)stored by   'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties (\"hbase.columns.mapping\"= \":key,cf:name,cf:ct\") tblproperties (\"hbase.table.name\" = \"hbase\");\n</code></pre>\n## 问题4 hive中建hbase关联表失败\n** 解决方法：**\n***\n首先要确保<pre><code>HIVE_HOME/lib </code></pre>下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用<pre><code>HBASE_HOME/lib/</code></pre>目录下得jar包：\n<pre><code>\nhbase-client-1.0.0-cdh5.4.0.jar \nhbase-common-1.0.0-cdh5.4.0-tests.jar\nhbase-common-1.0.0-cdh5.4.0.jar\nhbase-protocol-1.0.0-cdh5.4.0.jar\nhbase-server-1.0.0-cdh5.4.0.jar\nhtrace-core-3.0.4.jar\nhtrace-core-3.1.0-incubating.jar \n</code></pre>\n复制到<pre><code>HIVE_HOME/lib </code></pre>下。","slug":"Hbase数据迁移到Hive中问题汇总","published":1,"updated":"2016-03-26T12:44:35.766Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv5h00198cee8axmefrq","content":"<h2 id=\"问题1\"><a href=\"#问题1\" class=\"headerlink\" title=\"问题1\"></a>问题1</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apach e/hadoop/hbase/mapreduce/TableInputFormatBase </p>\n<p> <strong> 解决方法：</strong></p>\n<hr>\n<p>将hbase/lib下<br>     hbase-common-1.0.0-cdh5.4.0-tests.jar<br>     hbase-common-1.0.0-cdh5.4.0.jar<br>copy到hive/lib下面</p>\n<h2 id=\"问题2\"><a href=\"#问题2\" class=\"headerlink\" title=\"问题2\"></a>问题2</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaExcep tion(message:MetaException(message:java.io.IOException: java.lang.reflect.InvocationTargetExc eption<br> <strong> 解决方法：</strong></p>\n<hr>\n<p>将hbase/lib下<br>     htrace-core-3.0.4.jar<br>     htrace-core-3.1.0-incubating.jar<br>copy到hive/lib下面</p>\n<h2 id=\"问题3\"><a href=\"#问题3\" class=\"headerlink\" title=\"问题3\"></a>问题3</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Table truman already exists within HBase; use CREATE EXTERNAL TABLE instead to register it in Hive.)<br> <strong> 解决方法：</strong></p>\n<hr>\n<p>使用 external<br>例如：</p>\n<p><pre><code><br> create external table hive(id string,name string ,ct string)stored by   ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’ with serdeproperties (“hbase.columns.mapping”= “:key,cf:name,cf:ct”) tblproperties (“hbase.table.name” = “hbase”);<br></code></pre></p>\n<h2 id=\"问题4-hive中建hbase关联表失败\"><a href=\"#问题4-hive中建hbase关联表失败\" class=\"headerlink\" title=\"问题4 hive中建hbase关联表失败\"></a>问题4 hive中建hbase关联表失败</h2><p><strong> 解决方法：</strong></p>\n<hr>\n<p>首先要确保<pre><code>HIVE_HOME/lib </code></pre>下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用<pre><code>HBASE_HOME/lib/</code></pre>目录下得jar包：</p>\n<p><pre><code><br>hbase-client-1.0.0-cdh5.4.0.jar<br>hbase-common-1.0.0-cdh5.4.0-tests.jar<br>hbase-common-1.0.0-cdh5.4.0.jar<br>hbase-protocol-1.0.0-cdh5.4.0.jar<br>hbase-server-1.0.0-cdh5.4.0.jar<br>htrace-core-3.0.4.jar<br>htrace-core-3.1.0-incubating.jar<br></code></pre><br>复制到<pre><code>HIVE_HOME/lib </code></pre>下。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"问题1\"><a href=\"#问题1\" class=\"headerlink\" title=\"问题1\"></a>问题1</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apach e/hadoop/hbase/mapreduce/TableInputFormatBase </p>\n<p> <strong> 解决方法：</strong></p>\n<hr>\n<p>将hbase/lib下<br>     hbase-common-1.0.0-cdh5.4.0-tests.jar<br>     hbase-common-1.0.0-cdh5.4.0.jar<br>copy到hive/lib下面</p>\n<h2 id=\"问题2\"><a href=\"#问题2\" class=\"headerlink\" title=\"问题2\"></a>问题2</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaExcep tion(message:MetaException(message:java.io.IOException: java.lang.reflect.InvocationTargetExc eption<br> <strong> 解决方法：</strong></p>\n<hr>\n<p>将hbase/lib下<br>     htrace-core-3.0.4.jar<br>     htrace-core-3.1.0-incubating.jar<br>copy到hive/lib下面</p>\n<h2 id=\"问题3\"><a href=\"#问题3\" class=\"headerlink\" title=\"问题3\"></a>问题3</h2><p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:MetaException(message:Table truman already exists within HBase; use CREATE EXTERNAL TABLE instead to register it in Hive.)<br> <strong> 解决方法：</strong></p>\n<hr>\n<p>使用 external<br>例如：</p>\n<p><pre><code><br> create external table hive(id string,name string ,ct string)stored by   ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’ with serdeproperties (“hbase.columns.mapping”= “:key,cf:name,cf:ct”) tblproperties (“hbase.table.name” = “hbase”);<br></code></pre></p>\n<h2 id=\"问题4-hive中建hbase关联表失败\"><a href=\"#问题4-hive中建hbase关联表失败\" class=\"headerlink\" title=\"问题4 hive中建hbase关联表失败\"></a>问题4 hive中建hbase关联表失败</h2><p><strong> 解决方法：</strong></p>\n<hr>\n<p>首先要确保<pre><code>HIVE_HOME/lib </code></pre>下HBase的jar包的版本要和实际环境中HBase的版本一致，需要用<pre><code>HBASE_HOME/lib/</code></pre>目录下得jar包：</p>\n<p><pre><code><br>hbase-client-1.0.0-cdh5.4.0.jar<br>hbase-common-1.0.0-cdh5.4.0-tests.jar<br>hbase-common-1.0.0-cdh5.4.0.jar<br>hbase-protocol-1.0.0-cdh5.4.0.jar<br>hbase-server-1.0.0-cdh5.4.0.jar<br>htrace-core-3.0.4.jar<br>htrace-core-3.1.0-incubating.jar<br></code></pre><br>复制到<pre><code>HIVE_HOME/lib </code></pre>下。</p>\n"},{"title":"JVM基本结构,内存分配","date":"2019-03-02T14:28:00.000Z","_content":"## JVM的基本结构\n\n### 一.类加载器(ClassLoader)\n其作用是在程序运行时,将编译好的.class字节码文件装载到JVM的内存区域中.如下图所示流程,Java源码被编译器编译为字节码文件,字节码文件被类加载器加载到数据运行时区域(其实就是内存空间当中),然后再由执行引擎执行.class文件中的字节码指令.\n\n\n\n### 二.执行引擎\n执行.class字节码文件中的指令集,如果想了解class中的字节码指令,可以参考<<深入分析Java Web技术内幕>>的第5章深入class文件结构.\n\n### 三.本地库接口(本地方法库)\n我的理解这是JVM与本地操作系统交互的接口,调用一些由C语言等编写的本地方法,一般的开发者并不用细纠.\n\n### 四.JVM内存区(运行时数据区)\n这是JVM中非常重要的一部分,是Java程序运行时JVM所分配的内存区域,绝大部分开发者关注的重点都在此.\n\nJVM的内存区域分为5大块,如下图所示.\n\n#### 1.虚拟机栈(Stack)\n一般俗称栈区,是线程私有的.栈区一般与线程紧密相联,一旦有新的线程被创建,JVM就会为该线程分配一个对应的java栈区,在这个栈区中会有许多栈帧,每运行一个方法就创建一个栈帧,用于存储局部变量,方法返回值等.栈帧中存储的局部变量随着线程的结束而结束,其生命周期取决于线程的生命周期,所以讲java栈中的变量都是线程私有的.\n\n#### 2.堆(Heap)\n真正存储对象的区域,当进行Object obj = new Object()这样一个操作时,真正的obj对象实例就会在heap中.\n\n#### 3.方法区(Method Area)\n包含常量池,静态变量等,有人说常量池也属于heap的一部分,但是严格上讲方法区只是堆的逻辑部分,方法区还有个别名叫做非堆(non-heap),所以方法区和堆还是有不同的.\n\n#### 4.程序计数器(Program Couter Register)\n用于保存当前线程的执行的内存地址.因为JVM是支持多线程的,多线程同时执行的时候可能会轮流切换,为了保证线程切换回来后还能恢复到原先状态,就需要一个独立的计数器,记录之前中断的位置,由此可以看出程序计数器也是线程私有的.\n\n#### 5.本地方法栈(Native Method Stack)\n性质与虚拟机栈类似,是为了方便JVM去调用本地方法接口的栈区,此处开发者很少去关注,我也是了解有限,因此不深入探究其作用.","source":"_posts/JVM基本结构-内存分配.md","raw":"---\ntitle: 'JVM基本结构,内存分配'\ndate: 2019-03-02 22:28:00\ntags: 笔记\ncategories:\n- java\n---\n## JVM的基本结构\n\n### 一.类加载器(ClassLoader)\n其作用是在程序运行时,将编译好的.class字节码文件装载到JVM的内存区域中.如下图所示流程,Java源码被编译器编译为字节码文件,字节码文件被类加载器加载到数据运行时区域(其实就是内存空间当中),然后再由执行引擎执行.class文件中的字节码指令.\n\n\n\n### 二.执行引擎\n执行.class字节码文件中的指令集,如果想了解class中的字节码指令,可以参考<<深入分析Java Web技术内幕>>的第5章深入class文件结构.\n\n### 三.本地库接口(本地方法库)\n我的理解这是JVM与本地操作系统交互的接口,调用一些由C语言等编写的本地方法,一般的开发者并不用细纠.\n\n### 四.JVM内存区(运行时数据区)\n这是JVM中非常重要的一部分,是Java程序运行时JVM所分配的内存区域,绝大部分开发者关注的重点都在此.\n\nJVM的内存区域分为5大块,如下图所示.\n\n#### 1.虚拟机栈(Stack)\n一般俗称栈区,是线程私有的.栈区一般与线程紧密相联,一旦有新的线程被创建,JVM就会为该线程分配一个对应的java栈区,在这个栈区中会有许多栈帧,每运行一个方法就创建一个栈帧,用于存储局部变量,方法返回值等.栈帧中存储的局部变量随着线程的结束而结束,其生命周期取决于线程的生命周期,所以讲java栈中的变量都是线程私有的.\n\n#### 2.堆(Heap)\n真正存储对象的区域,当进行Object obj = new Object()这样一个操作时,真正的obj对象实例就会在heap中.\n\n#### 3.方法区(Method Area)\n包含常量池,静态变量等,有人说常量池也属于heap的一部分,但是严格上讲方法区只是堆的逻辑部分,方法区还有个别名叫做非堆(non-heap),所以方法区和堆还是有不同的.\n\n#### 4.程序计数器(Program Couter Register)\n用于保存当前线程的执行的内存地址.因为JVM是支持多线程的,多线程同时执行的时候可能会轮流切换,为了保证线程切换回来后还能恢复到原先状态,就需要一个独立的计数器,记录之前中断的位置,由此可以看出程序计数器也是线程私有的.\n\n#### 5.本地方法栈(Native Method Stack)\n性质与虚拟机栈类似,是为了方便JVM去调用本地方法接口的栈区,此处开发者很少去关注,我也是了解有限,因此不深入探究其作用.","slug":"JVM基本结构-内存分配","published":1,"updated":"2019-03-02T14:28:31.777Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobv5j001c8ceene9rkw4g","content":"<h2 id=\"JVM的基本结构\"><a href=\"#JVM的基本结构\" class=\"headerlink\" title=\"JVM的基本结构\"></a>JVM的基本结构</h2><h3 id=\"一-类加载器-ClassLoader\"><a href=\"#一-类加载器-ClassLoader\" class=\"headerlink\" title=\"一.类加载器(ClassLoader)\"></a>一.类加载器(ClassLoader)</h3><p>其作用是在程序运行时,将编译好的.class字节码文件装载到JVM的内存区域中.如下图所示流程,Java源码被编译器编译为字节码文件,字节码文件被类加载器加载到数据运行时区域(其实就是内存空间当中),然后再由执行引擎执行.class文件中的字节码指令.</p>\n<h3 id=\"二-执行引擎\"><a href=\"#二-执行引擎\" class=\"headerlink\" title=\"二.执行引擎\"></a>二.执行引擎</h3><p>执行.class字节码文件中的指令集,如果想了解class中的字节码指令,可以参考&lt;&lt;深入分析Java Web技术内幕&gt;&gt;的第5章深入class文件结构.</p>\n<h3 id=\"三-本地库接口-本地方法库\"><a href=\"#三-本地库接口-本地方法库\" class=\"headerlink\" title=\"三.本地库接口(本地方法库)\"></a>三.本地库接口(本地方法库)</h3><p>我的理解这是JVM与本地操作系统交互的接口,调用一些由C语言等编写的本地方法,一般的开发者并不用细纠.</p>\n<h3 id=\"四-JVM内存区-运行时数据区\"><a href=\"#四-JVM内存区-运行时数据区\" class=\"headerlink\" title=\"四.JVM内存区(运行时数据区)\"></a>四.JVM内存区(运行时数据区)</h3><p>这是JVM中非常重要的一部分,是Java程序运行时JVM所分配的内存区域,绝大部分开发者关注的重点都在此.</p>\n<p>JVM的内存区域分为5大块,如下图所示.</p>\n<h4 id=\"1-虚拟机栈-Stack\"><a href=\"#1-虚拟机栈-Stack\" class=\"headerlink\" title=\"1.虚拟机栈(Stack)\"></a>1.虚拟机栈(Stack)</h4><p>一般俗称栈区,是线程私有的.栈区一般与线程紧密相联,一旦有新的线程被创建,JVM就会为该线程分配一个对应的java栈区,在这个栈区中会有许多栈帧,每运行一个方法就创建一个栈帧,用于存储局部变量,方法返回值等.栈帧中存储的局部变量随着线程的结束而结束,其生命周期取决于线程的生命周期,所以讲java栈中的变量都是线程私有的.</p>\n<h4 id=\"2-堆-Heap\"><a href=\"#2-堆-Heap\" class=\"headerlink\" title=\"2.堆(Heap)\"></a>2.堆(Heap)</h4><p>真正存储对象的区域,当进行Object obj = new Object()这样一个操作时,真正的obj对象实例就会在heap中.</p>\n<h4 id=\"3-方法区-Method-Area\"><a href=\"#3-方法区-Method-Area\" class=\"headerlink\" title=\"3.方法区(Method Area)\"></a>3.方法区(Method Area)</h4><p>包含常量池,静态变量等,有人说常量池也属于heap的一部分,但是严格上讲方法区只是堆的逻辑部分,方法区还有个别名叫做非堆(non-heap),所以方法区和堆还是有不同的.</p>\n<h4 id=\"4-程序计数器-Program-Couter-Register\"><a href=\"#4-程序计数器-Program-Couter-Register\" class=\"headerlink\" title=\"4.程序计数器(Program Couter Register)\"></a>4.程序计数器(Program Couter Register)</h4><p>用于保存当前线程的执行的内存地址.因为JVM是支持多线程的,多线程同时执行的时候可能会轮流切换,为了保证线程切换回来后还能恢复到原先状态,就需要一个独立的计数器,记录之前中断的位置,由此可以看出程序计数器也是线程私有的.</p>\n<h4 id=\"5-本地方法栈-Native-Method-Stack\"><a href=\"#5-本地方法栈-Native-Method-Stack\" class=\"headerlink\" title=\"5.本地方法栈(Native Method Stack)\"></a>5.本地方法栈(Native Method Stack)</h4><p>性质与虚拟机栈类似,是为了方便JVM去调用本地方法接口的栈区,此处开发者很少去关注,我也是了解有限,因此不深入探究其作用.</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"JVM的基本结构\"><a href=\"#JVM的基本结构\" class=\"headerlink\" title=\"JVM的基本结构\"></a>JVM的基本结构</h2><h3 id=\"一-类加载器-ClassLoader\"><a href=\"#一-类加载器-ClassLoader\" class=\"headerlink\" title=\"一.类加载器(ClassLoader)\"></a>一.类加载器(ClassLoader)</h3><p>其作用是在程序运行时,将编译好的.class字节码文件装载到JVM的内存区域中.如下图所示流程,Java源码被编译器编译为字节码文件,字节码文件被类加载器加载到数据运行时区域(其实就是内存空间当中),然后再由执行引擎执行.class文件中的字节码指令.</p>\n<h3 id=\"二-执行引擎\"><a href=\"#二-执行引擎\" class=\"headerlink\" title=\"二.执行引擎\"></a>二.执行引擎</h3><p>执行.class字节码文件中的指令集,如果想了解class中的字节码指令,可以参考&lt;&lt;深入分析Java Web技术内幕&gt;&gt;的第5章深入class文件结构.</p>\n<h3 id=\"三-本地库接口-本地方法库\"><a href=\"#三-本地库接口-本地方法库\" class=\"headerlink\" title=\"三.本地库接口(本地方法库)\"></a>三.本地库接口(本地方法库)</h3><p>我的理解这是JVM与本地操作系统交互的接口,调用一些由C语言等编写的本地方法,一般的开发者并不用细纠.</p>\n<h3 id=\"四-JVM内存区-运行时数据区\"><a href=\"#四-JVM内存区-运行时数据区\" class=\"headerlink\" title=\"四.JVM内存区(运行时数据区)\"></a>四.JVM内存区(运行时数据区)</h3><p>这是JVM中非常重要的一部分,是Java程序运行时JVM所分配的内存区域,绝大部分开发者关注的重点都在此.</p>\n<p>JVM的内存区域分为5大块,如下图所示.</p>\n<h4 id=\"1-虚拟机栈-Stack\"><a href=\"#1-虚拟机栈-Stack\" class=\"headerlink\" title=\"1.虚拟机栈(Stack)\"></a>1.虚拟机栈(Stack)</h4><p>一般俗称栈区,是线程私有的.栈区一般与线程紧密相联,一旦有新的线程被创建,JVM就会为该线程分配一个对应的java栈区,在这个栈区中会有许多栈帧,每运行一个方法就创建一个栈帧,用于存储局部变量,方法返回值等.栈帧中存储的局部变量随着线程的结束而结束,其生命周期取决于线程的生命周期,所以讲java栈中的变量都是线程私有的.</p>\n<h4 id=\"2-堆-Heap\"><a href=\"#2-堆-Heap\" class=\"headerlink\" title=\"2.堆(Heap)\"></a>2.堆(Heap)</h4><p>真正存储对象的区域,当进行Object obj = new Object()这样一个操作时,真正的obj对象实例就会在heap中.</p>\n<h4 id=\"3-方法区-Method-Area\"><a href=\"#3-方法区-Method-Area\" class=\"headerlink\" title=\"3.方法区(Method Area)\"></a>3.方法区(Method Area)</h4><p>包含常量池,静态变量等,有人说常量池也属于heap的一部分,但是严格上讲方法区只是堆的逻辑部分,方法区还有个别名叫做非堆(non-heap),所以方法区和堆还是有不同的.</p>\n<h4 id=\"4-程序计数器-Program-Couter-Register\"><a href=\"#4-程序计数器-Program-Couter-Register\" class=\"headerlink\" title=\"4.程序计数器(Program Couter Register)\"></a>4.程序计数器(Program Couter Register)</h4><p>用于保存当前线程的执行的内存地址.因为JVM是支持多线程的,多线程同时执行的时候可能会轮流切换,为了保证线程切换回来后还能恢复到原先状态,就需要一个独立的计数器,记录之前中断的位置,由此可以看出程序计数器也是线程私有的.</p>\n<h4 id=\"5-本地方法栈-Native-Method-Stack\"><a href=\"#5-本地方法栈-Native-Method-Stack\" class=\"headerlink\" title=\"5.本地方法栈(Native Method Stack)\"></a>5.本地方法栈(Native Method Stack)</h4><p>性质与虚拟机栈类似,是为了方便JVM去调用本地方法接口的栈区,此处开发者很少去关注,我也是了解有限,因此不深入探究其作用.</p>\n"},{"title":"Kafka Blance Topic","date":"2017-03-05T12:48:13.000Z","_content":"# one step\n\n创建topic-move-test.json,迁移可以指定多个topic\n\n```\n{\"topics\": [{\n    \"topic\": \"upgrade-kafka\"\n  },{\n  \"topic\":\"upgrade-kafka1\"\n  }],\n  \"version\": 1\n}\n```\n# two step\n在kafka工作目录，执行以下生成迁移配置，其中91为需要迁移到的broker id\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --topics-to-move-json-file topic-move-test.json --broker-list \"91\" --generate\n```\n生成以下内容：\n```\nCurrent partition replica assignment\n\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[96]},{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[96]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[92]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[95]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[95]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[94]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[93]}]}\nProposed partition reassignment configuration\n\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[91]}]}\n\n```\n# three step\n\n新建expand-cluster-reassignment.json文件，将上一步输出放入，内容如下：\n```\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[91]}]}\n```\n# four step\n按自己需求可以重新编辑expand-cluster-reassignment.json内容，修改broker id.\n执行迁移命令\n\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --execute\n```\n检查执行结果\n\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --verify\n```\n\n以下即为迁移成功：\n```\nStatus of partition reassignment:\nReassignment of partition [upgrade-kafka,4] completed successfully\nReassignment of partition [upgrade-kafka,5] completed successfully\nReassignment of partition [upgrade-kafka1,0] completed successfully\nReassignment of partition [upgrade-kafka,2] completed successfully\nReassignment of partition [upgrade-kafka1,1] completed successfully\nReassignment of partition [upgrade-kafka,1] completed successfully\nReassignment of partition [upgrade-kafka,3] completed successfully\nReassignment of partition [upgrade-kafka,0] completed successfully\nReassignment of partition [upgrade-kafka1,2] completed successfully\n\n```\n\n# Reference\n1. [kafka.apache.org](https://kafka.apache.org/documentation/#basic_ops_cluster_expansion)\n","source":"_posts/Kafka-Blance-Topic.md","raw":"---\ntitle: Kafka Blance Topic\ndate: 2017-03-05 20:48:13\ntags: 大数据\ncategories:\n- kafka\n---\n# one step\n\n创建topic-move-test.json,迁移可以指定多个topic\n\n```\n{\"topics\": [{\n    \"topic\": \"upgrade-kafka\"\n  },{\n  \"topic\":\"upgrade-kafka1\"\n  }],\n  \"version\": 1\n}\n```\n# two step\n在kafka工作目录，执行以下生成迁移配置，其中91为需要迁移到的broker id\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --topics-to-move-json-file topic-move-test.json --broker-list \"91\" --generate\n```\n生成以下内容：\n```\nCurrent partition replica assignment\n\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[96]},{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[96]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[92]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[95]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[95]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[94]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[93]}]}\nProposed partition reassignment configuration\n\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[91]}]}\n\n```\n# three step\n\n新建expand-cluster-reassignment.json文件，将上一步输出放入，内容如下：\n```\n{\"version\":1,\"partitions\":[{\"topic\":\"upgrade-kafka\",\"partition\":5,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":1,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":4,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":2,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":3,\"replicas\":[91]},{\"topic\":\"upgrade-kafka1\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":0,\"replicas\":[91]},{\"topic\":\"upgrade-kafka\",\"partition\":2,\"replicas\":[91]}]}\n```\n# four step\n按自己需求可以重新编辑expand-cluster-reassignment.json内容，修改broker id.\n执行迁移命令\n\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --execute\n```\n检查执行结果\n\n```\nbin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --verify\n```\n\n以下即为迁移成功：\n```\nStatus of partition reassignment:\nReassignment of partition [upgrade-kafka,4] completed successfully\nReassignment of partition [upgrade-kafka,5] completed successfully\nReassignment of partition [upgrade-kafka1,0] completed successfully\nReassignment of partition [upgrade-kafka,2] completed successfully\nReassignment of partition [upgrade-kafka1,1] completed successfully\nReassignment of partition [upgrade-kafka,1] completed successfully\nReassignment of partition [upgrade-kafka,3] completed successfully\nReassignment of partition [upgrade-kafka,0] completed successfully\nReassignment of partition [upgrade-kafka1,2] completed successfully\n\n```\n\n# Reference\n1. [kafka.apache.org](https://kafka.apache.org/documentation/#basic_ops_cluster_expansion)\n","slug":"Kafka-Blance-Topic","published":1,"updated":"2017-03-05T12:50:13.340Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvat00248cee4xzkkqvq","content":"<h1 id=\"one-step\"><a href=\"#one-step\" class=\"headerlink\" title=\"one step\"></a>one step</h1><p>创建topic-move-test.json,迁移可以指定多个topic</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&quot;topics&quot;: [&#123;</span><br><span class=\"line\">    &quot;topic&quot;: &quot;upgrade-kafka&quot;</span><br><span class=\"line\">  &#125;,&#123;</span><br><span class=\"line\">  &quot;topic&quot;:&quot;upgrade-kafka1&quot;</span><br><span class=\"line\">  &#125;],</span><br><span class=\"line\">  &quot;version&quot;: 1</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"two-step\"><a href=\"#two-step\" class=\"headerlink\" title=\"two step\"></a>two step</h1><p>在kafka工作目录，执行以下生成迁移配置，其中91为需要迁移到的broker id<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --topics-to-move-json-file topic-move-test.json --broker-list &quot;91&quot; --generate</span><br></pre></td></tr></table></figure></p>\n<p>生成以下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Current partition replica assignment</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[96]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[96]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[92]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[95]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[95]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[94]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[93]&#125;]&#125;</span><br><span class=\"line\">Proposed partition reassignment configuration</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;]&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"three-step\"><a href=\"#three-step\" class=\"headerlink\" title=\"three step\"></a>three step</h1><p>新建expand-cluster-reassignment.json文件，将上一步输出放入，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;]&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"four-step\"><a href=\"#four-step\" class=\"headerlink\" title=\"four step\"></a>four step</h1><p>按自己需求可以重新编辑expand-cluster-reassignment.json内容，修改broker id.<br>执行迁移命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br></pre></td></tr></table></figure>\n<p>检查执行结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br></pre></td></tr></table></figure>\n<p>以下即为迁移成功：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Status of partition reassignment:</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,4] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,5] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,0] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,2] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,1] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,1] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,3] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,0] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,2] completed successfully</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li><a href=\"https://kafka.apache.org/documentation/#basic_ops_cluster_expansion\" target=\"_blank\" rel=\"noopener\">kafka.apache.org</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"one-step\"><a href=\"#one-step\" class=\"headerlink\" title=\"one step\"></a>one step</h1><p>创建topic-move-test.json,迁移可以指定多个topic</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&quot;topics&quot;: [&#123;</span><br><span class=\"line\">    &quot;topic&quot;: &quot;upgrade-kafka&quot;</span><br><span class=\"line\">  &#125;,&#123;</span><br><span class=\"line\">  &quot;topic&quot;:&quot;upgrade-kafka1&quot;</span><br><span class=\"line\">  &#125;],</span><br><span class=\"line\">  &quot;version&quot;: 1</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"two-step\"><a href=\"#two-step\" class=\"headerlink\" title=\"two step\"></a>two step</h1><p>在kafka工作目录，执行以下生成迁移配置，其中91为需要迁移到的broker id<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --topics-to-move-json-file topic-move-test.json --broker-list &quot;91&quot; --generate</span><br></pre></td></tr></table></figure></p>\n<p>生成以下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Current partition replica assignment</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[96]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[96]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[92]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[95]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[95]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[94]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[93]&#125;]&#125;</span><br><span class=\"line\">Proposed partition reassignment configuration</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;]&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"three-step\"><a href=\"#three-step\" class=\"headerlink\" title=\"three step\"></a>three step</h1><p>新建expand-cluster-reassignment.json文件，将上一步输出放入，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[91]&#125;,&#123;&quot;topic&quot;:&quot;upgrade-kafka&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[91]&#125;]&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"four-step\"><a href=\"#four-step\" class=\"headerlink\" title=\"four step\"></a>four step</h1><p>按自己需求可以重新编辑expand-cluster-reassignment.json内容，修改broker id.<br>执行迁移命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br></pre></td></tr></table></figure>\n<p>检查执行结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br></pre></td></tr></table></figure>\n<p>以下即为迁移成功：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Status of partition reassignment:</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,4] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,5] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,0] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,2] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,1] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,1] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,3] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka,0] completed successfully</span><br><span class=\"line\">Reassignment of partition [upgrade-kafka1,2] completed successfully</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h1><ol>\n<li><a href=\"https://kafka.apache.org/documentation/#basic_ops_cluster_expansion\" target=\"_blank\" rel=\"noopener\">kafka.apache.org</a></li>\n</ol>\n"},{"title":"HBase常用的操作一(shell)","date":"2016-06-14T12:27:16.000Z","_content":"HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。\nHBase访问接口如下:\n\n1. Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据\n2. HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用\n3. Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据\n4. REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制\n5. Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计\n6. Hive，当前Hive的Release版本尚没有加入对HBase的支持，在Hive 0.7.0中支持HBase，可以使用类似SQL语言来访问HBase\n本文针对HBase Shell，做一个详细讲解。\n\n<!-- more -->\n\n在hbase安装目录下执行 bin/hbase shell 即可进入shell操作\n#### 1. 一般命令\n##### 1.1status\n功能：查询服务器状态\n使用：\n```\n hbase(main):003:0> status\n4 servers, 0 dead, 1.2500 average load\n```\n \n##### 1.2version\n功能：查询HBase版本信息\n使用：\n```\n hbase(main):004:0> version\n1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015\n```\n##### 1.3 whoami\n功能：查看连接的用户\n#### 2. DDL命令\n##### 2.1 Create创建表\n功能：创建一个表。创建一个表时，不指定具体的列名，但要指定列族名。\n使用：create ‘表名’,’列族名1’,’列族名2’\n```\nhbase(main):005:0> create 'truman_test','user'\n0 row(s) in 1.3680 seconds\n\n=> Hbase::Table - truman_test\n```\n 备注：\n新建不带namespace表\n```\ncreate 'testTable','t1','t2'\n```\n新建带namespace表\n- 首先建一个namespace\n```\ncreate_namespace 'truman'\n```\n- 其次再新建表\n```\ncreate_namespace 'truman：test','t1','t2'\n```\n备注：t1,t2为column family\n```\ncreate'reason:user_test','bs',{NUMREGIONS=>2,SPLITALGO=>'HexStringSplit'}\n\n```\n\n##### 2.2 disable失效表\n功能：失效一个表。当需要修改表结构、删除表时，需要先执行此命令。\n使用：\n```\n hbase(main):007:0> disable 'truman_test'\n0 row(s) in 1.2660 seconds\n\nhbase(main):008:0> describe 'truman_test'\nTable truman_test is DISABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0210 seconds\n```\n \n##### 2.3 enable使失效表有效\n功能：使表有效。在失效表以后，需要执行此命令，以使得表可用。\n使用：\n```\n hbase(main):009:0> enable 'truman_test'\n0 row(s) in 0.4320 seconds\n\nhbase(main):010:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0260 seconds\n```\n \n##### 2.4 alter修改表结构\n功能：修改表结构，包括新增列族、删除列族等\n使用：\n新增列族（记得在执行alter之前，要先disable表）\n```\n hbase(main):011:0> disable 'truman_test'\n0 row(s) in 1.2480 seconds\n\nhbase(main):012:0> alter 'truman_test', 'order'\nUpdating all regions with the new schema...\n1/1 regions updated.\nDone.\n0 row(s) in 1.2320 seconds\n\nhbase(main):013:0> enable 'truman_test'\n0 row(s) in 0.4060 seconds\nhbase(main):014:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'order', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS =>\n'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\n BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n2 row(s) in 0.0240 seconds\n```\n \n删除列族\n```\n hbase(main):015:0> disable 'truman_test'\n0 row(s) in 1.2970 seconds\n\nhbase(main):017:0> alter 'truman_test',{NAME=>'order',method=>'delete'}\nUnknown argument ignored for column family order:\nUpdating all regions with the new schema...\n1/1 regions updated.\nDone.\n0 row(s) in 1.2100 seconds\n\nhbase(main):018:0> enable 'truman_test'\n0 row(s) in 0.4170 seconds\n```\n \n重命名列族\n列族不能被重命名。重命名一个列族的通常途径是使用API创建一个有着期望名称的新的列族，然后将数据复制过去，最后再删除旧的列族。\n##### 2.5 describe查看表结构\n功能：查看表结构\n使用：\n```\n hbase(main):010:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0260 seconds\n```\n \n##### 2.6 list列举数据库中的所有表\n功能：查看数据库中所有的表\n使用：\n```\nhbase(main):019:0> list\nTABLE\nreason\ntest\ntruman_test\n3 row(s) in 0.0090 seconds\n\n=> [\"reason\", \"test\", \"truman_test\"]\n```\n \n##### 2.7 drop删除表\n功能：删除指定的表\n使用：\n```\nhbase(main):022:0> disable 'tttt'\n0 row(s) in 1.2370 seconds\n\nhbase(main):023:0> drop 'tttt'\n0 row(s) in 0.2140 seconds\n```\n \n#### 3. DML命令\n##### 3.1 put插入数据\n功能：插入一条数据到指定的表中。对于同一个rowkey，如果执行两次put，则第二次被认为是更新操作。\n使用：put ‘表名’,’列族名1:列名1’,’值’\n```\nhbase(main):006:0> put 'truman_test','row1','user:name','truman'\n0 row(s) in 0.1470 seconds\n```\n \n##### 3.2 get获取数据\n功能：获取数据\n使用：\n获取指定rowkey的指定列族指定列的数据\n```\n hbase(main):027:0> get 'truman_test','row1','user:name'\nCOLUMN                     CELL\n user:name                 timestamp=1465268767543, value=truman\n1 row(s) in 0.0180 seconds\n```\n \n获取指定rowkey的指定列族所有的数据\n```\nhbase(main):028:0> get 'truman_test','row1','user'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0150 seconds \n```\n \n获取指定rowkey的所有数据\n```\n hbase(main):029:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0270 seconds\n```\n \n获取指定时间戳的数据\n```\n hbase(main):029:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0270 seconds\n\nhbase(main):030:0> put 'truman_test','row1','user:age','21'\n0 row(s) in 0.0090 seconds\n\nhbase(main):031:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277824052, value=21\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0140 seconds\n\nhbase(main):034:0> get 'truman_test','row1',{COLUMN=>'user:age',TIMESTAMP=>1465277742287}\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n1 row(s) in 0.0350 seconds\n```\n##### 3.3 Count计算表的行数\n功能：计算表的行数\n使用：\n```\n hbase(main):035:0> count 'truman_test'\n1 row(s) in 0.0370 seconds\n\n=> 1\n```\n \n##### 3.4 put更新数据\n详见3.1\n##### 3.5 scan全表扫描数据\n功能：扫描全表所有数据\n使用：\n```\n hbase(main):036:0> scan 'truman_test'\nROW                        COLUMN+CELL\n row1                      column=user:age, timestamp=1465277824052, value=21\n row1                      column=user:name, timestamp=1465268767543, value=truman\n1 row(s) in 0.0310 seconds\n```\n \n##### 3.6 delete删除数据\n功能：删除表中的数据\n使用：\n删除指定rowkey的指定列族的列名的数据\n```\n hbase(main):037:0> delete 'truman_test','row1','user:name'\n0 row(s) in 0.0610 seconds\n```\n \n删除指定rowkey的指定列族的数据\n```\nhbase(main):001:0> delete 'truman_test','row1','user'\n0 row(s) in 0.3730 seconds\n```\n \n##### 3.7 deleteall删除整行数据\n功能：删除整行数据\n使用：\n```\n hbase(main):003:0> deleteall 'truman_test','row1'\n0 row(s) in 0.0080 seconds\n```\n \n##### 3.8 truncate删除全表数据\n功能：删除表中所有的数据。正如你看到的，在HBase的help命令里并没有\n使用：\n```\n hbase(main):005:0> truncate 'truman_test'\nTruncating 'truman_test' table (it may take a while):\n - Disabling table...\n - Truncating table...\n0 row(s) in 1.8220 seconds\n```\n#### 4. HBase Snapshots\n##### 4.1 新增snapshot\n功能: 为指定表新增一个snapshot\n使用：\n```\nhbase(main):001:0> snapshot 'truman', 'trumanSnapshot-1'\n0 row(s) in 1.0820 seconds\n```\n##### 4.2 删除snapshot\n功能: 删除一个snapshot\n使用：\n```\nhbase(main):001:0> delete_snapshot 'trumanSnapshot-1'\n```\n##### 4.4 用snapshot创建一个新表\n功能: Clone a table from snapshot，新表必须不存在。\n使用：\n```\nhbase(main):007:0> clone_snapshot 'trumanSnapshot-1','truman_new'\n0 row(s) in 1.0260 seconds\n```\n##### 4.3 查看所有的snapshot\n功能: \n使用：\n```\nhbase(main):001:0> list_snapshots\nSNAPSHOT                   TABLE + CREATION TIME\n trumanSnapshot-1          truman (Wed Jun 08 01:43:36 -0400 2016)\n1 row(s) in 0.3410 seconds\n\n=> [\"trumanSnapshot-1\"]\n```\n##### 4.4 根据snapshot恢复数据\n功能: 恢复表数据\n使用：执行命令之前，首先要确认表的状态是disable,恢复完成之后，表的状态要更改成enable\n\n```\nhbase(main):008:0> disable 'truman'\n0 row(s) in 1.3080 seconds\n\nhbase(main):009:0> restore_snapshot 'trumanSnapshot-1'\n0 row(s) in 0.8200 seconds\n\nhbase(main):011:0> enable 'truman'\n0 row(s) in 0.4530 seconds\n```\n#### 5. 参考\n1. https://hbase.apache.org/book.html\n2. http://coderbase64.iteye.com/blog/2074601","source":"_posts/HBase常用的操作一-shell.md","raw":"---\ntitle: HBase常用的操作一(shell)\ndate: 2016-06-14 20:27:16\ntags: 大数据\ncategories:\n- hbase\n---\nHBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。\nHBase访问接口如下:\n\n1. Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据\n2. HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用\n3. Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据\n4. REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制\n5. Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计\n6. Hive，当前Hive的Release版本尚没有加入对HBase的支持，在Hive 0.7.0中支持HBase，可以使用类似SQL语言来访问HBase\n本文针对HBase Shell，做一个详细讲解。\n\n<!-- more -->\n\n在hbase安装目录下执行 bin/hbase shell 即可进入shell操作\n#### 1. 一般命令\n##### 1.1status\n功能：查询服务器状态\n使用：\n```\n hbase(main):003:0> status\n4 servers, 0 dead, 1.2500 average load\n```\n \n##### 1.2version\n功能：查询HBase版本信息\n使用：\n```\n hbase(main):004:0> version\n1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015\n```\n##### 1.3 whoami\n功能：查看连接的用户\n#### 2. DDL命令\n##### 2.1 Create创建表\n功能：创建一个表。创建一个表时，不指定具体的列名，但要指定列族名。\n使用：create ‘表名’,’列族名1’,’列族名2’\n```\nhbase(main):005:0> create 'truman_test','user'\n0 row(s) in 1.3680 seconds\n\n=> Hbase::Table - truman_test\n```\n 备注：\n新建不带namespace表\n```\ncreate 'testTable','t1','t2'\n```\n新建带namespace表\n- 首先建一个namespace\n```\ncreate_namespace 'truman'\n```\n- 其次再新建表\n```\ncreate_namespace 'truman：test','t1','t2'\n```\n备注：t1,t2为column family\n```\ncreate'reason:user_test','bs',{NUMREGIONS=>2,SPLITALGO=>'HexStringSplit'}\n\n```\n\n##### 2.2 disable失效表\n功能：失效一个表。当需要修改表结构、删除表时，需要先执行此命令。\n使用：\n```\n hbase(main):007:0> disable 'truman_test'\n0 row(s) in 1.2660 seconds\n\nhbase(main):008:0> describe 'truman_test'\nTable truman_test is DISABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0210 seconds\n```\n \n##### 2.3 enable使失效表有效\n功能：使表有效。在失效表以后，需要执行此命令，以使得表可用。\n使用：\n```\n hbase(main):009:0> enable 'truman_test'\n0 row(s) in 0.4320 seconds\n\nhbase(main):010:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0260 seconds\n```\n \n##### 2.4 alter修改表结构\n功能：修改表结构，包括新增列族、删除列族等\n使用：\n新增列族（记得在执行alter之前，要先disable表）\n```\n hbase(main):011:0> disable 'truman_test'\n0 row(s) in 1.2480 seconds\n\nhbase(main):012:0> alter 'truman_test', 'order'\nUpdating all regions with the new schema...\n1/1 regions updated.\nDone.\n0 row(s) in 1.2320 seconds\n\nhbase(main):013:0> enable 'truman_test'\n0 row(s) in 0.4060 seconds\nhbase(main):014:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'order', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS =>\n'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\n BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n2 row(s) in 0.0240 seconds\n```\n \n删除列族\n```\n hbase(main):015:0> disable 'truman_test'\n0 row(s) in 1.2970 seconds\n\nhbase(main):017:0> alter 'truman_test',{NAME=>'order',method=>'delete'}\nUnknown argument ignored for column family order:\nUpdating all regions with the new schema...\n1/1 regions updated.\nDone.\n0 row(s) in 1.2100 seconds\n\nhbase(main):018:0> enable 'truman_test'\n0 row(s) in 0.4170 seconds\n```\n \n重命名列族\n列族不能被重命名。重命名一个列族的通常途径是使用API创建一个有着期望名称的新的列族，然后将数据复制过去，最后再删除旧的列族。\n##### 2.5 describe查看表结构\n功能：查看表结构\n使用：\n```\n hbase(main):010:0> describe 'truman_test'\nTable truman_test is ENABLED\ntruman_test\nCOLUMN FAMILIES DESCRIPTION\n{NAME => 'user', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '\nFALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0',\nBLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n1 row(s) in 0.0260 seconds\n```\n \n##### 2.6 list列举数据库中的所有表\n功能：查看数据库中所有的表\n使用：\n```\nhbase(main):019:0> list\nTABLE\nreason\ntest\ntruman_test\n3 row(s) in 0.0090 seconds\n\n=> [\"reason\", \"test\", \"truman_test\"]\n```\n \n##### 2.7 drop删除表\n功能：删除指定的表\n使用：\n```\nhbase(main):022:0> disable 'tttt'\n0 row(s) in 1.2370 seconds\n\nhbase(main):023:0> drop 'tttt'\n0 row(s) in 0.2140 seconds\n```\n \n#### 3. DML命令\n##### 3.1 put插入数据\n功能：插入一条数据到指定的表中。对于同一个rowkey，如果执行两次put，则第二次被认为是更新操作。\n使用：put ‘表名’,’列族名1:列名1’,’值’\n```\nhbase(main):006:0> put 'truman_test','row1','user:name','truman'\n0 row(s) in 0.1470 seconds\n```\n \n##### 3.2 get获取数据\n功能：获取数据\n使用：\n获取指定rowkey的指定列族指定列的数据\n```\n hbase(main):027:0> get 'truman_test','row1','user:name'\nCOLUMN                     CELL\n user:name                 timestamp=1465268767543, value=truman\n1 row(s) in 0.0180 seconds\n```\n \n获取指定rowkey的指定列族所有的数据\n```\nhbase(main):028:0> get 'truman_test','row1','user'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0150 seconds \n```\n \n获取指定rowkey的所有数据\n```\n hbase(main):029:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0270 seconds\n```\n \n获取指定时间戳的数据\n```\n hbase(main):029:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0270 seconds\n\nhbase(main):030:0> put 'truman_test','row1','user:age','21'\n0 row(s) in 0.0090 seconds\n\nhbase(main):031:0> get 'truman_test','row1'\nCOLUMN                     CELL\n user:age                  timestamp=1465277824052, value=21\n user:name                 timestamp=1465268767543, value=truman\n2 row(s) in 0.0140 seconds\n\nhbase(main):034:0> get 'truman_test','row1',{COLUMN=>'user:age',TIMESTAMP=>1465277742287}\nCOLUMN                     CELL\n user:age                  timestamp=1465277742287, value=20\n1 row(s) in 0.0350 seconds\n```\n##### 3.3 Count计算表的行数\n功能：计算表的行数\n使用：\n```\n hbase(main):035:0> count 'truman_test'\n1 row(s) in 0.0370 seconds\n\n=> 1\n```\n \n##### 3.4 put更新数据\n详见3.1\n##### 3.5 scan全表扫描数据\n功能：扫描全表所有数据\n使用：\n```\n hbase(main):036:0> scan 'truman_test'\nROW                        COLUMN+CELL\n row1                      column=user:age, timestamp=1465277824052, value=21\n row1                      column=user:name, timestamp=1465268767543, value=truman\n1 row(s) in 0.0310 seconds\n```\n \n##### 3.6 delete删除数据\n功能：删除表中的数据\n使用：\n删除指定rowkey的指定列族的列名的数据\n```\n hbase(main):037:0> delete 'truman_test','row1','user:name'\n0 row(s) in 0.0610 seconds\n```\n \n删除指定rowkey的指定列族的数据\n```\nhbase(main):001:0> delete 'truman_test','row1','user'\n0 row(s) in 0.3730 seconds\n```\n \n##### 3.7 deleteall删除整行数据\n功能：删除整行数据\n使用：\n```\n hbase(main):003:0> deleteall 'truman_test','row1'\n0 row(s) in 0.0080 seconds\n```\n \n##### 3.8 truncate删除全表数据\n功能：删除表中所有的数据。正如你看到的，在HBase的help命令里并没有\n使用：\n```\n hbase(main):005:0> truncate 'truman_test'\nTruncating 'truman_test' table (it may take a while):\n - Disabling table...\n - Truncating table...\n0 row(s) in 1.8220 seconds\n```\n#### 4. HBase Snapshots\n##### 4.1 新增snapshot\n功能: 为指定表新增一个snapshot\n使用：\n```\nhbase(main):001:0> snapshot 'truman', 'trumanSnapshot-1'\n0 row(s) in 1.0820 seconds\n```\n##### 4.2 删除snapshot\n功能: 删除一个snapshot\n使用：\n```\nhbase(main):001:0> delete_snapshot 'trumanSnapshot-1'\n```\n##### 4.4 用snapshot创建一个新表\n功能: Clone a table from snapshot，新表必须不存在。\n使用：\n```\nhbase(main):007:0> clone_snapshot 'trumanSnapshot-1','truman_new'\n0 row(s) in 1.0260 seconds\n```\n##### 4.3 查看所有的snapshot\n功能: \n使用：\n```\nhbase(main):001:0> list_snapshots\nSNAPSHOT                   TABLE + CREATION TIME\n trumanSnapshot-1          truman (Wed Jun 08 01:43:36 -0400 2016)\n1 row(s) in 0.3410 seconds\n\n=> [\"trumanSnapshot-1\"]\n```\n##### 4.4 根据snapshot恢复数据\n功能: 恢复表数据\n使用：执行命令之前，首先要确认表的状态是disable,恢复完成之后，表的状态要更改成enable\n\n```\nhbase(main):008:0> disable 'truman'\n0 row(s) in 1.3080 seconds\n\nhbase(main):009:0> restore_snapshot 'trumanSnapshot-1'\n0 row(s) in 0.8200 seconds\n\nhbase(main):011:0> enable 'truman'\n0 row(s) in 0.4530 seconds\n```\n#### 5. 参考\n1. https://hbase.apache.org/book.html\n2. http://coderbase64.iteye.com/blog/2074601","slug":"HBase常用的操作一-shell","published":1,"updated":"2016-06-14T12:29:35.700Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvaw00268ceedsojr8ys","content":"<p>HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。<br>HBase访问接口如下:</p>\n<ol>\n<li>Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据</li>\n<li>HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用</li>\n<li>Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据</li>\n<li>REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制</li>\n<li>Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计</li>\n<li>Hive，当前Hive的Release版本尚没有加入对HBase的支持，在Hive 0.7.0中支持HBase，可以使用类似SQL语言来访问HBase<br>本文针对HBase Shell，做一个详细讲解。</li>\n</ol>\n<a id=\"more\"></a>\n<p>在hbase安装目录下执行 bin/hbase shell 即可进入shell操作</p>\n<h4 id=\"1-一般命令\"><a href=\"#1-一般命令\" class=\"headerlink\" title=\"1. 一般命令\"></a>1. 一般命令</h4><h5 id=\"1-1status\"><a href=\"#1-1status\" class=\"headerlink\" title=\"1.1status\"></a>1.1status</h5><p>功能：查询服务器状态<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):003:0&gt; status</span><br><span class=\"line\">4 servers, 0 dead, 1.2500 average load</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2version\"><a href=\"#1-2version\" class=\"headerlink\" title=\"1.2version\"></a>1.2version</h5><p>功能：查询HBase版本信息<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):004:0&gt; version</span><br><span class=\"line\">1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-3-whoami\"><a href=\"#1-3-whoami\" class=\"headerlink\" title=\"1.3 whoami\"></a>1.3 whoami</h5><p>功能：查看连接的用户</p>\n<h4 id=\"2-DDL命令\"><a href=\"#2-DDL命令\" class=\"headerlink\" title=\"2. DDL命令\"></a>2. DDL命令</h4><h5 id=\"2-1-Create创建表\"><a href=\"#2-1-Create创建表\" class=\"headerlink\" title=\"2.1 Create创建表\"></a>2.1 Create创建表</h5><p>功能：创建一个表。创建一个表时，不指定具体的列名，但要指定列族名。<br>使用：create ‘表名’,’列族名1’,’列族名2’<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):005:0&gt; create &apos;truman_test&apos;,&apos;user&apos;</span><br><span class=\"line\">0 row(s) in 1.3680 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; Hbase::Table - truman_test</span><br></pre></td></tr></table></figure></p>\n<p> 备注：<br>新建不带namespace表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create &apos;testTable&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure></p>\n<p>新建带namespace表</p>\n<ul>\n<li><p>首先建一个namespace</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>其次再新建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman：test&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>备注：t1,t2为column family<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create&apos;reason:user_test&apos;,&apos;bs&apos;,&#123;NUMREGIONS=&gt;2,SPLITALGO=&gt;&apos;HexStringSplit&apos;&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-2-disable失效表\"><a href=\"#2-2-disable失效表\" class=\"headerlink\" title=\"2.2 disable失效表\"></a>2.2 disable失效表</h5><p>功能：失效一个表。当需要修改表结构、删除表时，需要先执行此命令。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):007:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2660 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):008:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is DISABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0210 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-3-enable使失效表有效\"><a href=\"#2-3-enable使失效表有效\" class=\"headerlink\" title=\"2.3 enable使失效表有效\"></a>2.3 enable使失效表有效</h5><p>功能：使表有效。在失效表以后，需要执行此命令，以使得表可用。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):009:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4320 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):010:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-4-alter修改表结构\"><a href=\"#2-4-alter修改表结构\" class=\"headerlink\" title=\"2.4 alter修改表结构\"></a>2.4 alter修改表结构</h5><p>功能：修改表结构，包括新增列族、删除列族等<br>使用：<br>新增列族（记得在执行alter之前，要先disable表）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):011:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2480 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):012:0&gt; alter &apos;truman_test&apos;, &apos;order&apos;</span><br><span class=\"line\">Updating all regions with the new schema...</span><br><span class=\"line\">1/1 regions updated.</span><br><span class=\"line\">Done.</span><br><span class=\"line\">0 row(s) in 1.2320 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):013:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4060 seconds</span><br><span class=\"line\">hbase(main):014:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;order&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt;</span><br><span class=\"line\">&apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\"> BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">2 row(s) in 0.0240 seconds</span><br></pre></td></tr></table></figure></p>\n<p>删除列族<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):015:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2970 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):017:0&gt; alter &apos;truman_test&apos;,&#123;NAME=&gt;&apos;order&apos;,method=&gt;&apos;delete&apos;&#125;</span><br><span class=\"line\">Unknown argument ignored for column family order:</span><br><span class=\"line\">Updating all regions with the new schema...</span><br><span class=\"line\">1/1 regions updated.</span><br><span class=\"line\">Done.</span><br><span class=\"line\">0 row(s) in 1.2100 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):018:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4170 seconds</span><br></pre></td></tr></table></figure></p>\n<p>重命名列族<br>列族不能被重命名。重命名一个列族的通常途径是使用API创建一个有着期望名称的新的列族，然后将数据复制过去，最后再删除旧的列族。</p>\n<h5 id=\"2-5-describe查看表结构\"><a href=\"#2-5-describe查看表结构\" class=\"headerlink\" title=\"2.5 describe查看表结构\"></a>2.5 describe查看表结构</h5><p>功能：查看表结构<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):010:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-6-list列举数据库中的所有表\"><a href=\"#2-6-list列举数据库中的所有表\" class=\"headerlink\" title=\"2.6 list列举数据库中的所有表\"></a>2.6 list列举数据库中的所有表</h5><p>功能：查看数据库中所有的表<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):019:0&gt; list</span><br><span class=\"line\">TABLE</span><br><span class=\"line\">reason</span><br><span class=\"line\">test</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">3 row(s) in 0.0090 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; [&quot;reason&quot;, &quot;test&quot;, &quot;truman_test&quot;]</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-7-drop删除表\"><a href=\"#2-7-drop删除表\" class=\"headerlink\" title=\"2.7 drop删除表\"></a>2.7 drop删除表</h5><p>功能：删除指定的表<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):022:0&gt; disable &apos;tttt&apos;</span><br><span class=\"line\">0 row(s) in 1.2370 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):023:0&gt; drop &apos;tttt&apos;</span><br><span class=\"line\">0 row(s) in 0.2140 seconds</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-DML命令\"><a href=\"#3-DML命令\" class=\"headerlink\" title=\"3. DML命令\"></a>3. DML命令</h4><h5 id=\"3-1-put插入数据\"><a href=\"#3-1-put插入数据\" class=\"headerlink\" title=\"3.1 put插入数据\"></a>3.1 put插入数据</h5><p>功能：插入一条数据到指定的表中。对于同一个rowkey，如果执行两次put，则第二次被认为是更新操作。<br>使用：put ‘表名’,’列族名1:列名1’,’值’<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):006:0&gt; put &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;,&apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 0.1470 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-2-get获取数据\"><a href=\"#3-2-get获取数据\" class=\"headerlink\" title=\"3.2 get获取数据\"></a>3.2 get获取数据</h5><p>功能：获取数据<br>使用：<br>获取指定rowkey的指定列族指定列的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):027:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">1 row(s) in 0.0180 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定rowkey的指定列族所有的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):028:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&apos;user&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0150 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定rowkey的所有数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):029:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0270 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定时间戳的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):029:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0270 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):030:0&gt; put &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:age&apos;,&apos;21&apos;</span><br><span class=\"line\">0 row(s) in 0.0090 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):031:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277824052, value=21</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0140 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):034:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&#123;COLUMN=&gt;&apos;user:age&apos;,TIMESTAMP=&gt;1465277742287&#125;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\">1 row(s) in 0.0350 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-3-Count计算表的行数\"><a href=\"#3-3-Count计算表的行数\" class=\"headerlink\" title=\"3.3 Count计算表的行数\"></a>3.3 Count计算表的行数</h5><p>功能：计算表的行数<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):035:0&gt; count &apos;truman_test&apos;</span><br><span class=\"line\">1 row(s) in 0.0370 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; 1</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-4-put更新数据\"><a href=\"#3-4-put更新数据\" class=\"headerlink\" title=\"3.4 put更新数据\"></a>3.4 put更新数据</h5><p>详见3.1</p>\n<h5 id=\"3-5-scan全表扫描数据\"><a href=\"#3-5-scan全表扫描数据\" class=\"headerlink\" title=\"3.5 scan全表扫描数据\"></a>3.5 scan全表扫描数据</h5><p>功能：扫描全表所有数据<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):036:0&gt; scan &apos;truman_test&apos;</span><br><span class=\"line\">ROW                        COLUMN+CELL</span><br><span class=\"line\"> row1                      column=user:age, timestamp=1465277824052, value=21</span><br><span class=\"line\"> row1                      column=user:name, timestamp=1465268767543, value=truman</span><br><span class=\"line\">1 row(s) in 0.0310 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-6-delete删除数据\"><a href=\"#3-6-delete删除数据\" class=\"headerlink\" title=\"3.6 delete删除数据\"></a>3.6 delete删除数据</h5><p>功能：删除表中的数据<br>使用：<br>删除指定rowkey的指定列族的列名的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):037:0&gt; delete &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;</span><br><span class=\"line\">0 row(s) in 0.0610 seconds</span><br></pre></td></tr></table></figure></p>\n<p>删除指定rowkey的指定列族的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; delete &apos;truman_test&apos;,&apos;row1&apos;,&apos;user&apos;</span><br><span class=\"line\">0 row(s) in 0.3730 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-7-deleteall删除整行数据\"><a href=\"#3-7-deleteall删除整行数据\" class=\"headerlink\" title=\"3.7 deleteall删除整行数据\"></a>3.7 deleteall删除整行数据</h5><p>功能：删除整行数据<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):003:0&gt; deleteall &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">0 row(s) in 0.0080 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-8-truncate删除全表数据\"><a href=\"#3-8-truncate删除全表数据\" class=\"headerlink\" title=\"3.8 truncate删除全表数据\"></a>3.8 truncate删除全表数据</h5><p>功能：删除表中所有的数据。正如你看到的，在HBase的help命令里并没有<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):005:0&gt; truncate &apos;truman_test&apos;</span><br><span class=\"line\">Truncating &apos;truman_test&apos; table (it may take a while):</span><br><span class=\"line\"> - Disabling table...</span><br><span class=\"line\"> - Truncating table...</span><br><span class=\"line\">0 row(s) in 1.8220 seconds</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-HBase-Snapshots\"><a href=\"#4-HBase-Snapshots\" class=\"headerlink\" title=\"4. HBase Snapshots\"></a>4. HBase Snapshots</h4><h5 id=\"4-1-新增snapshot\"><a href=\"#4-1-新增snapshot\" class=\"headerlink\" title=\"4.1 新增snapshot\"></a>4.1 新增snapshot</h5><p>功能: 为指定表新增一个snapshot<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; snapshot &apos;truman&apos;, &apos;trumanSnapshot-1&apos;</span><br><span class=\"line\">0 row(s) in 1.0820 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-2-删除snapshot\"><a href=\"#4-2-删除snapshot\" class=\"headerlink\" title=\"4.2 删除snapshot\"></a>4.2 删除snapshot</h5><p>功能: 删除一个snapshot<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; delete_snapshot &apos;trumanSnapshot-1&apos;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-4-用snapshot创建一个新表\"><a href=\"#4-4-用snapshot创建一个新表\" class=\"headerlink\" title=\"4.4 用snapshot创建一个新表\"></a>4.4 用snapshot创建一个新表</h5><p>功能: Clone a table from snapshot，新表必须不存在。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):007:0&gt; clone_snapshot &apos;trumanSnapshot-1&apos;,&apos;truman_new&apos;</span><br><span class=\"line\">0 row(s) in 1.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-3-查看所有的snapshot\"><a href=\"#4-3-查看所有的snapshot\" class=\"headerlink\" title=\"4.3 查看所有的snapshot\"></a>4.3 查看所有的snapshot</h5><p>功能:<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; list_snapshots</span><br><span class=\"line\">SNAPSHOT                   TABLE + CREATION TIME</span><br><span class=\"line\"> trumanSnapshot-1          truman (Wed Jun 08 01:43:36 -0400 2016)</span><br><span class=\"line\">1 row(s) in 0.3410 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; [&quot;trumanSnapshot-1&quot;]</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-4-根据snapshot恢复数据\"><a href=\"#4-4-根据snapshot恢复数据\" class=\"headerlink\" title=\"4.4 根据snapshot恢复数据\"></a>4.4 根据snapshot恢复数据</h5><p>功能: 恢复表数据<br>使用：执行命令之前，首先要确认表的状态是disable,恢复完成之后，表的状态要更改成enable</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):008:0&gt; disable &apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 1.3080 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):009:0&gt; restore_snapshot &apos;trumanSnapshot-1&apos;</span><br><span class=\"line\">0 row(s) in 0.8200 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):011:0&gt; enable &apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 0.4530 seconds</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-参考\"><a href=\"#5-参考\" class=\"headerlink\" title=\"5. 参考\"></a>5. 参考</h4><ol>\n<li><a href=\"https://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">https://hbase.apache.org/book.html</a></li>\n<li><a href=\"http://coderbase64.iteye.com/blog/2074601\" target=\"_blank\" rel=\"noopener\">http://coderbase64.iteye.com/blog/2074601</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>HBase – Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群。<br>HBase访问接口如下:</p>\n<ol>\n<li>Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据</li>\n<li>HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用</li>\n<li>Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据</li>\n<li>REST Gateway，支持REST 风格的Http API访问HBase, 解除了语言限制</li>\n<li>Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计</li>\n<li>Hive，当前Hive的Release版本尚没有加入对HBase的支持，在Hive 0.7.0中支持HBase，可以使用类似SQL语言来访问HBase<br>本文针对HBase Shell，做一个详细讲解。</li>\n</ol>","more":"<p>在hbase安装目录下执行 bin/hbase shell 即可进入shell操作</p>\n<h4 id=\"1-一般命令\"><a href=\"#1-一般命令\" class=\"headerlink\" title=\"1. 一般命令\"></a>1. 一般命令</h4><h5 id=\"1-1status\"><a href=\"#1-1status\" class=\"headerlink\" title=\"1.1status\"></a>1.1status</h5><p>功能：查询服务器状态<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):003:0&gt; status</span><br><span class=\"line\">4 servers, 0 dead, 1.2500 average load</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-2version\"><a href=\"#1-2version\" class=\"headerlink\" title=\"1.2version\"></a>1.2version</h5><p>功能：查询HBase版本信息<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):004:0&gt; version</span><br><span class=\"line\">1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"1-3-whoami\"><a href=\"#1-3-whoami\" class=\"headerlink\" title=\"1.3 whoami\"></a>1.3 whoami</h5><p>功能：查看连接的用户</p>\n<h4 id=\"2-DDL命令\"><a href=\"#2-DDL命令\" class=\"headerlink\" title=\"2. DDL命令\"></a>2. DDL命令</h4><h5 id=\"2-1-Create创建表\"><a href=\"#2-1-Create创建表\" class=\"headerlink\" title=\"2.1 Create创建表\"></a>2.1 Create创建表</h5><p>功能：创建一个表。创建一个表时，不指定具体的列名，但要指定列族名。<br>使用：create ‘表名’,’列族名1’,’列族名2’<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):005:0&gt; create &apos;truman_test&apos;,&apos;user&apos;</span><br><span class=\"line\">0 row(s) in 1.3680 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; Hbase::Table - truman_test</span><br></pre></td></tr></table></figure></p>\n<p> 备注：<br>新建不带namespace表<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create &apos;testTable&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure></p>\n<p>新建带namespace表</p>\n<ul>\n<li><p>首先建一个namespace</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman&apos;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>其次再新建表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create_namespace &apos;truman：test&apos;,&apos;t1&apos;,&apos;t2&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>备注：t1,t2为column family<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create&apos;reason:user_test&apos;,&apos;bs&apos;,&#123;NUMREGIONS=&gt;2,SPLITALGO=&gt;&apos;HexStringSplit&apos;&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-2-disable失效表\"><a href=\"#2-2-disable失效表\" class=\"headerlink\" title=\"2.2 disable失效表\"></a>2.2 disable失效表</h5><p>功能：失效一个表。当需要修改表结构、删除表时，需要先执行此命令。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):007:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2660 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):008:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is DISABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0210 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-3-enable使失效表有效\"><a href=\"#2-3-enable使失效表有效\" class=\"headerlink\" title=\"2.3 enable使失效表有效\"></a>2.3 enable使失效表有效</h5><p>功能：使表有效。在失效表以后，需要执行此命令，以使得表可用。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):009:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4320 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):010:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-4-alter修改表结构\"><a href=\"#2-4-alter修改表结构\" class=\"headerlink\" title=\"2.4 alter修改表结构\"></a>2.4 alter修改表结构</h5><p>功能：修改表结构，包括新增列族、删除列族等<br>使用：<br>新增列族（记得在执行alter之前，要先disable表）<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):011:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2480 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):012:0&gt; alter &apos;truman_test&apos;, &apos;order&apos;</span><br><span class=\"line\">Updating all regions with the new schema...</span><br><span class=\"line\">1/1 regions updated.</span><br><span class=\"line\">Done.</span><br><span class=\"line\">0 row(s) in 1.2320 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):013:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4060 seconds</span><br><span class=\"line\">hbase(main):014:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;order&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt;</span><br><span class=\"line\">&apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\"> BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">2 row(s) in 0.0240 seconds</span><br></pre></td></tr></table></figure></p>\n<p>删除列族<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):015:0&gt; disable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 1.2970 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):017:0&gt; alter &apos;truman_test&apos;,&#123;NAME=&gt;&apos;order&apos;,method=&gt;&apos;delete&apos;&#125;</span><br><span class=\"line\">Unknown argument ignored for column family order:</span><br><span class=\"line\">Updating all regions with the new schema...</span><br><span class=\"line\">1/1 regions updated.</span><br><span class=\"line\">Done.</span><br><span class=\"line\">0 row(s) in 1.2100 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):018:0&gt; enable &apos;truman_test&apos;</span><br><span class=\"line\">0 row(s) in 0.4170 seconds</span><br></pre></td></tr></table></figure></p>\n<p>重命名列族<br>列族不能被重命名。重命名一个列族的通常途径是使用API创建一个有着期望名称的新的列族，然后将数据复制过去，最后再删除旧的列族。</p>\n<h5 id=\"2-5-describe查看表结构\"><a href=\"#2-5-describe查看表结构\" class=\"headerlink\" title=\"2.5 describe查看表结构\"></a>2.5 describe查看表结构</h5><p>功能：查看表结构<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):010:0&gt; describe &apos;truman_test&apos;</span><br><span class=\"line\">Table truman_test is ENABLED</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION</span><br><span class=\"line\">&#123;NAME =&gt; &apos;user&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;</span><br><span class=\"line\">FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;,</span><br><span class=\"line\">BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;</span><br><span class=\"line\">1 row(s) in 0.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-6-list列举数据库中的所有表\"><a href=\"#2-6-list列举数据库中的所有表\" class=\"headerlink\" title=\"2.6 list列举数据库中的所有表\"></a>2.6 list列举数据库中的所有表</h5><p>功能：查看数据库中所有的表<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):019:0&gt; list</span><br><span class=\"line\">TABLE</span><br><span class=\"line\">reason</span><br><span class=\"line\">test</span><br><span class=\"line\">truman_test</span><br><span class=\"line\">3 row(s) in 0.0090 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; [&quot;reason&quot;, &quot;test&quot;, &quot;truman_test&quot;]</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"2-7-drop删除表\"><a href=\"#2-7-drop删除表\" class=\"headerlink\" title=\"2.7 drop删除表\"></a>2.7 drop删除表</h5><p>功能：删除指定的表<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):022:0&gt; disable &apos;tttt&apos;</span><br><span class=\"line\">0 row(s) in 1.2370 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):023:0&gt; drop &apos;tttt&apos;</span><br><span class=\"line\">0 row(s) in 0.2140 seconds</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-DML命令\"><a href=\"#3-DML命令\" class=\"headerlink\" title=\"3. DML命令\"></a>3. DML命令</h4><h5 id=\"3-1-put插入数据\"><a href=\"#3-1-put插入数据\" class=\"headerlink\" title=\"3.1 put插入数据\"></a>3.1 put插入数据</h5><p>功能：插入一条数据到指定的表中。对于同一个rowkey，如果执行两次put，则第二次被认为是更新操作。<br>使用：put ‘表名’,’列族名1:列名1’,’值’<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):006:0&gt; put &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;,&apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 0.1470 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-2-get获取数据\"><a href=\"#3-2-get获取数据\" class=\"headerlink\" title=\"3.2 get获取数据\"></a>3.2 get获取数据</h5><p>功能：获取数据<br>使用：<br>获取指定rowkey的指定列族指定列的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):027:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">1 row(s) in 0.0180 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定rowkey的指定列族所有的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):028:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&apos;user&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0150 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定rowkey的所有数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):029:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0270 seconds</span><br></pre></td></tr></table></figure></p>\n<p>获取指定时间戳的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):029:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0270 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):030:0&gt; put &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:age&apos;,&apos;21&apos;</span><br><span class=\"line\">0 row(s) in 0.0090 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):031:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277824052, value=21</span><br><span class=\"line\"> user:name                 timestamp=1465268767543, value=truman</span><br><span class=\"line\">2 row(s) in 0.0140 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):034:0&gt; get &apos;truman_test&apos;,&apos;row1&apos;,&#123;COLUMN=&gt;&apos;user:age&apos;,TIMESTAMP=&gt;1465277742287&#125;</span><br><span class=\"line\">COLUMN                     CELL</span><br><span class=\"line\"> user:age                  timestamp=1465277742287, value=20</span><br><span class=\"line\">1 row(s) in 0.0350 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-3-Count计算表的行数\"><a href=\"#3-3-Count计算表的行数\" class=\"headerlink\" title=\"3.3 Count计算表的行数\"></a>3.3 Count计算表的行数</h5><p>功能：计算表的行数<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):035:0&gt; count &apos;truman_test&apos;</span><br><span class=\"line\">1 row(s) in 0.0370 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; 1</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-4-put更新数据\"><a href=\"#3-4-put更新数据\" class=\"headerlink\" title=\"3.4 put更新数据\"></a>3.4 put更新数据</h5><p>详见3.1</p>\n<h5 id=\"3-5-scan全表扫描数据\"><a href=\"#3-5-scan全表扫描数据\" class=\"headerlink\" title=\"3.5 scan全表扫描数据\"></a>3.5 scan全表扫描数据</h5><p>功能：扫描全表所有数据<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):036:0&gt; scan &apos;truman_test&apos;</span><br><span class=\"line\">ROW                        COLUMN+CELL</span><br><span class=\"line\"> row1                      column=user:age, timestamp=1465277824052, value=21</span><br><span class=\"line\"> row1                      column=user:name, timestamp=1465268767543, value=truman</span><br><span class=\"line\">1 row(s) in 0.0310 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-6-delete删除数据\"><a href=\"#3-6-delete删除数据\" class=\"headerlink\" title=\"3.6 delete删除数据\"></a>3.6 delete删除数据</h5><p>功能：删除表中的数据<br>使用：<br>删除指定rowkey的指定列族的列名的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):037:0&gt; delete &apos;truman_test&apos;,&apos;row1&apos;,&apos;user:name&apos;</span><br><span class=\"line\">0 row(s) in 0.0610 seconds</span><br></pre></td></tr></table></figure></p>\n<p>删除指定rowkey的指定列族的数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; delete &apos;truman_test&apos;,&apos;row1&apos;,&apos;user&apos;</span><br><span class=\"line\">0 row(s) in 0.3730 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-7-deleteall删除整行数据\"><a href=\"#3-7-deleteall删除整行数据\" class=\"headerlink\" title=\"3.7 deleteall删除整行数据\"></a>3.7 deleteall删除整行数据</h5><p>功能：删除整行数据<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):003:0&gt; deleteall &apos;truman_test&apos;,&apos;row1&apos;</span><br><span class=\"line\">0 row(s) in 0.0080 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"3-8-truncate删除全表数据\"><a href=\"#3-8-truncate删除全表数据\" class=\"headerlink\" title=\"3.8 truncate删除全表数据\"></a>3.8 truncate删除全表数据</h5><p>功能：删除表中所有的数据。正如你看到的，在HBase的help命令里并没有<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> hbase(main):005:0&gt; truncate &apos;truman_test&apos;</span><br><span class=\"line\">Truncating &apos;truman_test&apos; table (it may take a while):</span><br><span class=\"line\"> - Disabling table...</span><br><span class=\"line\"> - Truncating table...</span><br><span class=\"line\">0 row(s) in 1.8220 seconds</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-HBase-Snapshots\"><a href=\"#4-HBase-Snapshots\" class=\"headerlink\" title=\"4. HBase Snapshots\"></a>4. HBase Snapshots</h4><h5 id=\"4-1-新增snapshot\"><a href=\"#4-1-新增snapshot\" class=\"headerlink\" title=\"4.1 新增snapshot\"></a>4.1 新增snapshot</h5><p>功能: 为指定表新增一个snapshot<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; snapshot &apos;truman&apos;, &apos;trumanSnapshot-1&apos;</span><br><span class=\"line\">0 row(s) in 1.0820 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-2-删除snapshot\"><a href=\"#4-2-删除snapshot\" class=\"headerlink\" title=\"4.2 删除snapshot\"></a>4.2 删除snapshot</h5><p>功能: 删除一个snapshot<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; delete_snapshot &apos;trumanSnapshot-1&apos;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-4-用snapshot创建一个新表\"><a href=\"#4-4-用snapshot创建一个新表\" class=\"headerlink\" title=\"4.4 用snapshot创建一个新表\"></a>4.4 用snapshot创建一个新表</h5><p>功能: Clone a table from snapshot，新表必须不存在。<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):007:0&gt; clone_snapshot &apos;trumanSnapshot-1&apos;,&apos;truman_new&apos;</span><br><span class=\"line\">0 row(s) in 1.0260 seconds</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-3-查看所有的snapshot\"><a href=\"#4-3-查看所有的snapshot\" class=\"headerlink\" title=\"4.3 查看所有的snapshot\"></a>4.3 查看所有的snapshot</h5><p>功能:<br>使用：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):001:0&gt; list_snapshots</span><br><span class=\"line\">SNAPSHOT                   TABLE + CREATION TIME</span><br><span class=\"line\"> trumanSnapshot-1          truman (Wed Jun 08 01:43:36 -0400 2016)</span><br><span class=\"line\">1 row(s) in 0.3410 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">=&gt; [&quot;trumanSnapshot-1&quot;]</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"4-4-根据snapshot恢复数据\"><a href=\"#4-4-根据snapshot恢复数据\" class=\"headerlink\" title=\"4.4 根据snapshot恢复数据\"></a>4.4 根据snapshot恢复数据</h5><p>功能: 恢复表数据<br>使用：执行命令之前，首先要确认表的状态是disable,恢复完成之后，表的状态要更改成enable</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hbase(main):008:0&gt; disable &apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 1.3080 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):009:0&gt; restore_snapshot &apos;trumanSnapshot-1&apos;</span><br><span class=\"line\">0 row(s) in 0.8200 seconds</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):011:0&gt; enable &apos;truman&apos;</span><br><span class=\"line\">0 row(s) in 0.4530 seconds</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-参考\"><a href=\"#5-参考\" class=\"headerlink\" title=\"5. 参考\"></a>5. 参考</h4><ol>\n<li><a href=\"https://hbase.apache.org/book.html\" target=\"_blank\" rel=\"noopener\">https://hbase.apache.org/book.html</a></li>\n<li><a href=\"http://coderbase64.iteye.com/blog/2074601\" target=\"_blank\" rel=\"noopener\">http://coderbase64.iteye.com/blog/2074601</a></li>\n</ol>"},{"title":"HBase系统架构与存储格式","date":"2016-06-14T11:22:51.000Z","_content":"HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。\n\n<!-- more -->\n\n## HBase数据模型\n\n### Table & Region\n\n当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/1.jpg)\n\n### -ROOT- && .META. Table\n\n\n\nHBase中有两张特殊的Table，-ROOT-和.META.\n\nØ  .META.：记录了用户表的Region信息，.META.表本身可以有多个regoin\n\nØ  -ROOT-：记录了.META.表的Region信息，-ROOT-表本身只有一个region\n\nØ  Zookeeper中记录了-ROOT-表的location\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/2.jpg)\n\nClient访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存。\n\n### MapReduce on HBase\n\n在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如下图：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/3.jpg)\n\nHBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。\n\n读者可以将这张图与我们之前讲解Hadoop中MapReduce工作流程时的图解进行对比。\n\n## HBase系统架构\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/4.jpg)\n\n### Client\n\nHBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC\n\n### Zookeeper\n\nZookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题，见下文描述\n\n### HMaster\n\nHMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：\n\n    1. 管理用户对Table的增、删、改、查操作\n\n    2. 管理HRegionServer的负载均衡，调整Region分布\n\n    3. 在Region Split后，负责新Region的分配\n\n    4. 在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移\n\n### HRegionServer\n\nHRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/5.jpg)\nHRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。\n\nHStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：\n\n![Image.gif](http://o8fwlo8a3.bkt.clouddn.com/6.gif)\n\n在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。\n\n## HBase存储格式\n\n\n\nHBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：\n\n1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile\n\n2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File\n\n### HFile\n\n下图是HFile的存储格式：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/7.jpg)\n\n\n\n首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数 据块的起始点。File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。Data Index和Meta Index块记录了每个Data块和Meta块的起始点。\n\nData Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。\n\nHFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/8.jpg)\n\n开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。\n\n### HLogFile\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/9.jpg)\n\n上图中示意了HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。\n\nHLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 \n## 参考\n1. http://www.tianshouzhi.com/api/tutorials/hbase/142\n","source":"_posts/HBase系统架构与存储格式.md","raw":"---\ntitle: HBase系统架构与存储格式\ndate: 2016-06-14 19:22:51\ntags: 大数据\ncategories:\n- hbase\n---\nHBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。\n\n<!-- more -->\n\n## HBase数据模型\n\n### Table & Region\n\n当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/1.jpg)\n\n### -ROOT- && .META. Table\n\n\n\nHBase中有两张特殊的Table，-ROOT-和.META.\n\nØ  .META.：记录了用户表的Region信息，.META.表本身可以有多个regoin\n\nØ  -ROOT-：记录了.META.表的Region信息，-ROOT-表本身只有一个region\n\nØ  Zookeeper中记录了-ROOT-表的location\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/2.jpg)\n\nClient访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存。\n\n### MapReduce on HBase\n\n在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如下图：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/3.jpg)\n\nHBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。\n\n读者可以将这张图与我们之前讲解Hadoop中MapReduce工作流程时的图解进行对比。\n\n## HBase系统架构\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/4.jpg)\n\n### Client\n\nHBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC\n\n### Zookeeper\n\nZookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题，见下文描述\n\n### HMaster\n\nHMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：\n\n    1. 管理用户对Table的增、删、改、查操作\n\n    2. 管理HRegionServer的负载均衡，调整Region分布\n\n    3. 在Region Split后，负责新Region的分配\n\n    4. 在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移\n\n### HRegionServer\n\nHRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/5.jpg)\nHRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。\n\nHStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：\n\n![Image.gif](http://o8fwlo8a3.bkt.clouddn.com/6.gif)\n\n在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。\n\n## HBase存储格式\n\n\n\nHBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：\n\n1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile\n\n2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File\n\n### HFile\n\n下图是HFile的存储格式：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/7.jpg)\n\n\n\n首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数 据块的起始点。File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。Data Index和Meta Index块记录了每个Data块和Meta块的起始点。\n\nData Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。\n\nHFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/8.jpg)\n\n开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。\n\n### HLogFile\n\n![Image.jpg](http://o8fwlo8a3.bkt.clouddn.com/9.jpg)\n\n上图中示意了HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。\n\nHLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 \n## 参考\n1. http://www.tianshouzhi.com/api/tutorials/hbase/142\n","slug":"HBase系统架构与存储格式","published":1,"updated":"2016-06-14T11:24:23.041Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvb0002a8ceenfphscxu","content":"<p>HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。</p>\n<a id=\"more\"></a>\n<h2 id=\"HBase数据模型\"><a href=\"#HBase数据模型\" class=\"headerlink\" title=\"HBase数据模型\"></a>HBase数据模型</h2><h3 id=\"Table-amp-Region\"><a href=\"#Table-amp-Region\" class=\"headerlink\" title=\"Table &amp; Region\"></a>Table &amp; Region</h3><p>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/1.jpg\" alt=\"Image.jpg\"></p>\n<h3 id=\"ROOT-amp-amp-META-Table\"><a href=\"#ROOT-amp-amp-META-Table\" class=\"headerlink\" title=\"-ROOT- &amp;&amp; .META. Table\"></a>-ROOT- &amp;&amp; .META. Table</h3><p>HBase中有两张特殊的Table，-ROOT-和.META.</p>\n<p>Ø  .META.：记录了用户表的Region信息，.META.表本身可以有多个regoin</p>\n<p>Ø  -ROOT-：记录了.META.表的Region信息，-ROOT-表本身只有一个region</p>\n<p>Ø  Zookeeper中记录了-ROOT-表的location</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/2.jpg\" alt=\"Image.jpg\"></p>\n<p>Client访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存。</p>\n<h3 id=\"MapReduce-on-HBase\"><a href=\"#MapReduce-on-HBase\" class=\"headerlink\" title=\"MapReduce on HBase\"></a>MapReduce on HBase</h3><p>在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如下图：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/3.jpg\" alt=\"Image.jpg\"></p>\n<p>HBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。</p>\n<p>读者可以将这张图与我们之前讲解Hadoop中MapReduce工作流程时的图解进行对比。</p>\n<h2 id=\"HBase系统架构\"><a href=\"#HBase系统架构\" class=\"headerlink\" title=\"HBase系统架构\"></a>HBase系统架构</h2><p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/4.jpg\" alt=\"Image.jpg\"></p>\n<h3 id=\"Client\"><a href=\"#Client\" class=\"headerlink\" title=\"Client\"></a>Client</h3><p>HBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC</p>\n<h3 id=\"Zookeeper\"><a href=\"#Zookeeper\" class=\"headerlink\" title=\"Zookeeper\"></a>Zookeeper</h3><p>Zookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题，见下文描述</p>\n<h3 id=\"HMaster\"><a href=\"#HMaster\" class=\"headerlink\" title=\"HMaster\"></a>HMaster</h3><p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：</p>\n<pre><code>1. 管理用户对Table的增、删、改、查操作\n\n2. 管理HRegionServer的负载均衡，调整Region分布\n\n3. 在Region Split后，负责新Region的分配\n\n4. 在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移\n</code></pre><h3 id=\"HRegionServer\"><a href=\"#HRegionServer\" class=\"headerlink\" title=\"HRegionServer\"></a>HRegionServer</h3><p>HRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/5.jpg\" alt=\"Image.jpg\"><br>HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。</p>\n<p>HStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/6.gif\" alt=\"Image.gif\"></p>\n<p>在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>\n<h2 id=\"HBase存储格式\"><a href=\"#HBase存储格式\" class=\"headerlink\" title=\"HBase存储格式\"></a>HBase存储格式</h2><p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：</p>\n<p>1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile</p>\n<p>2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File</p>\n<h3 id=\"HFile\"><a href=\"#HFile\" class=\"headerlink\" title=\"HFile\"></a>HFile</h3><p>下图是HFile的存储格式：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/7.jpg\" alt=\"Image.jpg\"></p>\n<p>首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数 据块的起始点。File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。Data Index和Meta Index块记录了每个Data块和Meta块的起始点。</p>\n<p>Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。</p>\n<p>HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/8.jpg\" alt=\"Image.jpg\"></p>\n<p>开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。</p>\n<h3 id=\"HLogFile\"><a href=\"#HLogFile\" class=\"headerlink\" title=\"HLogFile\"></a>HLogFile</h3><p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/9.jpg\" alt=\"Image.jpg\"></p>\n<p>上图中示意了HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。</p>\n<p>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.tianshouzhi.com/api/tutorials/hbase/142\" target=\"_blank\" rel=\"noopener\">http://www.tianshouzhi.com/api/tutorials/hbase/142</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。</p>","more":"<h2 id=\"HBase数据模型\"><a href=\"#HBase数据模型\" class=\"headerlink\" title=\"HBase数据模型\"></a>HBase数据模型</h2><h3 id=\"Table-amp-Region\"><a href=\"#Table-amp-Region\" class=\"headerlink\" title=\"Table &amp; Region\"></a>Table &amp; Region</h3><p>当Table随着记录数不断增加而变大后，会逐渐分裂成多份splits，成为regions，一个region由[startkey,endkey)表示，不同的region会被Master分配给相应的RegionServer进行管理：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/1.jpg\" alt=\"Image.jpg\"></p>\n<h3 id=\"ROOT-amp-amp-META-Table\"><a href=\"#ROOT-amp-amp-META-Table\" class=\"headerlink\" title=\"-ROOT- &amp;&amp; .META. Table\"></a>-ROOT- &amp;&amp; .META. Table</h3><p>HBase中有两张特殊的Table，-ROOT-和.META.</p>\n<p>Ø  .META.：记录了用户表的Region信息，.META.表本身可以有多个regoin</p>\n<p>Ø  -ROOT-：记录了.META.表的Region信息，-ROOT-表本身只有一个region</p>\n<p>Ø  Zookeeper中记录了-ROOT-表的location</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/2.jpg\" alt=\"Image.jpg\"></p>\n<p>Client访问用户数据之前需要首先访问zookeeper，然后访问-ROOT-表，接着访问.META.表，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过client端会做cache缓存。</p>\n<h3 id=\"MapReduce-on-HBase\"><a href=\"#MapReduce-on-HBase\" class=\"headerlink\" title=\"MapReduce on HBase\"></a>MapReduce on HBase</h3><p>在HBase系统上运行批处理运算，最方便和实用的模型依然是MapReduce，如下图：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/3.jpg\" alt=\"Image.jpg\"></p>\n<p>HBase Table和Region的关系，比较类似HDFS File和Block的关系，HBase提供了配套的TableInputFormat和TableOutputFormat API，可以方便的将HBase Table作为Hadoop MapReduce的Source和Sink，对于MapReduce Job应用开发人员来说，基本不需要关注HBase系统自身的细节。</p>\n<p>读者可以将这张图与我们之前讲解Hadoop中MapReduce工作流程时的图解进行对比。</p>\n<h2 id=\"HBase系统架构\"><a href=\"#HBase系统架构\" class=\"headerlink\" title=\"HBase系统架构\"></a>HBase系统架构</h2><p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/4.jpg\" alt=\"Image.jpg\"></p>\n<h3 id=\"Client\"><a href=\"#Client\" class=\"headerlink\" title=\"Client\"></a>Client</h3><p>HBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信，对于管理类操作，Client与HMaster进行RPC；对于数据读写类操作，Client与HRegionServer进行RPC</p>\n<h3 id=\"Zookeeper\"><a href=\"#Zookeeper\" class=\"headerlink\" title=\"Zookeeper\"></a>Zookeeper</h3><p>Zookeeper Quorum中除了存储了-ROOT-表的地址和HMaster的地址，HRegionServer也会把自己以Ephemeral方式注册到 Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的健康状态。此外，Zookeeper也避免了HMaster的 单点问题，见下文描述</p>\n<h3 id=\"HMaster\"><a href=\"#HMaster\" class=\"headerlink\" title=\"HMaster\"></a>HMaster</h3><p>HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行，HMaster在功能上主要负责Table和Region的管理工作：</p>\n<pre><code>1. 管理用户对Table的增、删、改、查操作\n\n2. 管理HRegionServer的负载均衡，调整Region分布\n\n3. 在Region Split后，负责新Region的分配\n\n4. 在HRegionServer停机后，负责失效HRegionServer 上的Regions迁移\n</code></pre><h3 id=\"HRegionServer\"><a href=\"#HRegionServer\" class=\"headerlink\" title=\"HRegionServer\"></a>HRegionServer</h3><p>HRegionServer主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/5.jpg\" alt=\"Image.jpg\"><br>HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。</p>\n<p>HStore存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是 Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile）， 当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进 行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要 进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前 Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer 上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/6.gif\" alt=\"Image.gif\"></p>\n<p>在理解了上述HStore的基本原理后，还必须了解一下HLog的功能，因为上述的HStore在系统正常工作的前提下是没有问题的，但是在分布式 系统环境中，无法避免系统出错或者宕机，因此一旦HRegionServer意外退出，MemStore中的内存数据将会丢失，这就需要引入HLog了。 每个HRegionServer中都有一个HLog对象，HLog是一个实现Write Ahead Log的类，在每次用户操作写入MemStore的同时，也会写一份数据到HLog文件中（HLog文件格式见后续），HLog文件定期会滚动出新的，并 删除旧的文件（已持久化到StoreFile中的数据）。当HRegionServer意外终止后，HMaster会通过Zookeeper感知 到，HMaster首先会处理遗留的 HLog文件，将其中不同Region的Log数据进行拆分，分别放到相应region的目录下，然后再将失效的region重新分配，领取 到这些region的HRegionServer在Load Region的过程中，会发现有历史HLog需要处理，因此会Replay HLog中的数据到MemStore中，然后flush到StoreFiles，完成数据恢复。</p>\n<h2 id=\"HBase存储格式\"><a href=\"#HBase存储格式\" class=\"headerlink\" title=\"HBase存储格式\"></a>HBase存储格式</h2><p>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上，主要包括上述提出的两种文件类型：</p>\n<p>1.HFile， HBase中KeyValue数据的存储格式，HFile是Hadoop的二进制格式文件，实际上StoreFile就是对HFile做了轻量级包装，即StoreFile底层就是HFile</p>\n<p>2.HLog File，HBase中WAL（Write Ahead Log） 的存储格式，物理上是Hadoop的Sequence File</p>\n<h3 id=\"HFile\"><a href=\"#HFile\" class=\"headerlink\" title=\"HFile\"></a>HFile</h3><p>下图是HFile的存储格式：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/7.jpg\" alt=\"Image.jpg\"></p>\n<p>首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数 据块的起始点。File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。Data Index和Meta Index块记录了每个Data块和Meta块的起始点。</p>\n<p>Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。 每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。</p>\n<p>HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：</p>\n<p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/8.jpg\" alt=\"Image.jpg\"></p>\n<p>开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。</p>\n<h3 id=\"HLogFile\"><a href=\"#HLogFile\" class=\"headerlink\" title=\"HLogFile\"></a>HLogFile</h3><p><img src=\"http://o8fwlo8a3.bkt.clouddn.com/9.jpg\" alt=\"Image.jpg\"></p>\n<p>上图中示意了HLog文件的结构，其实HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是“写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。</p>\n<p>HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。 </p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.tianshouzhi.com/api/tutorials/hbase/142\" target=\"_blank\" rel=\"noopener\">http://www.tianshouzhi.com/api/tutorials/hbase/142</a></li>\n</ol>"},{"title":"ICE飞冰权限管理","toc":false,"date":"2019-04-07T13:14:43.000Z","_content":"\n## 前言\n前端页面权限在日常开发中，主要有以下：\n\n- 登录授权，用户没有登录只能访问登录页面，如果处于登录状态则跳转到当前用户的默认首页；\n\n- 路由授权，当前登录用户的角色，如果对一个 URL 没有权限访问，则跳转到 403 页面；\n\n- 数据授权，当访问一个没有权限的 API，则跳转到 403 页面；\n\n- 操作授权，当页面中某个按钮或者区域没有权限访问则在页面中隐藏\n\n本文主要针对ICE飞冰模板，介绍下如何开发权限管理，主要是使用，不涉及任何架构设计及原理，关于原理可以参考源码分析，或者参考本文参考链接。\n## 实践\n\n首先先安装依赖，因为飞冰权限是使用[Authorized](https://pro.ant.design/components/Authorized-cn/) 权限组件实现了基本的权限管理方案。\n```\nnpm install antd --save\nnpm install ant-design-pro@latest --save\n```\n因为ant-design-pro依赖antd，因此需要首先安装antd\n\n在开始之前需要新增两个util,一个设置登录权限，一个为了重新渲染权限\n\nauthority.js\n```\n// use localStorage to store the authority info, which might be sent from server in actual project.\nexport function getAuthority() {\n  return localStorage.getItem('ice-pro-authority') || 'admin';\n}\n\nexport function setAuthority(authority) {\n  return localStorage.setItem('ice-pro-authority', authority);\n}\n\nexport function removeAuthority() {\n  return localStorage.removeItem('ice-pro-authority');\n}\n\n```\n\nAuthorized.js\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from './authority';\n\nlet Authorized = RenderAuthorized(getAuthority()); // eslint-disable-line\n\n// 更新权限\nconst reloadAuthorized = () => {\n  Authorized = RenderAuthorized(getAuthority());\n};\n\nexport { reloadAuthorized };\nexport default Authorized;\n\n```\n### 登录授权\n\n在登录页面，引入依赖，然后再login方法处设置权限，如下：\n```\nimport { setAuthority } from '../../utils/authority';\nimport { reloadAuthorized } from '../../utils/Authorized';\n\n// 这里设置权限为admin,添加该方法到login验证通过的方法处。\nsetAuthority('admin');\n//重新渲染页面\nreloadAuthorized();\n```\n\n退出\n```\nremoveAuthority()\n```\n\n### 路由授权\n编辑routerConfig.js,在需要设置权限的地方添加authority属性，值可以是string,数组。例如：\n```\n  {\n    path: '/approve/tasks',\n    component: ApproveTask,\n    authority: 'admin',\n  }\n```\n在MainRoutes.jsx 设置重定向地址\n```\nimport {AuthorizedRoute} from 'ant-design-pro/lib/Authorized';\n/**\n   * 根据菜单取得重定向地址.\n   */\n  getRedirectData = () => {\n    const redirectData = [];\n    const getRedirect = (item) => {\n      if (item && item.children) {\n        if (item.children[0] && item.children[0].path) {\n          redirectData.push({\n            from: `${item.path}`,\n            to: `${item.children[0].path}`,\n          });\n          item.children.forEach((children) => {\n            getRedirect(children);\n          });\n        }\n      }\n    };\n\n    asideMenuConfig.forEach(getRedirect);\n\n    return redirectData;\n  };\n\n  /**\n   * 渲染权限路由组件\n   */\n  renderAuthorizedRoute = (item, index) => {\n    return item.component ? (\n      <AuthorizedRoute\n        key={index}\n        path={item.path}\n        component={item.component}\n        exact={item.exact}\n        authority={item.authority}\n        redirectPath=\"/exception/403\"\n      />\n    ) : null;\n  };\n\n  render() {\n    const redirectData = this.getRedirectData();\n    return (\n      <Switch>\n        {/* 渲染权限路由表 */}\n        \n        {routerData.map(this.renderAuthorizedRoute)}\n        \n        {/* 路由重定向，嵌套路由默认重定向到当前菜单的第一个路由 */}\n        \n        {redirectData.map((item, index) => {\n          return <Redirect key={index} exact from={item.from} to={item.to} />;\n        })}\n\n        {/* 根路由默认重定向到 /dashboard */}\n        <Redirect from=\"/\" to=\"/user/login\" />\n\n        {/* 未匹配到的路由重定向到 NotFound */}\n        <Route component={NotFound} />\n      </Switch>\n    );\n  }\n}\n```\n这里跳转到403页面，这里不再说明如何增加403页面\n### 菜单授权\n修改meConfig.js。在需要添加权限地方，设置角色名称，`authority: 'admin'`,子菜单会继承父菜单的权限。详细案例如下：\n```\nconst asideMenuConfig = [\n  {\n    name: 'Topics',\n    path: '/topics',\n    icon: 'home2',\n    children: [\n      { name: 'Topic List', path: '/topics/list', authority: ['admin', 'member'] },\n      { name: 'My Task', path: '/topics/task', authority: ['admin', 'member'] },\n    ],\n  },\n  {\n    name: 'Approve',\n    path: '/approve',\n    icon: 'publish',\n    authority: 'admin',\n    children: [\n      { name: 'Task List', path: '/approve/tasks' },\n    ],\n  },\n];\n```\n在`\\src\\layouts\\BasicLayout\\components\\Aside`页面添加权限校验：\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from '../../../../utils/authority';\nimport { asideMenuConfig } from '../../../../menuConfig';\n\n//省略部分方法\n/**\n   * 权限检查\n   */\n  checkPermissionItem = (authority, ItemDom) => {\n    if (Authorized.check) {\n      const { check } = Authorized;\n      return check(authority, ItemDom);\n    }\n\n    return ItemDom;\n  };\n\n\n  /**\n   * 获取菜单项数据\n   */\n  getNavMenuItems = (menusData) => {\n    if (!menusData) {\n      return [];\n    }\n\n    return menusData\n      .filter(item => item.name && !item.hideInMenu)\n      .map((item, index) => {\n        const ItemDom = this.getSubMenuOrItem(item, index);\n        return this.checkPermissionItem(item.authority, ItemDom);\n      })\n      .filter(item => item);\n  };\n\n\n  /**\n   * 二级导航\n   */\n  getSubMenuOrItem = (item, index) => {\n    if (item.children && item.children.some(child => child.name)) {\n      const childrenItems = this.getNavMenuItems(item.children);\n\n      if (childrenItems && childrenItems.length > 0) {\n        return (\n          <SubNav\n            key={index}\n            label={\n              <span>\n                {item.icon ? (\n                  <FoundationSymbol size=\"small\" type={item.icon} />\n                        ) : null}\n                <span className=\"ice-menu-collapse-hide\">\n                  {item.name}\n                </span>\n              </span>\n                    }\n          >\n            {childrenItems}\n          </SubNav>\n        );\n      }\n      return null;\n    }\n\n    const linkProps = {};\n    if (item.newWindow) {\n      linkProps.href = item.path;\n      linkProps.target = '_blank';\n    } else if (item.external) {\n      linkProps.href = item.path;\n    } else {\n      linkProps.to = item.path;\n    }\n    return (\n      <Item key={item.path}>\n        <Link {...linkProps}>\n          <span>\n            {item.icon ? (\n              <FoundationSymbol size=\"small\" type={item.icon} />\n            ) : null}\n            <span className=\"ice-menu-collapse-hide\">{item.name}</span>\n          </span>\n        </Link>\n      </Item>\n    );\n  };\n\n  render() {\n    const { location } = this.props;\n    const { pathname } = location;\n\n    return (\n      <Layout.Aside width=\"240\" theme=\"light\" className=\"custom-aside\">\n        <Nav\n          defaultSelectedKeys={[pathname]}\n          selectedKeys={[pathname]}\n          onOpen={this.onOpenChange}\n          openKeys={this.state.openKeys}\n          className=\"custom-menu\"\n        >\n          {this.getNavMenuItems(asideMenuConfig)}\n        </Nav>\n        {/* 侧边菜单项 end */}\n      </Layout.Aside>\n    );\n  }\n```\n### 操作授权\n这里说的数据操作授权，即根据不同权限在页面显示不同的菜单,这里是实现只有管理员角色才能操作的按钮。\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from '../../utils/authority';\nconst Authorized = RenderAuthorized(getAuthority());\n\nReactDOM.render(\n  <div>\n    <Authorized authority=\"admin\" noMatch={noMatch}>\n      按钮....\n    </Authorized>\n  </div>,\n  mountNode,\n);\n```\n## 参考\n1. [权限管理](https://alibaba.github.io/ice/docs/pro/authority)\n2. [Ant Design Pro权限(Authorized)管理模块深入解读](https://wsonh.com/article/76.html)","source":"_posts/ICE飞冰权限管理.md","raw":"---\ntitle: ICE飞冰权限管理\ntags:\n  - 教程\ncategories:\n  - 前端技术\ntoc: false\ndate: 2019-04-07 21:14:43\n---\n\n## 前言\n前端页面权限在日常开发中，主要有以下：\n\n- 登录授权，用户没有登录只能访问登录页面，如果处于登录状态则跳转到当前用户的默认首页；\n\n- 路由授权，当前登录用户的角色，如果对一个 URL 没有权限访问，则跳转到 403 页面；\n\n- 数据授权，当访问一个没有权限的 API，则跳转到 403 页面；\n\n- 操作授权，当页面中某个按钮或者区域没有权限访问则在页面中隐藏\n\n本文主要针对ICE飞冰模板，介绍下如何开发权限管理，主要是使用，不涉及任何架构设计及原理，关于原理可以参考源码分析，或者参考本文参考链接。\n## 实践\n\n首先先安装依赖，因为飞冰权限是使用[Authorized](https://pro.ant.design/components/Authorized-cn/) 权限组件实现了基本的权限管理方案。\n```\nnpm install antd --save\nnpm install ant-design-pro@latest --save\n```\n因为ant-design-pro依赖antd，因此需要首先安装antd\n\n在开始之前需要新增两个util,一个设置登录权限，一个为了重新渲染权限\n\nauthority.js\n```\n// use localStorage to store the authority info, which might be sent from server in actual project.\nexport function getAuthority() {\n  return localStorage.getItem('ice-pro-authority') || 'admin';\n}\n\nexport function setAuthority(authority) {\n  return localStorage.setItem('ice-pro-authority', authority);\n}\n\nexport function removeAuthority() {\n  return localStorage.removeItem('ice-pro-authority');\n}\n\n```\n\nAuthorized.js\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from './authority';\n\nlet Authorized = RenderAuthorized(getAuthority()); // eslint-disable-line\n\n// 更新权限\nconst reloadAuthorized = () => {\n  Authorized = RenderAuthorized(getAuthority());\n};\n\nexport { reloadAuthorized };\nexport default Authorized;\n\n```\n### 登录授权\n\n在登录页面，引入依赖，然后再login方法处设置权限，如下：\n```\nimport { setAuthority } from '../../utils/authority';\nimport { reloadAuthorized } from '../../utils/Authorized';\n\n// 这里设置权限为admin,添加该方法到login验证通过的方法处。\nsetAuthority('admin');\n//重新渲染页面\nreloadAuthorized();\n```\n\n退出\n```\nremoveAuthority()\n```\n\n### 路由授权\n编辑routerConfig.js,在需要设置权限的地方添加authority属性，值可以是string,数组。例如：\n```\n  {\n    path: '/approve/tasks',\n    component: ApproveTask,\n    authority: 'admin',\n  }\n```\n在MainRoutes.jsx 设置重定向地址\n```\nimport {AuthorizedRoute} from 'ant-design-pro/lib/Authorized';\n/**\n   * 根据菜单取得重定向地址.\n   */\n  getRedirectData = () => {\n    const redirectData = [];\n    const getRedirect = (item) => {\n      if (item && item.children) {\n        if (item.children[0] && item.children[0].path) {\n          redirectData.push({\n            from: `${item.path}`,\n            to: `${item.children[0].path}`,\n          });\n          item.children.forEach((children) => {\n            getRedirect(children);\n          });\n        }\n      }\n    };\n\n    asideMenuConfig.forEach(getRedirect);\n\n    return redirectData;\n  };\n\n  /**\n   * 渲染权限路由组件\n   */\n  renderAuthorizedRoute = (item, index) => {\n    return item.component ? (\n      <AuthorizedRoute\n        key={index}\n        path={item.path}\n        component={item.component}\n        exact={item.exact}\n        authority={item.authority}\n        redirectPath=\"/exception/403\"\n      />\n    ) : null;\n  };\n\n  render() {\n    const redirectData = this.getRedirectData();\n    return (\n      <Switch>\n        {/* 渲染权限路由表 */}\n        \n        {routerData.map(this.renderAuthorizedRoute)}\n        \n        {/* 路由重定向，嵌套路由默认重定向到当前菜单的第一个路由 */}\n        \n        {redirectData.map((item, index) => {\n          return <Redirect key={index} exact from={item.from} to={item.to} />;\n        })}\n\n        {/* 根路由默认重定向到 /dashboard */}\n        <Redirect from=\"/\" to=\"/user/login\" />\n\n        {/* 未匹配到的路由重定向到 NotFound */}\n        <Route component={NotFound} />\n      </Switch>\n    );\n  }\n}\n```\n这里跳转到403页面，这里不再说明如何增加403页面\n### 菜单授权\n修改meConfig.js。在需要添加权限地方，设置角色名称，`authority: 'admin'`,子菜单会继承父菜单的权限。详细案例如下：\n```\nconst asideMenuConfig = [\n  {\n    name: 'Topics',\n    path: '/topics',\n    icon: 'home2',\n    children: [\n      { name: 'Topic List', path: '/topics/list', authority: ['admin', 'member'] },\n      { name: 'My Task', path: '/topics/task', authority: ['admin', 'member'] },\n    ],\n  },\n  {\n    name: 'Approve',\n    path: '/approve',\n    icon: 'publish',\n    authority: 'admin',\n    children: [\n      { name: 'Task List', path: '/approve/tasks' },\n    ],\n  },\n];\n```\n在`\\src\\layouts\\BasicLayout\\components\\Aside`页面添加权限校验：\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from '../../../../utils/authority';\nimport { asideMenuConfig } from '../../../../menuConfig';\n\n//省略部分方法\n/**\n   * 权限检查\n   */\n  checkPermissionItem = (authority, ItemDom) => {\n    if (Authorized.check) {\n      const { check } = Authorized;\n      return check(authority, ItemDom);\n    }\n\n    return ItemDom;\n  };\n\n\n  /**\n   * 获取菜单项数据\n   */\n  getNavMenuItems = (menusData) => {\n    if (!menusData) {\n      return [];\n    }\n\n    return menusData\n      .filter(item => item.name && !item.hideInMenu)\n      .map((item, index) => {\n        const ItemDom = this.getSubMenuOrItem(item, index);\n        return this.checkPermissionItem(item.authority, ItemDom);\n      })\n      .filter(item => item);\n  };\n\n\n  /**\n   * 二级导航\n   */\n  getSubMenuOrItem = (item, index) => {\n    if (item.children && item.children.some(child => child.name)) {\n      const childrenItems = this.getNavMenuItems(item.children);\n\n      if (childrenItems && childrenItems.length > 0) {\n        return (\n          <SubNav\n            key={index}\n            label={\n              <span>\n                {item.icon ? (\n                  <FoundationSymbol size=\"small\" type={item.icon} />\n                        ) : null}\n                <span className=\"ice-menu-collapse-hide\">\n                  {item.name}\n                </span>\n              </span>\n                    }\n          >\n            {childrenItems}\n          </SubNav>\n        );\n      }\n      return null;\n    }\n\n    const linkProps = {};\n    if (item.newWindow) {\n      linkProps.href = item.path;\n      linkProps.target = '_blank';\n    } else if (item.external) {\n      linkProps.href = item.path;\n    } else {\n      linkProps.to = item.path;\n    }\n    return (\n      <Item key={item.path}>\n        <Link {...linkProps}>\n          <span>\n            {item.icon ? (\n              <FoundationSymbol size=\"small\" type={item.icon} />\n            ) : null}\n            <span className=\"ice-menu-collapse-hide\">{item.name}</span>\n          </span>\n        </Link>\n      </Item>\n    );\n  };\n\n  render() {\n    const { location } = this.props;\n    const { pathname } = location;\n\n    return (\n      <Layout.Aside width=\"240\" theme=\"light\" className=\"custom-aside\">\n        <Nav\n          defaultSelectedKeys={[pathname]}\n          selectedKeys={[pathname]}\n          onOpen={this.onOpenChange}\n          openKeys={this.state.openKeys}\n          className=\"custom-menu\"\n        >\n          {this.getNavMenuItems(asideMenuConfig)}\n        </Nav>\n        {/* 侧边菜单项 end */}\n      </Layout.Aside>\n    );\n  }\n```\n### 操作授权\n这里说的数据操作授权，即根据不同权限在页面显示不同的菜单,这里是实现只有管理员角色才能操作的按钮。\n```\nimport RenderAuthorized from 'ant-design-pro/lib/Authorized';\nimport { getAuthority } from '../../utils/authority';\nconst Authorized = RenderAuthorized(getAuthority());\n\nReactDOM.render(\n  <div>\n    <Authorized authority=\"admin\" noMatch={noMatch}>\n      按钮....\n    </Authorized>\n  </div>,\n  mountNode,\n);\n```\n## 参考\n1. [权限管理](https://alibaba.github.io/ice/docs/pro/authority)\n2. [Ant Design Pro权限(Authorized)管理模块深入解读](https://wsonh.com/article/76.html)","slug":"ICE飞冰权限管理","published":1,"updated":"2019-04-13T11:37:24.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvb2002d8cee4cyktd50","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>前端页面权限在日常开发中，主要有以下：</p>\n<ul>\n<li><p>登录授权，用户没有登录只能访问登录页面，如果处于登录状态则跳转到当前用户的默认首页；</p>\n</li>\n<li><p>路由授权，当前登录用户的角色，如果对一个 URL 没有权限访问，则跳转到 403 页面；</p>\n</li>\n<li><p>数据授权，当访问一个没有权限的 API，则跳转到 403 页面；</p>\n</li>\n<li><p>操作授权，当页面中某个按钮或者区域没有权限访问则在页面中隐藏</p>\n</li>\n</ul>\n<p>本文主要针对ICE飞冰模板，介绍下如何开发权限管理，主要是使用，不涉及任何架构设计及原理，关于原理可以参考源码分析，或者参考本文参考链接。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>首先先安装依赖，因为飞冰权限是使用<a href=\"https://pro.ant.design/components/Authorized-cn/\" target=\"_blank\" rel=\"noopener\">Authorized</a> 权限组件实现了基本的权限管理方案。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install antd --save</span><br><span class=\"line\">npm install ant-design-pro@latest --save</span><br></pre></td></tr></table></figure></p>\n<p>因为ant-design-pro依赖antd，因此需要首先安装antd</p>\n<p>在开始之前需要新增两个util,一个设置登录权限，一个为了重新渲染权限</p>\n<p>authority.js<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use localStorage to store the authority info, which might be sent from server in actual project.</span><br><span class=\"line\">export function getAuthority() &#123;</span><br><span class=\"line\">  return localStorage.getItem(&apos;ice-pro-authority&apos;) || &apos;admin&apos;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">export function setAuthority(authority) &#123;</span><br><span class=\"line\">  return localStorage.setItem(&apos;ice-pro-authority&apos;, authority);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">export function removeAuthority() &#123;</span><br><span class=\"line\">  return localStorage.removeItem(&apos;ice-pro-authority&apos;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Authorized.js<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;./authority&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">let Authorized = RenderAuthorized(getAuthority()); // eslint-disable-line</span><br><span class=\"line\"></span><br><span class=\"line\">// 更新权限</span><br><span class=\"line\">const reloadAuthorized = () =&gt; &#123;</span><br><span class=\"line\">  Authorized = RenderAuthorized(getAuthority());</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">export &#123; reloadAuthorized &#125;;</span><br><span class=\"line\">export default Authorized;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"登录授权\"><a href=\"#登录授权\" class=\"headerlink\" title=\"登录授权\"></a>登录授权</h3><p>在登录页面，引入依赖，然后再login方法处设置权限，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123; setAuthority &#125; from &apos;../../utils/authority&apos;;</span><br><span class=\"line\">import &#123; reloadAuthorized &#125; from &apos;../../utils/Authorized&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">// 这里设置权限为admin,添加该方法到login验证通过的方法处。</span><br><span class=\"line\">setAuthority(&apos;admin&apos;);</span><br><span class=\"line\">//重新渲染页面</span><br><span class=\"line\">reloadAuthorized();</span><br></pre></td></tr></table></figure></p>\n<p>退出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">removeAuthority()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"路由授权\"><a href=\"#路由授权\" class=\"headerlink\" title=\"路由授权\"></a>路由授权</h3><p>编辑routerConfig.js,在需要设置权限的地方添加authority属性，值可以是string,数组。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  path: &apos;/approve/tasks&apos;,</span><br><span class=\"line\">  component: ApproveTask,</span><br><span class=\"line\">  authority: &apos;admin&apos;,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在MainRoutes.jsx 设置重定向地址<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123;AuthorizedRoute&#125; from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">/**</span><br><span class=\"line\">   * 根据菜单取得重定向地址.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getRedirectData = () =&gt; &#123;</span><br><span class=\"line\">    const redirectData = [];</span><br><span class=\"line\">    const getRedirect = (item) =&gt; &#123;</span><br><span class=\"line\">      if (item &amp;&amp; item.children) &#123;</span><br><span class=\"line\">        if (item.children[0] &amp;&amp; item.children[0].path) &#123;</span><br><span class=\"line\">          redirectData.push(&#123;</span><br><span class=\"line\">            from: `$&#123;item.path&#125;`,</span><br><span class=\"line\">            to: `$&#123;item.children[0].path&#125;`,</span><br><span class=\"line\">          &#125;);</span><br><span class=\"line\">          item.children.forEach((children) =&gt; &#123;</span><br><span class=\"line\">            getRedirect(children);</span><br><span class=\"line\">          &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    asideMenuConfig.forEach(getRedirect);</span><br><span class=\"line\"></span><br><span class=\"line\">    return redirectData;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 渲染权限路由组件</span><br><span class=\"line\">   */</span><br><span class=\"line\">  renderAuthorizedRoute = (item, index) =&gt; &#123;</span><br><span class=\"line\">    return item.component ? (</span><br><span class=\"line\">      &lt;AuthorizedRoute</span><br><span class=\"line\">        key=&#123;index&#125;</span><br><span class=\"line\">        path=&#123;item.path&#125;</span><br><span class=\"line\">        component=&#123;item.component&#125;</span><br><span class=\"line\">        exact=&#123;item.exact&#125;</span><br><span class=\"line\">        authority=&#123;item.authority&#125;</span><br><span class=\"line\">        redirectPath=&quot;/exception/403&quot;</span><br><span class=\"line\">      /&gt;</span><br><span class=\"line\">    ) : null;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  render() &#123;</span><br><span class=\"line\">    const redirectData = this.getRedirectData();</span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Switch&gt;</span><br><span class=\"line\">        &#123;/* 渲染权限路由表 */&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;routerData.map(this.renderAuthorizedRoute)&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;/* 路由重定向，嵌套路由默认重定向到当前菜单的第一个路由 */&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;redirectData.map((item, index) =&gt; &#123;</span><br><span class=\"line\">          return &lt;Redirect key=&#123;index&#125; exact from=&#123;item.from&#125; to=&#123;item.to&#125; /&gt;;</span><br><span class=\"line\">        &#125;)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#123;/* 根路由默认重定向到 /dashboard */&#125;</span><br><span class=\"line\">        &lt;Redirect from=&quot;/&quot; to=&quot;/user/login&quot; /&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#123;/* 未匹配到的路由重定向到 NotFound */&#125;</span><br><span class=\"line\">        &lt;Route component=&#123;NotFound&#125; /&gt;</span><br><span class=\"line\">      &lt;/Switch&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这里跳转到403页面，这里不再说明如何增加403页面</p>\n<h3 id=\"菜单授权\"><a href=\"#菜单授权\" class=\"headerlink\" title=\"菜单授权\"></a>菜单授权</h3><p>修改meConfig.js。在需要添加权限地方，设置角色名称，<code>authority: &#39;admin&#39;</code>,子菜单会继承父菜单的权限。详细案例如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const asideMenuConfig = [</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    name: &apos;Topics&apos;,</span><br><span class=\"line\">    path: &apos;/topics&apos;,</span><br><span class=\"line\">    icon: &apos;home2&apos;,</span><br><span class=\"line\">    children: [</span><br><span class=\"line\">      &#123; name: &apos;Topic List&apos;, path: &apos;/topics/list&apos;, authority: [&apos;admin&apos;, &apos;member&apos;] &#125;,</span><br><span class=\"line\">      &#123; name: &apos;My Task&apos;, path: &apos;/topics/task&apos;, authority: [&apos;admin&apos;, &apos;member&apos;] &#125;,</span><br><span class=\"line\">    ],</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    name: &apos;Approve&apos;,</span><br><span class=\"line\">    path: &apos;/approve&apos;,</span><br><span class=\"line\">    icon: &apos;publish&apos;,</span><br><span class=\"line\">    authority: &apos;admin&apos;,</span><br><span class=\"line\">    children: [</span><br><span class=\"line\">      &#123; name: &apos;Task List&apos;, path: &apos;/approve/tasks&apos; &#125;,</span><br><span class=\"line\">    ],</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure></p>\n<p>在<code>\\src\\layouts\\BasicLayout\\components\\Aside</code>页面添加权限校验：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;../../../../utils/authority&apos;;</span><br><span class=\"line\">import &#123; asideMenuConfig &#125; from &apos;../../../../menuConfig&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">//省略部分方法</span><br><span class=\"line\">/**</span><br><span class=\"line\">   * 权限检查</span><br><span class=\"line\">   */</span><br><span class=\"line\">  checkPermissionItem = (authority, ItemDom) =&gt; &#123;</span><br><span class=\"line\">    if (Authorized.check) &#123;</span><br><span class=\"line\">      const &#123; check &#125; = Authorized;</span><br><span class=\"line\">      return check(authority, ItemDom);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return ItemDom;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 获取菜单项数据</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getNavMenuItems = (menusData) =&gt; &#123;</span><br><span class=\"line\">    if (!menusData) &#123;</span><br><span class=\"line\">      return [];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return menusData</span><br><span class=\"line\">      .filter(item =&gt; item.name &amp;&amp; !item.hideInMenu)</span><br><span class=\"line\">      .map((item, index) =&gt; &#123;</span><br><span class=\"line\">        const ItemDom = this.getSubMenuOrItem(item, index);</span><br><span class=\"line\">        return this.checkPermissionItem(item.authority, ItemDom);</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">      .filter(item =&gt; item);</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 二级导航</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getSubMenuOrItem = (item, index) =&gt; &#123;</span><br><span class=\"line\">    if (item.children &amp;&amp; item.children.some(child =&gt; child.name)) &#123;</span><br><span class=\"line\">      const childrenItems = this.getNavMenuItems(item.children);</span><br><span class=\"line\"></span><br><span class=\"line\">      if (childrenItems &amp;&amp; childrenItems.length &gt; 0) &#123;</span><br><span class=\"line\">        return (</span><br><span class=\"line\">          &lt;SubNav</span><br><span class=\"line\">            key=&#123;index&#125;</span><br><span class=\"line\">            label=&#123;</span><br><span class=\"line\">              &lt;span&gt;</span><br><span class=\"line\">                &#123;item.icon ? (</span><br><span class=\"line\">                  &lt;FoundationSymbol size=&quot;small&quot; type=&#123;item.icon&#125; /&gt;</span><br><span class=\"line\">                        ) : null&#125;</span><br><span class=\"line\">                &lt;span className=&quot;ice-menu-collapse-hide&quot;&gt;</span><br><span class=\"line\">                  &#123;item.name&#125;</span><br><span class=\"line\">                &lt;/span&gt;</span><br><span class=\"line\">              &lt;/span&gt;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">          &gt;</span><br><span class=\"line\">            &#123;childrenItems&#125;</span><br><span class=\"line\">          &lt;/SubNav&gt;</span><br><span class=\"line\">        );</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    const linkProps = &#123;&#125;;</span><br><span class=\"line\">    if (item.newWindow) &#123;</span><br><span class=\"line\">      linkProps.href = item.path;</span><br><span class=\"line\">      linkProps.target = &apos;_blank&apos;;</span><br><span class=\"line\">    &#125; else if (item.external) &#123;</span><br><span class=\"line\">      linkProps.href = item.path;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      linkProps.to = item.path;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Item key=&#123;item.path&#125;&gt;</span><br><span class=\"line\">        &lt;Link &#123;...linkProps&#125;&gt;</span><br><span class=\"line\">          &lt;span&gt;</span><br><span class=\"line\">            &#123;item.icon ? (</span><br><span class=\"line\">              &lt;FoundationSymbol size=&quot;small&quot; type=&#123;item.icon&#125; /&gt;</span><br><span class=\"line\">            ) : null&#125;</span><br><span class=\"line\">            &lt;span className=&quot;ice-menu-collapse-hide&quot;&gt;&#123;item.name&#125;&lt;/span&gt;</span><br><span class=\"line\">          &lt;/span&gt;</span><br><span class=\"line\">        &lt;/Link&gt;</span><br><span class=\"line\">      &lt;/Item&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  render() &#123;</span><br><span class=\"line\">    const &#123; location &#125; = this.props;</span><br><span class=\"line\">    const &#123; pathname &#125; = location;</span><br><span class=\"line\"></span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Layout.Aside width=&quot;240&quot; theme=&quot;light&quot; className=&quot;custom-aside&quot;&gt;</span><br><span class=\"line\">        &lt;Nav</span><br><span class=\"line\">          defaultSelectedKeys=&#123;[pathname]&#125;</span><br><span class=\"line\">          selectedKeys=&#123;[pathname]&#125;</span><br><span class=\"line\">          onOpen=&#123;this.onOpenChange&#125;</span><br><span class=\"line\">          openKeys=&#123;this.state.openKeys&#125;</span><br><span class=\"line\">          className=&quot;custom-menu&quot;</span><br><span class=\"line\">        &gt;</span><br><span class=\"line\">          &#123;this.getNavMenuItems(asideMenuConfig)&#125;</span><br><span class=\"line\">        &lt;/Nav&gt;</span><br><span class=\"line\">        &#123;/* 侧边菜单项 end */&#125;</span><br><span class=\"line\">      &lt;/Layout.Aside&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"操作授权\"><a href=\"#操作授权\" class=\"headerlink\" title=\"操作授权\"></a>操作授权</h3><p>这里说的数据操作授权，即根据不同权限在页面显示不同的菜单,这里是实现只有管理员角色才能操作的按钮。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;../../utils/authority&apos;;</span><br><span class=\"line\">const Authorized = RenderAuthorized(getAuthority());</span><br><span class=\"line\"></span><br><span class=\"line\">ReactDOM.render(</span><br><span class=\"line\">  &lt;div&gt;</span><br><span class=\"line\">    &lt;Authorized authority=&quot;admin&quot; noMatch=&#123;noMatch&#125;&gt;</span><br><span class=\"line\">      按钮....</span><br><span class=\"line\">    &lt;/Authorized&gt;</span><br><span class=\"line\">  &lt;/div&gt;,</span><br><span class=\"line\">  mountNode,</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://alibaba.github.io/ice/docs/pro/authority\" target=\"_blank\" rel=\"noopener\">权限管理</a></li>\n<li><a href=\"https://wsonh.com/article/76.html\" target=\"_blank\" rel=\"noopener\">Ant Design Pro权限(Authorized)管理模块深入解读</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>前端页面权限在日常开发中，主要有以下：</p>\n<ul>\n<li><p>登录授权，用户没有登录只能访问登录页面，如果处于登录状态则跳转到当前用户的默认首页；</p>\n</li>\n<li><p>路由授权，当前登录用户的角色，如果对一个 URL 没有权限访问，则跳转到 403 页面；</p>\n</li>\n<li><p>数据授权，当访问一个没有权限的 API，则跳转到 403 页面；</p>\n</li>\n<li><p>操作授权，当页面中某个按钮或者区域没有权限访问则在页面中隐藏</p>\n</li>\n</ul>\n<p>本文主要针对ICE飞冰模板，介绍下如何开发权限管理，主要是使用，不涉及任何架构设计及原理，关于原理可以参考源码分析，或者参考本文参考链接。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>首先先安装依赖，因为飞冰权限是使用<a href=\"https://pro.ant.design/components/Authorized-cn/\" target=\"_blank\" rel=\"noopener\">Authorized</a> 权限组件实现了基本的权限管理方案。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install antd --save</span><br><span class=\"line\">npm install ant-design-pro@latest --save</span><br></pre></td></tr></table></figure></p>\n<p>因为ant-design-pro依赖antd，因此需要首先安装antd</p>\n<p>在开始之前需要新增两个util,一个设置登录权限，一个为了重新渲染权限</p>\n<p>authority.js<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// use localStorage to store the authority info, which might be sent from server in actual project.</span><br><span class=\"line\">export function getAuthority() &#123;</span><br><span class=\"line\">  return localStorage.getItem(&apos;ice-pro-authority&apos;) || &apos;admin&apos;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">export function setAuthority(authority) &#123;</span><br><span class=\"line\">  return localStorage.setItem(&apos;ice-pro-authority&apos;, authority);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">export function removeAuthority() &#123;</span><br><span class=\"line\">  return localStorage.removeItem(&apos;ice-pro-authority&apos;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>Authorized.js<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;./authority&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">let Authorized = RenderAuthorized(getAuthority()); // eslint-disable-line</span><br><span class=\"line\"></span><br><span class=\"line\">// 更新权限</span><br><span class=\"line\">const reloadAuthorized = () =&gt; &#123;</span><br><span class=\"line\">  Authorized = RenderAuthorized(getAuthority());</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">export &#123; reloadAuthorized &#125;;</span><br><span class=\"line\">export default Authorized;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"登录授权\"><a href=\"#登录授权\" class=\"headerlink\" title=\"登录授权\"></a>登录授权</h3><p>在登录页面，引入依赖，然后再login方法处设置权限，如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123; setAuthority &#125; from &apos;../../utils/authority&apos;;</span><br><span class=\"line\">import &#123; reloadAuthorized &#125; from &apos;../../utils/Authorized&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">// 这里设置权限为admin,添加该方法到login验证通过的方法处。</span><br><span class=\"line\">setAuthority(&apos;admin&apos;);</span><br><span class=\"line\">//重新渲染页面</span><br><span class=\"line\">reloadAuthorized();</span><br></pre></td></tr></table></figure></p>\n<p>退出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">removeAuthority()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"路由授权\"><a href=\"#路由授权\" class=\"headerlink\" title=\"路由授权\"></a>路由授权</h3><p>编辑routerConfig.js,在需要设置权限的地方添加authority属性，值可以是string,数组。例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  path: &apos;/approve/tasks&apos;,</span><br><span class=\"line\">  component: ApproveTask,</span><br><span class=\"line\">  authority: &apos;admin&apos;,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在MainRoutes.jsx 设置重定向地址<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123;AuthorizedRoute&#125; from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">/**</span><br><span class=\"line\">   * 根据菜单取得重定向地址.</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getRedirectData = () =&gt; &#123;</span><br><span class=\"line\">    const redirectData = [];</span><br><span class=\"line\">    const getRedirect = (item) =&gt; &#123;</span><br><span class=\"line\">      if (item &amp;&amp; item.children) &#123;</span><br><span class=\"line\">        if (item.children[0] &amp;&amp; item.children[0].path) &#123;</span><br><span class=\"line\">          redirectData.push(&#123;</span><br><span class=\"line\">            from: `$&#123;item.path&#125;`,</span><br><span class=\"line\">            to: `$&#123;item.children[0].path&#125;`,</span><br><span class=\"line\">          &#125;);</span><br><span class=\"line\">          item.children.forEach((children) =&gt; &#123;</span><br><span class=\"line\">            getRedirect(children);</span><br><span class=\"line\">          &#125;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    asideMenuConfig.forEach(getRedirect);</span><br><span class=\"line\"></span><br><span class=\"line\">    return redirectData;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 渲染权限路由组件</span><br><span class=\"line\">   */</span><br><span class=\"line\">  renderAuthorizedRoute = (item, index) =&gt; &#123;</span><br><span class=\"line\">    return item.component ? (</span><br><span class=\"line\">      &lt;AuthorizedRoute</span><br><span class=\"line\">        key=&#123;index&#125;</span><br><span class=\"line\">        path=&#123;item.path&#125;</span><br><span class=\"line\">        component=&#123;item.component&#125;</span><br><span class=\"line\">        exact=&#123;item.exact&#125;</span><br><span class=\"line\">        authority=&#123;item.authority&#125;</span><br><span class=\"line\">        redirectPath=&quot;/exception/403&quot;</span><br><span class=\"line\">      /&gt;</span><br><span class=\"line\">    ) : null;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  render() &#123;</span><br><span class=\"line\">    const redirectData = this.getRedirectData();</span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Switch&gt;</span><br><span class=\"line\">        &#123;/* 渲染权限路由表 */&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;routerData.map(this.renderAuthorizedRoute)&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;/* 路由重定向，嵌套路由默认重定向到当前菜单的第一个路由 */&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &#123;redirectData.map((item, index) =&gt; &#123;</span><br><span class=\"line\">          return &lt;Redirect key=&#123;index&#125; exact from=&#123;item.from&#125; to=&#123;item.to&#125; /&gt;;</span><br><span class=\"line\">        &#125;)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#123;/* 根路由默认重定向到 /dashboard */&#125;</span><br><span class=\"line\">        &lt;Redirect from=&quot;/&quot; to=&quot;/user/login&quot; /&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#123;/* 未匹配到的路由重定向到 NotFound */&#125;</span><br><span class=\"line\">        &lt;Route component=&#123;NotFound&#125; /&gt;</span><br><span class=\"line\">      &lt;/Switch&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这里跳转到403页面，这里不再说明如何增加403页面</p>\n<h3 id=\"菜单授权\"><a href=\"#菜单授权\" class=\"headerlink\" title=\"菜单授权\"></a>菜单授权</h3><p>修改meConfig.js。在需要添加权限地方，设置角色名称，<code>authority: &#39;admin&#39;</code>,子菜单会继承父菜单的权限。详细案例如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const asideMenuConfig = [</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    name: &apos;Topics&apos;,</span><br><span class=\"line\">    path: &apos;/topics&apos;,</span><br><span class=\"line\">    icon: &apos;home2&apos;,</span><br><span class=\"line\">    children: [</span><br><span class=\"line\">      &#123; name: &apos;Topic List&apos;, path: &apos;/topics/list&apos;, authority: [&apos;admin&apos;, &apos;member&apos;] &#125;,</span><br><span class=\"line\">      &#123; name: &apos;My Task&apos;, path: &apos;/topics/task&apos;, authority: [&apos;admin&apos;, &apos;member&apos;] &#125;,</span><br><span class=\"line\">    ],</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    name: &apos;Approve&apos;,</span><br><span class=\"line\">    path: &apos;/approve&apos;,</span><br><span class=\"line\">    icon: &apos;publish&apos;,</span><br><span class=\"line\">    authority: &apos;admin&apos;,</span><br><span class=\"line\">    children: [</span><br><span class=\"line\">      &#123; name: &apos;Task List&apos;, path: &apos;/approve/tasks&apos; &#125;,</span><br><span class=\"line\">    ],</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">];</span><br></pre></td></tr></table></figure></p>\n<p>在<code>\\src\\layouts\\BasicLayout\\components\\Aside</code>页面添加权限校验：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;../../../../utils/authority&apos;;</span><br><span class=\"line\">import &#123; asideMenuConfig &#125; from &apos;../../../../menuConfig&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">//省略部分方法</span><br><span class=\"line\">/**</span><br><span class=\"line\">   * 权限检查</span><br><span class=\"line\">   */</span><br><span class=\"line\">  checkPermissionItem = (authority, ItemDom) =&gt; &#123;</span><br><span class=\"line\">    if (Authorized.check) &#123;</span><br><span class=\"line\">      const &#123; check &#125; = Authorized;</span><br><span class=\"line\">      return check(authority, ItemDom);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return ItemDom;</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 获取菜单项数据</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getNavMenuItems = (menusData) =&gt; &#123;</span><br><span class=\"line\">    if (!menusData) &#123;</span><br><span class=\"line\">      return [];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    return menusData</span><br><span class=\"line\">      .filter(item =&gt; item.name &amp;&amp; !item.hideInMenu)</span><br><span class=\"line\">      .map((item, index) =&gt; &#123;</span><br><span class=\"line\">        const ItemDom = this.getSubMenuOrItem(item, index);</span><br><span class=\"line\">        return this.checkPermissionItem(item.authority, ItemDom);</span><br><span class=\"line\">      &#125;)</span><br><span class=\"line\">      .filter(item =&gt; item);</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">  /**</span><br><span class=\"line\">   * 二级导航</span><br><span class=\"line\">   */</span><br><span class=\"line\">  getSubMenuOrItem = (item, index) =&gt; &#123;</span><br><span class=\"line\">    if (item.children &amp;&amp; item.children.some(child =&gt; child.name)) &#123;</span><br><span class=\"line\">      const childrenItems = this.getNavMenuItems(item.children);</span><br><span class=\"line\"></span><br><span class=\"line\">      if (childrenItems &amp;&amp; childrenItems.length &gt; 0) &#123;</span><br><span class=\"line\">        return (</span><br><span class=\"line\">          &lt;SubNav</span><br><span class=\"line\">            key=&#123;index&#125;</span><br><span class=\"line\">            label=&#123;</span><br><span class=\"line\">              &lt;span&gt;</span><br><span class=\"line\">                &#123;item.icon ? (</span><br><span class=\"line\">                  &lt;FoundationSymbol size=&quot;small&quot; type=&#123;item.icon&#125; /&gt;</span><br><span class=\"line\">                        ) : null&#125;</span><br><span class=\"line\">                &lt;span className=&quot;ice-menu-collapse-hide&quot;&gt;</span><br><span class=\"line\">                  &#123;item.name&#125;</span><br><span class=\"line\">                &lt;/span&gt;</span><br><span class=\"line\">              &lt;/span&gt;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">          &gt;</span><br><span class=\"line\">            &#123;childrenItems&#125;</span><br><span class=\"line\">          &lt;/SubNav&gt;</span><br><span class=\"line\">        );</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    const linkProps = &#123;&#125;;</span><br><span class=\"line\">    if (item.newWindow) &#123;</span><br><span class=\"line\">      linkProps.href = item.path;</span><br><span class=\"line\">      linkProps.target = &apos;_blank&apos;;</span><br><span class=\"line\">    &#125; else if (item.external) &#123;</span><br><span class=\"line\">      linkProps.href = item.path;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      linkProps.to = item.path;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Item key=&#123;item.path&#125;&gt;</span><br><span class=\"line\">        &lt;Link &#123;...linkProps&#125;&gt;</span><br><span class=\"line\">          &lt;span&gt;</span><br><span class=\"line\">            &#123;item.icon ? (</span><br><span class=\"line\">              &lt;FoundationSymbol size=&quot;small&quot; type=&#123;item.icon&#125; /&gt;</span><br><span class=\"line\">            ) : null&#125;</span><br><span class=\"line\">            &lt;span className=&quot;ice-menu-collapse-hide&quot;&gt;&#123;item.name&#125;&lt;/span&gt;</span><br><span class=\"line\">          &lt;/span&gt;</span><br><span class=\"line\">        &lt;/Link&gt;</span><br><span class=\"line\">      &lt;/Item&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">  render() &#123;</span><br><span class=\"line\">    const &#123; location &#125; = this.props;</span><br><span class=\"line\">    const &#123; pathname &#125; = location;</span><br><span class=\"line\"></span><br><span class=\"line\">    return (</span><br><span class=\"line\">      &lt;Layout.Aside width=&quot;240&quot; theme=&quot;light&quot; className=&quot;custom-aside&quot;&gt;</span><br><span class=\"line\">        &lt;Nav</span><br><span class=\"line\">          defaultSelectedKeys=&#123;[pathname]&#125;</span><br><span class=\"line\">          selectedKeys=&#123;[pathname]&#125;</span><br><span class=\"line\">          onOpen=&#123;this.onOpenChange&#125;</span><br><span class=\"line\">          openKeys=&#123;this.state.openKeys&#125;</span><br><span class=\"line\">          className=&quot;custom-menu&quot;</span><br><span class=\"line\">        &gt;</span><br><span class=\"line\">          &#123;this.getNavMenuItems(asideMenuConfig)&#125;</span><br><span class=\"line\">        &lt;/Nav&gt;</span><br><span class=\"line\">        &#123;/* 侧边菜单项 end */&#125;</span><br><span class=\"line\">      &lt;/Layout.Aside&gt;</span><br><span class=\"line\">    );</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"操作授权\"><a href=\"#操作授权\" class=\"headerlink\" title=\"操作授权\"></a>操作授权</h3><p>这里说的数据操作授权，即根据不同权限在页面显示不同的菜单,这里是实现只有管理员角色才能操作的按钮。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import RenderAuthorized from &apos;ant-design-pro/lib/Authorized&apos;;</span><br><span class=\"line\">import &#123; getAuthority &#125; from &apos;../../utils/authority&apos;;</span><br><span class=\"line\">const Authorized = RenderAuthorized(getAuthority());</span><br><span class=\"line\"></span><br><span class=\"line\">ReactDOM.render(</span><br><span class=\"line\">  &lt;div&gt;</span><br><span class=\"line\">    &lt;Authorized authority=&quot;admin&quot; noMatch=&#123;noMatch&#125;&gt;</span><br><span class=\"line\">      按钮....</span><br><span class=\"line\">    &lt;/Authorized&gt;</span><br><span class=\"line\">  &lt;/div&gt;,</span><br><span class=\"line\">  mountNode,</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://alibaba.github.io/ice/docs/pro/authority\" target=\"_blank\" rel=\"noopener\">权限管理</a></li>\n<li><a href=\"https://wsonh.com/article/76.html\" target=\"_blank\" rel=\"noopener\">Ant Design Pro权限(Authorized)管理模块深入解读</a></li>\n</ol>\n"},{"title":"Kafka 消息生产及消费原理","originContent":"","toc":false,"date":"2019-07-08T10:01:55.000Z","_content":"\n# Kafka 消息生产及消费原理\n\n## 开篇\n\n关于客户端生产和消费不在本文中探讨，本文主要集中在Kafka服务器端对消息如何存储和如何读取消息。\n\n本文主要探讨如下问题：\n\n1. 服务器端接收到消息后如何处理？\n2. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\n3. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\n\n## 正文\n\n### 服务器端接收到消息处理流程\n\n#### Kafka Server接受消息处理流程\n\n\n\n![](http://blogstatic.aibibang.com/kafka%20server%E6%8E%A5%E5%8F%97%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\nKafkaApis是Kafka server处理所有请求的入口，在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。副本由ReplicaManager管理，对于消息的写入操作在Log与LogSement中进行的。\n\n\n\n#### Log append处理流程\n\n![](http://blogstatic.aibibang.com/log_append%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\n\n\n\n\n真正的日志写入，还是在 LogSegment 的 `append()` 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。数据并不是实时持久化的，mmap只是写入了页缓存，并没有flush进磁盘，当满足`if (unflushedMessages >= config.flushInterval) `才会真正写入磁盘。\n\n## Consumer 获取消息\n\n![](http://matt33.com/images/kafka/kafka_fetch_request.png)\n### 指定offset，Kafka怎么查找到对应的消息\n\n1. 通过文件名前缀数字x找到该绝对offset 对应消息所在文件（log/index）\n2. offset-x为在文件中的相对偏移\n3. 通过相对偏移在index文件找到最近的消息的**位置**(使用二分查找)\n4. 在log文件从最近位置开始逐条寻找\n\n首先根据offset获取`LogSegment`，即`var segmentEntry = segments.floorEntry(startOffset)`segmentEntry 是个抽象的对象，包含log、index,timeindex等对象。\n\n接下来在index中获取position（物理位置），即`val startOffsetAndSize = translateOffset(startOffset)`\n\n源码为：\n\n```java\nprivate[log] def translateOffset(offset: Long, startingFilePosition: Int = 0): LogOffsetPosition = {\n    //二分查找index文件得到下标，再根据算法计算最近的position物理位置\n    val mapping = offsetIndex.lookup(offset)\n    //根据计算的起始位置开始遍历获取准确的position及size\n    log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))\n  }\npublic LogOffsetPosition searchForOffsetWithSize(long targetOffset, int startingPosition) {\n        for (FileChannelRecordBatch batch : batchesFrom(startingPosition)) {\n            long offset = batch.lastOffset();\n            if (offset >= targetOffset)\n                return new LogOffsetPosition(offset, batch.position(), batch.sizeInBytes());\n        }\n        return null;\n}\n```\n\n这里说明一下index中根据下标计算偏移量地址与物理地址：`物理地址=n*8+4`,`偏移量地址=n*8`\n\n```java\n  private def relativeOffset(buffer: ByteBuffer, n: Int): Int = buffer.getInt(n * entrySize)\n\n  private def physical(buffer: ByteBuffer, n: Int): Int = buffer.getInt(n * entrySize + 4)\n\n  override def parseEntry(buffer: ByteBuffer, n: Int): IndexEntry = {\n      OffsetPosition(baseOffset + relativeOffset(buffer, n), physical(buffer, n))\n  }\n```\n\n\n\n最后根据position在log中截取相应的message，`log.slice(startPosition, fetchSize)`\n\n```java\npublic FileRecords slice(int position, int size) throws IOException {\n        //省略校验代码\n        int end = this.start + position + size;\n        // handle integer overflow or if end is beyond the end of the file\n        if (end < 0 || end >= start + sizeInBytes())\n            end = start + sizeInBytes();\n        return new FileRecords(file, channel, this.start + position, end, true);\n}\n```\n\n\n\n看到`searchForOffsetWithSize`有个疑问，上面代码显示返回给客户端的records是批量的，假如提交的offset是这批次的中间一个，那么**返回给Consumer的message是有已经被消费过的信息**，我感觉不可能是这样的，查看了server端代码，再未发现删除已消费的message逻辑。\n\n**Kafka设计者真的这么蠢？？**\n\n随后我查看了客户端consumer源码有发现到如下代码：`if (record.offset() >= nextFetchOffset)`有对大于指定offset消息抛弃的逻辑。\n\n```java\nprivate Record nextFetchedRecord() {\n            while (true) {\n                if (records == null || !records.hasNext()) {\n                   //略\n                } else {\n                    Record record = records.next();\n                    // skip any records out of range\n                    if (record.offset() >= nextFetchOffset) {\n                        // we only do validation when the message should not be skipped.\n                        maybeEnsureValid(record);\n\n                        // control records are not returned to the user\n                        if (!currentBatch.isControlBatch()) {\n                            return record;\n                        } else {\n                            // Increment the next fetch offset when we skip a control batch.\n                            nextFetchOffset = record.offset() + 1;\n                        }\n                    }\n                }\n            }\n        }\n```\n\n至此得出结论：为了提高server端的响应速度，没有对批量消息进行解压缩，然后精准返回指定信息，而是在客户端解压消息，然后再抛弃已处理过的message，这样就不会存在重复消费的问题。这个问题纠结了半天，不知是否正确，仅是自己的理解，如果有哪位同学对这里有研究，欢迎指出问题。\n\n### 指定timestamp，Kafka怎么查找到对应的消息\n\n略\n\n类似根据offset获取消息，不过中间是从timeindex中获取position，然后遍历对比timestamp，获取相应的消息。\n\n## 参考\n\n1.[Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）](http://matt33.com/2018/03/18/kafka-server-handle-produce-request/)\n\n2.[Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）](http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/)","source":"_posts/Kafka-消息生产及消费原理.md","raw":"---\ntitle: Kafka 消息生产及消费原理\ntags:\n  - kafka\n  - 专栏\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-07-08 18:01:55\n---\n\n# Kafka 消息生产及消费原理\n\n## 开篇\n\n关于客户端生产和消费不在本文中探讨，本文主要集中在Kafka服务器端对消息如何存储和如何读取消息。\n\n本文主要探讨如下问题：\n\n1. 服务器端接收到消息后如何处理？\n2. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\n3. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\n\n## 正文\n\n### 服务器端接收到消息处理流程\n\n#### Kafka Server接受消息处理流程\n\n\n\n![](http://blogstatic.aibibang.com/kafka%20server%E6%8E%A5%E5%8F%97%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\nKafkaApis是Kafka server处理所有请求的入口，在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。副本由ReplicaManager管理，对于消息的写入操作在Log与LogSement中进行的。\n\n\n\n#### Log append处理流程\n\n![](http://blogstatic.aibibang.com/log_append%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\n\n\n\n\n真正的日志写入，还是在 LogSegment 的 `append()` 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。数据并不是实时持久化的，mmap只是写入了页缓存，并没有flush进磁盘，当满足`if (unflushedMessages >= config.flushInterval) `才会真正写入磁盘。\n\n## Consumer 获取消息\n\n![](http://matt33.com/images/kafka/kafka_fetch_request.png)\n### 指定offset，Kafka怎么查找到对应的消息\n\n1. 通过文件名前缀数字x找到该绝对offset 对应消息所在文件（log/index）\n2. offset-x为在文件中的相对偏移\n3. 通过相对偏移在index文件找到最近的消息的**位置**(使用二分查找)\n4. 在log文件从最近位置开始逐条寻找\n\n首先根据offset获取`LogSegment`，即`var segmentEntry = segments.floorEntry(startOffset)`segmentEntry 是个抽象的对象，包含log、index,timeindex等对象。\n\n接下来在index中获取position（物理位置），即`val startOffsetAndSize = translateOffset(startOffset)`\n\n源码为：\n\n```java\nprivate[log] def translateOffset(offset: Long, startingFilePosition: Int = 0): LogOffsetPosition = {\n    //二分查找index文件得到下标，再根据算法计算最近的position物理位置\n    val mapping = offsetIndex.lookup(offset)\n    //根据计算的起始位置开始遍历获取准确的position及size\n    log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))\n  }\npublic LogOffsetPosition searchForOffsetWithSize(long targetOffset, int startingPosition) {\n        for (FileChannelRecordBatch batch : batchesFrom(startingPosition)) {\n            long offset = batch.lastOffset();\n            if (offset >= targetOffset)\n                return new LogOffsetPosition(offset, batch.position(), batch.sizeInBytes());\n        }\n        return null;\n}\n```\n\n这里说明一下index中根据下标计算偏移量地址与物理地址：`物理地址=n*8+4`,`偏移量地址=n*8`\n\n```java\n  private def relativeOffset(buffer: ByteBuffer, n: Int): Int = buffer.getInt(n * entrySize)\n\n  private def physical(buffer: ByteBuffer, n: Int): Int = buffer.getInt(n * entrySize + 4)\n\n  override def parseEntry(buffer: ByteBuffer, n: Int): IndexEntry = {\n      OffsetPosition(baseOffset + relativeOffset(buffer, n), physical(buffer, n))\n  }\n```\n\n\n\n最后根据position在log中截取相应的message，`log.slice(startPosition, fetchSize)`\n\n```java\npublic FileRecords slice(int position, int size) throws IOException {\n        //省略校验代码\n        int end = this.start + position + size;\n        // handle integer overflow or if end is beyond the end of the file\n        if (end < 0 || end >= start + sizeInBytes())\n            end = start + sizeInBytes();\n        return new FileRecords(file, channel, this.start + position, end, true);\n}\n```\n\n\n\n看到`searchForOffsetWithSize`有个疑问，上面代码显示返回给客户端的records是批量的，假如提交的offset是这批次的中间一个，那么**返回给Consumer的message是有已经被消费过的信息**，我感觉不可能是这样的，查看了server端代码，再未发现删除已消费的message逻辑。\n\n**Kafka设计者真的这么蠢？？**\n\n随后我查看了客户端consumer源码有发现到如下代码：`if (record.offset() >= nextFetchOffset)`有对大于指定offset消息抛弃的逻辑。\n\n```java\nprivate Record nextFetchedRecord() {\n            while (true) {\n                if (records == null || !records.hasNext()) {\n                   //略\n                } else {\n                    Record record = records.next();\n                    // skip any records out of range\n                    if (record.offset() >= nextFetchOffset) {\n                        // we only do validation when the message should not be skipped.\n                        maybeEnsureValid(record);\n\n                        // control records are not returned to the user\n                        if (!currentBatch.isControlBatch()) {\n                            return record;\n                        } else {\n                            // Increment the next fetch offset when we skip a control batch.\n                            nextFetchOffset = record.offset() + 1;\n                        }\n                    }\n                }\n            }\n        }\n```\n\n至此得出结论：为了提高server端的响应速度，没有对批量消息进行解压缩，然后精准返回指定信息，而是在客户端解压消息，然后再抛弃已处理过的message，这样就不会存在重复消费的问题。这个问题纠结了半天，不知是否正确，仅是自己的理解，如果有哪位同学对这里有研究，欢迎指出问题。\n\n### 指定timestamp，Kafka怎么查找到对应的消息\n\n略\n\n类似根据offset获取消息，不过中间是从timeindex中获取position，然后遍历对比timestamp，获取相应的消息。\n\n## 参考\n\n1.[Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）](http://matt33.com/2018/03/18/kafka-server-handle-produce-request/)\n\n2.[Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）](http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/)","slug":"Kafka-消息生产及消费原理","published":1,"updated":"2019-07-08T10:01:55.197Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvb5002f8ceekvv058gw","content":"<h1 id=\"Kafka-消息生产及消费原理\"><a href=\"#Kafka-消息生产及消费原理\" class=\"headerlink\" title=\"Kafka 消息生产及消费原理\"></a>Kafka 消息生产及消费原理</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>关于客户端生产和消费不在本文中探讨，本文主要集中在Kafka服务器端对消息如何存储和如何读取消息。</p>\n<p>本文主要探讨如下问题：</p>\n<ol>\n<li>服务器端接收到消息后如何处理？</li>\n<li>如果我指定了一个offset，Kafka怎么查找到对应的消息？</li>\n<li>如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"服务器端接收到消息处理流程\"><a href=\"#服务器端接收到消息处理流程\" class=\"headerlink\" title=\"服务器端接收到消息处理流程\"></a>服务器端接收到消息处理流程</h3><h4 id=\"Kafka-Server接受消息处理流程\"><a href=\"#Kafka-Server接受消息处理流程\" class=\"headerlink\" title=\"Kafka Server接受消息处理流程\"></a>Kafka Server接受消息处理流程</h4><p><img src=\"http://blogstatic.aibibang.com/kafka%20server%E6%8E%A5%E5%8F%97%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<p>KafkaApis是Kafka server处理所有请求的入口，在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。副本由ReplicaManager管理，对于消息的写入操作在Log与LogSement中进行的。</p>\n<h4 id=\"Log-append处理流程\"><a href=\"#Log-append处理流程\" class=\"headerlink\" title=\"Log append处理流程\"></a>Log append处理流程</h4><p><img src=\"http://blogstatic.aibibang.com/log_append%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。数据并不是实时持久化的，mmap只是写入了页缓存，并没有flush进磁盘，当满足<code>if (unflushedMessages &gt;= config.flushInterval)</code>才会真正写入磁盘。</p>\n<h2 id=\"Consumer-获取消息\"><a href=\"#Consumer-获取消息\" class=\"headerlink\" title=\"Consumer 获取消息\"></a>Consumer 获取消息</h2><p><img src=\"http://matt33.com/images/kafka/kafka_fetch_request.png\" alt=\"\"></p>\n<h3 id=\"指定offset，Kafka怎么查找到对应的消息\"><a href=\"#指定offset，Kafka怎么查找到对应的消息\" class=\"headerlink\" title=\"指定offset，Kafka怎么查找到对应的消息\"></a>指定offset，Kafka怎么查找到对应的消息</h3><ol>\n<li>通过文件名前缀数字x找到该绝对offset 对应消息所在文件（log/index）</li>\n<li>offset-x为在文件中的相对偏移</li>\n<li>通过相对偏移在index文件找到最近的消息的<strong>位置</strong>(使用二分查找)</li>\n<li>在log文件从最近位置开始逐条寻找</li>\n</ol>\n<p>首先根据offset获取<code>LogSegment</code>，即<code>var segmentEntry = segments.floorEntry(startOffset)</code>segmentEntry 是个抽象的对象，包含log、index,timeindex等对象。</p>\n<p>接下来在index中获取position（物理位置），即<code>val startOffsetAndSize = translateOffset(startOffset)</code></p>\n<p>源码为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[log] <span class=\"function\">def <span class=\"title\">translateOffset</span><span class=\"params\">(offset: Long, startingFilePosition: Int = <span class=\"number\">0</span>)</span>: LogOffsetPosition </span>= &#123;</span><br><span class=\"line\">    <span class=\"comment\">//二分查找index文件得到下标，再根据算法计算最近的position物理位置</span></span><br><span class=\"line\">    val mapping = offsetIndex.lookup(offset)</span><br><span class=\"line\">    <span class=\"comment\">//根据计算的起始位置开始遍历获取准确的position及size</span></span><br><span class=\"line\">    log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> LogOffsetPosition <span class=\"title\">searchForOffsetWithSize</span><span class=\"params\">(<span class=\"keyword\">long</span> targetOffset, <span class=\"keyword\">int</span> startingPosition)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (FileChannelRecordBatch batch : batchesFrom(startingPosition)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> offset = batch.lastOffset();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (offset &gt;= targetOffset)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> LogOffsetPosition(offset, batch.position(), batch.sizeInBytes());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里说明一下index中根据下标计算偏移量地址与物理地址：<code>物理地址=n*8+4</code>,<code>偏移量地址=n*8</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> def <span class=\"title\">relativeOffset</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: Int </span>= buffer.getInt(n * entrySize)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> def <span class=\"title\">physical</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: Int </span>= buffer.getInt(n * entrySize + <span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">override def <span class=\"title\">parseEntry</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: IndexEntry </span>= &#123;</span><br><span class=\"line\">    OffsetPosition(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最后根据position在log中截取相应的message，<code>log.slice(startPosition, fetchSize)</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> FileRecords <span class=\"title\">slice</span><span class=\"params\">(<span class=\"keyword\">int</span> position, <span class=\"keyword\">int</span> size)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//省略校验代码</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> end = <span class=\"keyword\">this</span>.start + position + size;</span><br><span class=\"line\">        <span class=\"comment\">// handle integer overflow or if end is beyond the end of the file</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (end &lt; <span class=\"number\">0</span> || end &gt;= start + sizeInBytes())</span><br><span class=\"line\">            end = start + sizeInBytes();</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> FileRecords(file, channel, <span class=\"keyword\">this</span>.start + position, end, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>看到<code>searchForOffsetWithSize</code>有个疑问，上面代码显示返回给客户端的records是批量的，假如提交的offset是这批次的中间一个，那么<strong>返回给Consumer的message是有已经被消费过的信息</strong>，我感觉不可能是这样的，查看了server端代码，再未发现删除已消费的message逻辑。</p>\n<p><strong>Kafka设计者真的这么蠢？？</strong></p>\n<p>随后我查看了客户端consumer源码有发现到如下代码：<code>if (record.offset() &gt;= nextFetchOffset)</code>有对大于指定offset消息抛弃的逻辑。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Record <span class=\"title\">nextFetchedRecord</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (records == <span class=\"keyword\">null</span> || !records.hasNext()) &#123;</span><br><span class=\"line\">                   <span class=\"comment\">//略</span></span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    Record record = records.next();</span><br><span class=\"line\">                    <span class=\"comment\">// skip any records out of range</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// we only do validation when the message should not be skipped.</span></span><br><span class=\"line\">                        maybeEnsureValid(record);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"comment\">// control records are not returned to the user</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (!currentBatch.isControlBatch()) &#123;</span><br><span class=\"line\">                            <span class=\"keyword\">return</span> record;</span><br><span class=\"line\">                        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                            <span class=\"comment\">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class=\"line\">                            nextFetchOffset = record.offset() + <span class=\"number\">1</span>;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<p>至此得出结论：为了提高server端的响应速度，没有对批量消息进行解压缩，然后精准返回指定信息，而是在客户端解压消息，然后再抛弃已处理过的message，这样就不会存在重复消费的问题。这个问题纠结了半天，不知是否正确，仅是自己的理解，如果有哪位同学对这里有研究，欢迎指出问题。</p>\n<h3 id=\"指定timestamp，Kafka怎么查找到对应的消息\"><a href=\"#指定timestamp，Kafka怎么查找到对应的消息\" class=\"headerlink\" title=\"指定timestamp，Kafka怎么查找到对应的消息\"></a>指定timestamp，Kafka怎么查找到对应的消息</h3><p>略</p>\n<p>类似根据offset获取消息，不过中间是从timeindex中获取position，然后遍历对比timestamp，获取相应的消息。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://matt33.com/2018/03/18/kafka-server-handle-produce-request/\" target=\"_blank\" rel=\"noopener\">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a></p>\n<p>2.<a href=\"http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/\" target=\"_blank\" rel=\"noopener\">Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kafka-消息生产及消费原理\"><a href=\"#Kafka-消息生产及消费原理\" class=\"headerlink\" title=\"Kafka 消息生产及消费原理\"></a>Kafka 消息生产及消费原理</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>关于客户端生产和消费不在本文中探讨，本文主要集中在Kafka服务器端对消息如何存储和如何读取消息。</p>\n<p>本文主要探讨如下问题：</p>\n<ol>\n<li>服务器端接收到消息后如何处理？</li>\n<li>如果我指定了一个offset，Kafka怎么查找到对应的消息？</li>\n<li>如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"服务器端接收到消息处理流程\"><a href=\"#服务器端接收到消息处理流程\" class=\"headerlink\" title=\"服务器端接收到消息处理流程\"></a>服务器端接收到消息处理流程</h3><h4 id=\"Kafka-Server接受消息处理流程\"><a href=\"#Kafka-Server接受消息处理流程\" class=\"headerlink\" title=\"Kafka Server接受消息处理流程\"></a>Kafka Server接受消息处理流程</h4><p><img src=\"http://blogstatic.aibibang.com/kafka%20server%E6%8E%A5%E5%8F%97%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<p>KafkaApis是Kafka server处理所有请求的入口，在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。副本由ReplicaManager管理，对于消息的写入操作在Log与LogSement中进行的。</p>\n<h4 id=\"Log-append处理流程\"><a href=\"#Log-append处理流程\" class=\"headerlink\" title=\"Log append处理流程\"></a>Log append处理流程</h4><p><img src=\"http://blogstatic.aibibang.com/log_append%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。数据并不是实时持久化的，mmap只是写入了页缓存，并没有flush进磁盘，当满足<code>if (unflushedMessages &gt;= config.flushInterval)</code>才会真正写入磁盘。</p>\n<h2 id=\"Consumer-获取消息\"><a href=\"#Consumer-获取消息\" class=\"headerlink\" title=\"Consumer 获取消息\"></a>Consumer 获取消息</h2><p><img src=\"http://matt33.com/images/kafka/kafka_fetch_request.png\" alt=\"\"></p>\n<h3 id=\"指定offset，Kafka怎么查找到对应的消息\"><a href=\"#指定offset，Kafka怎么查找到对应的消息\" class=\"headerlink\" title=\"指定offset，Kafka怎么查找到对应的消息\"></a>指定offset，Kafka怎么查找到对应的消息</h3><ol>\n<li>通过文件名前缀数字x找到该绝对offset 对应消息所在文件（log/index）</li>\n<li>offset-x为在文件中的相对偏移</li>\n<li>通过相对偏移在index文件找到最近的消息的<strong>位置</strong>(使用二分查找)</li>\n<li>在log文件从最近位置开始逐条寻找</li>\n</ol>\n<p>首先根据offset获取<code>LogSegment</code>，即<code>var segmentEntry = segments.floorEntry(startOffset)</code>segmentEntry 是个抽象的对象，包含log、index,timeindex等对象。</p>\n<p>接下来在index中获取position（物理位置），即<code>val startOffsetAndSize = translateOffset(startOffset)</code></p>\n<p>源码为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[log] <span class=\"function\">def <span class=\"title\">translateOffset</span><span class=\"params\">(offset: Long, startingFilePosition: Int = <span class=\"number\">0</span>)</span>: LogOffsetPosition </span>= &#123;</span><br><span class=\"line\">    <span class=\"comment\">//二分查找index文件得到下标，再根据算法计算最近的position物理位置</span></span><br><span class=\"line\">    val mapping = offsetIndex.lookup(offset)</span><br><span class=\"line\">    <span class=\"comment\">//根据计算的起始位置开始遍历获取准确的position及size</span></span><br><span class=\"line\">    log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> LogOffsetPosition <span class=\"title\">searchForOffsetWithSize</span><span class=\"params\">(<span class=\"keyword\">long</span> targetOffset, <span class=\"keyword\">int</span> startingPosition)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (FileChannelRecordBatch batch : batchesFrom(startingPosition)) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">long</span> offset = batch.lastOffset();</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (offset &gt;= targetOffset)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> LogOffsetPosition(offset, batch.position(), batch.sizeInBytes());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">null</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里说明一下index中根据下标计算偏移量地址与物理地址：<code>物理地址=n*8+4</code>,<code>偏移量地址=n*8</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> def <span class=\"title\">relativeOffset</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: Int </span>= buffer.getInt(n * entrySize)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> def <span class=\"title\">physical</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: Int </span>= buffer.getInt(n * entrySize + <span class=\"number\">4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">override def <span class=\"title\">parseEntry</span><span class=\"params\">(buffer: ByteBuffer, n: Int)</span>: IndexEntry </span>= &#123;</span><br><span class=\"line\">    OffsetPosition(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最后根据position在log中截取相应的message，<code>log.slice(startPosition, fetchSize)</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> FileRecords <span class=\"title\">slice</span><span class=\"params\">(<span class=\"keyword\">int</span> position, <span class=\"keyword\">int</span> size)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">//省略校验代码</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span> end = <span class=\"keyword\">this</span>.start + position + size;</span><br><span class=\"line\">        <span class=\"comment\">// handle integer overflow or if end is beyond the end of the file</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (end &lt; <span class=\"number\">0</span> || end &gt;= start + sizeInBytes())</span><br><span class=\"line\">            end = start + sizeInBytes();</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> FileRecords(file, channel, <span class=\"keyword\">this</span>.start + position, end, <span class=\"keyword\">true</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>看到<code>searchForOffsetWithSize</code>有个疑问，上面代码显示返回给客户端的records是批量的，假如提交的offset是这批次的中间一个，那么<strong>返回给Consumer的message是有已经被消费过的信息</strong>，我感觉不可能是这样的，查看了server端代码，再未发现删除已消费的message逻辑。</p>\n<p><strong>Kafka设计者真的这么蠢？？</strong></p>\n<p>随后我查看了客户端consumer源码有发现到如下代码：<code>if (record.offset() &gt;= nextFetchOffset)</code>有对大于指定offset消息抛弃的逻辑。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> Record <span class=\"title\">nextFetchedRecord</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (records == <span class=\"keyword\">null</span> || !records.hasNext()) &#123;</span><br><span class=\"line\">                   <span class=\"comment\">//略</span></span><br><span class=\"line\">                &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                    Record record = records.next();</span><br><span class=\"line\">                    <span class=\"comment\">// skip any records out of range</span></span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class=\"line\">                        <span class=\"comment\">// we only do validation when the message should not be skipped.</span></span><br><span class=\"line\">                        maybeEnsureValid(record);</span><br><span class=\"line\"></span><br><span class=\"line\">                        <span class=\"comment\">// control records are not returned to the user</span></span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (!currentBatch.isControlBatch()) &#123;</span><br><span class=\"line\">                            <span class=\"keyword\">return</span> record;</span><br><span class=\"line\">                        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                            <span class=\"comment\">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class=\"line\">                            nextFetchOffset = record.offset() + <span class=\"number\">1</span>;</span><br><span class=\"line\">                        &#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<p>至此得出结论：为了提高server端的响应速度，没有对批量消息进行解压缩，然后精准返回指定信息，而是在客户端解压消息，然后再抛弃已处理过的message，这样就不会存在重复消费的问题。这个问题纠结了半天，不知是否正确，仅是自己的理解，如果有哪位同学对这里有研究，欢迎指出问题。</p>\n<h3 id=\"指定timestamp，Kafka怎么查找到对应的消息\"><a href=\"#指定timestamp，Kafka怎么查找到对应的消息\" class=\"headerlink\" title=\"指定timestamp，Kafka怎么查找到对应的消息\"></a>指定timestamp，Kafka怎么查找到对应的消息</h3><p>略</p>\n<p>类似根据offset获取消息，不过中间是从timeindex中获取position，然后遍历对比timestamp，获取相应的消息。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://matt33.com/2018/03/18/kafka-server-handle-produce-request/\" target=\"_blank\" rel=\"noopener\">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a></p>\n<p>2.<a href=\"http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/\" target=\"_blank\" rel=\"noopener\">Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</a></p>\n"},{"title":"Kafka原理及设计理论总结","date":"2017-03-05T12:32:03.000Z","_content":"# Kafka原理及设计理论总结\n\n## 一、数据可靠性保证\n\n当Producer向Leader发送数据时,可以通过request.required.acks参数设置数据可靠性的级别\n\n1. **0:** 不论写入是否成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;\n2. **1:** Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据\n3. **-1:** 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证\n仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:\n\n(1).request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;\n(2).min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica\nProducer要在吞吐率和数据可靠性之间做一个权衡\n\n## 二、数据一致性保证\n\n一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到\n\n1. **HighWaterMark**\n\n 简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark <= leader. LogEndOffset\n2. **对于Leader新写入的msg**，\n\n Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费，即Consumer最多只能消费到HW位置\n\n这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)\n","source":"_posts/Kafka原理及设计理论总结.md","raw":"---\ntitle: Kafka原理及设计理论总结\ndate: 2017-03-05 20:32:03\ntags: 大数据\ncategories:\n- kafka\n---\n# Kafka原理及设计理论总结\n\n## 一、数据可靠性保证\n\n当Producer向Leader发送数据时,可以通过request.required.acks参数设置数据可靠性的级别\n\n1. **0:** 不论写入是否成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;\n2. **1:** Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据\n3. **-1:** 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证\n仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:\n\n(1).request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;\n(2).min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica\nProducer要在吞吐率和数据可靠性之间做一个权衡\n\n## 二、数据一致性保证\n\n一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到\n\n1. **HighWaterMark**\n\n 简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark <= leader. LogEndOffset\n2. **对于Leader新写入的msg**，\n\n Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费，即Consumer最多只能消费到HW位置\n\n这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)\n","slug":"Kafka原理及设计理论总结","published":1,"updated":"2017-03-05T12:37:38.108Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvb7002j8ceeej094w85","content":"<h1 id=\"Kafka原理及设计理论总结\"><a href=\"#Kafka原理及设计理论总结\" class=\"headerlink\" title=\"Kafka原理及设计理论总结\"></a>Kafka原理及设计理论总结</h1><h2 id=\"一、数据可靠性保证\"><a href=\"#一、数据可靠性保证\" class=\"headerlink\" title=\"一、数据可靠性保证\"></a>一、数据可靠性保证</h2><p>当Producer向Leader发送数据时,可以通过request.required.acks参数设置数据可靠性的级别</p>\n<ol>\n<li><strong>0:</strong> 不论写入是否成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;</li>\n<li><strong>1:</strong> Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据</li>\n<li><strong>-1:</strong> 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证<br>仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:</li>\n</ol>\n<p>(1).request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;<br>(2).min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica<br>Producer要在吞吐率和数据可靠性之间做一个权衡</p>\n<h2 id=\"二、数据一致性保证\"><a href=\"#二、数据一致性保证\" class=\"headerlink\" title=\"二、数据一致性保证\"></a>二、数据一致性保证</h2><p>一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到</p>\n<ol>\n<li><p><strong>HighWaterMark</strong></p>\n<p>简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark &lt;= leader. LogEndOffset</p>\n</li>\n<li><p><strong>对于Leader新写入的msg</strong>，</p>\n<p>Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费，即Consumer最多只能消费到HW位置</p>\n</li>\n</ol>\n<p>这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kafka原理及设计理论总结\"><a href=\"#Kafka原理及设计理论总结\" class=\"headerlink\" title=\"Kafka原理及设计理论总结\"></a>Kafka原理及设计理论总结</h1><h2 id=\"一、数据可靠性保证\"><a href=\"#一、数据可靠性保证\" class=\"headerlink\" title=\"一、数据可靠性保证\"></a>一、数据可靠性保证</h2><p>当Producer向Leader发送数据时,可以通过request.required.acks参数设置数据可靠性的级别</p>\n<ol>\n<li><strong>0:</strong> 不论写入是否成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;</li>\n<li><strong>1:</strong> Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据</li>\n<li><strong>-1:</strong> 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证<br>仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:</li>\n</ol>\n<p>(1).request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;<br>(2).min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica<br>Producer要在吞吐率和数据可靠性之间做一个权衡</p>\n<h2 id=\"二、数据一致性保证\"><a href=\"#二、数据一致性保证\" class=\"headerlink\" title=\"二、数据一致性保证\"></a>二、数据一致性保证</h2><p>一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到</p>\n<ol>\n<li><p><strong>HighWaterMark</strong></p>\n<p>简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark &lt;= leader. LogEndOffset</p>\n</li>\n<li><p><strong>对于Leader新写入的msg</strong>，</p>\n<p>Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费，即Consumer最多只能消费到HW位置</p>\n</li>\n</ol>\n<p>这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)</p>\n"},{"title":"Kibana之sentinl slack使用教程","date":"2017-09-17T04:26:39.000Z","_content":"# Kibana之sentinl slack使用教程\n## 简介\n[Slack](https://slack.com/)是一个团队协作沟通平台，至于它的强大，就不在这里多说了，sentinl 集成slack,可以将监控告警发送至其中，这个平台支持PC和APP，可以让开发运维人员实时获取监控服务状态。\n## 申请slack\n1. 注册账户\n2. 点击右上方按钮创建workspace\n3. 新建channel ,进入创建好的workspace，在左侧菜单栏创建，将该chanel设置成public\n4. 集成[incoming-webhook](https://my.slack.com/services/new/incoming-webhook),获取Webhook URL。详细步骤详见[此处](https://www.elastic.co/guide/en/x-pack/current/actions-slack.html#configuring-slack)\n## kibana配置\n在kibana.yml配置文件中增加以下内容\n```\nsentinl:\n  settings:\n     slack:\n       active: true\n       username: truman\n       hook: 'https://hooks.slack.com/services/******/*****/*****'\n       channel: '#messagesend'\n\n```\n其中hook配置项为Webhook URL\n## sentinl 配置slack action\n在sentinl中增加slack action\n```\n\"slack action\": {\n        \"throttle_period\": \"0h0m1s\",\n        \"slack\": {\n          \"channel\": \"#messagesend\",\n          \"message\": \"payload.hits.total:{{payload.hits.total}}\",\n          \"stateless\": false\n        }\n```","source":"_posts/Kibana之sentinl-slack使用教程.md","raw":"---\ntitle: Kibana之sentinl slack使用教程\ndate: 2017-09-17 12:26:39\ntags: research\ncategories:\n- elasticsearch\n---\n# Kibana之sentinl slack使用教程\n## 简介\n[Slack](https://slack.com/)是一个团队协作沟通平台，至于它的强大，就不在这里多说了，sentinl 集成slack,可以将监控告警发送至其中，这个平台支持PC和APP，可以让开发运维人员实时获取监控服务状态。\n## 申请slack\n1. 注册账户\n2. 点击右上方按钮创建workspace\n3. 新建channel ,进入创建好的workspace，在左侧菜单栏创建，将该chanel设置成public\n4. 集成[incoming-webhook](https://my.slack.com/services/new/incoming-webhook),获取Webhook URL。详细步骤详见[此处](https://www.elastic.co/guide/en/x-pack/current/actions-slack.html#configuring-slack)\n## kibana配置\n在kibana.yml配置文件中增加以下内容\n```\nsentinl:\n  settings:\n     slack:\n       active: true\n       username: truman\n       hook: 'https://hooks.slack.com/services/******/*****/*****'\n       channel: '#messagesend'\n\n```\n其中hook配置项为Webhook URL\n## sentinl 配置slack action\n在sentinl中增加slack action\n```\n\"slack action\": {\n        \"throttle_period\": \"0h0m1s\",\n        \"slack\": {\n          \"channel\": \"#messagesend\",\n          \"message\": \"payload.hits.total:{{payload.hits.total}}\",\n          \"stateless\": false\n        }\n```","slug":"Kibana之sentinl-slack使用教程","published":1,"updated":"2017-09-17T04:26:59.510Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvb9002l8ceesb8j0l9y","content":"<h1 id=\"Kibana之sentinl-slack使用教程\"><a href=\"#Kibana之sentinl-slack使用教程\" class=\"headerlink\" title=\"Kibana之sentinl slack使用教程\"></a>Kibana之sentinl slack使用教程</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p><a href=\"https://slack.com/\" target=\"_blank\" rel=\"noopener\">Slack</a>是一个团队协作沟通平台，至于它的强大，就不在这里多说了，sentinl 集成slack,可以将监控告警发送至其中，这个平台支持PC和APP，可以让开发运维人员实时获取监控服务状态。</p>\n<h2 id=\"申请slack\"><a href=\"#申请slack\" class=\"headerlink\" title=\"申请slack\"></a>申请slack</h2><ol>\n<li>注册账户</li>\n<li>点击右上方按钮创建workspace</li>\n<li>新建channel ,进入创建好的workspace，在左侧菜单栏创建，将该chanel设置成public</li>\n<li>集成<a href=\"https://my.slack.com/services/new/incoming-webhook\" target=\"_blank\" rel=\"noopener\">incoming-webhook</a>,获取Webhook URL。详细步骤详见<a href=\"https://www.elastic.co/guide/en/x-pack/current/actions-slack.html#configuring-slack\" target=\"_blank\" rel=\"noopener\">此处</a><h2 id=\"kibana配置\"><a href=\"#kibana配置\" class=\"headerlink\" title=\"kibana配置\"></a>kibana配置</h2>在kibana.yml配置文件中增加以下内容<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sentinl:</span><br><span class=\"line\">  settings:</span><br><span class=\"line\">     slack:</span><br><span class=\"line\">       active: true</span><br><span class=\"line\">       username: truman</span><br><span class=\"line\">       hook: &apos;https://hooks.slack.com/services/******/*****/*****&apos;</span><br><span class=\"line\">       channel: &apos;#messagesend&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其中hook配置项为Webhook URL</p>\n<h2 id=\"sentinl-配置slack-action\"><a href=\"#sentinl-配置slack-action\" class=\"headerlink\" title=\"sentinl 配置slack action\"></a>sentinl 配置slack action</h2><p>在sentinl中增加slack action<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;slack action&quot;: &#123;</span><br><span class=\"line\">        &quot;throttle_period&quot;: &quot;0h0m1s&quot;,</span><br><span class=\"line\">        &quot;slack&quot;: &#123;</span><br><span class=\"line\">          &quot;channel&quot;: &quot;#messagesend&quot;,</span><br><span class=\"line\">          &quot;message&quot;: &quot;payload.hits.total:&#123;&#123;payload.hits.total&#125;&#125;&quot;,</span><br><span class=\"line\">          &quot;stateless&quot;: false</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kibana之sentinl-slack使用教程\"><a href=\"#Kibana之sentinl-slack使用教程\" class=\"headerlink\" title=\"Kibana之sentinl slack使用教程\"></a>Kibana之sentinl slack使用教程</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p><a href=\"https://slack.com/\" target=\"_blank\" rel=\"noopener\">Slack</a>是一个团队协作沟通平台，至于它的强大，就不在这里多说了，sentinl 集成slack,可以将监控告警发送至其中，这个平台支持PC和APP，可以让开发运维人员实时获取监控服务状态。</p>\n<h2 id=\"申请slack\"><a href=\"#申请slack\" class=\"headerlink\" title=\"申请slack\"></a>申请slack</h2><ol>\n<li>注册账户</li>\n<li>点击右上方按钮创建workspace</li>\n<li>新建channel ,进入创建好的workspace，在左侧菜单栏创建，将该chanel设置成public</li>\n<li>集成<a href=\"https://my.slack.com/services/new/incoming-webhook\" target=\"_blank\" rel=\"noopener\">incoming-webhook</a>,获取Webhook URL。详细步骤详见<a href=\"https://www.elastic.co/guide/en/x-pack/current/actions-slack.html#configuring-slack\" target=\"_blank\" rel=\"noopener\">此处</a><h2 id=\"kibana配置\"><a href=\"#kibana配置\" class=\"headerlink\" title=\"kibana配置\"></a>kibana配置</h2>在kibana.yml配置文件中增加以下内容<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sentinl:</span><br><span class=\"line\">  settings:</span><br><span class=\"line\">     slack:</span><br><span class=\"line\">       active: true</span><br><span class=\"line\">       username: truman</span><br><span class=\"line\">       hook: &apos;https://hooks.slack.com/services/******/*****/*****&apos;</span><br><span class=\"line\">       channel: &apos;#messagesend&apos;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其中hook配置项为Webhook URL</p>\n<h2 id=\"sentinl-配置slack-action\"><a href=\"#sentinl-配置slack-action\" class=\"headerlink\" title=\"sentinl 配置slack action\"></a>sentinl 配置slack action</h2><p>在sentinl中增加slack action<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;slack action&quot;: &#123;</span><br><span class=\"line\">        &quot;throttle_period&quot;: &quot;0h0m1s&quot;,</span><br><span class=\"line\">        &quot;slack&quot;: &#123;</span><br><span class=\"line\">          &quot;channel&quot;: &quot;#messagesend&quot;,</span><br><span class=\"line\">          &quot;message&quot;: &quot;payload.hits.total:&#123;&#123;payload.hits.total&#125;&#125;&quot;,</span><br><span class=\"line\">          &quot;stateless&quot;: false</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"MapReduce笔记","date":"2016-10-24T13:05:43.000Z","_content":"## MapReduce运行过程\n个人理解整个过程是先对数据分片（这个过程还未读取真正数据），将数据划分到多个map,一个job可以包含多个map,MapReduce框架将多个job发送到多个节点上执行，每个job中map读取自己分片数据，然后根据业务代码过滤，再根据map输出进行reduce操作，最后将生成结果输出到一个目录中。\n\n","source":"_posts/MapReduce笔记.md","raw":"---\ntitle: MapReduce笔记\ndate: 2016-10-24 21:05:43\ntags: 大数据\ncategories:\n- hadoop\n---\n## MapReduce运行过程\n个人理解整个过程是先对数据分片（这个过程还未读取真正数据），将数据划分到多个map,一个job可以包含多个map,MapReduce框架将多个job发送到多个节点上执行，每个job中map读取自己分片数据，然后根据业务代码过滤，再根据map输出进行reduce操作，最后将生成结果输出到一个目录中。\n\n","slug":"MapReduce笔记","published":1,"updated":"2016-10-24T13:06:59.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbb002o8ceeo5gnbfvd","content":"<h2 id=\"MapReduce运行过程\"><a href=\"#MapReduce运行过程\" class=\"headerlink\" title=\"MapReduce运行过程\"></a>MapReduce运行过程</h2><p>个人理解整个过程是先对数据分片（这个过程还未读取真正数据），将数据划分到多个map,一个job可以包含多个map,MapReduce框架将多个job发送到多个节点上执行，每个job中map读取自己分片数据，然后根据业务代码过滤，再根据map输出进行reduce操作，最后将生成结果输出到一个目录中。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"MapReduce运行过程\"><a href=\"#MapReduce运行过程\" class=\"headerlink\" title=\"MapReduce运行过程\"></a>MapReduce运行过程</h2><p>个人理解整个过程是先对数据分片（这个过程还未读取真正数据），将数据划分到多个map,一个job可以包含多个map,MapReduce框架将多个job发送到多个节点上执行，每个job中map读取自己分片数据，然后根据业务代码过滤，再根据map输出进行reduce操作，最后将生成结果输出到一个目录中。</p>\n"},{"title":"Markdown 语法学习","date":"2016-03-12T12:22:49.000Z","_content":"# Markdown语法学习\n## 第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n## 第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n\tprivate byte[] family;\n\tprivate byte[] column;\n\tprivate byte[] data;\n\t\n\t/**\n\t * @return the row\n\t */\n\tpublic byte[] getRow() {\n\t\treturn row;\n\t}\n</code></pre>\n\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com \"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n***\n原始代码\n***\n\n<pre><code>\n#Markdown语法学习\n##第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n##第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n\tprivate byte[] family;\n\tprivate byte[] column;\n\tprivate byte[] data;\n\t\n\t/**\n\t * @return the row\n\t */\n\tpublic byte[] getRow() {\n\t\treturn row;\n\t}\n</code></pre>\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com\"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\n</code></pre>\n","source":"_posts/Markdown-语法学习.md","raw":"---\ntitle: Markdown 语法学习\ndate: 2016-03-12 20:22:49\ntags: 写作技巧\n---\n# Markdown语法学习\n## 第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n## 第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n\tprivate byte[] family;\n\tprivate byte[] column;\n\tprivate byte[] data;\n\t\n\t/**\n\t * @return the row\n\t */\n\tpublic byte[] getRow() {\n\t\treturn row;\n\t}\n</code></pre>\n\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com \"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n***\n原始代码\n***\n\n<pre><code>\n#Markdown语法学习\n##第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n##第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n\tprivate byte[] family;\n\tprivate byte[] column;\n\tprivate byte[] data;\n\t\n\t/**\n\t * @return the row\n\t */\n\tpublic byte[] getRow() {\n\t\treturn row;\n\t}\n</code></pre>\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com\"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\n</code></pre>\n","slug":"Markdown-语法学习","published":1,"updated":"2016-03-12T12:48:08.575Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbe002s8ceehi743wza","content":"<h1 id=\"Markdown语法学习\"><a href=\"#Markdown语法学习\" class=\"headerlink\" title=\"Markdown语法学习\"></a>Markdown语法学习</h1><h2 id=\"第一节：章节-项目\"><a href=\"#第一节：章节-项目\" class=\"headerlink\" title=\"第一节：章节 项目\"></a>第一节：章节 项目</h2><ul>\n<li>部署</li>\n<li>实验</li>\n<li><p>测试</p>\n<p>1.开发</p>\n<p>2.发布</p>\n</li>\n</ul>\n<h2 id=\"第二节：杂乱\"><a href=\"#第二节：杂乱\" class=\"headerlink\" title=\"第二节：杂乱\"></a>第二节：杂乱</h2><p><strong>分割线</strong></p>\n<hr>\n<hr>\n<p><strong>代码</strong></p>\n<pre><code>\nprivate byte[] row;\n    private byte[] family;\n    private byte[] column;\n    private byte[] data;\n\n    /**\n     * @return the row\n     */\n    public byte[] getRow() {\n        return row;\n    }\n</code></pre>\n\n<hr>\n<hr>\n<p><strong>链接</strong></p>\n<p>对我的额非非<a href=\"http:www.baidu.com\" title=\"百度无所不能\" target=\"_blank\" rel=\"noopener\">医用</a>氨基酸S耗时</p>\n<p><img src=\"http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg\" alt=\"\"></p>\n<p>[医用]：http:<a href=\"http://www.baidu.com\" target=\"_blank\" rel=\"noopener\">www.baidu.com</a> “百度无所不能”</p>\n<p><strong>强调</strong></p>\n<p>斜体 <em>强调呢哦荣挨打的好多</em></p>\n<p>粗体 <strong>强调呢哦荣挨打的好多</strong></p>\n<p><strong>引用</strong></p>\n<blockquote>\n<p>&emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。</p>\n</blockquote>\n<p><strong>表格</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tables</th>\n<th style=\"text-align:center\">Are</th>\n<th style=\"text-align:right\">Cool</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>col 3 is</td>\n<td style=\"text-align:center\">right-aligned</td>\n<td style=\"text-align:right\">$1600</td>\n</tr>\n<tr>\n<td>col 2 is</td>\n<td style=\"text-align:center\">centered</td>\n<td style=\"text-align:right\">$12</td>\n</tr>\n<tr>\n<td>zebra stripes</td>\n<td style=\"text-align:center\">are neat</td>\n<td style=\"text-align:right\">$1</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p>原始代码</p>\n<hr>\n<pre><code>\n#Markdown语法学习\n##第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n##第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n    private byte[] family;\n    private byte[] column;\n    private byte[] data;\n\n    /**\n     * @return the row\n     */\n    public byte[] getRow() {\n        return row;\n    }\n</code></pre>\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com\"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Markdown语法学习\"><a href=\"#Markdown语法学习\" class=\"headerlink\" title=\"Markdown语法学习\"></a>Markdown语法学习</h1><h2 id=\"第一节：章节-项目\"><a href=\"#第一节：章节-项目\" class=\"headerlink\" title=\"第一节：章节 项目\"></a>第一节：章节 项目</h2><ul>\n<li>部署</li>\n<li>实验</li>\n<li><p>测试</p>\n<p>1.开发</p>\n<p>2.发布</p>\n</li>\n</ul>\n<h2 id=\"第二节：杂乱\"><a href=\"#第二节：杂乱\" class=\"headerlink\" title=\"第二节：杂乱\"></a>第二节：杂乱</h2><p><strong>分割线</strong></p>\n<hr>\n<hr>\n<p><strong>代码</strong></p>\n<pre><code>\nprivate byte[] row;\n    private byte[] family;\n    private byte[] column;\n    private byte[] data;\n\n    /**\n     * @return the row\n     */\n    public byte[] getRow() {\n        return row;\n    }\n</code></pre>\n\n<hr>\n<hr>\n<p><strong>链接</strong></p>\n<p>对我的额非非<a href=\"http:www.baidu.com\" title=\"百度无所不能\" target=\"_blank\" rel=\"noopener\">医用</a>氨基酸S耗时</p>\n<p><img src=\"http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg\" alt=\"\"></p>\n<p>[医用]：http:<a href=\"http://www.baidu.com\" target=\"_blank\" rel=\"noopener\">www.baidu.com</a> “百度无所不能”</p>\n<p><strong>强调</strong></p>\n<p>斜体 <em>强调呢哦荣挨打的好多</em></p>\n<p>粗体 <strong>强调呢哦荣挨打的好多</strong></p>\n<p><strong>引用</strong></p>\n<blockquote>\n<p>&emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。</p>\n</blockquote>\n<p><strong>表格</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tables</th>\n<th style=\"text-align:center\">Are</th>\n<th style=\"text-align:right\">Cool</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>col 3 is</td>\n<td style=\"text-align:center\">right-aligned</td>\n<td style=\"text-align:right\">$1600</td>\n</tr>\n<tr>\n<td>col 2 is</td>\n<td style=\"text-align:center\">centered</td>\n<td style=\"text-align:right\">$12</td>\n</tr>\n<tr>\n<td>zebra stripes</td>\n<td style=\"text-align:center\">are neat</td>\n<td style=\"text-align:right\">$1</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p>原始代码</p>\n<hr>\n<pre><code>\n#Markdown语法学习\n##第一节：章节 项目\n- 部署\n- 实验\n- 测试\n\n 1.开发\n\n 2.发布\n\n\n##第二节：杂乱\n**分割线**\n___\n***\n**代码**\n\n<pre><code>\nprivate byte[] row;\n    private byte[] family;\n    private byte[] column;\n    private byte[] data;\n\n    /**\n     * @return the row\n     */\n    public byte[] getRow() {\n        return row;\n    }\n</code></pre>\n***\n___\n**链接**\n\n对我的额非非[医用](http:www.baidu.com \"百度无所不能\")氨基酸S耗时\n\n![](http://ww4.sinaimg.cn/bmiddle/aa397b7fjw1dzplsgpdw5j.jpg)\n\n[医用]：http:www.baidu.com\"百度无所不能\"\n\n**强调**\n\n斜体 *强调呢哦荣挨打的好多*\n\n粗体 **强调呢哦荣挨打的好多**\n\n**引用**\n> &emsp;一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。\n\n**表格**\n\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\n</code></pre>\n"},{"title":"React问题笔记","date":"2018-10-15T09:46:16.000Z","_content":"## 1.this 作用域处理方案\n\n问题背景：手动触发事件，更改state中数据\n\n1. 箭头表达式\n```\n  mergeFilter(_this) {\n    let store = [];\n    if (!_this.state.merge) {\n      store = _this.mergetIndexName(this.state.indices, /[^a-z]+$/);\n    } else {\n      store = _this.reBuildOriginData(this.state.indices);\n    }\n    _this.setState({\n      store: store,\n      merge: !_this.state.merge,\n    });\n  } \n```\n   render组件中调用  \n```\n<EuiSwitch\n        label=\"Merge\"\n        key=\"MergeEuiSwitch\"\n        checked={\n          this.state.merge\n        }\n        onChange={()=>this.mergeFilter(this)}\n/>  \n```\n2. bind\n```\n  constructor(props) {\n    super(props);\n    this.state = {value: 'coconut'};\n\n    this.handleChange = this.handleChange.bind(this);\n  }\n\n  handleChange(event) {\n    this.setState({value: event.target.value});\n  }\n  \n render() {\n    return (\n      <form onSubmit={this.handleSubmit}>\n        <label>\n          Pick your favorite flavor:\n          <select value={this.state.value} onChange={this.handleChange}>\n            <option value=\"grapefruit\">Grapefruit</option>\n            <option value=\"lime\">Lime</option>\n            <option value=\"coconut\">Coconut</option>\n            <option value=\"mango\">Mango</option>\n          </select>\n        </label>\n        <input type=\"submit\" value=\"Submit\" />\n      </form>\n    );\n  }\n```\n\n\n\n\n\n\n## 参考\n1.[he-select-tag](https://reactjs.org/docs/forms.html#the-select-tag)\n\n","source":"_posts/React问题笔记.md","raw":"---\ntitle: React问题笔记\ndate: 2018-10-15 17:46:16\ntags: 笔记\ncategories:\n- React\n---\n## 1.this 作用域处理方案\n\n问题背景：手动触发事件，更改state中数据\n\n1. 箭头表达式\n```\n  mergeFilter(_this) {\n    let store = [];\n    if (!_this.state.merge) {\n      store = _this.mergetIndexName(this.state.indices, /[^a-z]+$/);\n    } else {\n      store = _this.reBuildOriginData(this.state.indices);\n    }\n    _this.setState({\n      store: store,\n      merge: !_this.state.merge,\n    });\n  } \n```\n   render组件中调用  \n```\n<EuiSwitch\n        label=\"Merge\"\n        key=\"MergeEuiSwitch\"\n        checked={\n          this.state.merge\n        }\n        onChange={()=>this.mergeFilter(this)}\n/>  \n```\n2. bind\n```\n  constructor(props) {\n    super(props);\n    this.state = {value: 'coconut'};\n\n    this.handleChange = this.handleChange.bind(this);\n  }\n\n  handleChange(event) {\n    this.setState({value: event.target.value});\n  }\n  \n render() {\n    return (\n      <form onSubmit={this.handleSubmit}>\n        <label>\n          Pick your favorite flavor:\n          <select value={this.state.value} onChange={this.handleChange}>\n            <option value=\"grapefruit\">Grapefruit</option>\n            <option value=\"lime\">Lime</option>\n            <option value=\"coconut\">Coconut</option>\n            <option value=\"mango\">Mango</option>\n          </select>\n        </label>\n        <input type=\"submit\" value=\"Submit\" />\n      </form>\n    );\n  }\n```\n\n\n\n\n\n\n## 参考\n1.[he-select-tag](https://reactjs.org/docs/forms.html#the-select-tag)\n\n","slug":"React问题笔记","published":1,"updated":"2018-10-15T10:07:29.541Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbg002w8ceekmfxaorl","content":"<h2 id=\"1-this-作用域处理方案\"><a href=\"#1-this-作用域处理方案\" class=\"headerlink\" title=\"1.this 作用域处理方案\"></a>1.this 作用域处理方案</h2><p>问题背景：手动触发事件，更改state中数据</p>\n<ol>\n<li><p>箭头表达式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mergeFilter(_this) &#123;</span><br><span class=\"line\">  let store = [];</span><br><span class=\"line\">  if (!_this.state.merge) &#123;</span><br><span class=\"line\">    store = _this.mergetIndexName(this.state.indices, /[^a-z]+$/);</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    store = _this.reBuildOriginData(this.state.indices);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _this.setState(&#123;</span><br><span class=\"line\">    store: store,</span><br><span class=\"line\">    merge: !_this.state.merge,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>render组件中调用  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;EuiSwitch</span><br><span class=\"line\">        label=&quot;Merge&quot;</span><br><span class=\"line\">        key=&quot;MergeEuiSwitch&quot;</span><br><span class=\"line\">        checked=&#123;</span><br><span class=\"line\">          this.state.merge</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        onChange=&#123;()=&gt;this.mergeFilter(this)&#125;</span><br><span class=\"line\">/&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>bind</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> constructor(props) &#123;</span><br><span class=\"line\">   super(props);</span><br><span class=\"line\">   this.state = &#123;value: &apos;coconut&apos;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   this.handleChange = this.handleChange.bind(this);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> handleChange(event) &#123;</span><br><span class=\"line\">   this.setState(&#123;value: event.target.value&#125;);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\">   return (</span><br><span class=\"line\">     &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;</span><br><span class=\"line\">       &lt;label&gt;</span><br><span class=\"line\">         Pick your favorite flavor:</span><br><span class=\"line\">         &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt;</span><br><span class=\"line\">           &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;lime&quot;&gt;Lime&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;coconut&quot;&gt;Coconut&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;mango&quot;&gt;Mango&lt;/option&gt;</span><br><span class=\"line\">         &lt;/select&gt;</span><br><span class=\"line\">       &lt;/label&gt;</span><br><span class=\"line\">       &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;</span><br><span class=\"line\">     &lt;/form&gt;</span><br><span class=\"line\">   );</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://reactjs.org/docs/forms.html#the-select-tag\" target=\"_blank\" rel=\"noopener\">he-select-tag</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-this-作用域处理方案\"><a href=\"#1-this-作用域处理方案\" class=\"headerlink\" title=\"1.this 作用域处理方案\"></a>1.this 作用域处理方案</h2><p>问题背景：手动触发事件，更改state中数据</p>\n<ol>\n<li><p>箭头表达式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mergeFilter(_this) &#123;</span><br><span class=\"line\">  let store = [];</span><br><span class=\"line\">  if (!_this.state.merge) &#123;</span><br><span class=\"line\">    store = _this.mergetIndexName(this.state.indices, /[^a-z]+$/);</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    store = _this.reBuildOriginData(this.state.indices);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  _this.setState(&#123;</span><br><span class=\"line\">    store: store,</span><br><span class=\"line\">    merge: !_this.state.merge,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>render组件中调用  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;EuiSwitch</span><br><span class=\"line\">        label=&quot;Merge&quot;</span><br><span class=\"line\">        key=&quot;MergeEuiSwitch&quot;</span><br><span class=\"line\">        checked=&#123;</span><br><span class=\"line\">          this.state.merge</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        onChange=&#123;()=&gt;this.mergeFilter(this)&#125;</span><br><span class=\"line\">/&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>bind</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> constructor(props) &#123;</span><br><span class=\"line\">   super(props);</span><br><span class=\"line\">   this.state = &#123;value: &apos;coconut&apos;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   this.handleChange = this.handleChange.bind(this);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> handleChange(event) &#123;</span><br><span class=\"line\">   this.setState(&#123;value: event.target.value&#125;);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\">   return (</span><br><span class=\"line\">     &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;</span><br><span class=\"line\">       &lt;label&gt;</span><br><span class=\"line\">         Pick your favorite flavor:</span><br><span class=\"line\">         &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt;</span><br><span class=\"line\">           &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;lime&quot;&gt;Lime&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;coconut&quot;&gt;Coconut&lt;/option&gt;</span><br><span class=\"line\">           &lt;option value=&quot;mango&quot;&gt;Mango&lt;/option&gt;</span><br><span class=\"line\">         &lt;/select&gt;</span><br><span class=\"line\">       &lt;/label&gt;</span><br><span class=\"line\">       &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;</span><br><span class=\"line\">     &lt;/form&gt;</span><br><span class=\"line\">   );</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://reactjs.org/docs/forms.html#the-select-tag\" target=\"_blank\" rel=\"noopener\">he-select-tag</a></p>\n"},{"title":"OutOfMemoryError-unable to create native thread问题追究","date":"2018-07-22T10:50:10.000Z","_content":"# OutOfMemoryError-unable to create native thread问题追究\n\n## 1.问题背景\n某天，发现线上elasticsearch 集群，有个节点down了，重启后，过了一个多小时，又down,这下才去认真的查看log,发现在down掉之前，有报以下错误：\n```\n[67329.555s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n\n[67329.557s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n\n[2018-07-12T02:47:53,549][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [e11redis28.mercury.corp] fatal error in thread [elasticsearch[e11redis28.mercury.corp][refresh][T#2]], exiting\n\njava.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n\nt java.lang.Thread.start0(Native Method) ~[?:?]\n\nt java.lang.Thread.start(Thread.java:813) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:944) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1012) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]\n\nt java.lang.Thread.run(Thread.java:844) [?:?]\n```\n困扰了好几天，修改各种jvm 配置发现无用，就在github 上提了个[issues](https://github.com/elastic/elasticsearch/issues/31982),里面详细的描述了该问题及环境的配置。\n## 2.该问题产生可能原因\n经过查阅大量资料，才发现该错误不是我们一眼看的那么简单。\n\nOutOfMemoryError: Unable to Create New Native Thread\n\n这个错误其实已经告诉我们，该问题可能是两个原因造成的：\n\n- 内存不足\n\n  该内存不足，其实不是指堆内存，而是创建栈内存不足。所以说出现该问题修改Xmx/Xms是无法解决的。默认情况下jvm 创建1个线程，会分配1m空间。这里所指内存不足是机器中可用内存不足。对于该原因的解决办法是给jvm 留够需要的内存，方法有多个：\n  >1. 减少Xss配置，这样话可以利用有限的内存创建更多的线程，但是需注意避免值过小栈溢出。\n  >2. 减少堆配置，主要还是留下足够内存供jvm 创建线程所用。\n  >3. 杀掉其他程序，留出空闲内存。\n  \n- 机器线程数受限制\n\n  众所周知，linux系统中对进程的创建是有限制的，不可能无限创建的，影响该数量主要是由以下三个方面：\n  >1. /proc/sys/kernel/threads-max \n  >2. max_user_process（ulimit –u）\n  >3. /proc/sys/kernel/pid_max\n  \n  通过这**三个参数共同**决定了linux机器中能创建线程数量\n## 3.linux 进程相关信息\n在这里主要是想写一些命令帮助人们分析linux进程下的线程等信息\n\n- 查看linux所有用户下所有进程数\n\n   ```\n   ps -eLo ruser|awk 'NR>1'|sort|uniq -c\n   ```\n- 查看进程中的线程数\n\n  ``` \n  ps –o nlwp 27989 \n  ```\n- 查看某个进程下的线程\n\n  ``` \n   top -H -p <pid>\n  ps -eLf | grep <pid>\n  ```\n## 4.检测系统参数\n1. 查看内存 free -g\n2. 查看limit\n\n   - ulimit -a\n   - cat /proc/sys/kernel/threads-max \n   - cat /proc/sys/kernel/pid_max\n\n\n## 5.问题模拟验证\n经过以上步骤分析，写了个脚本，监控该机器上内存使用情况及线程使用情况，发现问题出现在有个程序会创建大量的线程，该程序是java 程序。查看机器pid_max为32768，ulimit -u  未限制大小。 总结一下这个机器线程最多允许创建 32768。\n\n\n以下是验证代码：注意该代码有一定危险性，请勿在window 下运行。\n\n\n```\nint i =0;\nwhile(true) {\n\tThread thread = new Thread(new Runnable() {\n\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile(true) {\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t});\n\t\n\tthread.start();\n\ti++;\n\tSystem.out.println(\"curent thread num:\"+i);\n}\n```\n\n运行该代码，当线程增长到3.1w时，报该问题。\n\n## 6.本次问题原因\n本次问题是发现是有个程序创建了大量线程，造成达到linux 机器允许最大的线程数。切记这个不是一个参数控制的，多个参数控制pid_max，一般人可能会忽略！\n\n## 7.问题解决办法\n根据不同问题选择不同办法，详见第2点 [该问题产生可能原因](#2.该问题产生可能原因)\n## 8.参考\n\n1. [We are out of memory](https://www.elastic.co/blog/we-are-out-of-memory-systemd-process-limits)\n2. [为何线程有PID](https://blog.csdn.net/lh2016rocky/article/details/55671656)\n3. [Troubleshoot OutOfMemoryError: Unable to Create New Native Thread](https://dzone.com/articles/troubleshoot-outofmemoryerror-unable-to-create-new)","source":"_posts/OutOfMemoryError-unable-to-create-native-thread问题追究.md","raw":"---\ntitle: OutOfMemoryError-unable to create native thread问题追究\ndate: 2018-07-22 18:50:10\ntags: 问题答疑\ncategories:\n         - linux\n         - java\n---\n# OutOfMemoryError-unable to create native thread问题追究\n\n## 1.问题背景\n某天，发现线上elasticsearch 集群，有个节点down了，重启后，过了一个多小时，又down,这下才去认真的查看log,发现在down掉之前，有报以下错误：\n```\n[67329.555s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n\n[67329.557s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n\n[2018-07-12T02:47:53,549][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [e11redis28.mercury.corp] fatal error in thread [elasticsearch[e11redis28.mercury.corp][refresh][T#2]], exiting\n\njava.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n\nt java.lang.Thread.start0(Native Method) ~[?:?]\n\nt java.lang.Thread.start(Thread.java:813) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:944) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1012) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:?]\n\nt java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]\n\nt java.lang.Thread.run(Thread.java:844) [?:?]\n```\n困扰了好几天，修改各种jvm 配置发现无用，就在github 上提了个[issues](https://github.com/elastic/elasticsearch/issues/31982),里面详细的描述了该问题及环境的配置。\n## 2.该问题产生可能原因\n经过查阅大量资料，才发现该错误不是我们一眼看的那么简单。\n\nOutOfMemoryError: Unable to Create New Native Thread\n\n这个错误其实已经告诉我们，该问题可能是两个原因造成的：\n\n- 内存不足\n\n  该内存不足，其实不是指堆内存，而是创建栈内存不足。所以说出现该问题修改Xmx/Xms是无法解决的。默认情况下jvm 创建1个线程，会分配1m空间。这里所指内存不足是机器中可用内存不足。对于该原因的解决办法是给jvm 留够需要的内存，方法有多个：\n  >1. 减少Xss配置，这样话可以利用有限的内存创建更多的线程，但是需注意避免值过小栈溢出。\n  >2. 减少堆配置，主要还是留下足够内存供jvm 创建线程所用。\n  >3. 杀掉其他程序，留出空闲内存。\n  \n- 机器线程数受限制\n\n  众所周知，linux系统中对进程的创建是有限制的，不可能无限创建的，影响该数量主要是由以下三个方面：\n  >1. /proc/sys/kernel/threads-max \n  >2. max_user_process（ulimit –u）\n  >3. /proc/sys/kernel/pid_max\n  \n  通过这**三个参数共同**决定了linux机器中能创建线程数量\n## 3.linux 进程相关信息\n在这里主要是想写一些命令帮助人们分析linux进程下的线程等信息\n\n- 查看linux所有用户下所有进程数\n\n   ```\n   ps -eLo ruser|awk 'NR>1'|sort|uniq -c\n   ```\n- 查看进程中的线程数\n\n  ``` \n  ps –o nlwp 27989 \n  ```\n- 查看某个进程下的线程\n\n  ``` \n   top -H -p <pid>\n  ps -eLf | grep <pid>\n  ```\n## 4.检测系统参数\n1. 查看内存 free -g\n2. 查看limit\n\n   - ulimit -a\n   - cat /proc/sys/kernel/threads-max \n   - cat /proc/sys/kernel/pid_max\n\n\n## 5.问题模拟验证\n经过以上步骤分析，写了个脚本，监控该机器上内存使用情况及线程使用情况，发现问题出现在有个程序会创建大量的线程，该程序是java 程序。查看机器pid_max为32768，ulimit -u  未限制大小。 总结一下这个机器线程最多允许创建 32768。\n\n\n以下是验证代码：注意该代码有一定危险性，请勿在window 下运行。\n\n\n```\nint i =0;\nwhile(true) {\n\tThread thread = new Thread(new Runnable() {\n\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile(true) {\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t});\n\t\n\tthread.start();\n\ti++;\n\tSystem.out.println(\"curent thread num:\"+i);\n}\n```\n\n运行该代码，当线程增长到3.1w时，报该问题。\n\n## 6.本次问题原因\n本次问题是发现是有个程序创建了大量线程，造成达到linux 机器允许最大的线程数。切记这个不是一个参数控制的，多个参数控制pid_max，一般人可能会忽略！\n\n## 7.问题解决办法\n根据不同问题选择不同办法，详见第2点 [该问题产生可能原因](#2.该问题产生可能原因)\n## 8.参考\n\n1. [We are out of memory](https://www.elastic.co/blog/we-are-out-of-memory-systemd-process-limits)\n2. [为何线程有PID](https://blog.csdn.net/lh2016rocky/article/details/55671656)\n3. [Troubleshoot OutOfMemoryError: Unable to Create New Native Thread](https://dzone.com/articles/troubleshoot-outofmemoryerror-unable-to-create-new)","slug":"OutOfMemoryError-unable-to-create-native-thread问题追究","published":1,"updated":"2018-07-22T12:02:30.763Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbj00308ceeajiknuky","content":"<h1 id=\"OutOfMemoryError-unable-to-create-native-thread问题追究\"><a href=\"#OutOfMemoryError-unable-to-create-native-thread问题追究\" class=\"headerlink\" title=\"OutOfMemoryError-unable to create native thread问题追究\"></a>OutOfMemoryError-unable to create native thread问题追究</h1><h2 id=\"1-问题背景\"><a href=\"#1-问题背景\" class=\"headerlink\" title=\"1.问题背景\"></a>1.问题背景</h2><p>某天，发现线上elasticsearch 集群，有个节点down了，重启后，过了一个多小时，又down,这下才去认真的查看log,发现在down掉之前，有报以下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[67329.555s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.</span><br><span class=\"line\"></span><br><span class=\"line\">[67329.557s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.</span><br><span class=\"line\"></span><br><span class=\"line\">[2018-07-12T02:47:53,549][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [e11redis28.mercury.corp] fatal error in thread [elasticsearch[e11redis28.mercury.corp][refresh][T#2]], exiting</span><br><span class=\"line\"></span><br><span class=\"line\">java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.start0(Native Method) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.start(Thread.java:813) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:944) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1012) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.run(Thread.java:844) [?:?]</span><br></pre></td></tr></table></figure></p>\n<p>困扰了好几天，修改各种jvm 配置发现无用，就在github 上提了个<a href=\"https://github.com/elastic/elasticsearch/issues/31982\" target=\"_blank\" rel=\"noopener\">issues</a>,里面详细的描述了该问题及环境的配置。</p>\n<h2 id=\"2-该问题产生可能原因\"><a href=\"#2-该问题产生可能原因\" class=\"headerlink\" title=\"2.该问题产生可能原因\"></a>2.该问题产生可能原因</h2><p>经过查阅大量资料，才发现该错误不是我们一眼看的那么简单。</p>\n<p>OutOfMemoryError: Unable to Create New Native Thread</p>\n<p>这个错误其实已经告诉我们，该问题可能是两个原因造成的：</p>\n<ul>\n<li><p>内存不足</p>\n<p>该内存不足，其实不是指堆内存，而是创建栈内存不足。所以说出现该问题修改Xmx/Xms是无法解决的。默认情况下jvm 创建1个线程，会分配1m空间。这里所指内存不足是机器中可用内存不足。对于该原因的解决办法是给jvm 留够需要的内存，方法有多个：</p>\n<blockquote>\n<ol>\n<li>减少Xss配置，这样话可以利用有限的内存创建更多的线程，但是需注意避免值过小栈溢出。</li>\n<li>减少堆配置，主要还是留下足够内存供jvm 创建线程所用。</li>\n<li>杀掉其他程序，留出空闲内存。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>机器线程数受限制</p>\n<p>众所周知，linux系统中对进程的创建是有限制的，不可能无限创建的，影响该数量主要是由以下三个方面：</p>\n<blockquote>\n<ol>\n<li>/proc/sys/kernel/threads-max </li>\n<li>max_user_process（ulimit –u）</li>\n<li>/proc/sys/kernel/pid_max</li>\n</ol>\n</blockquote>\n<p>通过这<strong>三个参数共同</strong>决定了linux机器中能创建线程数量</p>\n<h2 id=\"3-linux-进程相关信息\"><a href=\"#3-linux-进程相关信息\" class=\"headerlink\" title=\"3.linux 进程相关信息\"></a>3.linux 进程相关信息</h2><p>在这里主要是想写一些命令帮助人们分析linux进程下的线程等信息</p>\n</li>\n<li><p>查看linux所有用户下所有进程数</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps -eLo ruser|awk &apos;NR&gt;1&apos;|sort|uniq -c</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看进程中的线程数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps –o nlwp 27989</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看某个进程下的线程</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> top -H -p &lt;pid&gt;</span><br><span class=\"line\">ps -eLf | grep &lt;pid&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"4-检测系统参数\"><a href=\"#4-检测系统参数\" class=\"headerlink\" title=\"4.检测系统参数\"></a>4.检测系统参数</h2><ol>\n<li>查看内存 free -g</li>\n<li><p>查看limit</p>\n<ul>\n<li>ulimit -a</li>\n<li>cat /proc/sys/kernel/threads-max </li>\n<li>cat /proc/sys/kernel/pid_max</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"5-问题模拟验证\"><a href=\"#5-问题模拟验证\" class=\"headerlink\" title=\"5.问题模拟验证\"></a>5.问题模拟验证</h2><p>经过以上步骤分析，写了个脚本，监控该机器上内存使用情况及线程使用情况，发现问题出现在有个程序会创建大量的线程，该程序是java 程序。查看机器pid_max为32768，ulimit -u  未限制大小。 总结一下这个机器线程最多允许创建 32768。</p>\n<p>以下是验证代码：注意该代码有一定危险性，请勿在window 下运行。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int i =0;</span><br><span class=\"line\">while(true) &#123;</span><br><span class=\"line\">\tThread thread = new Thread(new Runnable() &#123;</span><br><span class=\"line\">\t\t@Override</span><br><span class=\"line\">\t\tpublic void run() &#123;</span><br><span class=\"line\">\t\t\twhile(true) &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tThread.sleep(1000);</span><br><span class=\"line\">\t\t\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tthread.start();</span><br><span class=\"line\">\ti++;</span><br><span class=\"line\">\tSystem.out.println(&quot;curent thread num:&quot;+i);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>运行该代码，当线程增长到3.1w时，报该问题。</p>\n<h2 id=\"6-本次问题原因\"><a href=\"#6-本次问题原因\" class=\"headerlink\" title=\"6.本次问题原因\"></a>6.本次问题原因</h2><p>本次问题是发现是有个程序创建了大量线程，造成达到linux 机器允许最大的线程数。切记这个不是一个参数控制的，多个参数控制pid_max，一般人可能会忽略！</p>\n<h2 id=\"7-问题解决办法\"><a href=\"#7-问题解决办法\" class=\"headerlink\" title=\"7.问题解决办法\"></a>7.问题解决办法</h2><p>根据不同问题选择不同办法，详见第2点 <a href=\"#2.该问题产生可能原因\">该问题产生可能原因</a></p>\n<h2 id=\"8-参考\"><a href=\"#8-参考\" class=\"headerlink\" title=\"8.参考\"></a>8.参考</h2><ol>\n<li><a href=\"https://www.elastic.co/blog/we-are-out-of-memory-systemd-process-limits\" target=\"_blank\" rel=\"noopener\">We are out of memory</a></li>\n<li><a href=\"https://blog.csdn.net/lh2016rocky/article/details/55671656\" target=\"_blank\" rel=\"noopener\">为何线程有PID</a></li>\n<li><a href=\"https://dzone.com/articles/troubleshoot-outofmemoryerror-unable-to-create-new\" target=\"_blank\" rel=\"noopener\">Troubleshoot OutOfMemoryError: Unable to Create New Native Thread</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"OutOfMemoryError-unable-to-create-native-thread问题追究\"><a href=\"#OutOfMemoryError-unable-to-create-native-thread问题追究\" class=\"headerlink\" title=\"OutOfMemoryError-unable to create native thread问题追究\"></a>OutOfMemoryError-unable to create native thread问题追究</h1><h2 id=\"1-问题背景\"><a href=\"#1-问题背景\" class=\"headerlink\" title=\"1.问题背景\"></a>1.问题背景</h2><p>某天，发现线上elasticsearch 集群，有个节点down了，重启后，过了一个多小时，又down,这下才去认真的查看log,发现在down掉之前，有报以下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[67329.555s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.</span><br><span class=\"line\"></span><br><span class=\"line\">[67329.557s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.</span><br><span class=\"line\"></span><br><span class=\"line\">[2018-07-12T02:47:53,549][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [e11redis28.mercury.corp] fatal error in thread [elasticsearch[e11redis28.mercury.corp][refresh][T#2]], exiting</span><br><span class=\"line\"></span><br><span class=\"line\">java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.start0(Native Method) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.start(Thread.java:813) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:944) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1012) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]</span><br><span class=\"line\"></span><br><span class=\"line\">t java.lang.Thread.run(Thread.java:844) [?:?]</span><br></pre></td></tr></table></figure></p>\n<p>困扰了好几天，修改各种jvm 配置发现无用，就在github 上提了个<a href=\"https://github.com/elastic/elasticsearch/issues/31982\" target=\"_blank\" rel=\"noopener\">issues</a>,里面详细的描述了该问题及环境的配置。</p>\n<h2 id=\"2-该问题产生可能原因\"><a href=\"#2-该问题产生可能原因\" class=\"headerlink\" title=\"2.该问题产生可能原因\"></a>2.该问题产生可能原因</h2><p>经过查阅大量资料，才发现该错误不是我们一眼看的那么简单。</p>\n<p>OutOfMemoryError: Unable to Create New Native Thread</p>\n<p>这个错误其实已经告诉我们，该问题可能是两个原因造成的：</p>\n<ul>\n<li><p>内存不足</p>\n<p>该内存不足，其实不是指堆内存，而是创建栈内存不足。所以说出现该问题修改Xmx/Xms是无法解决的。默认情况下jvm 创建1个线程，会分配1m空间。这里所指内存不足是机器中可用内存不足。对于该原因的解决办法是给jvm 留够需要的内存，方法有多个：</p>\n<blockquote>\n<ol>\n<li>减少Xss配置，这样话可以利用有限的内存创建更多的线程，但是需注意避免值过小栈溢出。</li>\n<li>减少堆配置，主要还是留下足够内存供jvm 创建线程所用。</li>\n<li>杀掉其他程序，留出空闲内存。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>机器线程数受限制</p>\n<p>众所周知，linux系统中对进程的创建是有限制的，不可能无限创建的，影响该数量主要是由以下三个方面：</p>\n<blockquote>\n<ol>\n<li>/proc/sys/kernel/threads-max </li>\n<li>max_user_process（ulimit –u）</li>\n<li>/proc/sys/kernel/pid_max</li>\n</ol>\n</blockquote>\n<p>通过这<strong>三个参数共同</strong>决定了linux机器中能创建线程数量</p>\n<h2 id=\"3-linux-进程相关信息\"><a href=\"#3-linux-进程相关信息\" class=\"headerlink\" title=\"3.linux 进程相关信息\"></a>3.linux 进程相关信息</h2><p>在这里主要是想写一些命令帮助人们分析linux进程下的线程等信息</p>\n</li>\n<li><p>查看linux所有用户下所有进程数</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps -eLo ruser|awk &apos;NR&gt;1&apos;|sort|uniq -c</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看进程中的线程数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ps –o nlwp 27989</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>查看某个进程下的线程</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> top -H -p &lt;pid&gt;</span><br><span class=\"line\">ps -eLf | grep &lt;pid&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"4-检测系统参数\"><a href=\"#4-检测系统参数\" class=\"headerlink\" title=\"4.检测系统参数\"></a>4.检测系统参数</h2><ol>\n<li>查看内存 free -g</li>\n<li><p>查看limit</p>\n<ul>\n<li>ulimit -a</li>\n<li>cat /proc/sys/kernel/threads-max </li>\n<li>cat /proc/sys/kernel/pid_max</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"5-问题模拟验证\"><a href=\"#5-问题模拟验证\" class=\"headerlink\" title=\"5.问题模拟验证\"></a>5.问题模拟验证</h2><p>经过以上步骤分析，写了个脚本，监控该机器上内存使用情况及线程使用情况，发现问题出现在有个程序会创建大量的线程，该程序是java 程序。查看机器pid_max为32768，ulimit -u  未限制大小。 总结一下这个机器线程最多允许创建 32768。</p>\n<p>以下是验证代码：注意该代码有一定危险性，请勿在window 下运行。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int i =0;</span><br><span class=\"line\">while(true) &#123;</span><br><span class=\"line\">\tThread thread = new Thread(new Runnable() &#123;</span><br><span class=\"line\">\t\t@Override</span><br><span class=\"line\">\t\tpublic void run() &#123;</span><br><span class=\"line\">\t\t\twhile(true) &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tThread.sleep(1000);</span><br><span class=\"line\">\t\t\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tthread.start();</span><br><span class=\"line\">\ti++;</span><br><span class=\"line\">\tSystem.out.println(&quot;curent thread num:&quot;+i);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>运行该代码，当线程增长到3.1w时，报该问题。</p>\n<h2 id=\"6-本次问题原因\"><a href=\"#6-本次问题原因\" class=\"headerlink\" title=\"6.本次问题原因\"></a>6.本次问题原因</h2><p>本次问题是发现是有个程序创建了大量线程，造成达到linux 机器允许最大的线程数。切记这个不是一个参数控制的，多个参数控制pid_max，一般人可能会忽略！</p>\n<h2 id=\"7-问题解决办法\"><a href=\"#7-问题解决办法\" class=\"headerlink\" title=\"7.问题解决办法\"></a>7.问题解决办法</h2><p>根据不同问题选择不同办法，详见第2点 <a href=\"#2.该问题产生可能原因\">该问题产生可能原因</a></p>\n<h2 id=\"8-参考\"><a href=\"#8-参考\" class=\"headerlink\" title=\"8.参考\"></a>8.参考</h2><ol>\n<li><a href=\"https://www.elastic.co/blog/we-are-out-of-memory-systemd-process-limits\" target=\"_blank\" rel=\"noopener\">We are out of memory</a></li>\n<li><a href=\"https://blog.csdn.net/lh2016rocky/article/details/55671656\" target=\"_blank\" rel=\"noopener\">为何线程有PID</a></li>\n<li><a href=\"https://dzone.com/articles/troubleshoot-outofmemoryerror-unable-to-create-new\" target=\"_blank\" rel=\"noopener\">Troubleshoot OutOfMemoryError: Unable to Create New Native Thread</a></li>\n</ol>\n"},{"title":"React学习总结","originContent":"","toc":false,"date":"2019-04-07T13:12:40.000Z","_content":"\n\n## 资源\n看过很多文档，经过精心整理对比，觉得以下几个对我来说最有帮助，在此记录一下，分享给更多的人。\n\n1. [React 基础知识和介绍](https://alibaba.github.io/ice/docs/basis/intro-react)\n2. [JavaScript 标准库](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects)\n3. [React Getting Started](https://reactjs.org/docs/getting-started.html)\n\n## 常识\n\nprops 用来传递数据，state 用来存储组件内部的状态和数据。props 是只读的，state 当前组件 state 的值可以作为 props 传递给下层组件\n\n1. React 组件的变化是基于状态的\n\n     通过setState更改状态的内容\n```\nthis.setState({\n      switchStatus: !this.state.switchStatus\n    });\n```\n2. React 组件的生命周期\n![](https://img.alicdn.com/tps/TB1Ng7_MpXXXXamXFXXXXXXXXXX-2850-2945.jpg)\n更多详见[地址](https://alibaba.github.io/ice/docs/basis/intro-react#React%20%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F)\n\n## 问题解决\n\n### 1.this 作用域处理方案\n\n问题背景：手动触发事件，更改state中数据\n\n1. 箭头表达式\n```\n  mergeFilter = ()=> {\n    this.setState({\n      store: store,\n      merge: !this.state.merge,\n    });\n  } \n  \n```\n   render组件中调用  \n```\n<EuiSwitch\n        label=\"Merge\"\n        key=\"MergeEuiSwitch\"\n        checked={\n          this.state.merge\n        }\n        onChange={()=>this.mergeFilter()}\n/>  \n```\nhandlePaginationChange也是可以调用的\n2. bind\n```\n  constructor(props) {\n    super(props);\n    this.state = {value: 'coconut'};\n\n    this.handleChange = this.handleChange.bind(this);\n  }\n\n  handleChange(event) {\n    this.setState({value: event.target.value});\n  }\n  \n render() {\n    return (\n      <form onSubmit={this.handleSubmit}>\n         <select value={this.state.value} onChange={this.handleChange}>\n            <option value=\"grapefruit\">Grapefruit</option>\n          </select>\n        <input type=\"submit\" value=\"Submit\" />\n      </form>\n    );\n  }\n```\n\n### 2.传值\n这里的传值是组件化传值，属性在组件中是可以自定义的。\n在父组件中添加子组件后，可以添加任意属性，如下的dialogObj：\n```\n<DetailDialog dialogObj={dialogObj} } />\n```\n对于在子组件中可以在构造方法中获取该属性\n```\nconstructor(props) {\n      super(props);\n      this.state = {\n        dialogObj: this.props.dialogObj,\n      };\n    }\n```\n对于父组件该属性发生变化，如何更新呢，这里利用生命周期中的componentWillReceiveProps,这样即可获悉到属性的变化。\n```\n    // 接受父类props改变，修改子类中的属性\n    componentWillReceiveProps(nextProps) {\n      this.setState({\n        dialogObj: nextProps.dialogObj,\n      });\n    }\n```\n### 3.父调用子函数\n在开发的场景中，经常存在需要在父组件中触发子组件方法\n\n通过属性传值能解决业务问题的，优先建议使用属性传值解决。如果不能解决，可以考虑使用ref解决调用问题\n\n例如：\n```\n<SimpleFormDialog ref=\"getDialog\">\n\n父组件方法\nparentMethod = () => {\n            //调用组件进行通信\n            this.refs.getDialog.childMethod();\n}\n```\n### 4.子调用父函数\n在开发的场景中，经常存在需要在子组件中触发父组件方法\n\n在添加子组件的时候使用如下方式\n```\n<DetailDialog hideDetailDialog={this.hideDetailDialog} />\n```\n类似于属性传递，方法也可以传递的，这样在子组件中方法调用一下即可：\n```\nhideDetailDialog = () => {\n      this.props.hideDetailDialog();\n    }\n```\n\n## 高阶学习\n### react-route\n略，详见[简明React Router v4教程](https://juejin.im/post/5a7e9ee7f265da4e7832949c)\n\n### react-redux\n略,详见[redux](https://www.redux.org.cn/)\n## 参考\n\n1. [he-select-tag](https://reactjs.org/docs/forms.html#the-select-tag)\n2. [React 基础知识和介绍](https://alibaba.github.io/ice/docs/basis/intro-react)\n3. [React Refs](http://www.runoob.com/react/react-refs.html)","source":"_posts/React学习总结.md","raw":"---\ntitle: React学习总结\ntags:\n  - 笔记\noriginContent: ''\ncategories:\n  - React\ntoc: false\ndate: 2019-04-07 21:12:40\n---\n\n\n## 资源\n看过很多文档，经过精心整理对比，觉得以下几个对我来说最有帮助，在此记录一下，分享给更多的人。\n\n1. [React 基础知识和介绍](https://alibaba.github.io/ice/docs/basis/intro-react)\n2. [JavaScript 标准库](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects)\n3. [React Getting Started](https://reactjs.org/docs/getting-started.html)\n\n## 常识\n\nprops 用来传递数据，state 用来存储组件内部的状态和数据。props 是只读的，state 当前组件 state 的值可以作为 props 传递给下层组件\n\n1. React 组件的变化是基于状态的\n\n     通过setState更改状态的内容\n```\nthis.setState({\n      switchStatus: !this.state.switchStatus\n    });\n```\n2. React 组件的生命周期\n![](https://img.alicdn.com/tps/TB1Ng7_MpXXXXamXFXXXXXXXXXX-2850-2945.jpg)\n更多详见[地址](https://alibaba.github.io/ice/docs/basis/intro-react#React%20%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F)\n\n## 问题解决\n\n### 1.this 作用域处理方案\n\n问题背景：手动触发事件，更改state中数据\n\n1. 箭头表达式\n```\n  mergeFilter = ()=> {\n    this.setState({\n      store: store,\n      merge: !this.state.merge,\n    });\n  } \n  \n```\n   render组件中调用  \n```\n<EuiSwitch\n        label=\"Merge\"\n        key=\"MergeEuiSwitch\"\n        checked={\n          this.state.merge\n        }\n        onChange={()=>this.mergeFilter()}\n/>  \n```\nhandlePaginationChange也是可以调用的\n2. bind\n```\n  constructor(props) {\n    super(props);\n    this.state = {value: 'coconut'};\n\n    this.handleChange = this.handleChange.bind(this);\n  }\n\n  handleChange(event) {\n    this.setState({value: event.target.value});\n  }\n  \n render() {\n    return (\n      <form onSubmit={this.handleSubmit}>\n         <select value={this.state.value} onChange={this.handleChange}>\n            <option value=\"grapefruit\">Grapefruit</option>\n          </select>\n        <input type=\"submit\" value=\"Submit\" />\n      </form>\n    );\n  }\n```\n\n### 2.传值\n这里的传值是组件化传值，属性在组件中是可以自定义的。\n在父组件中添加子组件后，可以添加任意属性，如下的dialogObj：\n```\n<DetailDialog dialogObj={dialogObj} } />\n```\n对于在子组件中可以在构造方法中获取该属性\n```\nconstructor(props) {\n      super(props);\n      this.state = {\n        dialogObj: this.props.dialogObj,\n      };\n    }\n```\n对于父组件该属性发生变化，如何更新呢，这里利用生命周期中的componentWillReceiveProps,这样即可获悉到属性的变化。\n```\n    // 接受父类props改变，修改子类中的属性\n    componentWillReceiveProps(nextProps) {\n      this.setState({\n        dialogObj: nextProps.dialogObj,\n      });\n    }\n```\n### 3.父调用子函数\n在开发的场景中，经常存在需要在父组件中触发子组件方法\n\n通过属性传值能解决业务问题的，优先建议使用属性传值解决。如果不能解决，可以考虑使用ref解决调用问题\n\n例如：\n```\n<SimpleFormDialog ref=\"getDialog\">\n\n父组件方法\nparentMethod = () => {\n            //调用组件进行通信\n            this.refs.getDialog.childMethod();\n}\n```\n### 4.子调用父函数\n在开发的场景中，经常存在需要在子组件中触发父组件方法\n\n在添加子组件的时候使用如下方式\n```\n<DetailDialog hideDetailDialog={this.hideDetailDialog} />\n```\n类似于属性传递，方法也可以传递的，这样在子组件中方法调用一下即可：\n```\nhideDetailDialog = () => {\n      this.props.hideDetailDialog();\n    }\n```\n\n## 高阶学习\n### react-route\n略，详见[简明React Router v4教程](https://juejin.im/post/5a7e9ee7f265da4e7832949c)\n\n### react-redux\n略,详见[redux](https://www.redux.org.cn/)\n## 参考\n\n1. [he-select-tag](https://reactjs.org/docs/forms.html#the-select-tag)\n2. [React 基础知识和介绍](https://alibaba.github.io/ice/docs/basis/intro-react)\n3. [React Refs](http://www.runoob.com/react/react-refs.html)","slug":"React学习总结","published":1,"updated":"2019-04-07T13:12:40.224Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbl00338ceejc5rt3dc","content":"<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><p>看过很多文档，经过精心整理对比，觉得以下几个对我来说最有帮助，在此记录一下，分享给更多的人。</p>\n<ol>\n<li><a href=\"https://alibaba.github.io/ice/docs/basis/intro-react\" target=\"_blank\" rel=\"noopener\">React 基础知识和介绍</a></li>\n<li><a href=\"https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects\" target=\"_blank\" rel=\"noopener\">JavaScript 标准库</a></li>\n<li><a href=\"https://reactjs.org/docs/getting-started.html\" target=\"_blank\" rel=\"noopener\">React Getting Started</a></li>\n</ol>\n<h2 id=\"常识\"><a href=\"#常识\" class=\"headerlink\" title=\"常识\"></a>常识</h2><p>props 用来传递数据，state 用来存储组件内部的状态和数据。props 是只读的，state 当前组件 state 的值可以作为 props 传递给下层组件</p>\n<ol>\n<li><p>React 组件的变化是基于状态的</p>\n<p>  通过setState更改状态的内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">this.setState(&#123;</span><br><span class=\"line\">      switchStatus: !this.state.switchStatus</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>React 组件的生命周期<br><img src=\"https://img.alicdn.com/tps/TB1Ng7_MpXXXXamXFXXXXXXXXXX-2850-2945.jpg\" alt=\"\"><br>更多详见<a href=\"https://alibaba.github.io/ice/docs/basis/intro-react#React%20%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F\" target=\"_blank\" rel=\"noopener\">地址</a></p>\n</li>\n</ol>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><h3 id=\"1-this-作用域处理方案\"><a href=\"#1-this-作用域处理方案\" class=\"headerlink\" title=\"1.this 作用域处理方案\"></a>1.this 作用域处理方案</h3><p>问题背景：手动触发事件，更改state中数据</p>\n<ol>\n<li><p>箭头表达式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mergeFilter = ()=&gt; &#123;</span><br><span class=\"line\">  this.setState(&#123;</span><br><span class=\"line\">    store: store,</span><br><span class=\"line\">    merge: !this.state.merge,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>render组件中调用  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;EuiSwitch</span><br><span class=\"line\">        label=&quot;Merge&quot;</span><br><span class=\"line\">        key=&quot;MergeEuiSwitch&quot;</span><br><span class=\"line\">        checked=&#123;</span><br><span class=\"line\">          this.state.merge</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        onChange=&#123;()=&gt;this.mergeFilter()&#125;</span><br><span class=\"line\">/&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>handlePaginationChange也是可以调用的</p>\n<ol start=\"2\">\n<li>bind<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> constructor(props) &#123;</span><br><span class=\"line\">   super(props);</span><br><span class=\"line\">   this.state = &#123;value: &apos;coconut&apos;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   this.handleChange = this.handleChange.bind(this);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> handleChange(event) &#123;</span><br><span class=\"line\">   this.setState(&#123;value: event.target.value&#125;);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\">   return (</span><br><span class=\"line\">     &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;</span><br><span class=\"line\">        &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt;</span><br><span class=\"line\">           &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt;</span><br><span class=\"line\">         &lt;/select&gt;</span><br><span class=\"line\">       &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;</span><br><span class=\"line\">     &lt;/form&gt;</span><br><span class=\"line\">   );</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"2-传值\"><a href=\"#2-传值\" class=\"headerlink\" title=\"2.传值\"></a>2.传值</h3><p>这里的传值是组件化传值，属性在组件中是可以自定义的。<br>在父组件中添加子组件后，可以添加任意属性，如下的dialogObj：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;DetailDialog dialogObj=&#123;dialogObj&#125; &#125; /&gt;</span><br></pre></td></tr></table></figure></p>\n<p>对于在子组件中可以在构造方法中获取该属性<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">constructor(props) &#123;</span><br><span class=\"line\">      super(props);</span><br><span class=\"line\">      this.state = &#123;</span><br><span class=\"line\">        dialogObj: this.props.dialogObj,</span><br><span class=\"line\">      &#125;;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>对于父组件该属性发生变化，如何更新呢，这里利用生命周期中的componentWillReceiveProps,这样即可获悉到属性的变化。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 接受父类props改变，修改子类中的属性</span><br><span class=\"line\">componentWillReceiveProps(nextProps) &#123;</span><br><span class=\"line\">  this.setState(&#123;</span><br><span class=\"line\">    dialogObj: nextProps.dialogObj,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-父调用子函数\"><a href=\"#3-父调用子函数\" class=\"headerlink\" title=\"3.父调用子函数\"></a>3.父调用子函数</h3><p>在开发的场景中，经常存在需要在父组件中触发子组件方法</p>\n<p>通过属性传值能解决业务问题的，优先建议使用属性传值解决。如果不能解决，可以考虑使用ref解决调用问题</p>\n<p>例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;SimpleFormDialog ref=&quot;getDialog&quot;&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">父组件方法</span><br><span class=\"line\">parentMethod = () =&gt; &#123;</span><br><span class=\"line\">            //调用组件进行通信</span><br><span class=\"line\">            this.refs.getDialog.childMethod();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-子调用父函数\"><a href=\"#4-子调用父函数\" class=\"headerlink\" title=\"4.子调用父函数\"></a>4.子调用父函数</h3><p>在开发的场景中，经常存在需要在子组件中触发父组件方法</p>\n<p>在添加子组件的时候使用如下方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;DetailDialog hideDetailDialog=&#123;this.hideDetailDialog&#125; /&gt;</span><br></pre></td></tr></table></figure></p>\n<p>类似于属性传递，方法也可以传递的，这样在子组件中方法调用一下即可：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hideDetailDialog = () =&gt; &#123;</span><br><span class=\"line\">      this.props.hideDetailDialog();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"高阶学习\"><a href=\"#高阶学习\" class=\"headerlink\" title=\"高阶学习\"></a>高阶学习</h2><h3 id=\"react-route\"><a href=\"#react-route\" class=\"headerlink\" title=\"react-route\"></a>react-route</h3><p>略，详见<a href=\"https://juejin.im/post/5a7e9ee7f265da4e7832949c\" target=\"_blank\" rel=\"noopener\">简明React Router v4教程</a></p>\n<h3 id=\"react-redux\"><a href=\"#react-redux\" class=\"headerlink\" title=\"react-redux\"></a>react-redux</h3><p>略,详见<a href=\"https://www.redux.org.cn/\" target=\"_blank\" rel=\"noopener\">redux</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://reactjs.org/docs/forms.html#the-select-tag\" target=\"_blank\" rel=\"noopener\">he-select-tag</a></li>\n<li><a href=\"https://alibaba.github.io/ice/docs/basis/intro-react\" target=\"_blank\" rel=\"noopener\">React 基础知识和介绍</a></li>\n<li><a href=\"http://www.runoob.com/react/react-refs.html\" target=\"_blank\" rel=\"noopener\">React Refs</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h2><p>看过很多文档，经过精心整理对比，觉得以下几个对我来说最有帮助，在此记录一下，分享给更多的人。</p>\n<ol>\n<li><a href=\"https://alibaba.github.io/ice/docs/basis/intro-react\" target=\"_blank\" rel=\"noopener\">React 基础知识和介绍</a></li>\n<li><a href=\"https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects\" target=\"_blank\" rel=\"noopener\">JavaScript 标准库</a></li>\n<li><a href=\"https://reactjs.org/docs/getting-started.html\" target=\"_blank\" rel=\"noopener\">React Getting Started</a></li>\n</ol>\n<h2 id=\"常识\"><a href=\"#常识\" class=\"headerlink\" title=\"常识\"></a>常识</h2><p>props 用来传递数据，state 用来存储组件内部的状态和数据。props 是只读的，state 当前组件 state 的值可以作为 props 传递给下层组件</p>\n<ol>\n<li><p>React 组件的变化是基于状态的</p>\n<p>  通过setState更改状态的内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">this.setState(&#123;</span><br><span class=\"line\">      switchStatus: !this.state.switchStatus</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>React 组件的生命周期<br><img src=\"https://img.alicdn.com/tps/TB1Ng7_MpXXXXamXFXXXXXXXXXX-2850-2945.jpg\" alt=\"\"><br>更多详见<a href=\"https://alibaba.github.io/ice/docs/basis/intro-react#React%20%E7%BB%84%E4%BB%B6%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F\" target=\"_blank\" rel=\"noopener\">地址</a></p>\n</li>\n</ol>\n<h2 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h2><h3 id=\"1-this-作用域处理方案\"><a href=\"#1-this-作用域处理方案\" class=\"headerlink\" title=\"1.this 作用域处理方案\"></a>1.this 作用域处理方案</h3><p>问题背景：手动触发事件，更改state中数据</p>\n<ol>\n<li><p>箭头表达式</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mergeFilter = ()=&gt; &#123;</span><br><span class=\"line\">  this.setState(&#123;</span><br><span class=\"line\">    store: store,</span><br><span class=\"line\">    merge: !this.state.merge,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>render组件中调用  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;EuiSwitch</span><br><span class=\"line\">        label=&quot;Merge&quot;</span><br><span class=\"line\">        key=&quot;MergeEuiSwitch&quot;</span><br><span class=\"line\">        checked=&#123;</span><br><span class=\"line\">          this.state.merge</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        onChange=&#123;()=&gt;this.mergeFilter()&#125;</span><br><span class=\"line\">/&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>handlePaginationChange也是可以调用的</p>\n<ol start=\"2\">\n<li>bind<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> constructor(props) &#123;</span><br><span class=\"line\">   super(props);</span><br><span class=\"line\">   this.state = &#123;value: &apos;coconut&apos;&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">   this.handleChange = this.handleChange.bind(this);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"></span><br><span class=\"line\"> handleChange(event) &#123;</span><br><span class=\"line\">   this.setState(&#123;value: event.target.value&#125;);</span><br><span class=\"line\"> &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">render() &#123;</span><br><span class=\"line\">   return (</span><br><span class=\"line\">     &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt;</span><br><span class=\"line\">        &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt;</span><br><span class=\"line\">           &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt;</span><br><span class=\"line\">         &lt;/select&gt;</span><br><span class=\"line\">       &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;</span><br><span class=\"line\">     &lt;/form&gt;</span><br><span class=\"line\">   );</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"2-传值\"><a href=\"#2-传值\" class=\"headerlink\" title=\"2.传值\"></a>2.传值</h3><p>这里的传值是组件化传值，属性在组件中是可以自定义的。<br>在父组件中添加子组件后，可以添加任意属性，如下的dialogObj：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;DetailDialog dialogObj=&#123;dialogObj&#125; &#125; /&gt;</span><br></pre></td></tr></table></figure></p>\n<p>对于在子组件中可以在构造方法中获取该属性<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">constructor(props) &#123;</span><br><span class=\"line\">      super(props);</span><br><span class=\"line\">      this.state = &#123;</span><br><span class=\"line\">        dialogObj: this.props.dialogObj,</span><br><span class=\"line\">      &#125;;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<p>对于父组件该属性发生变化，如何更新呢，这里利用生命周期中的componentWillReceiveProps,这样即可获悉到属性的变化。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 接受父类props改变，修改子类中的属性</span><br><span class=\"line\">componentWillReceiveProps(nextProps) &#123;</span><br><span class=\"line\">  this.setState(&#123;</span><br><span class=\"line\">    dialogObj: nextProps.dialogObj,</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-父调用子函数\"><a href=\"#3-父调用子函数\" class=\"headerlink\" title=\"3.父调用子函数\"></a>3.父调用子函数</h3><p>在开发的场景中，经常存在需要在父组件中触发子组件方法</p>\n<p>通过属性传值能解决业务问题的，优先建议使用属性传值解决。如果不能解决，可以考虑使用ref解决调用问题</p>\n<p>例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;SimpleFormDialog ref=&quot;getDialog&quot;&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">父组件方法</span><br><span class=\"line\">parentMethod = () =&gt; &#123;</span><br><span class=\"line\">            //调用组件进行通信</span><br><span class=\"line\">            this.refs.getDialog.childMethod();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-子调用父函数\"><a href=\"#4-子调用父函数\" class=\"headerlink\" title=\"4.子调用父函数\"></a>4.子调用父函数</h3><p>在开发的场景中，经常存在需要在子组件中触发父组件方法</p>\n<p>在添加子组件的时候使用如下方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;DetailDialog hideDetailDialog=&#123;this.hideDetailDialog&#125; /&gt;</span><br></pre></td></tr></table></figure></p>\n<p>类似于属性传递，方法也可以传递的，这样在子组件中方法调用一下即可：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hideDetailDialog = () =&gt; &#123;</span><br><span class=\"line\">      this.props.hideDetailDialog();</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"高阶学习\"><a href=\"#高阶学习\" class=\"headerlink\" title=\"高阶学习\"></a>高阶学习</h2><h3 id=\"react-route\"><a href=\"#react-route\" class=\"headerlink\" title=\"react-route\"></a>react-route</h3><p>略，详见<a href=\"https://juejin.im/post/5a7e9ee7f265da4e7832949c\" target=\"_blank\" rel=\"noopener\">简明React Router v4教程</a></p>\n<h3 id=\"react-redux\"><a href=\"#react-redux\" class=\"headerlink\" title=\"react-redux\"></a>react-redux</h3><p>略,详见<a href=\"https://www.redux.org.cn/\" target=\"_blank\" rel=\"noopener\">redux</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://reactjs.org/docs/forms.html#the-select-tag\" target=\"_blank\" rel=\"noopener\">he-select-tag</a></li>\n<li><a href=\"https://alibaba.github.io/ice/docs/basis/intro-react\" target=\"_blank\" rel=\"noopener\">React 基础知识和介绍</a></li>\n<li><a href=\"http://www.runoob.com/react/react-refs.html\" target=\"_blank\" rel=\"noopener\">React Refs</a></li>\n</ol>\n"},{"title":"RediSearch 探索","date":"2017-07-19T14:17:29.000Z","_content":"# RediSearch 探索\n\n## 简介\n\nRediSearch是一个高性能的全文搜索引擎，可作为一个Redis Module 运行在Redis上，是由RedisLabs团队开发的。\n\n**特性**\n- 多字段全文检索\n- 增量索引无性能损失\n- 文档排序\n- 复杂的子查询 and,or,not\n- 可选查询子句。\n- 基于前缀的搜索\n- 字段权重\n- 自动完成建议（使用模糊前缀建议）\n- 精确词组搜索，基于Slop的搜索\n- 基于Stemming的查询扩展在许多语言（使用Snowball）\n- 支持用于查询扩展和评分的自定义函数（请参阅扩展）。\n- 限制搜索到特定文档字段（最多支持8个字段）。\n- 数字过滤以及范围查找\n- 使用Redis自己的地理命令进行地理过滤\n- 支持任何utf-8编码文本\n- 检索完整的文档内容或只是ids\n- 将现有的HASH键自动索引为文档\n- 使用索引垃圾回收文档删除和更新\n\n## 入门\n\n### 安装/运行\n```\nwget https://github.com/RedisLabsModules/RediSearch/archive/v0.19.1.tar.gz\ntar xvf v0.19.1.tar.gz\ncd RediSearch-0.19.1/src\nmake all\nnohup redis-server --loadmodule ./redisearch.so &\n```\n**1. 创建索引**\n\n```\n127.0.0.1:6379> FT.CREATE myIDs SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK\n\n```\n**2. 增加文档到该索引**\n```\n127.0.0.1:6379> FT.ADD myIDs doc1 1.0 FIELDS title \"hello world\" body \"lorem ipsum\" url \"http://redis.io\"\nOK\n\n```\n\n**3. 在该索引中搜索**\n```\n127.0.0.1:6379> ft.search myIDs \"hello world\" limit 0 10\n1) (integer) 1\n2) \"doc1\"\n3) 1) \"title\"\n   2) \"hello world\"\n   3) \"body\"\n   4) \"lorem ipsum\"\n   5) \"url\"\n   6) \"http://redis.io\"\n\n```\n\n**4. 删除索引**\n\n```\n127.0.0.1:6379> ft.drop myIDs\nOK\n\n```\n\n**5. 添加并获取自动完成建议**\n```\n127.0.0.1:6379> ft.sugadd autocomplete \"hello truman\" 100\n(integer) 1\n127.0.0.1:6379> ft.sugget autocomplete \"he\"\n1) \"hello truman\"\n\n```\n\n\n## 引用\n1. [redisearch.io](#http://redisearch.io/)\n","source":"_posts/RediSearch-探索.md","raw":"---\ntitle: RediSearch 探索\ndate: 2017-07-19 22:17:29\ntags: nosql\ncategories:\n- redis\n---\n# RediSearch 探索\n\n## 简介\n\nRediSearch是一个高性能的全文搜索引擎，可作为一个Redis Module 运行在Redis上，是由RedisLabs团队开发的。\n\n**特性**\n- 多字段全文检索\n- 增量索引无性能损失\n- 文档排序\n- 复杂的子查询 and,or,not\n- 可选查询子句。\n- 基于前缀的搜索\n- 字段权重\n- 自动完成建议（使用模糊前缀建议）\n- 精确词组搜索，基于Slop的搜索\n- 基于Stemming的查询扩展在许多语言（使用Snowball）\n- 支持用于查询扩展和评分的自定义函数（请参阅扩展）。\n- 限制搜索到特定文档字段（最多支持8个字段）。\n- 数字过滤以及范围查找\n- 使用Redis自己的地理命令进行地理过滤\n- 支持任何utf-8编码文本\n- 检索完整的文档内容或只是ids\n- 将现有的HASH键自动索引为文档\n- 使用索引垃圾回收文档删除和更新\n\n## 入门\n\n### 安装/运行\n```\nwget https://github.com/RedisLabsModules/RediSearch/archive/v0.19.1.tar.gz\ntar xvf v0.19.1.tar.gz\ncd RediSearch-0.19.1/src\nmake all\nnohup redis-server --loadmodule ./redisearch.so &\n```\n**1. 创建索引**\n\n```\n127.0.0.1:6379> FT.CREATE myIDs SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK\n\n```\n**2. 增加文档到该索引**\n```\n127.0.0.1:6379> FT.ADD myIDs doc1 1.0 FIELDS title \"hello world\" body \"lorem ipsum\" url \"http://redis.io\"\nOK\n\n```\n\n**3. 在该索引中搜索**\n```\n127.0.0.1:6379> ft.search myIDs \"hello world\" limit 0 10\n1) (integer) 1\n2) \"doc1\"\n3) 1) \"title\"\n   2) \"hello world\"\n   3) \"body\"\n   4) \"lorem ipsum\"\n   5) \"url\"\n   6) \"http://redis.io\"\n\n```\n\n**4. 删除索引**\n\n```\n127.0.0.1:6379> ft.drop myIDs\nOK\n\n```\n\n**5. 添加并获取自动完成建议**\n```\n127.0.0.1:6379> ft.sugadd autocomplete \"hello truman\" 100\n(integer) 1\n127.0.0.1:6379> ft.sugget autocomplete \"he\"\n1) \"hello truman\"\n\n```\n\n\n## 引用\n1. [redisearch.io](#http://redisearch.io/)\n","slug":"RediSearch-探索","published":1,"updated":"2017-07-19T14:21:21.737Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbn00368ceee8ut5p9s","content":"<h1 id=\"RediSearch-探索\"><a href=\"#RediSearch-探索\" class=\"headerlink\" title=\"RediSearch 探索\"></a>RediSearch 探索</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>RediSearch是一个高性能的全文搜索引擎，可作为一个Redis Module 运行在Redis上，是由RedisLabs团队开发的。</p>\n<p><strong>特性</strong></p>\n<ul>\n<li>多字段全文检索</li>\n<li>增量索引无性能损失</li>\n<li>文档排序</li>\n<li>复杂的子查询 and,or,not</li>\n<li>可选查询子句。</li>\n<li>基于前缀的搜索</li>\n<li>字段权重</li>\n<li>自动完成建议（使用模糊前缀建议）</li>\n<li>精确词组搜索，基于Slop的搜索</li>\n<li>基于Stemming的查询扩展在许多语言（使用Snowball）</li>\n<li>支持用于查询扩展和评分的自定义函数（请参阅扩展）。</li>\n<li>限制搜索到特定文档字段（最多支持8个字段）。</li>\n<li>数字过滤以及范围查找</li>\n<li>使用Redis自己的地理命令进行地理过滤</li>\n<li>支持任何utf-8编码文本</li>\n<li>检索完整的文档内容或只是ids</li>\n<li>将现有的HASH键自动索引为文档</li>\n<li>使用索引垃圾回收文档删除和更新</li>\n</ul>\n<h2 id=\"入门\"><a href=\"#入门\" class=\"headerlink\" title=\"入门\"></a>入门</h2><h3 id=\"安装-运行\"><a href=\"#安装-运行\" class=\"headerlink\" title=\"安装/运行\"></a>安装/运行</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/RedisLabsModules/RediSearch/archive/v0.19.1.tar.gz</span><br><span class=\"line\">tar xvf v0.19.1.tar.gz</span><br><span class=\"line\">cd RediSearch-0.19.1/src</span><br><span class=\"line\">make all</span><br><span class=\"line\">nohup redis-server --loadmodule ./redisearch.so &amp;</span><br></pre></td></tr></table></figure>\n<p><strong>1. 创建索引</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; FT.CREATE myIDs SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure>\n<p><strong>2. 增加文档到该索引</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; FT.ADD myIDs doc1 1.0 FIELDS title &quot;hello world&quot; body &quot;lorem ipsum&quot; url &quot;http://redis.io&quot;</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<p><strong>3. 在该索引中搜索</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.search myIDs &quot;hello world&quot; limit 0 10</span><br><span class=\"line\">1) (integer) 1</span><br><span class=\"line\">2) &quot;doc1&quot;</span><br><span class=\"line\">3) 1) &quot;title&quot;</span><br><span class=\"line\">   2) &quot;hello world&quot;</span><br><span class=\"line\">   3) &quot;body&quot;</span><br><span class=\"line\">   4) &quot;lorem ipsum&quot;</span><br><span class=\"line\">   5) &quot;url&quot;</span><br><span class=\"line\">   6) &quot;http://redis.io&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>4. 删除索引</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.drop myIDs</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure>\n<p><strong>5. 添加并获取自动完成建议</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.sugadd autocomplete &quot;hello truman&quot; 100</span><br><span class=\"line\">(integer) 1</span><br><span class=\"line\">127.0.0.1:6379&gt; ft.sugget autocomplete &quot;he&quot;</span><br><span class=\"line\">1) &quot;hello truman&quot;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"#http://redisearch.io/\">redisearch.io</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"RediSearch-探索\"><a href=\"#RediSearch-探索\" class=\"headerlink\" title=\"RediSearch 探索\"></a>RediSearch 探索</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>RediSearch是一个高性能的全文搜索引擎，可作为一个Redis Module 运行在Redis上，是由RedisLabs团队开发的。</p>\n<p><strong>特性</strong></p>\n<ul>\n<li>多字段全文检索</li>\n<li>增量索引无性能损失</li>\n<li>文档排序</li>\n<li>复杂的子查询 and,or,not</li>\n<li>可选查询子句。</li>\n<li>基于前缀的搜索</li>\n<li>字段权重</li>\n<li>自动完成建议（使用模糊前缀建议）</li>\n<li>精确词组搜索，基于Slop的搜索</li>\n<li>基于Stemming的查询扩展在许多语言（使用Snowball）</li>\n<li>支持用于查询扩展和评分的自定义函数（请参阅扩展）。</li>\n<li>限制搜索到特定文档字段（最多支持8个字段）。</li>\n<li>数字过滤以及范围查找</li>\n<li>使用Redis自己的地理命令进行地理过滤</li>\n<li>支持任何utf-8编码文本</li>\n<li>检索完整的文档内容或只是ids</li>\n<li>将现有的HASH键自动索引为文档</li>\n<li>使用索引垃圾回收文档删除和更新</li>\n</ul>\n<h2 id=\"入门\"><a href=\"#入门\" class=\"headerlink\" title=\"入门\"></a>入门</h2><h3 id=\"安装-运行\"><a href=\"#安装-运行\" class=\"headerlink\" title=\"安装/运行\"></a>安装/运行</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/RedisLabsModules/RediSearch/archive/v0.19.1.tar.gz</span><br><span class=\"line\">tar xvf v0.19.1.tar.gz</span><br><span class=\"line\">cd RediSearch-0.19.1/src</span><br><span class=\"line\">make all</span><br><span class=\"line\">nohup redis-server --loadmodule ./redisearch.so &amp;</span><br></pre></td></tr></table></figure>\n<p><strong>1. 创建索引</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; FT.CREATE myIDs SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure>\n<p><strong>2. 增加文档到该索引</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; FT.ADD myIDs doc1 1.0 FIELDS title &quot;hello world&quot; body &quot;lorem ipsum&quot; url &quot;http://redis.io&quot;</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<p><strong>3. 在该索引中搜索</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.search myIDs &quot;hello world&quot; limit 0 10</span><br><span class=\"line\">1) (integer) 1</span><br><span class=\"line\">2) &quot;doc1&quot;</span><br><span class=\"line\">3) 1) &quot;title&quot;</span><br><span class=\"line\">   2) &quot;hello world&quot;</span><br><span class=\"line\">   3) &quot;body&quot;</span><br><span class=\"line\">   4) &quot;lorem ipsum&quot;</span><br><span class=\"line\">   5) &quot;url&quot;</span><br><span class=\"line\">   6) &quot;http://redis.io&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>4. 删除索引</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.drop myIDs</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure>\n<p><strong>5. 添加并获取自动完成建议</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt; ft.sugadd autocomplete &quot;hello truman&quot; 100</span><br><span class=\"line\">(integer) 1</span><br><span class=\"line\">127.0.0.1:6379&gt; ft.sugget autocomplete &quot;he&quot;</span><br><span class=\"line\">1) &quot;hello truman&quot;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"#http://redisearch.io/\">redisearch.io</a></li>\n</ol>\n"},{"title":"Redis Cluster Resharding","date":"2016-08-21T12:27:06.000Z","_content":"# Redis Cluster Resharding实践\n## 简介\n在Redis Cluster运维过程中，会出现水平扩展集群，而水平扩展集群即新增master节点。Redis Cluster需要就需要重新划分slot，数据迁移等操作，本文只是探讨实现过程，用Redis-cli自带命令实现Resharding。\n## 实践\n### 过程简介\n真正开始Resharding之前，会先在源结点和目的结点上执行cluster setslot <slot> importing和cluster setslot <slot> migrating命令，将要迁移的槽分别标记为迁出中和导入中的状态。然后，执行cluster getkeysinslot获得Slot中的所有Key。最后就可以对每个Key执行migrate命令进行迁移了。槽迁移完成后，执行cluster setslot命令通知整个集群槽的指派已经发生变化。\n\n关于迁移过程中的数据访问，客户端访问源结点时，如果Key还在源结点上就直接操作。如果已经不在源结点了，就向客户端返回一个ASK错误，将客户端重定向到目的结点。\n### 实践过程\n#### 1.redis-cli 实现\n迁移备注：\n实现迁移9013node中952 slot到9015 node上,其中源9013 node_id为a92e4554aa2828b85f50f8f8318429d68f5213ca，目的9015node_id为c725cda7b314701c6892f035224700e9a8336699\n- 详解\n```\n在迁移目的节点执行cluster setslot <slot> IMPORTING <node ID>命令，指明需要迁移的slot和迁移源节点。\n在迁移源节点执行cluster setslot <slot> MIGRATING <node ID>命令，指明需要迁移的slot和迁移目的节点。\n在迁移源节点执行cluster getkeysinslot获取该slot的key列表。\n在迁移源节点执行对每个key执行migrate命令，该命令会同步把该key迁移到目的节点。\n在迁移源节点反复执行cluster getkeysinslot命令，直到该slot的列表为空。\n在迁移源节点或者目的节点执行cluster setslot <slot> NODE <node ID>，完成迁移操作。\n```\n- 实践\n```\nredis-cli -c -p 9015 cluster  setslot 952 importing a92e4554aa2828b85f50f8f8318429d68f5213ca//目的节点执行\nredis-cli -c -p 9013 cluster  setslot 952 migrating c725cda7b314701c6892f035224700e9a8336699//源节点执行\nredis-cli -c -h 192.168.0.101 -p 9013 migrate 192.168.0.101 9015 \"truman:00000829\" 0 1000\nredis-cli -c -p 9015 cluster  setslot 952  node c725cda7b314701c6892f035224700e9a8336699\n```\n相关命令参考如下：\n```\n//集群\nCLUSTER INFO 打印集群的信息\nCLUSTER NODES 列出集群当前已知的所有节点（node），以及这些节点的相关信息。\n//节点\nCLUSTER MEET <ip> <port> 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。\nCLUSTER FORGET <node_id> 从集群中移除 node_id 指定的节点。\nCLUSTER REPLICATE <node_id> 将当前节点设置为 node_id 指定的节点的从节点。\nCLUSTER SAVECONFIG 将节点的配置文件保存到硬盘里面。\n//槽(slot)\nCLUSTER ADDSLOTS <slot> [slot ...] 将一个或多个槽（slot）指派（assign）给当前节点。\nCLUSTER DELSLOTS <slot> [slot ...] 移除一个或多个槽对当前节点的指派。\nCLUSTER FLUSHSLOTS 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。\nCLUSTER SETSLOT <slot> NODE <node_id> 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽>，然后再进行指派。\nCLUSTER SETSLOT <slot> MIGRATING <node_id> 将本节点的槽 slot 迁移到 node_id 指定的节点中。\nCLUSTER SETSLOT <slot> IMPORTING <node_id> 从 node_id 指定的节点中导入槽 slot 到本节点。\nCLUSTER SETSLOT <slot> STABLE 取消对槽 slot 的导入（import）或者迁移（migrate）。\n//键\nCLUSTER KEYSLOT <key> 计算键 key 应该被放置在哪个槽上。\nCLUSTER COUNTKEYSINSLOT <slot> 返回槽 slot 目前包含的键值对数量。\nCLUSTER GETKEYSINSLOT <slot> <count> 返回 count 个 slot 槽中的键。\n```\n#### 2.redis-trib.rb 实现\n```\n#把127.0.0.1:9013当前master迁移到127.0.0.1:9015上  \nredis-trib.rb reshard 127.0.0.1:9013  \n#根据提示选择要迁移的slot数量(ps:这里选择500)  \nHow many slots do you want to move (from 1 to 16384)? 1(源master的需要迁移slot数量，不能单个指定)  \n#选择要接受这些slot的node-id(127.0.0.1:9015)  \nWhat is the receiving node ID? c725cda7b314701c6892f035224700e9a8336699 (ps:127.0.0.1:9015的node-id)  \nPlease enter all the source node IDs.  \n  Type 'all' to use all the nodes as source nodes for the hash slots.  \n  Type 'done' once you entered all the source nodes IDs.  \nSource node #1:a92e4554aa2828b85f50f8f8318429d68f5213ca(源master的node-id)  \nSource node #2:done  \n#打印被移动的slot后，输入yes开始移动slot以及对应的数据.  \n#Do you want to proceed with the proposed reshard plan (yes/no)? yes  \n```\n## 参考\n1. http://redis.io/commands/cluster-setslot\n2. http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）\n","source":"_posts/Redis-Cluster-Resharding.md","raw":"---\ntitle: Redis Cluster Resharding\ndate: 2016-08-21 20:27:06\ntags: 大数据\ncategories:\n- redis\n---\n# Redis Cluster Resharding实践\n## 简介\n在Redis Cluster运维过程中，会出现水平扩展集群，而水平扩展集群即新增master节点。Redis Cluster需要就需要重新划分slot，数据迁移等操作，本文只是探讨实现过程，用Redis-cli自带命令实现Resharding。\n## 实践\n### 过程简介\n真正开始Resharding之前，会先在源结点和目的结点上执行cluster setslot <slot> importing和cluster setslot <slot> migrating命令，将要迁移的槽分别标记为迁出中和导入中的状态。然后，执行cluster getkeysinslot获得Slot中的所有Key。最后就可以对每个Key执行migrate命令进行迁移了。槽迁移完成后，执行cluster setslot命令通知整个集群槽的指派已经发生变化。\n\n关于迁移过程中的数据访问，客户端访问源结点时，如果Key还在源结点上就直接操作。如果已经不在源结点了，就向客户端返回一个ASK错误，将客户端重定向到目的结点。\n### 实践过程\n#### 1.redis-cli 实现\n迁移备注：\n实现迁移9013node中952 slot到9015 node上,其中源9013 node_id为a92e4554aa2828b85f50f8f8318429d68f5213ca，目的9015node_id为c725cda7b314701c6892f035224700e9a8336699\n- 详解\n```\n在迁移目的节点执行cluster setslot <slot> IMPORTING <node ID>命令，指明需要迁移的slot和迁移源节点。\n在迁移源节点执行cluster setslot <slot> MIGRATING <node ID>命令，指明需要迁移的slot和迁移目的节点。\n在迁移源节点执行cluster getkeysinslot获取该slot的key列表。\n在迁移源节点执行对每个key执行migrate命令，该命令会同步把该key迁移到目的节点。\n在迁移源节点反复执行cluster getkeysinslot命令，直到该slot的列表为空。\n在迁移源节点或者目的节点执行cluster setslot <slot> NODE <node ID>，完成迁移操作。\n```\n- 实践\n```\nredis-cli -c -p 9015 cluster  setslot 952 importing a92e4554aa2828b85f50f8f8318429d68f5213ca//目的节点执行\nredis-cli -c -p 9013 cluster  setslot 952 migrating c725cda7b314701c6892f035224700e9a8336699//源节点执行\nredis-cli -c -h 192.168.0.101 -p 9013 migrate 192.168.0.101 9015 \"truman:00000829\" 0 1000\nredis-cli -c -p 9015 cluster  setslot 952  node c725cda7b314701c6892f035224700e9a8336699\n```\n相关命令参考如下：\n```\n//集群\nCLUSTER INFO 打印集群的信息\nCLUSTER NODES 列出集群当前已知的所有节点（node），以及这些节点的相关信息。\n//节点\nCLUSTER MEET <ip> <port> 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。\nCLUSTER FORGET <node_id> 从集群中移除 node_id 指定的节点。\nCLUSTER REPLICATE <node_id> 将当前节点设置为 node_id 指定的节点的从节点。\nCLUSTER SAVECONFIG 将节点的配置文件保存到硬盘里面。\n//槽(slot)\nCLUSTER ADDSLOTS <slot> [slot ...] 将一个或多个槽（slot）指派（assign）给当前节点。\nCLUSTER DELSLOTS <slot> [slot ...] 移除一个或多个槽对当前节点的指派。\nCLUSTER FLUSHSLOTS 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。\nCLUSTER SETSLOT <slot> NODE <node_id> 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽>，然后再进行指派。\nCLUSTER SETSLOT <slot> MIGRATING <node_id> 将本节点的槽 slot 迁移到 node_id 指定的节点中。\nCLUSTER SETSLOT <slot> IMPORTING <node_id> 从 node_id 指定的节点中导入槽 slot 到本节点。\nCLUSTER SETSLOT <slot> STABLE 取消对槽 slot 的导入（import）或者迁移（migrate）。\n//键\nCLUSTER KEYSLOT <key> 计算键 key 应该被放置在哪个槽上。\nCLUSTER COUNTKEYSINSLOT <slot> 返回槽 slot 目前包含的键值对数量。\nCLUSTER GETKEYSINSLOT <slot> <count> 返回 count 个 slot 槽中的键。\n```\n#### 2.redis-trib.rb 实现\n```\n#把127.0.0.1:9013当前master迁移到127.0.0.1:9015上  \nredis-trib.rb reshard 127.0.0.1:9013  \n#根据提示选择要迁移的slot数量(ps:这里选择500)  \nHow many slots do you want to move (from 1 to 16384)? 1(源master的需要迁移slot数量，不能单个指定)  \n#选择要接受这些slot的node-id(127.0.0.1:9015)  \nWhat is the receiving node ID? c725cda7b314701c6892f035224700e9a8336699 (ps:127.0.0.1:9015的node-id)  \nPlease enter all the source node IDs.  \n  Type 'all' to use all the nodes as source nodes for the hash slots.  \n  Type 'done' once you entered all the source nodes IDs.  \nSource node #1:a92e4554aa2828b85f50f8f8318429d68f5213ca(源master的node-id)  \nSource node #2:done  \n#打印被移动的slot后，输入yes开始移动slot以及对应的数据.  \n#Do you want to proceed with the proposed reshard plan (yes/no)? yes  \n```\n## 参考\n1. http://redis.io/commands/cluster-setslot\n2. http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）\n","slug":"Redis-Cluster-Resharding","published":1,"updated":"2019-08-22T12:31:46.151Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbp00388ceeyuu0zezg","content":"<h1 id=\"Redis-Cluster-Resharding实践\"><a href=\"#Redis-Cluster-Resharding实践\" class=\"headerlink\" title=\"Redis Cluster Resharding实践\"></a>Redis Cluster Resharding实践</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>在Redis Cluster运维过程中，会出现水平扩展集群，而水平扩展集群即新增master节点。Redis Cluster需要就需要重新划分slot，数据迁移等操作，本文只是探讨实现过程，用Redis-cli自带命令实现Resharding。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><h3 id=\"过程简介\"><a href=\"#过程简介\" class=\"headerlink\" title=\"过程简介\"></a>过程简介</h3><p>真正开始Resharding之前，会先在源结点和目的结点上执行cluster setslot <slot> importing和cluster setslot <slot> migrating命令，将要迁移的槽分别标记为迁出中和导入中的状态。然后，执行cluster getkeysinslot获得Slot中的所有Key。最后就可以对每个Key执行migrate命令进行迁移了。槽迁移完成后，执行cluster setslot命令通知整个集群槽的指派已经发生变化。</slot></slot></p>\n<p>关于迁移过程中的数据访问，客户端访问源结点时，如果Key还在源结点上就直接操作。如果已经不在源结点了，就向客户端返回一个ASK错误，将客户端重定向到目的结点。</p>\n<h3 id=\"实践过程\"><a href=\"#实践过程\" class=\"headerlink\" title=\"实践过程\"></a>实践过程</h3><h4 id=\"1-redis-cli-实现\"><a href=\"#1-redis-cli-实现\" class=\"headerlink\" title=\"1.redis-cli 实现\"></a>1.redis-cli 实现</h4><p>迁移备注：<br>实现迁移9013node中952 slot到9015 node上,其中源9013 node_id为a92e4554aa2828b85f50f8f8318429d68f5213ca，目的9015node_id为c725cda7b314701c6892f035224700e9a8336699</p>\n<ul>\n<li><p>详解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在迁移目的节点执行cluster setslot &lt;slot&gt; IMPORTING &lt;node ID&gt;命令，指明需要迁移的slot和迁移源节点。</span><br><span class=\"line\">在迁移源节点执行cluster setslot &lt;slot&gt; MIGRATING &lt;node ID&gt;命令，指明需要迁移的slot和迁移目的节点。</span><br><span class=\"line\">在迁移源节点执行cluster getkeysinslot获取该slot的key列表。</span><br><span class=\"line\">在迁移源节点执行对每个key执行migrate命令，该命令会同步把该key迁移到目的节点。</span><br><span class=\"line\">在迁移源节点反复执行cluster getkeysinslot命令，直到该slot的列表为空。</span><br><span class=\"line\">在迁移源节点或者目的节点执行cluster setslot &lt;slot&gt; NODE &lt;node ID&gt;，完成迁移操作。</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>实践</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli -c -p 9015 cluster  setslot 952 importing a92e4554aa2828b85f50f8f8318429d68f5213ca//目的节点执行</span><br><span class=\"line\">redis-cli -c -p 9013 cluster  setslot 952 migrating c725cda7b314701c6892f035224700e9a8336699//源节点执行</span><br><span class=\"line\">redis-cli -c -h 192.168.0.101 -p 9013 migrate 192.168.0.101 9015 &quot;truman:00000829&quot; 0 1000</span><br><span class=\"line\">redis-cli -c -p 9015 cluster  setslot 952  node c725cda7b314701c6892f035224700e9a8336699</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>相关命令参考如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//集群</span><br><span class=\"line\">CLUSTER INFO 打印集群的信息</span><br><span class=\"line\">CLUSTER NODES 列出集群当前已知的所有节点（node），以及这些节点的相关信息。</span><br><span class=\"line\">//节点</span><br><span class=\"line\">CLUSTER MEET &lt;ip&gt; &lt;port&gt; 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。</span><br><span class=\"line\">CLUSTER FORGET &lt;node_id&gt; 从集群中移除 node_id 指定的节点。</span><br><span class=\"line\">CLUSTER REPLICATE &lt;node_id&gt; 将当前节点设置为 node_id 指定的节点的从节点。</span><br><span class=\"line\">CLUSTER SAVECONFIG 将节点的配置文件保存到硬盘里面。</span><br><span class=\"line\">//槽(slot)</span><br><span class=\"line\">CLUSTER ADDSLOTS &lt;slot&gt; [slot ...] 将一个或多个槽（slot）指派（assign）给当前节点。</span><br><span class=\"line\">CLUSTER DELSLOTS &lt;slot&gt; [slot ...] 移除一个或多个槽对当前节点的指派。</span><br><span class=\"line\">CLUSTER FLUSHSLOTS 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; NODE &lt;node_id&gt; 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;node_id&gt; 将本节点的槽 slot 迁移到 node_id 指定的节点中。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;node_id&gt; 从 node_id 指定的节点中导入槽 slot 到本节点。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; STABLE 取消对槽 slot 的导入（import）或者迁移（migrate）。</span><br><span class=\"line\">//键</span><br><span class=\"line\">CLUSTER KEYSLOT &lt;key&gt; 计算键 key 应该被放置在哪个槽上。</span><br><span class=\"line\">CLUSTER COUNTKEYSINSLOT &lt;slot&gt; 返回槽 slot 目前包含的键值对数量。</span><br><span class=\"line\">CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt; 返回 count 个 slot 槽中的键。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-redis-trib-rb-实现\"><a href=\"#2-redis-trib-rb-实现\" class=\"headerlink\" title=\"2.redis-trib.rb 实现\"></a>2.redis-trib.rb 实现</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#把127.0.0.1:9013当前master迁移到127.0.0.1:9015上  </span><br><span class=\"line\">redis-trib.rb reshard 127.0.0.1:9013  </span><br><span class=\"line\">#根据提示选择要迁移的slot数量(ps:这里选择500)  </span><br><span class=\"line\">How many slots do you want to move (from 1 to 16384)? 1(源master的需要迁移slot数量，不能单个指定)  </span><br><span class=\"line\">#选择要接受这些slot的node-id(127.0.0.1:9015)  </span><br><span class=\"line\">What is the receiving node ID? c725cda7b314701c6892f035224700e9a8336699 (ps:127.0.0.1:9015的node-id)  </span><br><span class=\"line\">Please enter all the source node IDs.  </span><br><span class=\"line\">  Type &apos;all&apos; to use all the nodes as source nodes for the hash slots.  </span><br><span class=\"line\">  Type &apos;done&apos; once you entered all the source nodes IDs.  </span><br><span class=\"line\">Source node #1:a92e4554aa2828b85f50f8f8318429d68f5213ca(源master的node-id)  </span><br><span class=\"line\">Source node #2:done  </span><br><span class=\"line\">#打印被移动的slot后，输入yes开始移动slot以及对应的数据.  </span><br><span class=\"line\">#Do you want to proceed with the proposed reshard plan (yes/no)? yes</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://redis.io/commands/cluster-setslot\" target=\"_blank\" rel=\"noopener\">http://redis.io/commands/cluster-setslot</a></li>\n<li><a href=\"http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）\" target=\"_blank\" rel=\"noopener\">http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Redis-Cluster-Resharding实践\"><a href=\"#Redis-Cluster-Resharding实践\" class=\"headerlink\" title=\"Redis Cluster Resharding实践\"></a>Redis Cluster Resharding实践</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>在Redis Cluster运维过程中，会出现水平扩展集群，而水平扩展集群即新增master节点。Redis Cluster需要就需要重新划分slot，数据迁移等操作，本文只是探讨实现过程，用Redis-cli自带命令实现Resharding。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><h3 id=\"过程简介\"><a href=\"#过程简介\" class=\"headerlink\" title=\"过程简介\"></a>过程简介</h3><p>真正开始Resharding之前，会先在源结点和目的结点上执行cluster setslot <slot> importing和cluster setslot <slot> migrating命令，将要迁移的槽分别标记为迁出中和导入中的状态。然后，执行cluster getkeysinslot获得Slot中的所有Key。最后就可以对每个Key执行migrate命令进行迁移了。槽迁移完成后，执行cluster setslot命令通知整个集群槽的指派已经发生变化。</slot></slot></p>\n<p>关于迁移过程中的数据访问，客户端访问源结点时，如果Key还在源结点上就直接操作。如果已经不在源结点了，就向客户端返回一个ASK错误，将客户端重定向到目的结点。</p>\n<h3 id=\"实践过程\"><a href=\"#实践过程\" class=\"headerlink\" title=\"实践过程\"></a>实践过程</h3><h4 id=\"1-redis-cli-实现\"><a href=\"#1-redis-cli-实现\" class=\"headerlink\" title=\"1.redis-cli 实现\"></a>1.redis-cli 实现</h4><p>迁移备注：<br>实现迁移9013node中952 slot到9015 node上,其中源9013 node_id为a92e4554aa2828b85f50f8f8318429d68f5213ca，目的9015node_id为c725cda7b314701c6892f035224700e9a8336699</p>\n<ul>\n<li><p>详解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在迁移目的节点执行cluster setslot &lt;slot&gt; IMPORTING &lt;node ID&gt;命令，指明需要迁移的slot和迁移源节点。</span><br><span class=\"line\">在迁移源节点执行cluster setslot &lt;slot&gt; MIGRATING &lt;node ID&gt;命令，指明需要迁移的slot和迁移目的节点。</span><br><span class=\"line\">在迁移源节点执行cluster getkeysinslot获取该slot的key列表。</span><br><span class=\"line\">在迁移源节点执行对每个key执行migrate命令，该命令会同步把该key迁移到目的节点。</span><br><span class=\"line\">在迁移源节点反复执行cluster getkeysinslot命令，直到该slot的列表为空。</span><br><span class=\"line\">在迁移源节点或者目的节点执行cluster setslot &lt;slot&gt; NODE &lt;node ID&gt;，完成迁移操作。</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>实践</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli -c -p 9015 cluster  setslot 952 importing a92e4554aa2828b85f50f8f8318429d68f5213ca//目的节点执行</span><br><span class=\"line\">redis-cli -c -p 9013 cluster  setslot 952 migrating c725cda7b314701c6892f035224700e9a8336699//源节点执行</span><br><span class=\"line\">redis-cli -c -h 192.168.0.101 -p 9013 migrate 192.168.0.101 9015 &quot;truman:00000829&quot; 0 1000</span><br><span class=\"line\">redis-cli -c -p 9015 cluster  setslot 952  node c725cda7b314701c6892f035224700e9a8336699</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>相关命令参考如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//集群</span><br><span class=\"line\">CLUSTER INFO 打印集群的信息</span><br><span class=\"line\">CLUSTER NODES 列出集群当前已知的所有节点（node），以及这些节点的相关信息。</span><br><span class=\"line\">//节点</span><br><span class=\"line\">CLUSTER MEET &lt;ip&gt; &lt;port&gt; 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。</span><br><span class=\"line\">CLUSTER FORGET &lt;node_id&gt; 从集群中移除 node_id 指定的节点。</span><br><span class=\"line\">CLUSTER REPLICATE &lt;node_id&gt; 将当前节点设置为 node_id 指定的节点的从节点。</span><br><span class=\"line\">CLUSTER SAVECONFIG 将节点的配置文件保存到硬盘里面。</span><br><span class=\"line\">//槽(slot)</span><br><span class=\"line\">CLUSTER ADDSLOTS &lt;slot&gt; [slot ...] 将一个或多个槽（slot）指派（assign）给当前节点。</span><br><span class=\"line\">CLUSTER DELSLOTS &lt;slot&gt; [slot ...] 移除一个或多个槽对当前节点的指派。</span><br><span class=\"line\">CLUSTER FLUSHSLOTS 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; NODE &lt;node_id&gt; 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;node_id&gt; 将本节点的槽 slot 迁移到 node_id 指定的节点中。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;node_id&gt; 从 node_id 指定的节点中导入槽 slot 到本节点。</span><br><span class=\"line\">CLUSTER SETSLOT &lt;slot&gt; STABLE 取消对槽 slot 的导入（import）或者迁移（migrate）。</span><br><span class=\"line\">//键</span><br><span class=\"line\">CLUSTER KEYSLOT &lt;key&gt; 计算键 key 应该被放置在哪个槽上。</span><br><span class=\"line\">CLUSTER COUNTKEYSINSLOT &lt;slot&gt; 返回槽 slot 目前包含的键值对数量。</span><br><span class=\"line\">CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt; 返回 count 个 slot 槽中的键。</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"2-redis-trib-rb-实现\"><a href=\"#2-redis-trib-rb-实现\" class=\"headerlink\" title=\"2.redis-trib.rb 实现\"></a>2.redis-trib.rb 实现</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#把127.0.0.1:9013当前master迁移到127.0.0.1:9015上  </span><br><span class=\"line\">redis-trib.rb reshard 127.0.0.1:9013  </span><br><span class=\"line\">#根据提示选择要迁移的slot数量(ps:这里选择500)  </span><br><span class=\"line\">How many slots do you want to move (from 1 to 16384)? 1(源master的需要迁移slot数量，不能单个指定)  </span><br><span class=\"line\">#选择要接受这些slot的node-id(127.0.0.1:9015)  </span><br><span class=\"line\">What is the receiving node ID? c725cda7b314701c6892f035224700e9a8336699 (ps:127.0.0.1:9015的node-id)  </span><br><span class=\"line\">Please enter all the source node IDs.  </span><br><span class=\"line\">  Type &apos;all&apos; to use all the nodes as source nodes for the hash slots.  </span><br><span class=\"line\">  Type &apos;done&apos; once you entered all the source nodes IDs.  </span><br><span class=\"line\">Source node #1:a92e4554aa2828b85f50f8f8318429d68f5213ca(源master的node-id)  </span><br><span class=\"line\">Source node #2:done  </span><br><span class=\"line\">#打印被移动的slot后，输入yes开始移动slot以及对应的数据.  </span><br><span class=\"line\">#Do you want to proceed with the proposed reshard plan (yes/no)? yes</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://redis.io/commands/cluster-setslot\" target=\"_blank\" rel=\"noopener\">http://redis.io/commands/cluster-setslot</a></li>\n<li><a href=\"http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）\" target=\"_blank\" rel=\"noopener\">http://carlosfu.iteye.com/blog/2243056（该博客较详细，本文未参考，只是做记录）</a></li>\n</ol>\n"},{"title":"Redis Cluster慢查询修复总结","date":"2016-08-28T11:27:16.000Z","_content":"## RedisCluster集群定位\n### 故障排除\n\n如果在产线中Redis 操作变慢，可按以下操作排除故障\n\n- slowlog查看引发延迟的慢命令\n\n获取10条慢操作日志\n```\nslowlog get 10\n```\n结果为查询ID、发生时间、运行时长和原命令。默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为**微秒**，注意这个时间是不包含网络延迟的。\n\n- 监控客户端的连接\n\n在Redis-cli工具中输入info clients可以查看到当前实例的所有客户端连接信息,过多的连接数也会影响redis性能\n```\ninfo clients\n```\n\n- 机器固有延迟\n\n如果使用的是虚拟机的话，会存在一个固有延迟.可以使用如下命令，在redis server中执行：\n```\n./redis-cli --intrinsic-latency 100\n```\n100位一秒内测试数量\n\n- 计算延迟\n\n如果你正在经历redis 延迟问题，可能需要衡量延迟到底是多大，用以下命令即可实现\n```\nredis-cli --latency -h `host` -p `port`\n```\n\n- AOF和磁盘IO引起延迟\n\n查看redis中fdatasync(2)，追踪进程\n```\nsudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write\n```\n\n- 过期key引起延迟\n\n当数据库有超过1/4key在同一时间过期，会引起redis 阻塞\n\n\n### 产线调整\n\n- aof配置调整\n\n经过查看redis log,我们发现aof延迟比较厉害，猜测写aof影响了redis 性能。但为了保证数据安全，又无法关闭aof文件。经过查看官方文档，未更改aof保存策略维持fsync every second，更改\n```\nno-appendfsync-on-rewrite yes\n```\n这个配置是指在重写aof时，不持久化数据到aof中。\n\n- rdb配置调整\n\n线上机器IO操作特别频繁，用命令\n```\niostat -d -k 2\n```\n查看当前IO情况。经过查看发现redis rdb temp文件变更比较频繁。将配置改为\n```\nsave 600 30000\n```\n线上机器IO才降下来。\n## Client使用定位\n- pipeline操作串行改并行\n\n经过测试，高并发情况下，key批量pipeline处理已经成为性能瓶颈。经过分析将这块的处理更改成并行处理，并行处理的线程数为当前master的数量，这样会将Client的处理能力提升n倍，经过测试，效果明显。\n- 集群节点信息单独线程维护\n\n之前Client为了实时获取集群node信息，在每次操作都会主动获取集群信息。这里操作不用这么频繁，如果并发特别大的话，也会消耗大量的时间，经过分析后将此处做更改。有两种修改方法(1.)单独新建线程，专门维护集群信息（2.）更改成处理失败情况下，再去更新集群信息。目前采用第二种\n- 增加slow log记录\n\n系统如果不存在对操作时间log统计，那么就会出现不容易定位是哪一块出的问题，但也是一把双刃剑，增加slow  log 统计会浪费时间，增加系统负担。基于以上综合考虑：特将slow log架构设计成通过rest API动态改变统计维度，统计分为set和get两种(set数据理论上会比get数据慢)。\n\n**问题1**\n\n更改成并行处理后，出现许多莫名其妙的问题，错误如下：\n```\nredis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream.\n```\n**解决**\n分析log,发现有同时多线程处理同一个key情况，更改key处理顺序，避免多线程同时操作一个key，此问题再未复现。\n\n**问题2**\n\n更改成并行处理后，出现开始一部分操作报null错误\n\n**解决**\n\n分析log,发现多线程同时调用一个获取集群信息方法，该方法避免多次调用，已经加lock,这会导致多数线程得不到集群信息，因此会出现null,将该方法提到初始化时即调用，彻底避免多线程调用。\n\n## 参考\n以下链接为我收藏的网址，对追踪Redis latency有很大帮助\n1. http://redis.io/topics/latency\n2. http://www.tuicool.com/articles/2Q7nauM\n3. http://www.jianshu.com/p/ee2aa7fe341b\n4. http://www.cnblogs.com/mushroom/p/4738170.html\n\n","source":"_posts/Redis-Cluster慢查询修复总结.md","raw":"---\ntitle: Redis Cluster慢查询修复总结\ndate: 2016-08-28 19:27:16\ntags: nosql\ncategories:\n- redis\n---\n## RedisCluster集群定位\n### 故障排除\n\n如果在产线中Redis 操作变慢，可按以下操作排除故障\n\n- slowlog查看引发延迟的慢命令\n\n获取10条慢操作日志\n```\nslowlog get 10\n```\n结果为查询ID、发生时间、运行时长和原命令。默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为**微秒**，注意这个时间是不包含网络延迟的。\n\n- 监控客户端的连接\n\n在Redis-cli工具中输入info clients可以查看到当前实例的所有客户端连接信息,过多的连接数也会影响redis性能\n```\ninfo clients\n```\n\n- 机器固有延迟\n\n如果使用的是虚拟机的话，会存在一个固有延迟.可以使用如下命令，在redis server中执行：\n```\n./redis-cli --intrinsic-latency 100\n```\n100位一秒内测试数量\n\n- 计算延迟\n\n如果你正在经历redis 延迟问题，可能需要衡量延迟到底是多大，用以下命令即可实现\n```\nredis-cli --latency -h `host` -p `port`\n```\n\n- AOF和磁盘IO引起延迟\n\n查看redis中fdatasync(2)，追踪进程\n```\nsudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write\n```\n\n- 过期key引起延迟\n\n当数据库有超过1/4key在同一时间过期，会引起redis 阻塞\n\n\n### 产线调整\n\n- aof配置调整\n\n经过查看redis log,我们发现aof延迟比较厉害，猜测写aof影响了redis 性能。但为了保证数据安全，又无法关闭aof文件。经过查看官方文档，未更改aof保存策略维持fsync every second，更改\n```\nno-appendfsync-on-rewrite yes\n```\n这个配置是指在重写aof时，不持久化数据到aof中。\n\n- rdb配置调整\n\n线上机器IO操作特别频繁，用命令\n```\niostat -d -k 2\n```\n查看当前IO情况。经过查看发现redis rdb temp文件变更比较频繁。将配置改为\n```\nsave 600 30000\n```\n线上机器IO才降下来。\n## Client使用定位\n- pipeline操作串行改并行\n\n经过测试，高并发情况下，key批量pipeline处理已经成为性能瓶颈。经过分析将这块的处理更改成并行处理，并行处理的线程数为当前master的数量，这样会将Client的处理能力提升n倍，经过测试，效果明显。\n- 集群节点信息单独线程维护\n\n之前Client为了实时获取集群node信息，在每次操作都会主动获取集群信息。这里操作不用这么频繁，如果并发特别大的话，也会消耗大量的时间，经过分析后将此处做更改。有两种修改方法(1.)单独新建线程，专门维护集群信息（2.）更改成处理失败情况下，再去更新集群信息。目前采用第二种\n- 增加slow log记录\n\n系统如果不存在对操作时间log统计，那么就会出现不容易定位是哪一块出的问题，但也是一把双刃剑，增加slow  log 统计会浪费时间，增加系统负担。基于以上综合考虑：特将slow log架构设计成通过rest API动态改变统计维度，统计分为set和get两种(set数据理论上会比get数据慢)。\n\n**问题1**\n\n更改成并行处理后，出现许多莫名其妙的问题，错误如下：\n```\nredis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream.\n```\n**解决**\n分析log,发现有同时多线程处理同一个key情况，更改key处理顺序，避免多线程同时操作一个key，此问题再未复现。\n\n**问题2**\n\n更改成并行处理后，出现开始一部分操作报null错误\n\n**解决**\n\n分析log,发现多线程同时调用一个获取集群信息方法，该方法避免多次调用，已经加lock,这会导致多数线程得不到集群信息，因此会出现null,将该方法提到初始化时即调用，彻底避免多线程调用。\n\n## 参考\n以下链接为我收藏的网址，对追踪Redis latency有很大帮助\n1. http://redis.io/topics/latency\n2. http://www.tuicool.com/articles/2Q7nauM\n3. http://www.jianshu.com/p/ee2aa7fe341b\n4. http://www.cnblogs.com/mushroom/p/4738170.html\n\n","slug":"Redis-Cluster慢查询修复总结","published":1,"updated":"2016-08-28T11:51:40.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbt003d8ceeqpdayxo3","content":"<h2 id=\"RedisCluster集群定位\"><a href=\"#RedisCluster集群定位\" class=\"headerlink\" title=\"RedisCluster集群定位\"></a>RedisCluster集群定位</h2><h3 id=\"故障排除\"><a href=\"#故障排除\" class=\"headerlink\" title=\"故障排除\"></a>故障排除</h3><p>如果在产线中Redis 操作变慢，可按以下操作排除故障</p>\n<ul>\n<li>slowlog查看引发延迟的慢命令</li>\n</ul>\n<p>获取10条慢操作日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">slowlog get 10</span><br></pre></td></tr></table></figure></p>\n<p>结果为查询ID、发生时间、运行时长和原命令。默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为<strong>微秒</strong>，注意这个时间是不包含网络延迟的。</p>\n<ul>\n<li>监控客户端的连接</li>\n</ul>\n<p>在Redis-cli工具中输入info clients可以查看到当前实例的所有客户端连接信息,过多的连接数也会影响redis性能<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">info clients</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>机器固有延迟</li>\n</ul>\n<p>如果使用的是虚拟机的话，会存在一个固有延迟.可以使用如下命令，在redis server中执行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./redis-cli --intrinsic-latency 100</span><br></pre></td></tr></table></figure></p>\n<p>100位一秒内测试数量</p>\n<ul>\n<li>计算延迟</li>\n</ul>\n<p>如果你正在经历redis 延迟问题，可能需要衡量延迟到底是多大，用以下命令即可实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli --latency -h `host` -p `port`</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>AOF和磁盘IO引起延迟</li>\n</ul>\n<p>查看redis中fdatasync(2)，追踪进程<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>过期key引起延迟</li>\n</ul>\n<p>当数据库有超过1/4key在同一时间过期，会引起redis 阻塞</p>\n<h3 id=\"产线调整\"><a href=\"#产线调整\" class=\"headerlink\" title=\"产线调整\"></a>产线调整</h3><ul>\n<li>aof配置调整</li>\n</ul>\n<p>经过查看redis log,我们发现aof延迟比较厉害，猜测写aof影响了redis 性能。但为了保证数据安全，又无法关闭aof文件。经过查看官方文档，未更改aof保存策略维持fsync every second，更改<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">no-appendfsync-on-rewrite yes</span><br></pre></td></tr></table></figure></p>\n<p>这个配置是指在重写aof时，不持久化数据到aof中。</p>\n<ul>\n<li>rdb配置调整</li>\n</ul>\n<p>线上机器IO操作特别频繁，用命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat -d -k 2</span><br></pre></td></tr></table></figure></p>\n<p>查看当前IO情况。经过查看发现redis rdb temp文件变更比较频繁。将配置改为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">save 600 30000</span><br></pre></td></tr></table></figure></p>\n<p>线上机器IO才降下来。</p>\n<h2 id=\"Client使用定位\"><a href=\"#Client使用定位\" class=\"headerlink\" title=\"Client使用定位\"></a>Client使用定位</h2><ul>\n<li>pipeline操作串行改并行</li>\n</ul>\n<p>经过测试，高并发情况下，key批量pipeline处理已经成为性能瓶颈。经过分析将这块的处理更改成并行处理，并行处理的线程数为当前master的数量，这样会将Client的处理能力提升n倍，经过测试，效果明显。</p>\n<ul>\n<li>集群节点信息单独线程维护</li>\n</ul>\n<p>之前Client为了实时获取集群node信息，在每次操作都会主动获取集群信息。这里操作不用这么频繁，如果并发特别大的话，也会消耗大量的时间，经过分析后将此处做更改。有两种修改方法(1.)单独新建线程，专门维护集群信息（2.）更改成处理失败情况下，再去更新集群信息。目前采用第二种</p>\n<ul>\n<li>增加slow log记录</li>\n</ul>\n<p>系统如果不存在对操作时间log统计，那么就会出现不容易定位是哪一块出的问题，但也是一把双刃剑，增加slow  log 统计会浪费时间，增加系统负担。基于以上综合考虑：特将slow log架构设计成通过rest API动态改变统计维度，统计分为set和get两种(set数据理论上会比get数据慢)。</p>\n<p><strong>问题1</strong></p>\n<p>更改成并行处理后，出现许多莫名其妙的问题，错误如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream.</span><br></pre></td></tr></table></figure></p>\n<p><strong>解决</strong><br>分析log,发现有同时多线程处理同一个key情况，更改key处理顺序，避免多线程同时操作一个key，此问题再未复现。</p>\n<p><strong>问题2</strong></p>\n<p>更改成并行处理后，出现开始一部分操作报null错误</p>\n<p><strong>解决</strong></p>\n<p>分析log,发现多线程同时调用一个获取集群信息方法，该方法避免多次调用，已经加lock,这会导致多数线程得不到集群信息，因此会出现null,将该方法提到初始化时即调用，彻底避免多线程调用。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>以下链接为我收藏的网址，对追踪Redis latency有很大帮助</p>\n<ol>\n<li><a href=\"http://redis.io/topics/latency\" target=\"_blank\" rel=\"noopener\">http://redis.io/topics/latency</a></li>\n<li><a href=\"http://www.tuicool.com/articles/2Q7nauM\" target=\"_blank\" rel=\"noopener\">http://www.tuicool.com/articles/2Q7nauM</a></li>\n<li><a href=\"http://www.jianshu.com/p/ee2aa7fe341b\" target=\"_blank\" rel=\"noopener\">http://www.jianshu.com/p/ee2aa7fe341b</a></li>\n<li><a href=\"http://www.cnblogs.com/mushroom/p/4738170.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/mushroom/p/4738170.html</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"RedisCluster集群定位\"><a href=\"#RedisCluster集群定位\" class=\"headerlink\" title=\"RedisCluster集群定位\"></a>RedisCluster集群定位</h2><h3 id=\"故障排除\"><a href=\"#故障排除\" class=\"headerlink\" title=\"故障排除\"></a>故障排除</h3><p>如果在产线中Redis 操作变慢，可按以下操作排除故障</p>\n<ul>\n<li>slowlog查看引发延迟的慢命令</li>\n</ul>\n<p>获取10条慢操作日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">slowlog get 10</span><br></pre></td></tr></table></figure></p>\n<p>结果为查询ID、发生时间、运行时长和原命令。默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为<strong>微秒</strong>，注意这个时间是不包含网络延迟的。</p>\n<ul>\n<li>监控客户端的连接</li>\n</ul>\n<p>在Redis-cli工具中输入info clients可以查看到当前实例的所有客户端连接信息,过多的连接数也会影响redis性能<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">info clients</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>机器固有延迟</li>\n</ul>\n<p>如果使用的是虚拟机的话，会存在一个固有延迟.可以使用如下命令，在redis server中执行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./redis-cli --intrinsic-latency 100</span><br></pre></td></tr></table></figure></p>\n<p>100位一秒内测试数量</p>\n<ul>\n<li>计算延迟</li>\n</ul>\n<p>如果你正在经历redis 延迟问题，可能需要衡量延迟到底是多大，用以下命令即可实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-cli --latency -h `host` -p `port`</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>AOF和磁盘IO引起延迟</li>\n</ul>\n<p>查看redis中fdatasync(2)，追踪进程<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>过期key引起延迟</li>\n</ul>\n<p>当数据库有超过1/4key在同一时间过期，会引起redis 阻塞</p>\n<h3 id=\"产线调整\"><a href=\"#产线调整\" class=\"headerlink\" title=\"产线调整\"></a>产线调整</h3><ul>\n<li>aof配置调整</li>\n</ul>\n<p>经过查看redis log,我们发现aof延迟比较厉害，猜测写aof影响了redis 性能。但为了保证数据安全，又无法关闭aof文件。经过查看官方文档，未更改aof保存策略维持fsync every second，更改<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">no-appendfsync-on-rewrite yes</span><br></pre></td></tr></table></figure></p>\n<p>这个配置是指在重写aof时，不持久化数据到aof中。</p>\n<ul>\n<li>rdb配置调整</li>\n</ul>\n<p>线上机器IO操作特别频繁，用命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat -d -k 2</span><br></pre></td></tr></table></figure></p>\n<p>查看当前IO情况。经过查看发现redis rdb temp文件变更比较频繁。将配置改为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">save 600 30000</span><br></pre></td></tr></table></figure></p>\n<p>线上机器IO才降下来。</p>\n<h2 id=\"Client使用定位\"><a href=\"#Client使用定位\" class=\"headerlink\" title=\"Client使用定位\"></a>Client使用定位</h2><ul>\n<li>pipeline操作串行改并行</li>\n</ul>\n<p>经过测试，高并发情况下，key批量pipeline处理已经成为性能瓶颈。经过分析将这块的处理更改成并行处理，并行处理的线程数为当前master的数量，这样会将Client的处理能力提升n倍，经过测试，效果明显。</p>\n<ul>\n<li>集群节点信息单独线程维护</li>\n</ul>\n<p>之前Client为了实时获取集群node信息，在每次操作都会主动获取集群信息。这里操作不用这么频繁，如果并发特别大的话，也会消耗大量的时间，经过分析后将此处做更改。有两种修改方法(1.)单独新建线程，专门维护集群信息（2.）更改成处理失败情况下，再去更新集群信息。目前采用第二种</p>\n<ul>\n<li>增加slow log记录</li>\n</ul>\n<p>系统如果不存在对操作时间log统计，那么就会出现不容易定位是哪一块出的问题，但也是一把双刃剑，增加slow  log 统计会浪费时间，增加系统负担。基于以上综合考虑：特将slow log架构设计成通过rest API动态改变统计维度，统计分为set和get两种(set数据理论上会比get数据慢)。</p>\n<p><strong>问题1</strong></p>\n<p>更改成并行处理后，出现许多莫名其妙的问题，错误如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream.</span><br></pre></td></tr></table></figure></p>\n<p><strong>解决</strong><br>分析log,发现有同时多线程处理同一个key情况，更改key处理顺序，避免多线程同时操作一个key，此问题再未复现。</p>\n<p><strong>问题2</strong></p>\n<p>更改成并行处理后，出现开始一部分操作报null错误</p>\n<p><strong>解决</strong></p>\n<p>分析log,发现多线程同时调用一个获取集群信息方法，该方法避免多次调用，已经加lock,这会导致多数线程得不到集群信息，因此会出现null,将该方法提到初始化时即调用，彻底避免多线程调用。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>以下链接为我收藏的网址，对追踪Redis latency有很大帮助</p>\n<ol>\n<li><a href=\"http://redis.io/topics/latency\" target=\"_blank\" rel=\"noopener\">http://redis.io/topics/latency</a></li>\n<li><a href=\"http://www.tuicool.com/articles/2Q7nauM\" target=\"_blank\" rel=\"noopener\">http://www.tuicool.com/articles/2Q7nauM</a></li>\n<li><a href=\"http://www.jianshu.com/p/ee2aa7fe341b\" target=\"_blank\" rel=\"noopener\">http://www.jianshu.com/p/ee2aa7fe341b</a></li>\n<li><a href=\"http://www.cnblogs.com/mushroom/p/4738170.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/mushroom/p/4738170.html</a></li>\n</ol>\n"},{"title":"Redis Info信息详情","date":"2016-06-01T14:00:06.000Z","_content":"#### Info信息\n---\n以下信息是在redis-cli中执行info命令，可能和redis cluster版本有点不同的地方，仅此作为参考。\n<!-- more -->\n```\n# Server\nredis_version:2.8.19 ###redis版本号\nredis_git_sha1:00000000  ###git SHA1\nredis_git_dirty:0   ###git dirty flag\nredis_build_id:78796c63e58b72dc\nredis_mode:standalone   ###redis运行模式\nos:Linux 2.6.32-431.el6.x86_64 x86_64   ###os版本号\narch_bits:64  ###64位架构\nmultiplexing_api:epoll  ###调用epoll算法\ngcc_version:4.4.7   ###gcc版本号\nprocess_id:25899   ###服务器进程PID\nrun_id:eae356ac1098c13b68f2b00fd7e1c9f93b1c6a2c   ###Redis的随机标识符(用于sentinel和集群)\ntcp_port:6379   ###Redis监听的端口号\nuptime_in_seconds:6419 ###Redis运行时长(s为单位)\nuptime_in_days:0  ###Redis运行时长(天为单位)\nhz:10\nlru_clock:10737922  ###以分钟为单位的自增时钟,用于LRU管理\nconfig_file:/etc/redis/redis.conf   ###redis配置文件\n\n# Clients\nconnected_clients:1   ###已连接客户端的数量（不包括通过从属服务器连接的客户端）这个参数也要一定关注，有飙升和明显下降时都会有问题。即使不操作\nclient_longest_output_list:0   ###当前连接的客户端中最长的输出列表\nclient_biggest_input_buf:0   ###当前连接的客户端中最大的。输出缓存\nblocked_clients:0  ###正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 需监控\n\n# Memory\nused_memory:2281560   ###由 Redis 分配器分配的内存总量，以字节（byte）为单位\nused_memory_human:2.18M   ###以更友好的格式输出redis占用的内存\nused_memory_rss:2699264   ###从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致，包含了used_memory和内存碎片。\nused_memory_peak:22141272  ### Redis 的内存消耗峰值（以字节为单位）\nused_memory_peak_human:21.12M  ###以更友好的格式输出redis峰值内存占用\nused_memory_lua:35840  ###LUA引擎所使用的内存大小\nmem_fragmentation_ratio:1.18  ###   =used_memory_rss /used_memory 这两个参数都包含保存用户k-v数据的内存和redis内部不同数据结构需要占用的内存，并且RSS指的是包含操作系统给redis实例分配的内存，这里面还包含不连续分配所带来的开销。因此在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。当 rss > used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。当 used > rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。可以说这个值大于1.5或者小于1都是有问题的。当大于1.5的时候需要择机进行服务器重启。当小于1的时候需要对redis进行数据清理\nmem_allocator:jemalloc-3.6.0\n\n# Persistence\nloading:0  ###记录服务器是否正在载入持久化文件，1为正在加载\nrdb_changes_since_last_save:0   ###距离最近一次成功创建持久化文件之后，产生了多少次修改数据集的操作\nrdb_bgsave_in_progress:0   ###记录了服务器是否正在创建 RDB 文件，1为正在进行\nrdb_last_save_time:1420023749  ###最近一次成功创建 RDB 文件的 UNIX 时间戳\nrdb_last_bgsave_status:ok   ###最近一次创建 RDB 文件的结果是成功还是失败,失败标识为err，这个时候写入redis 的操作可能会停止，因为默认stop-writes-on-bgsave-error是开启的，这个时候如果需要尽快恢复写操作，可以手工将这个选项设置为no。\nrdb_last_bgsave_time_sec:0  ###最近一次创建 RDB 文件耗费的秒数\nrdb_current_bgsave_time_sec:-1  ###如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数\naof_enabled:1   ###AOF 是否处于打开状态，1为启用\naof_rewrite_in_progress:0   ###服务器是否正在创建 AOF 文件\naof_rewrite_scheduled:0   ###RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作（因为在RDB时aof的rewrite会被阻塞一直到RDB结束）\naof_last_rewrite_time_sec:-1  ###最近一次创建 AOF 文件耗费的时长\naof_current_rewrite_time_sec:-1  ###如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数\naof_last_bgrewrite_status:ok  ###最近一次创建 AOF 文件的结果是成功还是失败\naof_last_write_status:ok \naof_current_size:176265  ###AOF 文件目前的大小\naof_base_size:176265  ###服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小\naof_pending_rewrite:0  ###是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行\naof_buffer_length:0   ###AOF 缓冲区的大小\naof_rewrite_buffer_length:0  ###AOF 重写缓冲区的大小\naof_pending_bio_fsync:0  ###后台 I/O 队列里面，等待执行的 fsync 调用数量\naof_delayed_fsync:0###被延迟的 fsync 调用数量\nloading_start_time:1441769386   loading启动时间戳\nloading_total_bytes:1787767808   loading需要加载数据量\nloading_loaded_bytes:1587418182  已经加载的数据量\nloading_loaded_perc:88.79 加载百分比\nloading_eta_seconds:7   剩余时间\n\n# Stats\ntotal_connections_received:8466  ###服务器已接受的连接请求数量，注意这是个累计值。\ntotal_commands_processed:900668   ###服务器已执行的命令数量，这个数值需要持续监控，如果在一段时间内出现大范围波动说明系统要么出现大量请求，要么出现执行缓慢的操作。\ninstantaneous_ops_per_sec:1   ###服务器每秒钟执行的命令数量\ntotal_net_input_bytes:82724170\ntotal_net_output_bytes:39509080\ninstantaneous_input_kbps:0.07\ninstantaneous_output_kbps:0.02\nrejected_connections:0  ###因为最大客户端数量限制而被拒绝的连接请求数量\nsync_full:2\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0   ###因为过期而被自动删除的数据库键数量\nevicted_keys:0   ###因为最大内存容量限制而被驱逐（evict）的键数量。这个数值如果不是0则说明maxmemory被触发，并且    evicted_keys一直大于0，则系统的latency增加，此时可以临时提高最大内存，但这只是临时措施，需要从应用着手分析。\nkeyspace_hits:0  ###查找数据库键成功的次数。可以计算命中率\nkeyspace_misses:500000   ###查找数据库键失败的次数。\npubsub_channels:0  ###目前被订阅的频道数量\npubsub_patterns:0  ###目前被订阅的模式数量\nlatest_fork_usec:402  ###最近一次 fork() 操作耗费的毫秒数\n\n# Replication\nrole:master   ###如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器\nconnected_slaves:2   ###2个slaves\nslave0:ip=192.168.65.130,port=6379,state=online,offset=1639,lag=1\nslave1:ip=192.168.65.129,port=6379,state=online,offset=1639,lag=0\nmaster_repl_offset:1639\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:2\nrepl_backlog_histlen:1638\n\n# CPU\nused_cpu_sys:41.87  ###Redis 服务器耗费的系统 CPU\nused_cpu_user:17.82  ###Redis 服务器耗费的用户 CPU\nused_cpu_sys_children:0.01  ###后台进程耗费的系统 CPU\nused_cpu_user_children:0.01  ###后台进程耗费的用户 CPU\n\n# Keyspace\ndb0:keys=3101,expires=0,avg_ttl=0   ###keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库过期键数量等。对于每个数据库，这个部分都会添加一行此信息\n```\n\n","source":"_posts/Redis-Info信息详情.md","raw":"---\ntitle: Redis Info信息详情\ndate: 2016-06-01 22:00:06\ntags: 大数据\ncategories:\n- redis\n---\n#### Info信息\n---\n以下信息是在redis-cli中执行info命令，可能和redis cluster版本有点不同的地方，仅此作为参考。\n<!-- more -->\n```\n# Server\nredis_version:2.8.19 ###redis版本号\nredis_git_sha1:00000000  ###git SHA1\nredis_git_dirty:0   ###git dirty flag\nredis_build_id:78796c63e58b72dc\nredis_mode:standalone   ###redis运行模式\nos:Linux 2.6.32-431.el6.x86_64 x86_64   ###os版本号\narch_bits:64  ###64位架构\nmultiplexing_api:epoll  ###调用epoll算法\ngcc_version:4.4.7   ###gcc版本号\nprocess_id:25899   ###服务器进程PID\nrun_id:eae356ac1098c13b68f2b00fd7e1c9f93b1c6a2c   ###Redis的随机标识符(用于sentinel和集群)\ntcp_port:6379   ###Redis监听的端口号\nuptime_in_seconds:6419 ###Redis运行时长(s为单位)\nuptime_in_days:0  ###Redis运行时长(天为单位)\nhz:10\nlru_clock:10737922  ###以分钟为单位的自增时钟,用于LRU管理\nconfig_file:/etc/redis/redis.conf   ###redis配置文件\n\n# Clients\nconnected_clients:1   ###已连接客户端的数量（不包括通过从属服务器连接的客户端）这个参数也要一定关注，有飙升和明显下降时都会有问题。即使不操作\nclient_longest_output_list:0   ###当前连接的客户端中最长的输出列表\nclient_biggest_input_buf:0   ###当前连接的客户端中最大的。输出缓存\nblocked_clients:0  ###正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 需监控\n\n# Memory\nused_memory:2281560   ###由 Redis 分配器分配的内存总量，以字节（byte）为单位\nused_memory_human:2.18M   ###以更友好的格式输出redis占用的内存\nused_memory_rss:2699264   ###从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致，包含了used_memory和内存碎片。\nused_memory_peak:22141272  ### Redis 的内存消耗峰值（以字节为单位）\nused_memory_peak_human:21.12M  ###以更友好的格式输出redis峰值内存占用\nused_memory_lua:35840  ###LUA引擎所使用的内存大小\nmem_fragmentation_ratio:1.18  ###   =used_memory_rss /used_memory 这两个参数都包含保存用户k-v数据的内存和redis内部不同数据结构需要占用的内存，并且RSS指的是包含操作系统给redis实例分配的内存，这里面还包含不连续分配所带来的开销。因此在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。当 rss > used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。当 used > rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。可以说这个值大于1.5或者小于1都是有问题的。当大于1.5的时候需要择机进行服务器重启。当小于1的时候需要对redis进行数据清理\nmem_allocator:jemalloc-3.6.0\n\n# Persistence\nloading:0  ###记录服务器是否正在载入持久化文件，1为正在加载\nrdb_changes_since_last_save:0   ###距离最近一次成功创建持久化文件之后，产生了多少次修改数据集的操作\nrdb_bgsave_in_progress:0   ###记录了服务器是否正在创建 RDB 文件，1为正在进行\nrdb_last_save_time:1420023749  ###最近一次成功创建 RDB 文件的 UNIX 时间戳\nrdb_last_bgsave_status:ok   ###最近一次创建 RDB 文件的结果是成功还是失败,失败标识为err，这个时候写入redis 的操作可能会停止，因为默认stop-writes-on-bgsave-error是开启的，这个时候如果需要尽快恢复写操作，可以手工将这个选项设置为no。\nrdb_last_bgsave_time_sec:0  ###最近一次创建 RDB 文件耗费的秒数\nrdb_current_bgsave_time_sec:-1  ###如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数\naof_enabled:1   ###AOF 是否处于打开状态，1为启用\naof_rewrite_in_progress:0   ###服务器是否正在创建 AOF 文件\naof_rewrite_scheduled:0   ###RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作（因为在RDB时aof的rewrite会被阻塞一直到RDB结束）\naof_last_rewrite_time_sec:-1  ###最近一次创建 AOF 文件耗费的时长\naof_current_rewrite_time_sec:-1  ###如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数\naof_last_bgrewrite_status:ok  ###最近一次创建 AOF 文件的结果是成功还是失败\naof_last_write_status:ok \naof_current_size:176265  ###AOF 文件目前的大小\naof_base_size:176265  ###服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小\naof_pending_rewrite:0  ###是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行\naof_buffer_length:0   ###AOF 缓冲区的大小\naof_rewrite_buffer_length:0  ###AOF 重写缓冲区的大小\naof_pending_bio_fsync:0  ###后台 I/O 队列里面，等待执行的 fsync 调用数量\naof_delayed_fsync:0###被延迟的 fsync 调用数量\nloading_start_time:1441769386   loading启动时间戳\nloading_total_bytes:1787767808   loading需要加载数据量\nloading_loaded_bytes:1587418182  已经加载的数据量\nloading_loaded_perc:88.79 加载百分比\nloading_eta_seconds:7   剩余时间\n\n# Stats\ntotal_connections_received:8466  ###服务器已接受的连接请求数量，注意这是个累计值。\ntotal_commands_processed:900668   ###服务器已执行的命令数量，这个数值需要持续监控，如果在一段时间内出现大范围波动说明系统要么出现大量请求，要么出现执行缓慢的操作。\ninstantaneous_ops_per_sec:1   ###服务器每秒钟执行的命令数量\ntotal_net_input_bytes:82724170\ntotal_net_output_bytes:39509080\ninstantaneous_input_kbps:0.07\ninstantaneous_output_kbps:0.02\nrejected_connections:0  ###因为最大客户端数量限制而被拒绝的连接请求数量\nsync_full:2\nsync_partial_ok:0\nsync_partial_err:0\nexpired_keys:0   ###因为过期而被自动删除的数据库键数量\nevicted_keys:0   ###因为最大内存容量限制而被驱逐（evict）的键数量。这个数值如果不是0则说明maxmemory被触发，并且    evicted_keys一直大于0，则系统的latency增加，此时可以临时提高最大内存，但这只是临时措施，需要从应用着手分析。\nkeyspace_hits:0  ###查找数据库键成功的次数。可以计算命中率\nkeyspace_misses:500000   ###查找数据库键失败的次数。\npubsub_channels:0  ###目前被订阅的频道数量\npubsub_patterns:0  ###目前被订阅的模式数量\nlatest_fork_usec:402  ###最近一次 fork() 操作耗费的毫秒数\n\n# Replication\nrole:master   ###如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器\nconnected_slaves:2   ###2个slaves\nslave0:ip=192.168.65.130,port=6379,state=online,offset=1639,lag=1\nslave1:ip=192.168.65.129,port=6379,state=online,offset=1639,lag=0\nmaster_repl_offset:1639\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:2\nrepl_backlog_histlen:1638\n\n# CPU\nused_cpu_sys:41.87  ###Redis 服务器耗费的系统 CPU\nused_cpu_user:17.82  ###Redis 服务器耗费的用户 CPU\nused_cpu_sys_children:0.01  ###后台进程耗费的系统 CPU\nused_cpu_user_children:0.01  ###后台进程耗费的用户 CPU\n\n# Keyspace\ndb0:keys=3101,expires=0,avg_ttl=0   ###keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库过期键数量等。对于每个数据库，这个部分都会添加一行此信息\n```\n\n","slug":"Redis-Info信息详情","published":1,"updated":"2016-06-01T14:20:53.498Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbv003f8cee6co6gr9c","content":"<h4 id=\"Info信息\"><a href=\"#Info信息\" class=\"headerlink\" title=\"Info信息\"></a>Info信息</h4><hr>\n<p>以下信息是在redis-cli中执行info命令，可能和redis cluster版本有点不同的地方，仅此作为参考。<br><a id=\"more\"></a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Server</span><br><span class=\"line\">redis_version:2.8.19 ###redis版本号</span><br><span class=\"line\">redis_git_sha1:00000000  ###git SHA1</span><br><span class=\"line\">redis_git_dirty:0   ###git dirty flag</span><br><span class=\"line\">redis_build_id:78796c63e58b72dc</span><br><span class=\"line\">redis_mode:standalone   ###redis运行模式</span><br><span class=\"line\">os:Linux 2.6.32-431.el6.x86_64 x86_64   ###os版本号</span><br><span class=\"line\">arch_bits:64  ###64位架构</span><br><span class=\"line\">multiplexing_api:epoll  ###调用epoll算法</span><br><span class=\"line\">gcc_version:4.4.7   ###gcc版本号</span><br><span class=\"line\">process_id:25899   ###服务器进程PID</span><br><span class=\"line\">run_id:eae356ac1098c13b68f2b00fd7e1c9f93b1c6a2c   ###Redis的随机标识符(用于sentinel和集群)</span><br><span class=\"line\">tcp_port:6379   ###Redis监听的端口号</span><br><span class=\"line\">uptime_in_seconds:6419 ###Redis运行时长(s为单位)</span><br><span class=\"line\">uptime_in_days:0  ###Redis运行时长(天为单位)</span><br><span class=\"line\">hz:10</span><br><span class=\"line\">lru_clock:10737922  ###以分钟为单位的自增时钟,用于LRU管理</span><br><span class=\"line\">config_file:/etc/redis/redis.conf   ###redis配置文件</span><br><span class=\"line\"></span><br><span class=\"line\"># Clients</span><br><span class=\"line\">connected_clients:1   ###已连接客户端的数量（不包括通过从属服务器连接的客户端）这个参数也要一定关注，有飙升和明显下降时都会有问题。即使不操作</span><br><span class=\"line\">client_longest_output_list:0   ###当前连接的客户端中最长的输出列表</span><br><span class=\"line\">client_biggest_input_buf:0   ###当前连接的客户端中最大的。输出缓存</span><br><span class=\"line\">blocked_clients:0  ###正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 需监控</span><br><span class=\"line\"></span><br><span class=\"line\"># Memory</span><br><span class=\"line\">used_memory:2281560   ###由 Redis 分配器分配的内存总量，以字节（byte）为单位</span><br><span class=\"line\">used_memory_human:2.18M   ###以更友好的格式输出redis占用的内存</span><br><span class=\"line\">used_memory_rss:2699264   ###从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致，包含了used_memory和内存碎片。</span><br><span class=\"line\">used_memory_peak:22141272  ### Redis 的内存消耗峰值（以字节为单位）</span><br><span class=\"line\">used_memory_peak_human:21.12M  ###以更友好的格式输出redis峰值内存占用</span><br><span class=\"line\">used_memory_lua:35840  ###LUA引擎所使用的内存大小</span><br><span class=\"line\">mem_fragmentation_ratio:1.18  ###   =used_memory_rss /used_memory 这两个参数都包含保存用户k-v数据的内存和redis内部不同数据结构需要占用的内存，并且RSS指的是包含操作系统给redis实例分配的内存，这里面还包含不连续分配所带来的开销。因此在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。可以说这个值大于1.5或者小于1都是有问题的。当大于1.5的时候需要择机进行服务器重启。当小于1的时候需要对redis进行数据清理</span><br><span class=\"line\">mem_allocator:jemalloc-3.6.0</span><br><span class=\"line\"></span><br><span class=\"line\"># Persistence</span><br><span class=\"line\">loading:0  ###记录服务器是否正在载入持久化文件，1为正在加载</span><br><span class=\"line\">rdb_changes_since_last_save:0   ###距离最近一次成功创建持久化文件之后，产生了多少次修改数据集的操作</span><br><span class=\"line\">rdb_bgsave_in_progress:0   ###记录了服务器是否正在创建 RDB 文件，1为正在进行</span><br><span class=\"line\">rdb_last_save_time:1420023749  ###最近一次成功创建 RDB 文件的 UNIX 时间戳</span><br><span class=\"line\">rdb_last_bgsave_status:ok   ###最近一次创建 RDB 文件的结果是成功还是失败,失败标识为err，这个时候写入redis 的操作可能会停止，因为默认stop-writes-on-bgsave-error是开启的，这个时候如果需要尽快恢复写操作，可以手工将这个选项设置为no。</span><br><span class=\"line\">rdb_last_bgsave_time_sec:0  ###最近一次创建 RDB 文件耗费的秒数</span><br><span class=\"line\">rdb_current_bgsave_time_sec:-1  ###如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数</span><br><span class=\"line\">aof_enabled:1   ###AOF 是否处于打开状态，1为启用</span><br><span class=\"line\">aof_rewrite_in_progress:0   ###服务器是否正在创建 AOF 文件</span><br><span class=\"line\">aof_rewrite_scheduled:0   ###RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作（因为在RDB时aof的rewrite会被阻塞一直到RDB结束）</span><br><span class=\"line\">aof_last_rewrite_time_sec:-1  ###最近一次创建 AOF 文件耗费的时长</span><br><span class=\"line\">aof_current_rewrite_time_sec:-1  ###如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数</span><br><span class=\"line\">aof_last_bgrewrite_status:ok  ###最近一次创建 AOF 文件的结果是成功还是失败</span><br><span class=\"line\">aof_last_write_status:ok </span><br><span class=\"line\">aof_current_size:176265  ###AOF 文件目前的大小</span><br><span class=\"line\">aof_base_size:176265  ###服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小</span><br><span class=\"line\">aof_pending_rewrite:0  ###是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行</span><br><span class=\"line\">aof_buffer_length:0   ###AOF 缓冲区的大小</span><br><span class=\"line\">aof_rewrite_buffer_length:0  ###AOF 重写缓冲区的大小</span><br><span class=\"line\">aof_pending_bio_fsync:0  ###后台 I/O 队列里面，等待执行的 fsync 调用数量</span><br><span class=\"line\">aof_delayed_fsync:0###被延迟的 fsync 调用数量</span><br><span class=\"line\">loading_start_time:1441769386   loading启动时间戳</span><br><span class=\"line\">loading_total_bytes:1787767808   loading需要加载数据量</span><br><span class=\"line\">loading_loaded_bytes:1587418182  已经加载的数据量</span><br><span class=\"line\">loading_loaded_perc:88.79 加载百分比</span><br><span class=\"line\">loading_eta_seconds:7   剩余时间</span><br><span class=\"line\"></span><br><span class=\"line\"># Stats</span><br><span class=\"line\">total_connections_received:8466  ###服务器已接受的连接请求数量，注意这是个累计值。</span><br><span class=\"line\">total_commands_processed:900668   ###服务器已执行的命令数量，这个数值需要持续监控，如果在一段时间内出现大范围波动说明系统要么出现大量请求，要么出现执行缓慢的操作。</span><br><span class=\"line\">instantaneous_ops_per_sec:1   ###服务器每秒钟执行的命令数量</span><br><span class=\"line\">total_net_input_bytes:82724170</span><br><span class=\"line\">total_net_output_bytes:39509080</span><br><span class=\"line\">instantaneous_input_kbps:0.07</span><br><span class=\"line\">instantaneous_output_kbps:0.02</span><br><span class=\"line\">rejected_connections:0  ###因为最大客户端数量限制而被拒绝的连接请求数量</span><br><span class=\"line\">sync_full:2</span><br><span class=\"line\">sync_partial_ok:0</span><br><span class=\"line\">sync_partial_err:0</span><br><span class=\"line\">expired_keys:0   ###因为过期而被自动删除的数据库键数量</span><br><span class=\"line\">evicted_keys:0   ###因为最大内存容量限制而被驱逐（evict）的键数量。这个数值如果不是0则说明maxmemory被触发，并且    evicted_keys一直大于0，则系统的latency增加，此时可以临时提高最大内存，但这只是临时措施，需要从应用着手分析。</span><br><span class=\"line\">keyspace_hits:0  ###查找数据库键成功的次数。可以计算命中率</span><br><span class=\"line\">keyspace_misses:500000   ###查找数据库键失败的次数。</span><br><span class=\"line\">pubsub_channels:0  ###目前被订阅的频道数量</span><br><span class=\"line\">pubsub_patterns:0  ###目前被订阅的模式数量</span><br><span class=\"line\">latest_fork_usec:402  ###最近一次 fork() 操作耗费的毫秒数</span><br><span class=\"line\"></span><br><span class=\"line\"># Replication</span><br><span class=\"line\">role:master   ###如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器</span><br><span class=\"line\">connected_slaves:2   ###2个slaves</span><br><span class=\"line\">slave0:ip=192.168.65.130,port=6379,state=online,offset=1639,lag=1</span><br><span class=\"line\">slave1:ip=192.168.65.129,port=6379,state=online,offset=1639,lag=0</span><br><span class=\"line\">master_repl_offset:1639</span><br><span class=\"line\">repl_backlog_active:1</span><br><span class=\"line\">repl_backlog_size:1048576</span><br><span class=\"line\">repl_backlog_first_byte_offset:2</span><br><span class=\"line\">repl_backlog_histlen:1638</span><br><span class=\"line\"></span><br><span class=\"line\"># CPU</span><br><span class=\"line\">used_cpu_sys:41.87  ###Redis 服务器耗费的系统 CPU</span><br><span class=\"line\">used_cpu_user:17.82  ###Redis 服务器耗费的用户 CPU</span><br><span class=\"line\">used_cpu_sys_children:0.01  ###后台进程耗费的系统 CPU</span><br><span class=\"line\">used_cpu_user_children:0.01  ###后台进程耗费的用户 CPU</span><br><span class=\"line\"></span><br><span class=\"line\"># Keyspace</span><br><span class=\"line\">db0:keys=3101,expires=0,avg_ttl=0   ###keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库过期键数量等。对于每个数据库，这个部分都会添加一行此信息</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<h4 id=\"Info信息\"><a href=\"#Info信息\" class=\"headerlink\" title=\"Info信息\"></a>Info信息</h4><hr>\n<p>以下信息是在redis-cli中执行info命令，可能和redis cluster版本有点不同的地方，仅此作为参考。<br>","more":"<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Server</span><br><span class=\"line\">redis_version:2.8.19 ###redis版本号</span><br><span class=\"line\">redis_git_sha1:00000000  ###git SHA1</span><br><span class=\"line\">redis_git_dirty:0   ###git dirty flag</span><br><span class=\"line\">redis_build_id:78796c63e58b72dc</span><br><span class=\"line\">redis_mode:standalone   ###redis运行模式</span><br><span class=\"line\">os:Linux 2.6.32-431.el6.x86_64 x86_64   ###os版本号</span><br><span class=\"line\">arch_bits:64  ###64位架构</span><br><span class=\"line\">multiplexing_api:epoll  ###调用epoll算法</span><br><span class=\"line\">gcc_version:4.4.7   ###gcc版本号</span><br><span class=\"line\">process_id:25899   ###服务器进程PID</span><br><span class=\"line\">run_id:eae356ac1098c13b68f2b00fd7e1c9f93b1c6a2c   ###Redis的随机标识符(用于sentinel和集群)</span><br><span class=\"line\">tcp_port:6379   ###Redis监听的端口号</span><br><span class=\"line\">uptime_in_seconds:6419 ###Redis运行时长(s为单位)</span><br><span class=\"line\">uptime_in_days:0  ###Redis运行时长(天为单位)</span><br><span class=\"line\">hz:10</span><br><span class=\"line\">lru_clock:10737922  ###以分钟为单位的自增时钟,用于LRU管理</span><br><span class=\"line\">config_file:/etc/redis/redis.conf   ###redis配置文件</span><br><span class=\"line\"></span><br><span class=\"line\"># Clients</span><br><span class=\"line\">connected_clients:1   ###已连接客户端的数量（不包括通过从属服务器连接的客户端）这个参数也要一定关注，有飙升和明显下降时都会有问题。即使不操作</span><br><span class=\"line\">client_longest_output_list:0   ###当前连接的客户端中最长的输出列表</span><br><span class=\"line\">client_biggest_input_buf:0   ###当前连接的客户端中最大的。输出缓存</span><br><span class=\"line\">blocked_clients:0  ###正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 需监控</span><br><span class=\"line\"></span><br><span class=\"line\"># Memory</span><br><span class=\"line\">used_memory:2281560   ###由 Redis 分配器分配的内存总量，以字节（byte）为单位</span><br><span class=\"line\">used_memory_human:2.18M   ###以更友好的格式输出redis占用的内存</span><br><span class=\"line\">used_memory_rss:2699264   ###从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致，包含了used_memory和内存碎片。</span><br><span class=\"line\">used_memory_peak:22141272  ### Redis 的内存消耗峰值（以字节为单位）</span><br><span class=\"line\">used_memory_peak_human:21.12M  ###以更友好的格式输出redis峰值内存占用</span><br><span class=\"line\">used_memory_lua:35840  ###LUA引擎所使用的内存大小</span><br><span class=\"line\">mem_fragmentation_ratio:1.18  ###   =used_memory_rss /used_memory 这两个参数都包含保存用户k-v数据的内存和redis内部不同数据结构需要占用的内存，并且RSS指的是包含操作系统给redis实例分配的内存，这里面还包含不连续分配所带来的开销。因此在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。可以说这个值大于1.5或者小于1都是有问题的。当大于1.5的时候需要择机进行服务器重启。当小于1的时候需要对redis进行数据清理</span><br><span class=\"line\">mem_allocator:jemalloc-3.6.0</span><br><span class=\"line\"></span><br><span class=\"line\"># Persistence</span><br><span class=\"line\">loading:0  ###记录服务器是否正在载入持久化文件，1为正在加载</span><br><span class=\"line\">rdb_changes_since_last_save:0   ###距离最近一次成功创建持久化文件之后，产生了多少次修改数据集的操作</span><br><span class=\"line\">rdb_bgsave_in_progress:0   ###记录了服务器是否正在创建 RDB 文件，1为正在进行</span><br><span class=\"line\">rdb_last_save_time:1420023749  ###最近一次成功创建 RDB 文件的 UNIX 时间戳</span><br><span class=\"line\">rdb_last_bgsave_status:ok   ###最近一次创建 RDB 文件的结果是成功还是失败,失败标识为err，这个时候写入redis 的操作可能会停止，因为默认stop-writes-on-bgsave-error是开启的，这个时候如果需要尽快恢复写操作，可以手工将这个选项设置为no。</span><br><span class=\"line\">rdb_last_bgsave_time_sec:0  ###最近一次创建 RDB 文件耗费的秒数</span><br><span class=\"line\">rdb_current_bgsave_time_sec:-1  ###如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数</span><br><span class=\"line\">aof_enabled:1   ###AOF 是否处于打开状态，1为启用</span><br><span class=\"line\">aof_rewrite_in_progress:0   ###服务器是否正在创建 AOF 文件</span><br><span class=\"line\">aof_rewrite_scheduled:0   ###RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作（因为在RDB时aof的rewrite会被阻塞一直到RDB结束）</span><br><span class=\"line\">aof_last_rewrite_time_sec:-1  ###最近一次创建 AOF 文件耗费的时长</span><br><span class=\"line\">aof_current_rewrite_time_sec:-1  ###如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数</span><br><span class=\"line\">aof_last_bgrewrite_status:ok  ###最近一次创建 AOF 文件的结果是成功还是失败</span><br><span class=\"line\">aof_last_write_status:ok </span><br><span class=\"line\">aof_current_size:176265  ###AOF 文件目前的大小</span><br><span class=\"line\">aof_base_size:176265  ###服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小</span><br><span class=\"line\">aof_pending_rewrite:0  ###是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行</span><br><span class=\"line\">aof_buffer_length:0   ###AOF 缓冲区的大小</span><br><span class=\"line\">aof_rewrite_buffer_length:0  ###AOF 重写缓冲区的大小</span><br><span class=\"line\">aof_pending_bio_fsync:0  ###后台 I/O 队列里面，等待执行的 fsync 调用数量</span><br><span class=\"line\">aof_delayed_fsync:0###被延迟的 fsync 调用数量</span><br><span class=\"line\">loading_start_time:1441769386   loading启动时间戳</span><br><span class=\"line\">loading_total_bytes:1787767808   loading需要加载数据量</span><br><span class=\"line\">loading_loaded_bytes:1587418182  已经加载的数据量</span><br><span class=\"line\">loading_loaded_perc:88.79 加载百分比</span><br><span class=\"line\">loading_eta_seconds:7   剩余时间</span><br><span class=\"line\"></span><br><span class=\"line\"># Stats</span><br><span class=\"line\">total_connections_received:8466  ###服务器已接受的连接请求数量，注意这是个累计值。</span><br><span class=\"line\">total_commands_processed:900668   ###服务器已执行的命令数量，这个数值需要持续监控，如果在一段时间内出现大范围波动说明系统要么出现大量请求，要么出现执行缓慢的操作。</span><br><span class=\"line\">instantaneous_ops_per_sec:1   ###服务器每秒钟执行的命令数量</span><br><span class=\"line\">total_net_input_bytes:82724170</span><br><span class=\"line\">total_net_output_bytes:39509080</span><br><span class=\"line\">instantaneous_input_kbps:0.07</span><br><span class=\"line\">instantaneous_output_kbps:0.02</span><br><span class=\"line\">rejected_connections:0  ###因为最大客户端数量限制而被拒绝的连接请求数量</span><br><span class=\"line\">sync_full:2</span><br><span class=\"line\">sync_partial_ok:0</span><br><span class=\"line\">sync_partial_err:0</span><br><span class=\"line\">expired_keys:0   ###因为过期而被自动删除的数据库键数量</span><br><span class=\"line\">evicted_keys:0   ###因为最大内存容量限制而被驱逐（evict）的键数量。这个数值如果不是0则说明maxmemory被触发，并且    evicted_keys一直大于0，则系统的latency增加，此时可以临时提高最大内存，但这只是临时措施，需要从应用着手分析。</span><br><span class=\"line\">keyspace_hits:0  ###查找数据库键成功的次数。可以计算命中率</span><br><span class=\"line\">keyspace_misses:500000   ###查找数据库键失败的次数。</span><br><span class=\"line\">pubsub_channels:0  ###目前被订阅的频道数量</span><br><span class=\"line\">pubsub_patterns:0  ###目前被订阅的模式数量</span><br><span class=\"line\">latest_fork_usec:402  ###最近一次 fork() 操作耗费的毫秒数</span><br><span class=\"line\"></span><br><span class=\"line\"># Replication</span><br><span class=\"line\">role:master   ###如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器</span><br><span class=\"line\">connected_slaves:2   ###2个slaves</span><br><span class=\"line\">slave0:ip=192.168.65.130,port=6379,state=online,offset=1639,lag=1</span><br><span class=\"line\">slave1:ip=192.168.65.129,port=6379,state=online,offset=1639,lag=0</span><br><span class=\"line\">master_repl_offset:1639</span><br><span class=\"line\">repl_backlog_active:1</span><br><span class=\"line\">repl_backlog_size:1048576</span><br><span class=\"line\">repl_backlog_first_byte_offset:2</span><br><span class=\"line\">repl_backlog_histlen:1638</span><br><span class=\"line\"></span><br><span class=\"line\"># CPU</span><br><span class=\"line\">used_cpu_sys:41.87  ###Redis 服务器耗费的系统 CPU</span><br><span class=\"line\">used_cpu_user:17.82  ###Redis 服务器耗费的用户 CPU</span><br><span class=\"line\">used_cpu_sys_children:0.01  ###后台进程耗费的系统 CPU</span><br><span class=\"line\">used_cpu_user_children:0.01  ###后台进程耗费的用户 CPU</span><br><span class=\"line\"></span><br><span class=\"line\"># Keyspace</span><br><span class=\"line\">db0:keys=3101,expires=0,avg_ttl=0   ###keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库过期键数量等。对于每个数据库，这个部分都会添加一行此信息</span><br></pre></td></tr></table></figure></p>"},{"title":"Redis Cluster 批量操作实现","date":"2016-06-05T02:27:55.000Z","_content":"## 前言\n之前发过一篇[**RedisCluster构建批量操作探讨**](http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/)，这篇文章针对其中第2种方案，做一次实现。在使用redis cluster 过程中，经常会需要用到批量操作，本次探讨因此产生。\n<!-- more -->\n## 实现\n原理：将key分批计算所在node,然后在单个node上执行pipeline,即可完成批量操作\n### key 槽计算\n因为redis cluster 设计原理，在处理key时，会将 CRC16(key) mod 16384 散列的分布在集群的node 上，因此，我们需要计算该key所在slot.(利用jedis中自带的算法)\n```\nint slot = JedisClusterCRC16.getSlot(key);\n```\n### node及slot获取\n同样基于jedis(2.8.1),核心用法如下：\n```\nJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());\nList<Object> list = jedisNode.clusterSlots();\n```\n该方法获取到整个集群中slot在node节点的分布。例如：0,5000，node.\n通过以上方法整合，即可实现批量操作。\n\n### 整合案例\n以下是我自己写的一个set类型读取的批量操作的实现\n```\n\nprivate static Map<String, JedisPool> nodeMap = jedis.getClusterNodes();\n\nprivate static TreeMap<Long, String> slotHostMap = getSlotHostMap(redisConf[0]);\n/**\n\t * 将key按slort分批整理\n\t * @param keys\n\t * @return\n\t */\nprivate static Map<JedisPool, List<String>> getPoolKeyMap(List<String> keys) {\n\tMap<JedisPool, List<String>> poolKeysMap = new LinkedHashMap<JedisPool, List<String>>();\n\ttry {\n\t\tfor (String key : keys) {\n\n\t\t\tint slot = JedisClusterCRC16.getSlot(key);\n\n\t\t\t//获取到对应的Jedis对象，此处+1解决临界问题\n\t\t\tMap.Entry<Long, String> entry = slotHostMap.lowerEntry(Long.valueOf(slot+1));\n\n\t\t\tJedisPool jedisPool = nodeMap.get(entry.getValue());\n\n\t\t\tif (poolKeysMap.containsKey(jedisPool)) {\n\t\t\t\tpoolKeysMap.get(jedisPool).add(key);\n\t\t\t} else {\n\t\t\t\tList<String> subKeyList = new ArrayList<String>();\n\t\t\t\tsubKeyList.add(key);\n\t\t\t\tpoolKeysMap.put(jedisPool, subKeyList);\n\t\t\t}\n\t\t}\n\t} catch (Exception e) {\n\t\te.getMessage();\n\t}\n\treturn poolKeysMap;\n}\n\n/**\n * slort对应node\n * @param anyHostAndPortStr\n * @return\n */\nprivate static TreeMap<Long, String> getSlotHostMap(String anyHostAndPortStr) {\n\tTreeMap<Long, String> tree = new TreeMap<Long, String>();\n\tString parts[] = anyHostAndPortStr.split(\":\");\n\tHostAndPort anyHostAndPort = new HostAndPort(parts[0], Integer.parseInt(parts[1]));\n\ttry {\n\t\tJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());\n\t\tList<Object> list = jedisNode.clusterSlots();\n\t\tfor (Object object : list) {\n\t\t\tList<Object> list1 = (List<Object>) object;\n\t\t\tList<Object> master = (List<Object>) list1.get(2);\n\t\t\tString hostAndPort = new String((byte[]) master.get(0)) + \":\" + master.get(1);\n\t\t\ttree.put((Long) list1.get(0), hostAndPort);\n\t\t\ttree.put((Long) list1.get(1), hostAndPort);\n\t\t}\n\t\tjedisNode.close();\n\t} catch (Exception e) {\n\n\t}\n\treturn tree;\n}\n\n/**\n * 批量获取set类型数据\n * @param keys\n * @return\n */\npublic static Map<String, Set<String>> batchGetSetData(List<String> keys) {\n\tif (keys == null || keys.isEmpty()) {\n\t\treturn null;\n\t}\n\tMap<JedisPool, List<String>> poolKeysMap = getPoolKeyMap(keys);\n\tMap<String, Set<String>> resultMap = new HashMap<String, Set<String>>();\n\tfor (Map.Entry<JedisPool, List<String>> entry : poolKeysMap.entrySet()) {\n\t\tJedisPool jedisPool = entry.getKey();\n\t\tList<String> subkeys = entry.getValue();\n\t\tif (subkeys == null || subkeys.isEmpty()) {\n\t\t\tcontinue;\n\t\t}\n\t\t//申请jedis对象\n\t\tJedis jedis = null;\n\t\tPipeline pipeline = null;\n\t\tList<Object> subResultList = null;\n\t\ttry {\n\t\t\tjedis = jedisPool.getResource();\n\t\t\tpipeline = jedis.pipelined();\n\n\t\t\tfor (String key : subkeys) {\n\t\t\t\tpipeline.smembers(key);\n\t\t\t}\n\n\t\t\tsubResultList = pipeline.syncAndReturnAll();\n\t\t} catch (JedisConnectionException e) {\n\t\t\te.getMessage();\n\t\t} catch (Exception e) {\n\t\t\te.getMessage();\n\t\t} finally {\n\t\t\tif (pipeline != null)\n\t\t\t\ttry {\n\t\t\t\t\tpipeline.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t//释放jedis对象\n\t\t\tif (jedis != null) {\n\t\t\t\tjedis.close();\n\t\t\t}\n\t\t}\n\t\tif (subResultList == null || subResultList.isEmpty()) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (subResultList.size() == subkeys.size()) {\n\t\t\tfor (int i = 0; i < subkeys.size(); i++) {\n\t\t\t\tString key = subkeys.get(i);\n\t\t\t\tObject result = subResultList.get(i);\n\t\t\t\tresultMap.put(key, (Set<String>) result);\n\t\t\t}\n\t\t} else {\n\t\t\tSystem.out.println(\"redis cluster pipeline error!\");\n\t\t}\n\t}\n\treturn resultMap;\n}\n```\n## 参考\n[1] cachecloud client中实现","source":"_posts/Redis-Cluster-批量操作实现.md","raw":"---\ntitle: Redis Cluster 批量操作实现\ndate: 2016-06-05 10:27:55\ntags: nosql\ncategories:\n- redis\n---\n## 前言\n之前发过一篇[**RedisCluster构建批量操作探讨**](http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/)，这篇文章针对其中第2种方案，做一次实现。在使用redis cluster 过程中，经常会需要用到批量操作，本次探讨因此产生。\n<!-- more -->\n## 实现\n原理：将key分批计算所在node,然后在单个node上执行pipeline,即可完成批量操作\n### key 槽计算\n因为redis cluster 设计原理，在处理key时，会将 CRC16(key) mod 16384 散列的分布在集群的node 上，因此，我们需要计算该key所在slot.(利用jedis中自带的算法)\n```\nint slot = JedisClusterCRC16.getSlot(key);\n```\n### node及slot获取\n同样基于jedis(2.8.1),核心用法如下：\n```\nJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());\nList<Object> list = jedisNode.clusterSlots();\n```\n该方法获取到整个集群中slot在node节点的分布。例如：0,5000，node.\n通过以上方法整合，即可实现批量操作。\n\n### 整合案例\n以下是我自己写的一个set类型读取的批量操作的实现\n```\n\nprivate static Map<String, JedisPool> nodeMap = jedis.getClusterNodes();\n\nprivate static TreeMap<Long, String> slotHostMap = getSlotHostMap(redisConf[0]);\n/**\n\t * 将key按slort分批整理\n\t * @param keys\n\t * @return\n\t */\nprivate static Map<JedisPool, List<String>> getPoolKeyMap(List<String> keys) {\n\tMap<JedisPool, List<String>> poolKeysMap = new LinkedHashMap<JedisPool, List<String>>();\n\ttry {\n\t\tfor (String key : keys) {\n\n\t\t\tint slot = JedisClusterCRC16.getSlot(key);\n\n\t\t\t//获取到对应的Jedis对象，此处+1解决临界问题\n\t\t\tMap.Entry<Long, String> entry = slotHostMap.lowerEntry(Long.valueOf(slot+1));\n\n\t\t\tJedisPool jedisPool = nodeMap.get(entry.getValue());\n\n\t\t\tif (poolKeysMap.containsKey(jedisPool)) {\n\t\t\t\tpoolKeysMap.get(jedisPool).add(key);\n\t\t\t} else {\n\t\t\t\tList<String> subKeyList = new ArrayList<String>();\n\t\t\t\tsubKeyList.add(key);\n\t\t\t\tpoolKeysMap.put(jedisPool, subKeyList);\n\t\t\t}\n\t\t}\n\t} catch (Exception e) {\n\t\te.getMessage();\n\t}\n\treturn poolKeysMap;\n}\n\n/**\n * slort对应node\n * @param anyHostAndPortStr\n * @return\n */\nprivate static TreeMap<Long, String> getSlotHostMap(String anyHostAndPortStr) {\n\tTreeMap<Long, String> tree = new TreeMap<Long, String>();\n\tString parts[] = anyHostAndPortStr.split(\":\");\n\tHostAndPort anyHostAndPort = new HostAndPort(parts[0], Integer.parseInt(parts[1]));\n\ttry {\n\t\tJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());\n\t\tList<Object> list = jedisNode.clusterSlots();\n\t\tfor (Object object : list) {\n\t\t\tList<Object> list1 = (List<Object>) object;\n\t\t\tList<Object> master = (List<Object>) list1.get(2);\n\t\t\tString hostAndPort = new String((byte[]) master.get(0)) + \":\" + master.get(1);\n\t\t\ttree.put((Long) list1.get(0), hostAndPort);\n\t\t\ttree.put((Long) list1.get(1), hostAndPort);\n\t\t}\n\t\tjedisNode.close();\n\t} catch (Exception e) {\n\n\t}\n\treturn tree;\n}\n\n/**\n * 批量获取set类型数据\n * @param keys\n * @return\n */\npublic static Map<String, Set<String>> batchGetSetData(List<String> keys) {\n\tif (keys == null || keys.isEmpty()) {\n\t\treturn null;\n\t}\n\tMap<JedisPool, List<String>> poolKeysMap = getPoolKeyMap(keys);\n\tMap<String, Set<String>> resultMap = new HashMap<String, Set<String>>();\n\tfor (Map.Entry<JedisPool, List<String>> entry : poolKeysMap.entrySet()) {\n\t\tJedisPool jedisPool = entry.getKey();\n\t\tList<String> subkeys = entry.getValue();\n\t\tif (subkeys == null || subkeys.isEmpty()) {\n\t\t\tcontinue;\n\t\t}\n\t\t//申请jedis对象\n\t\tJedis jedis = null;\n\t\tPipeline pipeline = null;\n\t\tList<Object> subResultList = null;\n\t\ttry {\n\t\t\tjedis = jedisPool.getResource();\n\t\t\tpipeline = jedis.pipelined();\n\n\t\t\tfor (String key : subkeys) {\n\t\t\t\tpipeline.smembers(key);\n\t\t\t}\n\n\t\t\tsubResultList = pipeline.syncAndReturnAll();\n\t\t} catch (JedisConnectionException e) {\n\t\t\te.getMessage();\n\t\t} catch (Exception e) {\n\t\t\te.getMessage();\n\t\t} finally {\n\t\t\tif (pipeline != null)\n\t\t\t\ttry {\n\t\t\t\t\tpipeline.close();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\t// TODO Auto-generated catch block\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\t\t\t//释放jedis对象\n\t\t\tif (jedis != null) {\n\t\t\t\tjedis.close();\n\t\t\t}\n\t\t}\n\t\tif (subResultList == null || subResultList.isEmpty()) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (subResultList.size() == subkeys.size()) {\n\t\t\tfor (int i = 0; i < subkeys.size(); i++) {\n\t\t\t\tString key = subkeys.get(i);\n\t\t\t\tObject result = subResultList.get(i);\n\t\t\t\tresultMap.put(key, (Set<String>) result);\n\t\t\t}\n\t\t} else {\n\t\t\tSystem.out.println(\"redis cluster pipeline error!\");\n\t\t}\n\t}\n\treturn resultMap;\n}\n```\n## 参考\n[1] cachecloud client中实现","slug":"Redis-Cluster-批量操作实现","published":1,"updated":"2016-06-05T02:34:15.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbx003j8ceecg8n0b3q","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>之前发过一篇<a href=\"http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/\"><strong>RedisCluster构建批量操作探讨</strong></a>，这篇文章针对其中第2种方案，做一次实现。在使用redis cluster 过程中，经常会需要用到批量操作，本次探讨因此产生。<br><a id=\"more\"></a></p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>原理：将key分批计算所在node,然后在单个node上执行pipeline,即可完成批量操作</p>\n<h3 id=\"key-槽计算\"><a href=\"#key-槽计算\" class=\"headerlink\" title=\"key 槽计算\"></a>key 槽计算</h3><p>因为redis cluster 设计原理，在处理key时，会将 CRC16(key) mod 16384 散列的分布在集群的node 上，因此，我们需要计算该key所在slot.(利用jedis中自带的算法)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int slot = JedisClusterCRC16.getSlot(key);</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"node及slot获取\"><a href=\"#node及slot获取\" class=\"headerlink\" title=\"node及slot获取\"></a>node及slot获取</h3><p>同样基于jedis(2.8.1),核心用法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Jedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());</span><br><span class=\"line\">List&lt;Object&gt; list = jedisNode.clusterSlots();</span><br></pre></td></tr></table></figure></p>\n<p>该方法获取到整个集群中slot在node节点的分布。例如：0,5000，node.<br>通过以上方法整合，即可实现批量操作。</p>\n<h3 id=\"整合案例\"><a href=\"#整合案例\" class=\"headerlink\" title=\"整合案例\"></a>整合案例</h3><p>以下是我自己写的一个set类型读取的批量操作的实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">private static Map&lt;String, JedisPool&gt; nodeMap = jedis.getClusterNodes();</span><br><span class=\"line\"></span><br><span class=\"line\">private static TreeMap&lt;Long, String&gt; slotHostMap = getSlotHostMap(redisConf[0]);</span><br><span class=\"line\">/**</span><br><span class=\"line\">\t * 将key按slort分批整理</span><br><span class=\"line\">\t * @param keys</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">private static Map&lt;JedisPool, List&lt;String&gt;&gt; getPoolKeyMap(List&lt;String&gt; keys) &#123;</span><br><span class=\"line\">\tMap&lt;JedisPool, List&lt;String&gt;&gt; poolKeysMap = new LinkedHashMap&lt;JedisPool, List&lt;String&gt;&gt;();</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tfor (String key : keys) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tint slot = JedisClusterCRC16.getSlot(key);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t//获取到对应的Jedis对象，此处+1解决临界问题</span><br><span class=\"line\">\t\t\tMap.Entry&lt;Long, String&gt; entry = slotHostMap.lowerEntry(Long.valueOf(slot+1));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tJedisPool jedisPool = nodeMap.get(entry.getValue());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tif (poolKeysMap.containsKey(jedisPool)) &#123;</span><br><span class=\"line\">\t\t\t\tpoolKeysMap.get(jedisPool).add(key);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\tList&lt;String&gt; subKeyList = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">\t\t\t\tsubKeyList.add(key);</span><br><span class=\"line\">\t\t\t\tpoolKeysMap.put(jedisPool, subKeyList);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\te.getMessage();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn poolKeysMap;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * slort对应node</span><br><span class=\"line\"> * @param anyHostAndPortStr</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">private static TreeMap&lt;Long, String&gt; getSlotHostMap(String anyHostAndPortStr) &#123;</span><br><span class=\"line\">\tTreeMap&lt;Long, String&gt; tree = new TreeMap&lt;Long, String&gt;();</span><br><span class=\"line\">\tString parts[] = anyHostAndPortStr.split(&quot;:&quot;);</span><br><span class=\"line\">\tHostAndPort anyHostAndPort = new HostAndPort(parts[0], Integer.parseInt(parts[1]));</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());</span><br><span class=\"line\">\t\tList&lt;Object&gt; list = jedisNode.clusterSlots();</span><br><span class=\"line\">\t\tfor (Object object : list) &#123;</span><br><span class=\"line\">\t\t\tList&lt;Object&gt; list1 = (List&lt;Object&gt;) object;</span><br><span class=\"line\">\t\t\tList&lt;Object&gt; master = (List&lt;Object&gt;) list1.get(2);</span><br><span class=\"line\">\t\t\tString hostAndPort = new String((byte[]) master.get(0)) + &quot;:&quot; + master.get(1);</span><br><span class=\"line\">\t\t\ttree.put((Long) list1.get(0), hostAndPort);</span><br><span class=\"line\">\t\t\ttree.put((Long) list1.get(1), hostAndPort);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tjedisNode.close();</span><br><span class=\"line\">\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn tree;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 批量获取set类型数据</span><br><span class=\"line\"> * @param keys</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static Map&lt;String, Set&lt;String&gt;&gt; batchGetSetData(List&lt;String&gt; keys) &#123;</span><br><span class=\"line\">\tif (keys == null || keys.isEmpty()) &#123;</span><br><span class=\"line\">\t\treturn null;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tMap&lt;JedisPool, List&lt;String&gt;&gt; poolKeysMap = getPoolKeyMap(keys);</span><br><span class=\"line\">\tMap&lt;String, Set&lt;String&gt;&gt; resultMap = new HashMap&lt;String, Set&lt;String&gt;&gt;();</span><br><span class=\"line\">\tfor (Map.Entry&lt;JedisPool, List&lt;String&gt;&gt; entry : poolKeysMap.entrySet()) &#123;</span><br><span class=\"line\">\t\tJedisPool jedisPool = entry.getKey();</span><br><span class=\"line\">\t\tList&lt;String&gt; subkeys = entry.getValue();</span><br><span class=\"line\">\t\tif (subkeys == null || subkeys.isEmpty()) &#123;</span><br><span class=\"line\">\t\t\tcontinue;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t//申请jedis对象</span><br><span class=\"line\">\t\tJedis jedis = null;</span><br><span class=\"line\">\t\tPipeline pipeline = null;</span><br><span class=\"line\">\t\tList&lt;Object&gt; subResultList = null;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tjedis = jedisPool.getResource();</span><br><span class=\"line\">\t\t\tpipeline = jedis.pipelined();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tfor (String key : subkeys) &#123;</span><br><span class=\"line\">\t\t\t\tpipeline.smembers(key);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsubResultList = pipeline.syncAndReturnAll();</span><br><span class=\"line\">\t\t&#125; catch (JedisConnectionException e) &#123;</span><br><span class=\"line\">\t\t\te.getMessage();</span><br><span class=\"line\">\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\te.getMessage();</span><br><span class=\"line\">\t\t&#125; finally &#123;</span><br><span class=\"line\">\t\t\tif (pipeline != null)</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tpipeline.close();</span><br><span class=\"line\">\t\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t//释放jedis对象</span><br><span class=\"line\">\t\t\tif (jedis != null) &#123;</span><br><span class=\"line\">\t\t\t\tjedis.close();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (subResultList == null || subResultList.isEmpty()) &#123;</span><br><span class=\"line\">\t\t\tcontinue;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (subResultList.size() == subkeys.size()) &#123;</span><br><span class=\"line\">\t\t\tfor (int i = 0; i &lt; subkeys.size(); i++) &#123;</span><br><span class=\"line\">\t\t\t\tString key = subkeys.get(i);</span><br><span class=\"line\">\t\t\t\tObject result = subResultList.get(i);</span><br><span class=\"line\">\t\t\t\tresultMap.put(key, (Set&lt;String&gt;) result);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(&quot;redis cluster pipeline error!&quot;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn resultMap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>[1] cachecloud client中实现</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>之前发过一篇<a href=\"http://trumandu.github.io/2016/05/09/RedisCluster%E6%9E%84%E5%BB%BA%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C%E6%8E%A2%E8%AE%A8/\"><strong>RedisCluster构建批量操作探讨</strong></a>，这篇文章针对其中第2种方案，做一次实现。在使用redis cluster 过程中，经常会需要用到批量操作，本次探讨因此产生。<br>","more":"</p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>原理：将key分批计算所在node,然后在单个node上执行pipeline,即可完成批量操作</p>\n<h3 id=\"key-槽计算\"><a href=\"#key-槽计算\" class=\"headerlink\" title=\"key 槽计算\"></a>key 槽计算</h3><p>因为redis cluster 设计原理，在处理key时，会将 CRC16(key) mod 16384 散列的分布在集群的node 上，因此，我们需要计算该key所在slot.(利用jedis中自带的算法)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int slot = JedisClusterCRC16.getSlot(key);</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"node及slot获取\"><a href=\"#node及slot获取\" class=\"headerlink\" title=\"node及slot获取\"></a>node及slot获取</h3><p>同样基于jedis(2.8.1),核心用法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Jedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());</span><br><span class=\"line\">List&lt;Object&gt; list = jedisNode.clusterSlots();</span><br></pre></td></tr></table></figure></p>\n<p>该方法获取到整个集群中slot在node节点的分布。例如：0,5000，node.<br>通过以上方法整合，即可实现批量操作。</p>\n<h3 id=\"整合案例\"><a href=\"#整合案例\" class=\"headerlink\" title=\"整合案例\"></a>整合案例</h3><p>以下是我自己写的一个set类型读取的批量操作的实现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">private static Map&lt;String, JedisPool&gt; nodeMap = jedis.getClusterNodes();</span><br><span class=\"line\"></span><br><span class=\"line\">private static TreeMap&lt;Long, String&gt; slotHostMap = getSlotHostMap(redisConf[0]);</span><br><span class=\"line\">/**</span><br><span class=\"line\">\t * 将key按slort分批整理</span><br><span class=\"line\">\t * @param keys</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">private static Map&lt;JedisPool, List&lt;String&gt;&gt; getPoolKeyMap(List&lt;String&gt; keys) &#123;</span><br><span class=\"line\">\tMap&lt;JedisPool, List&lt;String&gt;&gt; poolKeysMap = new LinkedHashMap&lt;JedisPool, List&lt;String&gt;&gt;();</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tfor (String key : keys) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tint slot = JedisClusterCRC16.getSlot(key);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t//获取到对应的Jedis对象，此处+1解决临界问题</span><br><span class=\"line\">\t\t\tMap.Entry&lt;Long, String&gt; entry = slotHostMap.lowerEntry(Long.valueOf(slot+1));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tJedisPool jedisPool = nodeMap.get(entry.getValue());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tif (poolKeysMap.containsKey(jedisPool)) &#123;</span><br><span class=\"line\">\t\t\t\tpoolKeysMap.get(jedisPool).add(key);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\tList&lt;String&gt; subKeyList = new ArrayList&lt;String&gt;();</span><br><span class=\"line\">\t\t\t\tsubKeyList.add(key);</span><br><span class=\"line\">\t\t\t\tpoolKeysMap.put(jedisPool, subKeyList);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\te.getMessage();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn poolKeysMap;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * slort对应node</span><br><span class=\"line\"> * @param anyHostAndPortStr</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">private static TreeMap&lt;Long, String&gt; getSlotHostMap(String anyHostAndPortStr) &#123;</span><br><span class=\"line\">\tTreeMap&lt;Long, String&gt; tree = new TreeMap&lt;Long, String&gt;();</span><br><span class=\"line\">\tString parts[] = anyHostAndPortStr.split(&quot;:&quot;);</span><br><span class=\"line\">\tHostAndPort anyHostAndPort = new HostAndPort(parts[0], Integer.parseInt(parts[1]));</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tJedis jedisNode = new Jedis(anyHostAndPort.getHost(), anyHostAndPort.getPort());</span><br><span class=\"line\">\t\tList&lt;Object&gt; list = jedisNode.clusterSlots();</span><br><span class=\"line\">\t\tfor (Object object : list) &#123;</span><br><span class=\"line\">\t\t\tList&lt;Object&gt; list1 = (List&lt;Object&gt;) object;</span><br><span class=\"line\">\t\t\tList&lt;Object&gt; master = (List&lt;Object&gt;) list1.get(2);</span><br><span class=\"line\">\t\t\tString hostAndPort = new String((byte[]) master.get(0)) + &quot;:&quot; + master.get(1);</span><br><span class=\"line\">\t\t\ttree.put((Long) list1.get(0), hostAndPort);</span><br><span class=\"line\">\t\t\ttree.put((Long) list1.get(1), hostAndPort);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tjedisNode.close();</span><br><span class=\"line\">\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn tree;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 批量获取set类型数据</span><br><span class=\"line\"> * @param keys</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static Map&lt;String, Set&lt;String&gt;&gt; batchGetSetData(List&lt;String&gt; keys) &#123;</span><br><span class=\"line\">\tif (keys == null || keys.isEmpty()) &#123;</span><br><span class=\"line\">\t\treturn null;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tMap&lt;JedisPool, List&lt;String&gt;&gt; poolKeysMap = getPoolKeyMap(keys);</span><br><span class=\"line\">\tMap&lt;String, Set&lt;String&gt;&gt; resultMap = new HashMap&lt;String, Set&lt;String&gt;&gt;();</span><br><span class=\"line\">\tfor (Map.Entry&lt;JedisPool, List&lt;String&gt;&gt; entry : poolKeysMap.entrySet()) &#123;</span><br><span class=\"line\">\t\tJedisPool jedisPool = entry.getKey();</span><br><span class=\"line\">\t\tList&lt;String&gt; subkeys = entry.getValue();</span><br><span class=\"line\">\t\tif (subkeys == null || subkeys.isEmpty()) &#123;</span><br><span class=\"line\">\t\t\tcontinue;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t//申请jedis对象</span><br><span class=\"line\">\t\tJedis jedis = null;</span><br><span class=\"line\">\t\tPipeline pipeline = null;</span><br><span class=\"line\">\t\tList&lt;Object&gt; subResultList = null;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tjedis = jedisPool.getResource();</span><br><span class=\"line\">\t\t\tpipeline = jedis.pipelined();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tfor (String key : subkeys) &#123;</span><br><span class=\"line\">\t\t\t\tpipeline.smembers(key);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tsubResultList = pipeline.syncAndReturnAll();</span><br><span class=\"line\">\t\t&#125; catch (JedisConnectionException e) &#123;</span><br><span class=\"line\">\t\t\te.getMessage();</span><br><span class=\"line\">\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\te.getMessage();</span><br><span class=\"line\">\t\t&#125; finally &#123;</span><br><span class=\"line\">\t\t\tif (pipeline != null)</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tpipeline.close();</span><br><span class=\"line\">\t\t\t\t&#125; catch (IOException e) &#123;</span><br><span class=\"line\">\t\t\t\t\t// TODO Auto-generated catch block</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t//释放jedis对象</span><br><span class=\"line\">\t\t\tif (jedis != null) &#123;</span><br><span class=\"line\">\t\t\t\tjedis.close();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (subResultList == null || subResultList.isEmpty()) &#123;</span><br><span class=\"line\">\t\t\tcontinue;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif (subResultList.size() == subkeys.size()) &#123;</span><br><span class=\"line\">\t\t\tfor (int i = 0; i &lt; subkeys.size(); i++) &#123;</span><br><span class=\"line\">\t\t\t\tString key = subkeys.get(i);</span><br><span class=\"line\">\t\t\t\tObject result = subResultList.get(i);</span><br><span class=\"line\">\t\t\t\tresultMap.put(key, (Set&lt;String&gt;) result);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\tSystem.out.println(&quot;redis cluster pipeline error!&quot;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn resultMap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>[1] cachecloud client中实现</p>"},{"title":"Redis 运维shell 工具","date":"2017-07-19T14:17:15.000Z","_content":"# Redis 运维shell 工具\n\n## 介绍\n\n在Redis 集群运维过程中，经常需要做一些重复性工作，因为Redis 无中心设计，这就需要在每个节点中执行相同命令，对于这些重复性劳动工作完全可以通过shell处理，降低运维难度，减少工作量，以下是我在工作冲总结的脚本，仅供大家参考。\n\n## 工具集\n\n### 1. 集群健康状态检测\n\n新建脚本**redis_tool.sh**,然后执行\n\n```\nsh redis_tool.sh clusterStatus 127.0.0.1 7000\n```\n\n### 2. 移除fail节点\n\n新建脚本**redis_tool.sh**,然后执行\n\n```\nsh redis_tool.sh forget 127.0.0.1 7000\n```\n\n\n\n### 脚本内容\n\n```\n#!/bin/sh\n\nfunction clusterStatus(){\ninstans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk '{print $2}'`\nfor var in  ${instans[@]};\ndo\n         echo $var\n         host=${var%:*}\n         port=${var#*:}\n\tredis-cli -c -h $host -p $port cluster info |grep cluster_state:fail\ndone\n}\n\nfunction forget(){\ninstans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk '{print $2}'`\nfor var in  ${instans[@]};\ndo\n         echo $var\n         host=${var%:*}\n         port=${var#*:}\n          failnodes=`redis-cli -c -h $host -p $port cluster nodes|grep  fail|awk '{print $1}'`\n         for nodeid in  ${failnodes[@]};\n         do  \n             echo $nodeid\n             redis-cli -c -h $host -p $port cluster forget $nodeid\n         done\n\ndone\n}\n\n# main start\ncase $1 in\n  clusterStatus)\n    echo \"check redis cluster cluster_state...\"\n    clusterStatus $2 $3;\n  ;;\n   forget)\n    echo \"forget redis fail nodes ...\"\n    forget $2 $3;\n  ;;\n  *)\n    echo \"Usage: the options [clusterStatus|forget]\"\n  ;;\nesac\n```\n","source":"_posts/Redis-运维shell-工具.md","raw":"---\ntitle: Redis 运维shell 工具\ndate: 2017-07-19 22:17:15\ntags: nosql\ncategories:\n- redis\n---\n# Redis 运维shell 工具\n\n## 介绍\n\n在Redis 集群运维过程中，经常需要做一些重复性工作，因为Redis 无中心设计，这就需要在每个节点中执行相同命令，对于这些重复性劳动工作完全可以通过shell处理，降低运维难度，减少工作量，以下是我在工作冲总结的脚本，仅供大家参考。\n\n## 工具集\n\n### 1. 集群健康状态检测\n\n新建脚本**redis_tool.sh**,然后执行\n\n```\nsh redis_tool.sh clusterStatus 127.0.0.1 7000\n```\n\n### 2. 移除fail节点\n\n新建脚本**redis_tool.sh**,然后执行\n\n```\nsh redis_tool.sh forget 127.0.0.1 7000\n```\n\n\n\n### 脚本内容\n\n```\n#!/bin/sh\n\nfunction clusterStatus(){\ninstans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk '{print $2}'`\nfor var in  ${instans[@]};\ndo\n         echo $var\n         host=${var%:*}\n         port=${var#*:}\n\tredis-cli -c -h $host -p $port cluster info |grep cluster_state:fail\ndone\n}\n\nfunction forget(){\ninstans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk '{print $2}'`\nfor var in  ${instans[@]};\ndo\n         echo $var\n         host=${var%:*}\n         port=${var#*:}\n          failnodes=`redis-cli -c -h $host -p $port cluster nodes|grep  fail|awk '{print $1}'`\n         for nodeid in  ${failnodes[@]};\n         do  \n             echo $nodeid\n             redis-cli -c -h $host -p $port cluster forget $nodeid\n         done\n\ndone\n}\n\n# main start\ncase $1 in\n  clusterStatus)\n    echo \"check redis cluster cluster_state...\"\n    clusterStatus $2 $3;\n  ;;\n   forget)\n    echo \"forget redis fail nodes ...\"\n    forget $2 $3;\n  ;;\n  *)\n    echo \"Usage: the options [clusterStatus|forget]\"\n  ;;\nesac\n```\n","slug":"Redis-运维shell-工具","published":1,"updated":"2017-07-19T14:29:16.998Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvbz003m8ceerwail3vn","content":"<h1 id=\"Redis-运维shell-工具\"><a href=\"#Redis-运维shell-工具\" class=\"headerlink\" title=\"Redis 运维shell 工具\"></a>Redis 运维shell 工具</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>在Redis 集群运维过程中，经常需要做一些重复性工作，因为Redis 无中心设计，这就需要在每个节点中执行相同命令，对于这些重复性劳动工作完全可以通过shell处理，降低运维难度，减少工作量，以下是我在工作冲总结的脚本，仅供大家参考。</p>\n<h2 id=\"工具集\"><a href=\"#工具集\" class=\"headerlink\" title=\"工具集\"></a>工具集</h2><h3 id=\"1-集群健康状态检测\"><a href=\"#1-集群健康状态检测\" class=\"headerlink\" title=\"1. 集群健康状态检测\"></a>1. 集群健康状态检测</h3><p>新建脚本<strong>redis_tool.sh</strong>,然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh redis_tool.sh clusterStatus 127.0.0.1 7000</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-移除fail节点\"><a href=\"#2-移除fail节点\" class=\"headerlink\" title=\"2. 移除fail节点\"></a>2. 移除fail节点</h3><p>新建脚本<strong>redis_tool.sh</strong>,然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh redis_tool.sh forget 127.0.0.1 7000</span><br></pre></td></tr></table></figure>\n<h3 id=\"脚本内容\"><a href=\"#脚本内容\" class=\"headerlink\" title=\"脚本内容\"></a>脚本内容</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">function clusterStatus()&#123;</span><br><span class=\"line\">instans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for var in  $&#123;instans[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">         echo $var</span><br><span class=\"line\">         host=$&#123;var%:*&#125;</span><br><span class=\"line\">         port=$&#123;var#*:&#125;</span><br><span class=\"line\">\tredis-cli -c -h $host -p $port cluster info |grep cluster_state:fail</span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">function forget()&#123;</span><br><span class=\"line\">instans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for var in  $&#123;instans[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">         echo $var</span><br><span class=\"line\">         host=$&#123;var%:*&#125;</span><br><span class=\"line\">         port=$&#123;var#*:&#125;</span><br><span class=\"line\">          failnodes=`redis-cli -c -h $host -p $port cluster nodes|grep  fail|awk &apos;&#123;print $1&#125;&apos;`</span><br><span class=\"line\">         for nodeid in  $&#123;failnodes[@]&#125;;</span><br><span class=\"line\">         do  </span><br><span class=\"line\">             echo $nodeid</span><br><span class=\"line\">             redis-cli -c -h $host -p $port cluster forget $nodeid</span><br><span class=\"line\">         done</span><br><span class=\"line\"></span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># main start</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">  clusterStatus)</span><br><span class=\"line\">    echo &quot;check redis cluster cluster_state...&quot;</span><br><span class=\"line\">    clusterStatus $2 $3;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">   forget)</span><br><span class=\"line\">    echo &quot;forget redis fail nodes ...&quot;</span><br><span class=\"line\">    forget $2 $3;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: the options [clusterStatus|forget]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Redis-运维shell-工具\"><a href=\"#Redis-运维shell-工具\" class=\"headerlink\" title=\"Redis 运维shell 工具\"></a>Redis 运维shell 工具</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>在Redis 集群运维过程中，经常需要做一些重复性工作，因为Redis 无中心设计，这就需要在每个节点中执行相同命令，对于这些重复性劳动工作完全可以通过shell处理，降低运维难度，减少工作量，以下是我在工作冲总结的脚本，仅供大家参考。</p>\n<h2 id=\"工具集\"><a href=\"#工具集\" class=\"headerlink\" title=\"工具集\"></a>工具集</h2><h3 id=\"1-集群健康状态检测\"><a href=\"#1-集群健康状态检测\" class=\"headerlink\" title=\"1. 集群健康状态检测\"></a>1. 集群健康状态检测</h3><p>新建脚本<strong>redis_tool.sh</strong>,然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh redis_tool.sh clusterStatus 127.0.0.1 7000</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-移除fail节点\"><a href=\"#2-移除fail节点\" class=\"headerlink\" title=\"2. 移除fail节点\"></a>2. 移除fail节点</h3><p>新建脚本<strong>redis_tool.sh</strong>,然后执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh redis_tool.sh forget 127.0.0.1 7000</span><br></pre></td></tr></table></figure>\n<h3 id=\"脚本内容\"><a href=\"#脚本内容\" class=\"headerlink\" title=\"脚本内容\"></a>脚本内容</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\"></span><br><span class=\"line\">function clusterStatus()&#123;</span><br><span class=\"line\">instans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for var in  $&#123;instans[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">         echo $var</span><br><span class=\"line\">         host=$&#123;var%:*&#125;</span><br><span class=\"line\">         port=$&#123;var#*:&#125;</span><br><span class=\"line\">\tredis-cli -c -h $host -p $port cluster info |grep cluster_state:fail</span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">function forget()&#123;</span><br><span class=\"line\">instans=`redis-cli -c -h $1 -p $2 cluster nodes|grep -v fail|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class=\"line\">for var in  $&#123;instans[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">         echo $var</span><br><span class=\"line\">         host=$&#123;var%:*&#125;</span><br><span class=\"line\">         port=$&#123;var#*:&#125;</span><br><span class=\"line\">          failnodes=`redis-cli -c -h $host -p $port cluster nodes|grep  fail|awk &apos;&#123;print $1&#125;&apos;`</span><br><span class=\"line\">         for nodeid in  $&#123;failnodes[@]&#125;;</span><br><span class=\"line\">         do  </span><br><span class=\"line\">             echo $nodeid</span><br><span class=\"line\">             redis-cli -c -h $host -p $port cluster forget $nodeid</span><br><span class=\"line\">         done</span><br><span class=\"line\"></span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># main start</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">  clusterStatus)</span><br><span class=\"line\">    echo &quot;check redis cluster cluster_state...&quot;</span><br><span class=\"line\">    clusterStatus $2 $3;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">   forget)</span><br><span class=\"line\">    echo &quot;forget redis fail nodes ...&quot;</span><br><span class=\"line\">    forget $2 $3;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: the options [clusterStatus|forget]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n"},{"title":"RedisCluster 集群配置文件损坏修复","date":"2017-06-01T13:15:33.000Z","_content":"# RedisCluster 集群配置文件损坏修复\n\n## 问题现象\n\n启动集群后，log文件显示如下信息：\n```\nUnrecoverable error: corrupted cluster config file.\n\n```\n这种情况，因为集群cluster-config-file文件损坏引起，导致该节点无法启动\n\n## 修复方案\n\n1. 首先在各个node上移除该出错节点\n2. 删除该cluster-config-file文件\n3. 重新启动该节点\n4. 将该节点加入集群\n5. 指定该节点的master,将该节点以slave加入集群(本次修复未执行，集群自动恢复正常)\n\n\n\n##  实施步骤\n\n**一、首先在各个node上移除该出错节点**\n\n执行以下脚本，将fail节点移除出集群,该操作仅移除一个机器中的无用节点信息，如果是多个机器，请在host_array中添加多个机器IP\n```\n#!/bin/bash\nhost_array=(192.168.0.101)\n  for var in  ${host_array[@]};\n  do\n          for i in $(seq 7000 7012)\n          do\n             nodeids=`src/redis-cli -c -h $var -p $i cluster nodes|grep fail|awk '{print $1}'`\n              for d in $nodeids\n              do\n                    echo $d\n                    src/redis-cli -c -h $var -p $i cluster forget $d\n              done\n          done\n  done\n```\n\n**二、删除该cluster-config-file文件**\n```\n mv nodes-7004.conf nodes-7004.conf.bak\n```\n\n**三、重新启动该节点**\n\n```\nredis-server redis.conf\n```\n\n\n**四、将该节点加入集群**\n\n在集群任意instance 执行以下命令\n```\nCLUSTER MEET <ip> <port>   //将ip和port所指定的节点添加到集群当中，让它成为集群的一份子\n```\n\n\n**五、指定该节点的master,将该节点以slave加入集群**\n\n**在此次修复场景中未进行第五步，集群自动分配成为不均衡节点的slave**\n\n如果集群未自动分配，在需要加入集群的实例上执行，指定为某一个master的slave\n```\nCLUSTER REPLICATE <node_id> //将当前节点设置为 node_id 指定的节点的从节点。\n```\n","source":"_posts/RedisCluster-集群配置文件损坏修复.md","raw":"---\ntitle: RedisCluster 集群配置文件损坏修复\ndate: 2017-06-01 21:15:33\ntags: 大数据\ncategories:\n- redis\n---\n# RedisCluster 集群配置文件损坏修复\n\n## 问题现象\n\n启动集群后，log文件显示如下信息：\n```\nUnrecoverable error: corrupted cluster config file.\n\n```\n这种情况，因为集群cluster-config-file文件损坏引起，导致该节点无法启动\n\n## 修复方案\n\n1. 首先在各个node上移除该出错节点\n2. 删除该cluster-config-file文件\n3. 重新启动该节点\n4. 将该节点加入集群\n5. 指定该节点的master,将该节点以slave加入集群(本次修复未执行，集群自动恢复正常)\n\n\n\n##  实施步骤\n\n**一、首先在各个node上移除该出错节点**\n\n执行以下脚本，将fail节点移除出集群,该操作仅移除一个机器中的无用节点信息，如果是多个机器，请在host_array中添加多个机器IP\n```\n#!/bin/bash\nhost_array=(192.168.0.101)\n  for var in  ${host_array[@]};\n  do\n          for i in $(seq 7000 7012)\n          do\n             nodeids=`src/redis-cli -c -h $var -p $i cluster nodes|grep fail|awk '{print $1}'`\n              for d in $nodeids\n              do\n                    echo $d\n                    src/redis-cli -c -h $var -p $i cluster forget $d\n              done\n          done\n  done\n```\n\n**二、删除该cluster-config-file文件**\n```\n mv nodes-7004.conf nodes-7004.conf.bak\n```\n\n**三、重新启动该节点**\n\n```\nredis-server redis.conf\n```\n\n\n**四、将该节点加入集群**\n\n在集群任意instance 执行以下命令\n```\nCLUSTER MEET <ip> <port>   //将ip和port所指定的节点添加到集群当中，让它成为集群的一份子\n```\n\n\n**五、指定该节点的master,将该节点以slave加入集群**\n\n**在此次修复场景中未进行第五步，集群自动分配成为不均衡节点的slave**\n\n如果集群未自动分配，在需要加入集群的实例上执行，指定为某一个master的slave\n```\nCLUSTER REPLICATE <node_id> //将当前节点设置为 node_id 指定的节点的从节点。\n```\n","slug":"RedisCluster-集群配置文件损坏修复","published":1,"updated":"2017-06-01T13:19:12.526Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvc2003p8ceemcvk3i8p","content":"<h1 id=\"RedisCluster-集群配置文件损坏修复\"><a href=\"#RedisCluster-集群配置文件损坏修复\" class=\"headerlink\" title=\"RedisCluster 集群配置文件损坏修复\"></a>RedisCluster 集群配置文件损坏修复</h1><h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>启动集群后，log文件显示如下信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Unrecoverable error: corrupted cluster config file.</span><br></pre></td></tr></table></figure></p>\n<p>这种情况，因为集群cluster-config-file文件损坏引起，导致该节点无法启动</p>\n<h2 id=\"修复方案\"><a href=\"#修复方案\" class=\"headerlink\" title=\"修复方案\"></a>修复方案</h2><ol>\n<li>首先在各个node上移除该出错节点</li>\n<li>删除该cluster-config-file文件</li>\n<li>重新启动该节点</li>\n<li>将该节点加入集群</li>\n<li>指定该节点的master,将该节点以slave加入集群(本次修复未执行，集群自动恢复正常)</li>\n</ol>\n<h2 id=\"实施步骤\"><a href=\"#实施步骤\" class=\"headerlink\" title=\"实施步骤\"></a>实施步骤</h2><p><strong>一、首先在各个node上移除该出错节点</strong></p>\n<p>执行以下脚本，将fail节点移除出集群,该操作仅移除一个机器中的无用节点信息，如果是多个机器，请在host_array中添加多个机器IP<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">host_array=(192.168.0.101)</span><br><span class=\"line\">  for var in  $&#123;host_array[@]&#125;;</span><br><span class=\"line\">  do</span><br><span class=\"line\">          for i in $(seq 7000 7012)</span><br><span class=\"line\">          do</span><br><span class=\"line\">             nodeids=`src/redis-cli -c -h $var -p $i cluster nodes|grep fail|awk &apos;&#123;print $1&#125;&apos;`</span><br><span class=\"line\">              for d in $nodeids</span><br><span class=\"line\">              do</span><br><span class=\"line\">                    echo $d</span><br><span class=\"line\">                    src/redis-cli -c -h $var -p $i cluster forget $d</span><br><span class=\"line\">              done</span><br><span class=\"line\">          done</span><br><span class=\"line\">  done</span><br></pre></td></tr></table></figure></p>\n<p><strong>二、删除该cluster-config-file文件</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv nodes-7004.conf nodes-7004.conf.bak</span><br></pre></td></tr></table></figure></p>\n<p><strong>三、重新启动该节点</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-server redis.conf</span><br></pre></td></tr></table></figure>\n<p><strong>四、将该节点加入集群</strong></p>\n<p>在集群任意instance 执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CLUSTER MEET &lt;ip&gt; &lt;port&gt;   //将ip和port所指定的节点添加到集群当中，让它成为集群的一份子</span><br></pre></td></tr></table></figure></p>\n<p><strong>五、指定该节点的master,将该节点以slave加入集群</strong></p>\n<p><strong>在此次修复场景中未进行第五步，集群自动分配成为不均衡节点的slave</strong></p>\n<p>如果集群未自动分配，在需要加入集群的实例上执行，指定为某一个master的slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CLUSTER REPLICATE &lt;node_id&gt; //将当前节点设置为 node_id 指定的节点的从节点。</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"RedisCluster-集群配置文件损坏修复\"><a href=\"#RedisCluster-集群配置文件损坏修复\" class=\"headerlink\" title=\"RedisCluster 集群配置文件损坏修复\"></a>RedisCluster 集群配置文件损坏修复</h1><h2 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h2><p>启动集群后，log文件显示如下信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Unrecoverable error: corrupted cluster config file.</span><br></pre></td></tr></table></figure></p>\n<p>这种情况，因为集群cluster-config-file文件损坏引起，导致该节点无法启动</p>\n<h2 id=\"修复方案\"><a href=\"#修复方案\" class=\"headerlink\" title=\"修复方案\"></a>修复方案</h2><ol>\n<li>首先在各个node上移除该出错节点</li>\n<li>删除该cluster-config-file文件</li>\n<li>重新启动该节点</li>\n<li>将该节点加入集群</li>\n<li>指定该节点的master,将该节点以slave加入集群(本次修复未执行，集群自动恢复正常)</li>\n</ol>\n<h2 id=\"实施步骤\"><a href=\"#实施步骤\" class=\"headerlink\" title=\"实施步骤\"></a>实施步骤</h2><p><strong>一、首先在各个node上移除该出错节点</strong></p>\n<p>执行以下脚本，将fail节点移除出集群,该操作仅移除一个机器中的无用节点信息，如果是多个机器，请在host_array中添加多个机器IP<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">host_array=(192.168.0.101)</span><br><span class=\"line\">  for var in  $&#123;host_array[@]&#125;;</span><br><span class=\"line\">  do</span><br><span class=\"line\">          for i in $(seq 7000 7012)</span><br><span class=\"line\">          do</span><br><span class=\"line\">             nodeids=`src/redis-cli -c -h $var -p $i cluster nodes|grep fail|awk &apos;&#123;print $1&#125;&apos;`</span><br><span class=\"line\">              for d in $nodeids</span><br><span class=\"line\">              do</span><br><span class=\"line\">                    echo $d</span><br><span class=\"line\">                    src/redis-cli -c -h $var -p $i cluster forget $d</span><br><span class=\"line\">              done</span><br><span class=\"line\">          done</span><br><span class=\"line\">  done</span><br></pre></td></tr></table></figure></p>\n<p><strong>二、删除该cluster-config-file文件</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mv nodes-7004.conf nodes-7004.conf.bak</span><br></pre></td></tr></table></figure></p>\n<p><strong>三、重新启动该节点</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis-server redis.conf</span><br></pre></td></tr></table></figure>\n<p><strong>四、将该节点加入集群</strong></p>\n<p>在集群任意instance 执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CLUSTER MEET &lt;ip&gt; &lt;port&gt;   //将ip和port所指定的节点添加到集群当中，让它成为集群的一份子</span><br></pre></td></tr></table></figure></p>\n<p><strong>五、指定该节点的master,将该节点以slave加入集群</strong></p>\n<p><strong>在此次修复场景中未进行第五步，集群自动分配成为不均衡节点的slave</strong></p>\n<p>如果集群未自动分配，在需要加入集群的实例上执行，指定为某一个master的slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CLUSTER REPLICATE &lt;node_id&gt; //将当前节点设置为 node_id 指定的节点的从节点。</span><br></pre></td></tr></table></figure></p>\n"},{"layout":"n","title":"RedisCluster搭建","date":"2016-07-18T13:15:05.000Z","_content":"# RedisCluster搭建\n## 前言\n现在越来越多的公司开始使用redis cluster,这就要求大家能够学会快速搭建redis cluster集群，在官网上也是有很详细的入门指导，本文章只是自己搭建后总结，主要使用两种方式搭建：1. 官网上使用的redis-trib.rb脚本构建集群；2. 使用shel 命令搭建。\n\n## 准备\n\n因为两种方式都需要提前启动好相应的redis实例，因为将公共步骤放在准备工作中。这一步比较简单，主要是按端口新建不同文件夹，然后复制redis.conf,修改其中port对应的端口。集群高可用需要至少三个node,本次采用3 master、3slave架构。步骤如下：\n1.\n```\nmkdir 9000\ncp redis.conf 9000/\n```\n2.\n修改复制后的redis.conf：\ncluster-enabled yes\nport 9000\n\n3.\n```\nsrc/redis-server 9000/redis.conf\n```\n然后重复以上操作 9001-9005\n## rb脚本创建\n准备操作中已经建立了6个独立的node,接下来执行以下脚本：\n```\n/src/redis-trib.rb create --replicas 1 127.0.0.1:9000 127.0.0.1:9001 127.0.0.1:9002 127.0.0.1:9003 127.0.0.1:9004 127.0.0.1:9005\n```\n在执行脚本后，确认自动计算的分配方案，即可搭建好redis cluster集群\n## shell创建\n1.集群meet\n```\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9001\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9002\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9003\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9004\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9005\n```\n2.划分master,slave\n```\nsrc/redis-cli -c -h 127.0.0.1 -p 9001 cluster nodes\n14fca7a90ad3f680c9466c018cb890b5a8717c15 127.0.0.1:9005 master - 0 1468651448427 0 connected\na3fb3e01351afad2aa67affaaef5e2f689c3a007 127.0.0.1:9001 myself,master - 0 0 5 connected\nfcb1557f223b6a7e34d1653e3e255f60eedf8d1f 127.0.0.1:9002 master - 0 1468651450831 0 connected\n0966d47c01d6725206fa04c0d673072a87cbc76a 127.0.0.1:9003 master - 0 1468651449829 2 connected\n5ba760249e5dfc478f5066a98922e0f3268075fd 127.0.0.1:9000 master - 0 1468651448827 1 connected\nc0688867266abd0e08781a1acff4d12bf140b017 127.0.0.1:9004 master - 0 1468651451871 3 connected\n```\n根绝ID指定为哪个node的slave\n```\nsrc/redis-cli -c -h 127.0.0.1 -p 9003 cluster replicate 5ba760249e5dfc478f5066a98922e0f3268075fd\nsrc/redis-cli -c -h 127.0.0.1 -p 9004 cluster replicate a3fb3e01351afad2aa67affaaef5e2f689c3a007\nsrc/redis-cli -c -h 127.0.0.1 -p 9005 cluster replicate fcb1557f223b6a7e34d1653e3e255f60eedf8d1f\n```\n3.分配slot\n因为cluster addslots命令不支持区间分配slot，因此只能借助shell实现分配slot\nredis.sh如下：\n```\n#/bin/sh\nfunction addslots()\n{\nfor i in `seq  $2 $3`;do\n src/redis-cli  -p $1 cluster addslots $i  \ndone\n}\ncase $1 in\n    addslots)\n    echo \"add slots ...\"\n    addslots $2 $3 $4;\n  ;;\n  *)\n    echo \"Usage: inetpanel [addslots]\"\n  ;;\nesac\n\n```\n执行shell\n```\n./redis.sh addslots 9000 0 5460\n./redis.sh addslots 9001 5461 10921\n./redis.sh addslots 9002 10922 16383\n```\n至此redis cluster 搭建完毕！\n","source":"_posts/RedisCluster搭建.md","raw":"layout: 'n'\ntitle: RedisCluster搭建\ndate: 2016-07-18 21:15:05\ntags: nosql\ncategories:\n- redis\n---\n# RedisCluster搭建\n## 前言\n现在越来越多的公司开始使用redis cluster,这就要求大家能够学会快速搭建redis cluster集群，在官网上也是有很详细的入门指导，本文章只是自己搭建后总结，主要使用两种方式搭建：1. 官网上使用的redis-trib.rb脚本构建集群；2. 使用shel 命令搭建。\n\n## 准备\n\n因为两种方式都需要提前启动好相应的redis实例，因为将公共步骤放在准备工作中。这一步比较简单，主要是按端口新建不同文件夹，然后复制redis.conf,修改其中port对应的端口。集群高可用需要至少三个node,本次采用3 master、3slave架构。步骤如下：\n1.\n```\nmkdir 9000\ncp redis.conf 9000/\n```\n2.\n修改复制后的redis.conf：\ncluster-enabled yes\nport 9000\n\n3.\n```\nsrc/redis-server 9000/redis.conf\n```\n然后重复以上操作 9001-9005\n## rb脚本创建\n准备操作中已经建立了6个独立的node,接下来执行以下脚本：\n```\n/src/redis-trib.rb create --replicas 1 127.0.0.1:9000 127.0.0.1:9001 127.0.0.1:9002 127.0.0.1:9003 127.0.0.1:9004 127.0.0.1:9005\n```\n在执行脚本后，确认自动计算的分配方案，即可搭建好redis cluster集群\n## shell创建\n1.集群meet\n```\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9001\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9002\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9003\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9004\nsrc/redis-cli -p 9000 cluster meet 127.0.0.1 9005\n```\n2.划分master,slave\n```\nsrc/redis-cli -c -h 127.0.0.1 -p 9001 cluster nodes\n14fca7a90ad3f680c9466c018cb890b5a8717c15 127.0.0.1:9005 master - 0 1468651448427 0 connected\na3fb3e01351afad2aa67affaaef5e2f689c3a007 127.0.0.1:9001 myself,master - 0 0 5 connected\nfcb1557f223b6a7e34d1653e3e255f60eedf8d1f 127.0.0.1:9002 master - 0 1468651450831 0 connected\n0966d47c01d6725206fa04c0d673072a87cbc76a 127.0.0.1:9003 master - 0 1468651449829 2 connected\n5ba760249e5dfc478f5066a98922e0f3268075fd 127.0.0.1:9000 master - 0 1468651448827 1 connected\nc0688867266abd0e08781a1acff4d12bf140b017 127.0.0.1:9004 master - 0 1468651451871 3 connected\n```\n根绝ID指定为哪个node的slave\n```\nsrc/redis-cli -c -h 127.0.0.1 -p 9003 cluster replicate 5ba760249e5dfc478f5066a98922e0f3268075fd\nsrc/redis-cli -c -h 127.0.0.1 -p 9004 cluster replicate a3fb3e01351afad2aa67affaaef5e2f689c3a007\nsrc/redis-cli -c -h 127.0.0.1 -p 9005 cluster replicate fcb1557f223b6a7e34d1653e3e255f60eedf8d1f\n```\n3.分配slot\n因为cluster addslots命令不支持区间分配slot，因此只能借助shell实现分配slot\nredis.sh如下：\n```\n#/bin/sh\nfunction addslots()\n{\nfor i in `seq  $2 $3`;do\n src/redis-cli  -p $1 cluster addslots $i  \ndone\n}\ncase $1 in\n    addslots)\n    echo \"add slots ...\"\n    addslots $2 $3 $4;\n  ;;\n  *)\n    echo \"Usage: inetpanel [addslots]\"\n  ;;\nesac\n\n```\n执行shell\n```\n./redis.sh addslots 9000 0 5460\n./redis.sh addslots 9001 5461 10921\n./redis.sh addslots 9002 10922 16383\n```\n至此redis cluster 搭建完毕！\n","slug":"RedisCluster搭建","published":1,"updated":"2016-07-18T13:23:29.181Z","comments":1,"photos":[],"link":"","_id":"cjzmobvc4003s8ceez5ggzfog","content":"<h1 id=\"RedisCluster搭建\"><a href=\"#RedisCluster搭建\" class=\"headerlink\" title=\"RedisCluster搭建\"></a>RedisCluster搭建</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>现在越来越多的公司开始使用redis cluster,这就要求大家能够学会快速搭建redis cluster集群，在官网上也是有很详细的入门指导，本文章只是自己搭建后总结，主要使用两种方式搭建：1. 官网上使用的redis-trib.rb脚本构建集群；2. 使用shel 命令搭建。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>因为两种方式都需要提前启动好相应的redis实例，因为将公共步骤放在准备工作中。这一步比较简单，主要是按端口新建不同文件夹，然后复制redis.conf,修改其中port对应的端口。集群高可用需要至少三个node,本次采用3 master、3slave架构。步骤如下：<br>1.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir 9000</span><br><span class=\"line\">cp redis.conf 9000/</span><br></pre></td></tr></table></figure></p>\n<p>2.<br>修改复制后的redis.conf：<br>cluster-enabled yes<br>port 9000</p>\n<p>3.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-server 9000/redis.conf</span><br></pre></td></tr></table></figure></p>\n<p>然后重复以上操作 9001-9005</p>\n<h2 id=\"rb脚本创建\"><a href=\"#rb脚本创建\" class=\"headerlink\" title=\"rb脚本创建\"></a>rb脚本创建</h2><p>准备操作中已经建立了6个独立的node,接下来执行以下脚本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/src/redis-trib.rb create --replicas 1 127.0.0.1:9000 127.0.0.1:9001 127.0.0.1:9002 127.0.0.1:9003 127.0.0.1:9004 127.0.0.1:9005</span><br></pre></td></tr></table></figure></p>\n<p>在执行脚本后，确认自动计算的分配方案，即可搭建好redis cluster集群</p>\n<h2 id=\"shell创建\"><a href=\"#shell创建\" class=\"headerlink\" title=\"shell创建\"></a>shell创建</h2><p>1.集群meet<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9001</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9002</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9003</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9004</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9005</span><br></pre></td></tr></table></figure></p>\n<p>2.划分master,slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9001 cluster nodes</span><br><span class=\"line\">14fca7a90ad3f680c9466c018cb890b5a8717c15 127.0.0.1:9005 master - 0 1468651448427 0 connected</span><br><span class=\"line\">a3fb3e01351afad2aa67affaaef5e2f689c3a007 127.0.0.1:9001 myself,master - 0 0 5 connected</span><br><span class=\"line\">fcb1557f223b6a7e34d1653e3e255f60eedf8d1f 127.0.0.1:9002 master - 0 1468651450831 0 connected</span><br><span class=\"line\">0966d47c01d6725206fa04c0d673072a87cbc76a 127.0.0.1:9003 master - 0 1468651449829 2 connected</span><br><span class=\"line\">5ba760249e5dfc478f5066a98922e0f3268075fd 127.0.0.1:9000 master - 0 1468651448827 1 connected</span><br><span class=\"line\">c0688867266abd0e08781a1acff4d12bf140b017 127.0.0.1:9004 master - 0 1468651451871 3 connected</span><br></pre></td></tr></table></figure></p>\n<p>根绝ID指定为哪个node的slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9003 cluster replicate 5ba760249e5dfc478f5066a98922e0f3268075fd</span><br><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9004 cluster replicate a3fb3e01351afad2aa67affaaef5e2f689c3a007</span><br><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9005 cluster replicate fcb1557f223b6a7e34d1653e3e255f60eedf8d1f</span><br></pre></td></tr></table></figure></p>\n<p>3.分配slot<br>因为cluster addslots命令不支持区间分配slot，因此只能借助shell实现分配slot<br>redis.sh如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#/bin/sh</span><br><span class=\"line\">function addslots()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">for i in `seq  $2 $3`;do</span><br><span class=\"line\"> src/redis-cli  -p $1 cluster addslots $i  </span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">    addslots)</span><br><span class=\"line\">    echo &quot;add slots ...&quot;</span><br><span class=\"line\">    addslots $2 $3 $4;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: inetpanel [addslots]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure></p>\n<p>执行shell<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./redis.sh addslots 9000 0 5460</span><br><span class=\"line\">./redis.sh addslots 9001 5461 10921</span><br><span class=\"line\">./redis.sh addslots 9002 10922 16383</span><br></pre></td></tr></table></figure></p>\n<p>至此redis cluster 搭建完毕！</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"RedisCluster搭建\"><a href=\"#RedisCluster搭建\" class=\"headerlink\" title=\"RedisCluster搭建\"></a>RedisCluster搭建</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>现在越来越多的公司开始使用redis cluster,这就要求大家能够学会快速搭建redis cluster集群，在官网上也是有很详细的入门指导，本文章只是自己搭建后总结，主要使用两种方式搭建：1. 官网上使用的redis-trib.rb脚本构建集群；2. 使用shel 命令搭建。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>因为两种方式都需要提前启动好相应的redis实例，因为将公共步骤放在准备工作中。这一步比较简单，主要是按端口新建不同文件夹，然后复制redis.conf,修改其中port对应的端口。集群高可用需要至少三个node,本次采用3 master、3slave架构。步骤如下：<br>1.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir 9000</span><br><span class=\"line\">cp redis.conf 9000/</span><br></pre></td></tr></table></figure></p>\n<p>2.<br>修改复制后的redis.conf：<br>cluster-enabled yes<br>port 9000</p>\n<p>3.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-server 9000/redis.conf</span><br></pre></td></tr></table></figure></p>\n<p>然后重复以上操作 9001-9005</p>\n<h2 id=\"rb脚本创建\"><a href=\"#rb脚本创建\" class=\"headerlink\" title=\"rb脚本创建\"></a>rb脚本创建</h2><p>准备操作中已经建立了6个独立的node,接下来执行以下脚本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/src/redis-trib.rb create --replicas 1 127.0.0.1:9000 127.0.0.1:9001 127.0.0.1:9002 127.0.0.1:9003 127.0.0.1:9004 127.0.0.1:9005</span><br></pre></td></tr></table></figure></p>\n<p>在执行脚本后，确认自动计算的分配方案，即可搭建好redis cluster集群</p>\n<h2 id=\"shell创建\"><a href=\"#shell创建\" class=\"headerlink\" title=\"shell创建\"></a>shell创建</h2><p>1.集群meet<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9001</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9002</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9003</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9004</span><br><span class=\"line\">src/redis-cli -p 9000 cluster meet 127.0.0.1 9005</span><br></pre></td></tr></table></figure></p>\n<p>2.划分master,slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9001 cluster nodes</span><br><span class=\"line\">14fca7a90ad3f680c9466c018cb890b5a8717c15 127.0.0.1:9005 master - 0 1468651448427 0 connected</span><br><span class=\"line\">a3fb3e01351afad2aa67affaaef5e2f689c3a007 127.0.0.1:9001 myself,master - 0 0 5 connected</span><br><span class=\"line\">fcb1557f223b6a7e34d1653e3e255f60eedf8d1f 127.0.0.1:9002 master - 0 1468651450831 0 connected</span><br><span class=\"line\">0966d47c01d6725206fa04c0d673072a87cbc76a 127.0.0.1:9003 master - 0 1468651449829 2 connected</span><br><span class=\"line\">5ba760249e5dfc478f5066a98922e0f3268075fd 127.0.0.1:9000 master - 0 1468651448827 1 connected</span><br><span class=\"line\">c0688867266abd0e08781a1acff4d12bf140b017 127.0.0.1:9004 master - 0 1468651451871 3 connected</span><br></pre></td></tr></table></figure></p>\n<p>根绝ID指定为哪个node的slave<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9003 cluster replicate 5ba760249e5dfc478f5066a98922e0f3268075fd</span><br><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9004 cluster replicate a3fb3e01351afad2aa67affaaef5e2f689c3a007</span><br><span class=\"line\">src/redis-cli -c -h 127.0.0.1 -p 9005 cluster replicate fcb1557f223b6a7e34d1653e3e255f60eedf8d1f</span><br></pre></td></tr></table></figure></p>\n<p>3.分配slot<br>因为cluster addslots命令不支持区间分配slot，因此只能借助shell实现分配slot<br>redis.sh如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#/bin/sh</span><br><span class=\"line\">function addslots()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">for i in `seq  $2 $3`;do</span><br><span class=\"line\"> src/redis-cli  -p $1 cluster addslots $i  </span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">    addslots)</span><br><span class=\"line\">    echo &quot;add slots ...&quot;</span><br><span class=\"line\">    addslots $2 $3 $4;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: inetpanel [addslots]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure></p>\n<p>执行shell<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./redis.sh addslots 9000 0 5460</span><br><span class=\"line\">./redis.sh addslots 9001 5461 10921</span><br><span class=\"line\">./redis.sh addslots 9002 10922 16383</span><br></pre></td></tr></table></figure></p>\n<p>至此redis cluster 搭建完毕！</p>\n"},{"title":"Redis维护总结","date":"2016-06-01T13:58:44.000Z","_content":"#### 内存淘汰\n---\n获取内存淘汰配置策略\n```\n127.0.0.1:6379>config get maxmemory-policy\n```\n- volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。\n- volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。\n- volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。\n- allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。\n- allkeys-random：从数据集合中任意选择数据淘汰\n- no-enviction：禁止淘汰数据。\n\n#### 延时追踪及慢查询\n---\nRedis的延迟数据是无法从info信息中获取的。倘若想要查看延迟时间，可以用\tRedis-cli工具加--latency参数运行，如:\n\n```\nRedis-cli --latency -h 127.0.0.1 -p 6379\n```\n\n结果为查询ID、发生时间、运行时长和原命令 默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为微秒，注意这个时间是不包含网络延迟的。\n```\n127.0.0.1:6379>slowlog get\n```\n查看aof是否影响延迟,在info Stats中查看，数字代表有多少次fork延迟操作\n```\nlatest_fork_usec:1724##单位微妙\n\n```\n查看最近一次fork延迟耗时\n```\naof_delayed_fsync:0###被延迟的 fsync 调用数量\n```\n#### 主从切换\n在运维过程中，整体断电后，集群重启后，master与slave节点分布可能会出现变化，为了保证集群master分布均匀，可使用命令主动切换master.\n在需要提升为master的节点执行以下命令\n```\n127.0.0.1:6379>cluster failover\n```\n#### 指定slave数量\n可在redis.conf中配置\n```\ncluster-migration-barrier 1\n```\n该参数指定slave最小数量为1，当前节点无法满足的话，会从别的节点漂移一个作为master的slave\n","source":"_posts/Redis维护总结.md","raw":"---\ntitle: Redis维护总结\ndate: 2016-06-01 21:58:44\ntags: 大数据\ncategories:\n- redis\n---\n#### 内存淘汰\n---\n获取内存淘汰配置策略\n```\n127.0.0.1:6379>config get maxmemory-policy\n```\n- volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。\n- volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。\n- volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。\n- allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。\n- allkeys-random：从数据集合中任意选择数据淘汰\n- no-enviction：禁止淘汰数据。\n\n#### 延时追踪及慢查询\n---\nRedis的延迟数据是无法从info信息中获取的。倘若想要查看延迟时间，可以用\tRedis-cli工具加--latency参数运行，如:\n\n```\nRedis-cli --latency -h 127.0.0.1 -p 6379\n```\n\n结果为查询ID、发生时间、运行时长和原命令 默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为微秒，注意这个时间是不包含网络延迟的。\n```\n127.0.0.1:6379>slowlog get\n```\n查看aof是否影响延迟,在info Stats中查看，数字代表有多少次fork延迟操作\n```\nlatest_fork_usec:1724##单位微妙\n\n```\n查看最近一次fork延迟耗时\n```\naof_delayed_fsync:0###被延迟的 fsync 调用数量\n```\n#### 主从切换\n在运维过程中，整体断电后，集群重启后，master与slave节点分布可能会出现变化，为了保证集群master分布均匀，可使用命令主动切换master.\n在需要提升为master的节点执行以下命令\n```\n127.0.0.1:6379>cluster failover\n```\n#### 指定slave数量\n可在redis.conf中配置\n```\ncluster-migration-barrier 1\n```\n该参数指定slave最小数量为1，当前节点无法满足的话，会从别的节点漂移一个作为master的slave\n","slug":"Redis维护总结","published":1,"updated":"2016-08-21T12:48:07.395Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvc6003w8ceeknjr3mca","content":"<h4 id=\"内存淘汰\"><a href=\"#内存淘汰\" class=\"headerlink\" title=\"内存淘汰\"></a>内存淘汰</h4><hr>\n<p>获取内存淘汰配置策略<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;config get maxmemory-policy</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。</li>\n<li>volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。</li>\n<li>volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。</li>\n<li>allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。</li>\n<li>allkeys-random：从数据集合中任意选择数据淘汰</li>\n<li>no-enviction：禁止淘汰数据。</li>\n</ul>\n<h4 id=\"延时追踪及慢查询\"><a href=\"#延时追踪及慢查询\" class=\"headerlink\" title=\"延时追踪及慢查询\"></a>延时追踪及慢查询</h4><hr>\n<p>Redis的延迟数据是无法从info信息中获取的。倘若想要查看延迟时间，可以用    Redis-cli工具加–latency参数运行，如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Redis-cli --latency -h 127.0.0.1 -p 6379</span><br></pre></td></tr></table></figure>\n<p>结果为查询ID、发生时间、运行时长和原命令 默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为微秒，注意这个时间是不包含网络延迟的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;slowlog get</span><br></pre></td></tr></table></figure></p>\n<p>查看aof是否影响延迟,在info Stats中查看，数字代表有多少次fork延迟操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">latest_fork_usec:1724##单位微妙</span><br></pre></td></tr></table></figure></p>\n<p>查看最近一次fork延迟耗时<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">aof_delayed_fsync:0###被延迟的 fsync 调用数量</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"主从切换\"><a href=\"#主从切换\" class=\"headerlink\" title=\"主从切换\"></a>主从切换</h4><p>在运维过程中，整体断电后，集群重启后，master与slave节点分布可能会出现变化，为了保证集群master分布均匀，可使用命令主动切换master.<br>在需要提升为master的节点执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;cluster failover</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"指定slave数量\"><a href=\"#指定slave数量\" class=\"headerlink\" title=\"指定slave数量\"></a>指定slave数量</h4><p>可在redis.conf中配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster-migration-barrier 1</span><br></pre></td></tr></table></figure></p>\n<p>该参数指定slave最小数量为1，当前节点无法满足的话，会从别的节点漂移一个作为master的slave</p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"内存淘汰\"><a href=\"#内存淘汰\" class=\"headerlink\" title=\"内存淘汰\"></a>内存淘汰</h4><hr>\n<p>获取内存淘汰配置策略<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;config get maxmemory-policy</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。</li>\n<li>volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。</li>\n<li>volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。</li>\n<li>allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。</li>\n<li>allkeys-random：从数据集合中任意选择数据淘汰</li>\n<li>no-enviction：禁止淘汰数据。</li>\n</ul>\n<h4 id=\"延时追踪及慢查询\"><a href=\"#延时追踪及慢查询\" class=\"headerlink\" title=\"延时追踪及慢查询\"></a>延时追踪及慢查询</h4><hr>\n<p>Redis的延迟数据是无法从info信息中获取的。倘若想要查看延迟时间，可以用    Redis-cli工具加–latency参数运行，如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Redis-cli --latency -h 127.0.0.1 -p 6379</span><br></pre></td></tr></table></figure>\n<p>结果为查询ID、发生时间、运行时长和原命令 默认10毫秒，默认只保留最后的128条。单线程的模型下，一个请求占掉10毫秒是件大事情，注意设置和显示的单位为微秒，注意这个时间是不包含网络延迟的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;slowlog get</span><br></pre></td></tr></table></figure></p>\n<p>查看aof是否影响延迟,在info Stats中查看，数字代表有多少次fork延迟操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">latest_fork_usec:1724##单位微妙</span><br></pre></td></tr></table></figure></p>\n<p>查看最近一次fork延迟耗时<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">aof_delayed_fsync:0###被延迟的 fsync 调用数量</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"主从切换\"><a href=\"#主从切换\" class=\"headerlink\" title=\"主从切换\"></a>主从切换</h4><p>在运维过程中，整体断电后，集群重启后，master与slave节点分布可能会出现变化，为了保证集群master分布均匀，可使用命令主动切换master.<br>在需要提升为master的节点执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:6379&gt;cluster failover</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"指定slave数量\"><a href=\"#指定slave数量\" class=\"headerlink\" title=\"指定slave数量\"></a>指定slave数量</h4><p>可在redis.conf中配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cluster-migration-barrier 1</span><br></pre></td></tr></table></figure></p>\n<p>该参数指定slave最小数量为1，当前节点无法满足的话，会从别的节点漂移一个作为master的slave</p>\n"},{"title":"RedisCluster构建批量操作探讨","date":"2016-05-09T01:50:03.000Z","_content":"# RedisCluster构建批量操作探讨\n## 前言 ##\n众所周知，jedis仅支持redis standalone mset,mget等批量操作，在最新的redis cluster中是不支持的，这个和redis cluster的设计有关，将不同的实例划分不同的槽。不同的key会落到不同的槽上，所在的实例也就不同，这就对jedis的批量操作提出问题，即无法同时支持多个实例上的批量操作。（理解可能不是很深入，希望有人看到可以指教一下）\n### 分布式储存产品存储方式\n在分布式存储产品中，哈希存储与顺序存储是两种重要的数据存储和分布方式，这两种方式不同也直接决定了批量获取数据的不同，所以这里需要对这两种数据的分布式方式进行简要说明：\n   1. hash分布：\n   hash分布应用于大部分key-value系统中，例如memcache, redis-cluster, twemproxy，即使像MySQL在分库分表时候，也经常会用user%100这样的方式。\n   hash分布的主要作用是将key均匀的分布到各个机器，所以它的一个特点就是数据分散度较高，实现方式通常是hash(key)得到的整数再和分布式节点的某台机器做映射，以redis-cluster为例子：\n    ![](http://dl2.iteye.com/upload/attachment/0113/6187/1b8483c0-502e-3a3b-93ca-4bcf12b01e89.jpg)\n   问题：和业务没什么关系，不支持范围查询。\n   2. 顺序分布\n ![](http://dl2.iteye.com/upload/attachment/0113/6193/def09084-469c-3513-9dab-8e789a2c20c2.jpg) \n   3. 两种分布方式的比较：\n   \n| 分布方式 | 特点 | 典型产品 |\n| -------- | ---- | -------- |\n| 哈希分布 | 1. 数据分散度高</br>2.键值分布与业务无关</br>3.无法顺序访问</br>4.支持批量操作 | 一致性哈希memcache </br>redisCluster其他缓存产品 |\n| 顺序分布 | 1.数据分散度易倾斜</br>2.键值分布与业务相关</br>3.可以顺序访问</br>4.支持批量操作 | BigTable </br>Hbase |\n\n## 分布式缓存/存储四种Mget解决方案\n### 1. IO的优化思路：\n  * 命令本身的效率：例如sql优化，命令优化\n  * 网络次数：减少通信次数\n  * 降低接入成本:长连/连接池,NIO等。\n  * IO访问合并:O(n)到O(1)过程:批量接口(mget),\n\n### 2. 如果只考虑减少网络次数的话，mget会有如下模型：\n![](http://dl2.iteye.com/upload/attachment/0113/6195/8dc22bf0-c55a-3a29-8285-4a11ef0c67be.jpg)\n\n### 3. 四种解决方案：\n#### (1).串行mget\n将Mget操作(n个key)拆分为逐次执行N次get操作, 很明显这种操作时间复杂度较高，它的操作时间=n次网络时间+n次命令时间，网络次数是n，很显然这种方案不是最优的，但是足够简单。\n ![](http://dl2.iteye.com/upload/attachment/0113/5649/734b8416-0dde-3ea5-bd85-6b19c058d8ee.png)\n#### (2). 串行IO\n将Mget操作(n个key)，利用已知的hash函数算出key对应的节点，这样就可以得到一个这样的关系：Map<node, somekeys>，也就是每个节点对应的一些keys 。它的操作时间=node次网络时间+n次命令时间，网络次数是node的个数，很明显这种方案比第一种要好很多，但是如果节点数足够多，还是有一定的性能问题。\n ![](http://dl2.iteye.com/upload/attachment/0113/5599/a6e24459-5bca-3c42-b555-97f3c7c2d4f7.png)\n#### (3). 并行IO\n此方案是将方案（2）中的最后一步，改为多线程执行，网络次数虽然还是nodes.size()，但网络时间变为o(1)，但是这种方案会增加编程的复杂度。它的操作时间=1次网络时间+n次命令时间\n ![](http://dl2.iteye.com/upload/attachment/0113/5653/668355e5-34f7-30a2-aee3-b4eb8b8dae68.png)\n#### (4).hash-tag实现\n第二节提到过，由于hash函数会造成key随机分配到各个节点，那么有没有一种方法能够强制一些key到指定节点到指定的节点呢? redis提供了这样的功能，叫做hash-tag。什么意思呢？假如我们现在使用的是redis-cluster（10个redis节点组成），我们现在有1000个k-v，那么按照hash函数(crc16)规则，这1000个key会被打散到10个节点上，那么时间复杂度还是上述(1)~(3)\n ![](http://dl2.iteye.com/upload/attachment/0113/5655/0cc9ab10-39ec-3114-a82a-68cb7d5075e4.png)\n那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。\n例如：\n ![](http://dl2.iteye.com/upload/attachment/0113/5657/8b42b6fb-91d0-367b-b72d-fd01f81c78d4.png)\n例如下图：它的操作时间=1次网络时间+n次命令时间\n![](http://dl2.iteye.com/upload/attachment/0113/5603/b17e3697-9d98-39ae-9500-a4365a3b2c69.png)\n### 4. 四种批量操作解决方案对比：\n|方案|优点|缺点|网络IO|\n|----|----|----|------|\n|串行mget|1.编程简单</br>2.少量keys，性能满足要求|大量keys请求延迟严重|o(keys)|\n|串行IO|1.编程简单</br>2.少量节点，性能满足要求|大量node延迟严重|o(nodes)|\n|并行IO|1.利用并行特性</br>2.延迟取决于最慢的节点|1.编程复杂</br>2.超时定位较难|o(max_slow(node))|\n|hash tags|性能最高|1.tag-key业务维护成本较高</br>2.tag分布容易出现数据倾斜|o(1)|\n\n\n## 总结 \n目前我们项目中采用第一种方式，使用多线程解决批量问题，减少带宽时延，提高效率，这种做法就如上面所说简单便捷（我们目前批量操作类型比较多），有效。但问题比较明显。批量操作数量不大即可满足。最近研究cachecloud发现搜狐他们采用第二点，先将key获取槽点，然后分node pipeline操作。这种做法相对比第一种做法较优。推荐第二种方法，后期对性能有要求的话，考虑修改成第二种方式。\n附加cachecloud-client代码中mget追踪路径\n类PipelineCluster======>mget======>类pipelineCommand=====>run=====>getPoolKeyMap====>getResult\n## 参考\n[1]http://carlosfu.iteye.com/blog/2263813\n[2]https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3\n\n","source":"_posts/RedisCluster构建批量操作探讨.md","raw":"---\ntitle: RedisCluster构建批量操作探讨\ndate: 2016-05-09 09:50:03\ntags: nosql\ncategories:\n- redis\n---\n# RedisCluster构建批量操作探讨\n## 前言 ##\n众所周知，jedis仅支持redis standalone mset,mget等批量操作，在最新的redis cluster中是不支持的，这个和redis cluster的设计有关，将不同的实例划分不同的槽。不同的key会落到不同的槽上，所在的实例也就不同，这就对jedis的批量操作提出问题，即无法同时支持多个实例上的批量操作。（理解可能不是很深入，希望有人看到可以指教一下）\n### 分布式储存产品存储方式\n在分布式存储产品中，哈希存储与顺序存储是两种重要的数据存储和分布方式，这两种方式不同也直接决定了批量获取数据的不同，所以这里需要对这两种数据的分布式方式进行简要说明：\n   1. hash分布：\n   hash分布应用于大部分key-value系统中，例如memcache, redis-cluster, twemproxy，即使像MySQL在分库分表时候，也经常会用user%100这样的方式。\n   hash分布的主要作用是将key均匀的分布到各个机器，所以它的一个特点就是数据分散度较高，实现方式通常是hash(key)得到的整数再和分布式节点的某台机器做映射，以redis-cluster为例子：\n    ![](http://dl2.iteye.com/upload/attachment/0113/6187/1b8483c0-502e-3a3b-93ca-4bcf12b01e89.jpg)\n   问题：和业务没什么关系，不支持范围查询。\n   2. 顺序分布\n ![](http://dl2.iteye.com/upload/attachment/0113/6193/def09084-469c-3513-9dab-8e789a2c20c2.jpg) \n   3. 两种分布方式的比较：\n   \n| 分布方式 | 特点 | 典型产品 |\n| -------- | ---- | -------- |\n| 哈希分布 | 1. 数据分散度高</br>2.键值分布与业务无关</br>3.无法顺序访问</br>4.支持批量操作 | 一致性哈希memcache </br>redisCluster其他缓存产品 |\n| 顺序分布 | 1.数据分散度易倾斜</br>2.键值分布与业务相关</br>3.可以顺序访问</br>4.支持批量操作 | BigTable </br>Hbase |\n\n## 分布式缓存/存储四种Mget解决方案\n### 1. IO的优化思路：\n  * 命令本身的效率：例如sql优化，命令优化\n  * 网络次数：减少通信次数\n  * 降低接入成本:长连/连接池,NIO等。\n  * IO访问合并:O(n)到O(1)过程:批量接口(mget),\n\n### 2. 如果只考虑减少网络次数的话，mget会有如下模型：\n![](http://dl2.iteye.com/upload/attachment/0113/6195/8dc22bf0-c55a-3a29-8285-4a11ef0c67be.jpg)\n\n### 3. 四种解决方案：\n#### (1).串行mget\n将Mget操作(n个key)拆分为逐次执行N次get操作, 很明显这种操作时间复杂度较高，它的操作时间=n次网络时间+n次命令时间，网络次数是n，很显然这种方案不是最优的，但是足够简单。\n ![](http://dl2.iteye.com/upload/attachment/0113/5649/734b8416-0dde-3ea5-bd85-6b19c058d8ee.png)\n#### (2). 串行IO\n将Mget操作(n个key)，利用已知的hash函数算出key对应的节点，这样就可以得到一个这样的关系：Map<node, somekeys>，也就是每个节点对应的一些keys 。它的操作时间=node次网络时间+n次命令时间，网络次数是node的个数，很明显这种方案比第一种要好很多，但是如果节点数足够多，还是有一定的性能问题。\n ![](http://dl2.iteye.com/upload/attachment/0113/5599/a6e24459-5bca-3c42-b555-97f3c7c2d4f7.png)\n#### (3). 并行IO\n此方案是将方案（2）中的最后一步，改为多线程执行，网络次数虽然还是nodes.size()，但网络时间变为o(1)，但是这种方案会增加编程的复杂度。它的操作时间=1次网络时间+n次命令时间\n ![](http://dl2.iteye.com/upload/attachment/0113/5653/668355e5-34f7-30a2-aee3-b4eb8b8dae68.png)\n#### (4).hash-tag实现\n第二节提到过，由于hash函数会造成key随机分配到各个节点，那么有没有一种方法能够强制一些key到指定节点到指定的节点呢? redis提供了这样的功能，叫做hash-tag。什么意思呢？假如我们现在使用的是redis-cluster（10个redis节点组成），我们现在有1000个k-v，那么按照hash函数(crc16)规则，这1000个key会被打散到10个节点上，那么时间复杂度还是上述(1)~(3)\n ![](http://dl2.iteye.com/upload/attachment/0113/5655/0cc9ab10-39ec-3114-a82a-68cb7d5075e4.png)\n那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。\n例如：\n ![](http://dl2.iteye.com/upload/attachment/0113/5657/8b42b6fb-91d0-367b-b72d-fd01f81c78d4.png)\n例如下图：它的操作时间=1次网络时间+n次命令时间\n![](http://dl2.iteye.com/upload/attachment/0113/5603/b17e3697-9d98-39ae-9500-a4365a3b2c69.png)\n### 4. 四种批量操作解决方案对比：\n|方案|优点|缺点|网络IO|\n|----|----|----|------|\n|串行mget|1.编程简单</br>2.少量keys，性能满足要求|大量keys请求延迟严重|o(keys)|\n|串行IO|1.编程简单</br>2.少量节点，性能满足要求|大量node延迟严重|o(nodes)|\n|并行IO|1.利用并行特性</br>2.延迟取决于最慢的节点|1.编程复杂</br>2.超时定位较难|o(max_slow(node))|\n|hash tags|性能最高|1.tag-key业务维护成本较高</br>2.tag分布容易出现数据倾斜|o(1)|\n\n\n## 总结 \n目前我们项目中采用第一种方式，使用多线程解决批量问题，减少带宽时延，提高效率，这种做法就如上面所说简单便捷（我们目前批量操作类型比较多），有效。但问题比较明显。批量操作数量不大即可满足。最近研究cachecloud发现搜狐他们采用第二点，先将key获取槽点，然后分node pipeline操作。这种做法相对比第一种做法较优。推荐第二种方法，后期对性能有要求的话，考虑修改成第二种方式。\n附加cachecloud-client代码中mget追踪路径\n类PipelineCluster======>mget======>类pipelineCommand=====>run=====>getPoolKeyMap====>getResult\n## 参考\n[1]http://carlosfu.iteye.com/blog/2263813\n[2]https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3\n\n","slug":"RedisCluster构建批量操作探讨","published":1,"updated":"2016-08-28T11:36:47.953Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvc9003z8ceejt4nu3mh","content":"<h1 id=\"RedisCluster构建批量操作探讨\"><a href=\"#RedisCluster构建批量操作探讨\" class=\"headerlink\" title=\"RedisCluster构建批量操作探讨\"></a>RedisCluster构建批量操作探讨</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>众所周知，jedis仅支持redis standalone mset,mget等批量操作，在最新的redis cluster中是不支持的，这个和redis cluster的设计有关，将不同的实例划分不同的槽。不同的key会落到不同的槽上，所在的实例也就不同，这就对jedis的批量操作提出问题，即无法同时支持多个实例上的批量操作。（理解可能不是很深入，希望有人看到可以指教一下）</p>\n<h3 id=\"分布式储存产品存储方式\"><a href=\"#分布式储存产品存储方式\" class=\"headerlink\" title=\"分布式储存产品存储方式\"></a>分布式储存产品存储方式</h3><p>在分布式存储产品中，哈希存储与顺序存储是两种重要的数据存储和分布方式，这两种方式不同也直接决定了批量获取数据的不同，所以这里需要对这两种数据的分布式方式进行简要说明：</p>\n<ol>\n<li>hash分布：<br>hash分布应用于大部分key-value系统中，例如memcache, redis-cluster, twemproxy，即使像MySQL在分库分表时候，也经常会用user%100这样的方式。<br>hash分布的主要作用是将key均匀的分布到各个机器，所以它的一个特点就是数据分散度较高，实现方式通常是hash(key)得到的整数再和分布式节点的某台机器做映射，以redis-cluster为例子：<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/6187/1b8483c0-502e-3a3b-93ca-4bcf12b01e89.jpg\" alt=\"\"><br>问题：和业务没什么关系，不支持范围查询。</li>\n<li>顺序分布<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/6193/def09084-469c-3513-9dab-8e789a2c20c2.jpg\" alt=\"\"> </li>\n<li>两种分布方式的比较：</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>分布方式</th>\n<th>特点</th>\n<th>典型产品</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>哈希分布</td>\n<td>1. 数据分散度高<br>2.键值分布与业务无关<br>3.无法顺序访问<br>4.支持批量操作</td>\n<td>一致性哈希memcache <br>redisCluster其他缓存产品</td>\n</tr>\n<tr>\n<td>顺序分布</td>\n<td>1.数据分散度易倾斜<br>2.键值分布与业务相关<br>3.可以顺序访问<br>4.支持批量操作</td>\n<td>BigTable <br>Hbase</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"分布式缓存-存储四种Mget解决方案\"><a href=\"#分布式缓存-存储四种Mget解决方案\" class=\"headerlink\" title=\"分布式缓存/存储四种Mget解决方案\"></a>分布式缓存/存储四种Mget解决方案</h2><h3 id=\"1-IO的优化思路：\"><a href=\"#1-IO的优化思路：\" class=\"headerlink\" title=\"1. IO的优化思路：\"></a>1. IO的优化思路：</h3><ul>\n<li>命令本身的效率：例如sql优化，命令优化</li>\n<li>网络次数：减少通信次数</li>\n<li>降低接入成本:长连/连接池,NIO等。</li>\n<li>IO访问合并:O(n)到O(1)过程:批量接口(mget),</li>\n</ul>\n<h3 id=\"2-如果只考虑减少网络次数的话，mget会有如下模型：\"><a href=\"#2-如果只考虑减少网络次数的话，mget会有如下模型：\" class=\"headerlink\" title=\"2. 如果只考虑减少网络次数的话，mget会有如下模型：\"></a>2. 如果只考虑减少网络次数的话，mget会有如下模型：</h3><p><img src=\"http://dl2.iteye.com/upload/attachment/0113/6195/8dc22bf0-c55a-3a29-8285-4a11ef0c67be.jpg\" alt=\"\"></p>\n<h3 id=\"3-四种解决方案：\"><a href=\"#3-四种解决方案：\" class=\"headerlink\" title=\"3. 四种解决方案：\"></a>3. 四种解决方案：</h3><h4 id=\"1-串行mget\"><a href=\"#1-串行mget\" class=\"headerlink\" title=\"(1).串行mget\"></a>(1).串行mget</h4><p>将Mget操作(n个key)拆分为逐次执行N次get操作, 很明显这种操作时间复杂度较高，它的操作时间=n次网络时间+n次命令时间，网络次数是n，很显然这种方案不是最优的，但是足够简单。<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5649/734b8416-0dde-3ea5-bd85-6b19c058d8ee.png\" alt=\"\"></p>\n<h4 id=\"2-串行IO\"><a href=\"#2-串行IO\" class=\"headerlink\" title=\"(2). 串行IO\"></a>(2). 串行IO</h4><p>将Mget操作(n个key)，利用已知的hash函数算出key对应的节点，这样就可以得到一个这样的关系：Map&lt;node, somekeys&gt;，也就是每个节点对应的一些keys 。它的操作时间=node次网络时间+n次命令时间，网络次数是node的个数，很明显这种方案比第一种要好很多，但是如果节点数足够多，还是有一定的性能问题。<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5599/a6e24459-5bca-3c42-b555-97f3c7c2d4f7.png\" alt=\"\"></p>\n<h4 id=\"3-并行IO\"><a href=\"#3-并行IO\" class=\"headerlink\" title=\"(3). 并行IO\"></a>(3). 并行IO</h4><p>此方案是将方案（2）中的最后一步，改为多线程执行，网络次数虽然还是nodes.size()，但网络时间变为o(1)，但是这种方案会增加编程的复杂度。它的操作时间=1次网络时间+n次命令时间<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5653/668355e5-34f7-30a2-aee3-b4eb8b8dae68.png\" alt=\"\"></p>\n<h4 id=\"4-hash-tag实现\"><a href=\"#4-hash-tag实现\" class=\"headerlink\" title=\"(4).hash-tag实现\"></a>(4).hash-tag实现</h4><p>第二节提到过，由于hash函数会造成key随机分配到各个节点，那么有没有一种方法能够强制一些key到指定节点到指定的节点呢? redis提供了这样的功能，叫做hash-tag。什么意思呢？假如我们现在使用的是redis-cluster（10个redis节点组成），我们现在有1000个k-v，那么按照hash函数(crc16)规则，这1000个key会被打散到10个节点上，那么时间复杂度还是上述(1)~(3)<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5655/0cc9ab10-39ec-3114-a82a-68cb7d5075e4.png\" alt=\"\"><br>那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。<br>例如：<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5657/8b42b6fb-91d0-367b-b72d-fd01f81c78d4.png\" alt=\"\"><br>例如下图：它的操作时间=1次网络时间+n次命令时间<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/5603/b17e3697-9d98-39ae-9500-a4365a3b2c69.png\" alt=\"\"></p>\n<h3 id=\"4-四种批量操作解决方案对比：\"><a href=\"#4-四种批量操作解决方案对比：\" class=\"headerlink\" title=\"4. 四种批量操作解决方案对比：\"></a>4. 四种批量操作解决方案对比：</h3><table>\n<thead>\n<tr>\n<th>方案</th>\n<th>优点</th>\n<th>缺点</th>\n<th>网络IO</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>串行mget</td>\n<td>1.编程简单<br>2.少量keys，性能满足要求</td>\n<td>大量keys请求延迟严重</td>\n<td>o(keys)</td>\n</tr>\n<tr>\n<td>串行IO</td>\n<td>1.编程简单<br>2.少量节点，性能满足要求</td>\n<td>大量node延迟严重</td>\n<td>o(nodes)</td>\n</tr>\n<tr>\n<td>并行IO</td>\n<td>1.利用并行特性<br>2.延迟取决于最慢的节点</td>\n<td>1.编程复杂<br>2.超时定位较难</td>\n<td>o(max_slow(node))</td>\n</tr>\n<tr>\n<td>hash tags</td>\n<td>性能最高</td>\n<td>1.tag-key业务维护成本较高<br>2.tag分布容易出现数据倾斜</td>\n<td>o(1)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>目前我们项目中采用第一种方式，使用多线程解决批量问题，减少带宽时延，提高效率，这种做法就如上面所说简单便捷（我们目前批量操作类型比较多），有效。但问题比较明显。批量操作数量不大即可满足。最近研究cachecloud发现搜狐他们采用第二点，先将key获取槽点，然后分node pipeline操作。这种做法相对比第一种做法较优。推荐第二种方法，后期对性能有要求的话，考虑修改成第二种方式。<br>附加cachecloud-client代码中mget追踪路径<br>类PipelineCluster======&gt;mget======&gt;类pipelineCommand=====&gt;run=====&gt;getPoolKeyMap====&gt;getResult</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>[1]<a href=\"http://carlosfu.iteye.com/blog/2263813\" target=\"_blank\" rel=\"noopener\">http://carlosfu.iteye.com/blog/2263813</a><br>[2]<a href=\"https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3\" target=\"_blank\" rel=\"noopener\">https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"RedisCluster构建批量操作探讨\"><a href=\"#RedisCluster构建批量操作探讨\" class=\"headerlink\" title=\"RedisCluster构建批量操作探讨\"></a>RedisCluster构建批量操作探讨</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>众所周知，jedis仅支持redis standalone mset,mget等批量操作，在最新的redis cluster中是不支持的，这个和redis cluster的设计有关，将不同的实例划分不同的槽。不同的key会落到不同的槽上，所在的实例也就不同，这就对jedis的批量操作提出问题，即无法同时支持多个实例上的批量操作。（理解可能不是很深入，希望有人看到可以指教一下）</p>\n<h3 id=\"分布式储存产品存储方式\"><a href=\"#分布式储存产品存储方式\" class=\"headerlink\" title=\"分布式储存产品存储方式\"></a>分布式储存产品存储方式</h3><p>在分布式存储产品中，哈希存储与顺序存储是两种重要的数据存储和分布方式，这两种方式不同也直接决定了批量获取数据的不同，所以这里需要对这两种数据的分布式方式进行简要说明：</p>\n<ol>\n<li>hash分布：<br>hash分布应用于大部分key-value系统中，例如memcache, redis-cluster, twemproxy，即使像MySQL在分库分表时候，也经常会用user%100这样的方式。<br>hash分布的主要作用是将key均匀的分布到各个机器，所以它的一个特点就是数据分散度较高，实现方式通常是hash(key)得到的整数再和分布式节点的某台机器做映射，以redis-cluster为例子：<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/6187/1b8483c0-502e-3a3b-93ca-4bcf12b01e89.jpg\" alt=\"\"><br>问题：和业务没什么关系，不支持范围查询。</li>\n<li>顺序分布<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/6193/def09084-469c-3513-9dab-8e789a2c20c2.jpg\" alt=\"\"> </li>\n<li>两种分布方式的比较：</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>分布方式</th>\n<th>特点</th>\n<th>典型产品</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>哈希分布</td>\n<td>1. 数据分散度高<br>2.键值分布与业务无关<br>3.无法顺序访问<br>4.支持批量操作</td>\n<td>一致性哈希memcache <br>redisCluster其他缓存产品</td>\n</tr>\n<tr>\n<td>顺序分布</td>\n<td>1.数据分散度易倾斜<br>2.键值分布与业务相关<br>3.可以顺序访问<br>4.支持批量操作</td>\n<td>BigTable <br>Hbase</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"分布式缓存-存储四种Mget解决方案\"><a href=\"#分布式缓存-存储四种Mget解决方案\" class=\"headerlink\" title=\"分布式缓存/存储四种Mget解决方案\"></a>分布式缓存/存储四种Mget解决方案</h2><h3 id=\"1-IO的优化思路：\"><a href=\"#1-IO的优化思路：\" class=\"headerlink\" title=\"1. IO的优化思路：\"></a>1. IO的优化思路：</h3><ul>\n<li>命令本身的效率：例如sql优化，命令优化</li>\n<li>网络次数：减少通信次数</li>\n<li>降低接入成本:长连/连接池,NIO等。</li>\n<li>IO访问合并:O(n)到O(1)过程:批量接口(mget),</li>\n</ul>\n<h3 id=\"2-如果只考虑减少网络次数的话，mget会有如下模型：\"><a href=\"#2-如果只考虑减少网络次数的话，mget会有如下模型：\" class=\"headerlink\" title=\"2. 如果只考虑减少网络次数的话，mget会有如下模型：\"></a>2. 如果只考虑减少网络次数的话，mget会有如下模型：</h3><p><img src=\"http://dl2.iteye.com/upload/attachment/0113/6195/8dc22bf0-c55a-3a29-8285-4a11ef0c67be.jpg\" alt=\"\"></p>\n<h3 id=\"3-四种解决方案：\"><a href=\"#3-四种解决方案：\" class=\"headerlink\" title=\"3. 四种解决方案：\"></a>3. 四种解决方案：</h3><h4 id=\"1-串行mget\"><a href=\"#1-串行mget\" class=\"headerlink\" title=\"(1).串行mget\"></a>(1).串行mget</h4><p>将Mget操作(n个key)拆分为逐次执行N次get操作, 很明显这种操作时间复杂度较高，它的操作时间=n次网络时间+n次命令时间，网络次数是n，很显然这种方案不是最优的，但是足够简单。<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5649/734b8416-0dde-3ea5-bd85-6b19c058d8ee.png\" alt=\"\"></p>\n<h4 id=\"2-串行IO\"><a href=\"#2-串行IO\" class=\"headerlink\" title=\"(2). 串行IO\"></a>(2). 串行IO</h4><p>将Mget操作(n个key)，利用已知的hash函数算出key对应的节点，这样就可以得到一个这样的关系：Map&lt;node, somekeys&gt;，也就是每个节点对应的一些keys 。它的操作时间=node次网络时间+n次命令时间，网络次数是node的个数，很明显这种方案比第一种要好很多，但是如果节点数足够多，还是有一定的性能问题。<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5599/a6e24459-5bca-3c42-b555-97f3c7c2d4f7.png\" alt=\"\"></p>\n<h4 id=\"3-并行IO\"><a href=\"#3-并行IO\" class=\"headerlink\" title=\"(3). 并行IO\"></a>(3). 并行IO</h4><p>此方案是将方案（2）中的最后一步，改为多线程执行，网络次数虽然还是nodes.size()，但网络时间变为o(1)，但是这种方案会增加编程的复杂度。它的操作时间=1次网络时间+n次命令时间<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5653/668355e5-34f7-30a2-aee3-b4eb8b8dae68.png\" alt=\"\"></p>\n<h4 id=\"4-hash-tag实现\"><a href=\"#4-hash-tag实现\" class=\"headerlink\" title=\"(4).hash-tag实现\"></a>(4).hash-tag实现</h4><p>第二节提到过，由于hash函数会造成key随机分配到各个节点，那么有没有一种方法能够强制一些key到指定节点到指定的节点呢? redis提供了这样的功能，叫做hash-tag。什么意思呢？假如我们现在使用的是redis-cluster（10个redis节点组成），我们现在有1000个k-v，那么按照hash函数(crc16)规则，这1000个key会被打散到10个节点上，那么时间复杂度还是上述(1)~(3)<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5655/0cc9ab10-39ec-3114-a82a-68cb7d5075e4.png\" alt=\"\"><br>那么我们能不能像使用单机redis一样，一次IO将所有的key取出来呢？hash-tag提供了这样的功能，如果将上述的key改为如下，也就是用大括号括起来相同的内容，那么这些key就会到指定的一个节点上。<br>例如：<br> <img src=\"http://dl2.iteye.com/upload/attachment/0113/5657/8b42b6fb-91d0-367b-b72d-fd01f81c78d4.png\" alt=\"\"><br>例如下图：它的操作时间=1次网络时间+n次命令时间<br><img src=\"http://dl2.iteye.com/upload/attachment/0113/5603/b17e3697-9d98-39ae-9500-a4365a3b2c69.png\" alt=\"\"></p>\n<h3 id=\"4-四种批量操作解决方案对比：\"><a href=\"#4-四种批量操作解决方案对比：\" class=\"headerlink\" title=\"4. 四种批量操作解决方案对比：\"></a>4. 四种批量操作解决方案对比：</h3><table>\n<thead>\n<tr>\n<th>方案</th>\n<th>优点</th>\n<th>缺点</th>\n<th>网络IO</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>串行mget</td>\n<td>1.编程简单<br>2.少量keys，性能满足要求</td>\n<td>大量keys请求延迟严重</td>\n<td>o(keys)</td>\n</tr>\n<tr>\n<td>串行IO</td>\n<td>1.编程简单<br>2.少量节点，性能满足要求</td>\n<td>大量node延迟严重</td>\n<td>o(nodes)</td>\n</tr>\n<tr>\n<td>并行IO</td>\n<td>1.利用并行特性<br>2.延迟取决于最慢的节点</td>\n<td>1.编程复杂<br>2.超时定位较难</td>\n<td>o(max_slow(node))</td>\n</tr>\n<tr>\n<td>hash tags</td>\n<td>性能最高</td>\n<td>1.tag-key业务维护成本较高<br>2.tag分布容易出现数据倾斜</td>\n<td>o(1)</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>目前我们项目中采用第一种方式，使用多线程解决批量问题，减少带宽时延，提高效率，这种做法就如上面所说简单便捷（我们目前批量操作类型比较多），有效。但问题比较明显。批量操作数量不大即可满足。最近研究cachecloud发现搜狐他们采用第二点，先将key获取槽点，然后分node pipeline操作。这种做法相对比第一种做法较优。推荐第二种方法，后期对性能有要求的话，考虑修改成第二种方式。<br>附加cachecloud-client代码中mget追踪路径<br>类PipelineCluster======&gt;mget======&gt;类pipelineCommand=====&gt;run=====&gt;getPoolKeyMap====&gt;getResult</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>[1]<a href=\"http://carlosfu.iteye.com/blog/2263813\" target=\"_blank\" rel=\"noopener\">http://carlosfu.iteye.com/blog/2263813</a><br>[2]<a href=\"https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3\" target=\"_blank\" rel=\"noopener\">https://github.com/sohutv/cachecloud/wiki/5.%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3</a></p>\n"},{"title":"RedisCluster监控系统","date":"2016-10-24T13:35:24.000Z","_content":"## 一、项目介绍\n为了更好监控RedisCluster集群状态信息，提升性能，抛弃之前通过java api获取info信息。此次项目分为两个方面：\n\n1. 通过redis自带info信息监控集群\n2. 监控网络流量获取对集群的使用情况的监控。\n\n总之，通过不同的粒度监控RedisCluster集群运行状况，提供良好的管理运维平台。\n\n## 二、技术架构\n1.info信息\n\n\n```\ngraph LR\nLogstash-->ElasticSearch\nElasticSearch-->Kibana\n```\nLogstash良好的插件结构设计，我们可以根据不同场景选择合适的input,filter,output插件。为了高效配置监控集群，input插件我们基于exec自定义了自己的插件redis-exec插件。output插件直接选择elasticsearch插件。\n\n2.网络流量\n\n```\ngraph LR\nPackagebeat-->ElasticSearch\nElasticSearch-->Kibana\n```\nPackagebeat是一个分布式网络数据抓包软件，可以直接监控redis协议信息。\n\nKibana是一个es可视化的作图工具，根据不同的搜索条件可制定监控不同指标信息的动态图，让使用人员可以直观监控集群运行状况。\n## 三、难点\n1.redis-exec插件开发\n\n详见logstash插件开发\n2.监控指标\n\n[详见国外案例](http://logz.io/blog/redis-performance-monitoring-elk-stack/)","source":"_posts/RedisCluster监控系统.md","raw":"---\ntitle: RedisCluster监控系统\ndate: 2016-10-24 21:35:24\ntags: nosql\ncategories:\n- redis\n---\n## 一、项目介绍\n为了更好监控RedisCluster集群状态信息，提升性能，抛弃之前通过java api获取info信息。此次项目分为两个方面：\n\n1. 通过redis自带info信息监控集群\n2. 监控网络流量获取对集群的使用情况的监控。\n\n总之，通过不同的粒度监控RedisCluster集群运行状况，提供良好的管理运维平台。\n\n## 二、技术架构\n1.info信息\n\n\n```\ngraph LR\nLogstash-->ElasticSearch\nElasticSearch-->Kibana\n```\nLogstash良好的插件结构设计，我们可以根据不同场景选择合适的input,filter,output插件。为了高效配置监控集群，input插件我们基于exec自定义了自己的插件redis-exec插件。output插件直接选择elasticsearch插件。\n\n2.网络流量\n\n```\ngraph LR\nPackagebeat-->ElasticSearch\nElasticSearch-->Kibana\n```\nPackagebeat是一个分布式网络数据抓包软件，可以直接监控redis协议信息。\n\nKibana是一个es可视化的作图工具，根据不同的搜索条件可制定监控不同指标信息的动态图，让使用人员可以直观监控集群运行状况。\n## 三、难点\n1.redis-exec插件开发\n\n详见logstash插件开发\n2.监控指标\n\n[详见国外案例](http://logz.io/blog/redis-performance-monitoring-elk-stack/)","slug":"RedisCluster监控系统","published":1,"updated":"2016-10-24T13:37:16.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcb00438cee0n5kglf6","content":"<h2 id=\"一、项目介绍\"><a href=\"#一、项目介绍\" class=\"headerlink\" title=\"一、项目介绍\"></a>一、项目介绍</h2><p>为了更好监控RedisCluster集群状态信息，提升性能，抛弃之前通过java api获取info信息。此次项目分为两个方面：</p>\n<ol>\n<li>通过redis自带info信息监控集群</li>\n<li>监控网络流量获取对集群的使用情况的监控。</li>\n</ol>\n<p>总之，通过不同的粒度监控RedisCluster集群运行状况，提供良好的管理运维平台。</p>\n<h2 id=\"二、技术架构\"><a href=\"#二、技术架构\" class=\"headerlink\" title=\"二、技术架构\"></a>二、技术架构</h2><p>1.info信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph LR</span><br><span class=\"line\">Logstash--&gt;ElasticSearch</span><br><span class=\"line\">ElasticSearch--&gt;Kibana</span><br></pre></td></tr></table></figure>\n<p>Logstash良好的插件结构设计，我们可以根据不同场景选择合适的input,filter,output插件。为了高效配置监控集群，input插件我们基于exec自定义了自己的插件redis-exec插件。output插件直接选择elasticsearch插件。</p>\n<p>2.网络流量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph LR</span><br><span class=\"line\">Packagebeat--&gt;ElasticSearch</span><br><span class=\"line\">ElasticSearch--&gt;Kibana</span><br></pre></td></tr></table></figure>\n<p>Packagebeat是一个分布式网络数据抓包软件，可以直接监控redis协议信息。</p>\n<p>Kibana是一个es可视化的作图工具，根据不同的搜索条件可制定监控不同指标信息的动态图，让使用人员可以直观监控集群运行状况。</p>\n<h2 id=\"三、难点\"><a href=\"#三、难点\" class=\"headerlink\" title=\"三、难点\"></a>三、难点</h2><p>1.redis-exec插件开发</p>\n<p>详见logstash插件开发<br>2.监控指标</p>\n<p><a href=\"http://logz.io/blog/redis-performance-monitoring-elk-stack/\" target=\"_blank\" rel=\"noopener\">详见国外案例</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、项目介绍\"><a href=\"#一、项目介绍\" class=\"headerlink\" title=\"一、项目介绍\"></a>一、项目介绍</h2><p>为了更好监控RedisCluster集群状态信息，提升性能，抛弃之前通过java api获取info信息。此次项目分为两个方面：</p>\n<ol>\n<li>通过redis自带info信息监控集群</li>\n<li>监控网络流量获取对集群的使用情况的监控。</li>\n</ol>\n<p>总之，通过不同的粒度监控RedisCluster集群运行状况，提供良好的管理运维平台。</p>\n<h2 id=\"二、技术架构\"><a href=\"#二、技术架构\" class=\"headerlink\" title=\"二、技术架构\"></a>二、技术架构</h2><p>1.info信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph LR</span><br><span class=\"line\">Logstash--&gt;ElasticSearch</span><br><span class=\"line\">ElasticSearch--&gt;Kibana</span><br></pre></td></tr></table></figure>\n<p>Logstash良好的插件结构设计，我们可以根据不同场景选择合适的input,filter,output插件。为了高效配置监控集群，input插件我们基于exec自定义了自己的插件redis-exec插件。output插件直接选择elasticsearch插件。</p>\n<p>2.网络流量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph LR</span><br><span class=\"line\">Packagebeat--&gt;ElasticSearch</span><br><span class=\"line\">ElasticSearch--&gt;Kibana</span><br></pre></td></tr></table></figure>\n<p>Packagebeat是一个分布式网络数据抓包软件，可以直接监控redis协议信息。</p>\n<p>Kibana是一个es可视化的作图工具，根据不同的搜索条件可制定监控不同指标信息的动态图，让使用人员可以直观监控集群运行状况。</p>\n<h2 id=\"三、难点\"><a href=\"#三、难点\" class=\"headerlink\" title=\"三、难点\"></a>三、难点</h2><p>1.redis-exec插件开发</p>\n<p>详见logstash插件开发<br>2.监控指标</p>\n<p><a href=\"http://logz.io/blog/redis-performance-monitoring-elk-stack/\" target=\"_blank\" rel=\"noopener\">详见国外案例</a></p>\n"},{"title":"Shell学习","date":"2016-07-10T02:36:52.000Z","_content":"# Shell学习\n## 1.流程控制\n- for\n使用格式如下：\n```\nfor loop in 1 2 3 4 5\ndo\n    echo \"The value is: $loop\"\ndone\n```\n其中需要循环的数据用空格分离开\n- while\n```\nint=1\nwhile(( $int<=5 ))\ndo\n        echo $int\n        let \"int++\"\ndone\n```\n- if/else\n以下实例判断两个变量是否相等：\n```\na=10\nb=20\nif [ $a == $b ]\nthen\n   echo \"a 等于 b\"\nelif [ $a -gt $b ]\nthen\n   echo \"a 大于 b\"\nelif [ $a -lt $b ]\nthen\n   echo \"a 小于 b\"\nelse\n   echo \"没有符合的条件\"\nfi\n```\n## 2.数组\n- 数组定义\n```\narray_name=(value1 ... valuen)\n```\n数组用括号来表示，元素用\"空格\"符号分割开\n- 读取数组\n```\n${array_name[index]}\n```\n- 数组长度\n```\n${#my_array[*]}\n```\n- 遍历数组\n```\narr=(1 2 3 4 5)\nfor var in ${arr[@]};\ndo\n    echo $var\ndone\n```\n循环输出\n```\nfor j in $(seq 7000 7005)\ndo\n  echo $j\ndone\n\n```\n## 3.函数\n- 函数传参\n```\n#/bin/sh\nfunction addslots()\n{\nfor i in `seq  $2 $3`;do\n src/redis-cli  -p $1 cluster addslots $i  \ndone\n}\ncase $1 in\n    addslots)\n    echo \"add slots ...\"\n    addslots $2 $3 $4;\n  ;;\n  *)\n    echo \"Usage: inetpanel [addslots]\"\n  ;;\nesac\n```\n## 4.$0,$?,$!等的特殊用法\n变量|说明\n---|---\n$$|Shell本身的PID（ProcessID）\n$!|Shell最后运行的后台Process的PID\n$?|最后运行的命令的结束代码（返回值）\n$-|使用Set命令设定的Flag一览\n$*|所有参数列表。如\"$*\"用「\"」括起来的情况、以\"$1 $2 … $n\"的形式输出所有参数。\n$@|所有参数列表。如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。\n$#|添加到Shell的参数个数\n$0|Shell本身的文件名\n$1～$n|添加到Shell的各参数值。$1是第1参数、$2是第2参数…\n\n## 5.变量判断\n- -n\n可以使用-n来判断一个string不是NULL值\n```\nif [ ! -f \"/bin/redis-cli\" ]; then \n   ln -s $LOGSTASH_HOME/bin/redis-cli /bin/redis-cli\nfi \n```\n- -f\n可以使用-f来判断一个文件是否存在\n```\nPID=`cat logs/$confName.pid`\n    if [ -n \"$PID\" ];then\n       kill -9 $PID\n       rm -f logs/$confName.pid\n    fi\n```\n- 更多\n```\n[ -f \"$file\" ] 判断$file是否是一个文件\n\n[ $a -lt 3 ] 判断$a的值是否小于3，同样-gt和-le分别表示大于或小于等于\n\n[ -x \"$file\" ] 判断$file是否存在且有可执行权限，同样-r测试文件可读性\n\n[ -n \"$a\" ] 判断变量$a是否有值(空)，测试空串用-z\n\n[ -n $a ] 判断变量$a是否为null\n\n[ \"$a\" = \"$b\" ] 判断$a和$b的取值是否相等\n\n[ cond1 -a cond2 ] 判断cond1和cond2是否同时成立，-o表示cond1和cond2有一成立\n```\n## 6.写文件\n- 两种方式\n\n两种方式：>重写>>追加\n- 写log\n```\nshell上:\n0表示标准输入\n1表示标准输出\n2表示标准错误输出\n> 默认为标准输出重定向，与 1> 相同\n2>&1 意思是把 标准错误输出 重定向到 标准输出.\n&>file 意思是把 标准输出 和 标准错误输出 都重定向到文件file中\n```","source":"_posts/Shell学习.md","raw":"---\ntitle: Shell学习\ndate: 2016-07-10 10:36:52\ntags: 开发笔记\ncategories:\n- shell\n---\n# Shell学习\n## 1.流程控制\n- for\n使用格式如下：\n```\nfor loop in 1 2 3 4 5\ndo\n    echo \"The value is: $loop\"\ndone\n```\n其中需要循环的数据用空格分离开\n- while\n```\nint=1\nwhile(( $int<=5 ))\ndo\n        echo $int\n        let \"int++\"\ndone\n```\n- if/else\n以下实例判断两个变量是否相等：\n```\na=10\nb=20\nif [ $a == $b ]\nthen\n   echo \"a 等于 b\"\nelif [ $a -gt $b ]\nthen\n   echo \"a 大于 b\"\nelif [ $a -lt $b ]\nthen\n   echo \"a 小于 b\"\nelse\n   echo \"没有符合的条件\"\nfi\n```\n## 2.数组\n- 数组定义\n```\narray_name=(value1 ... valuen)\n```\n数组用括号来表示，元素用\"空格\"符号分割开\n- 读取数组\n```\n${array_name[index]}\n```\n- 数组长度\n```\n${#my_array[*]}\n```\n- 遍历数组\n```\narr=(1 2 3 4 5)\nfor var in ${arr[@]};\ndo\n    echo $var\ndone\n```\n循环输出\n```\nfor j in $(seq 7000 7005)\ndo\n  echo $j\ndone\n\n```\n## 3.函数\n- 函数传参\n```\n#/bin/sh\nfunction addslots()\n{\nfor i in `seq  $2 $3`;do\n src/redis-cli  -p $1 cluster addslots $i  \ndone\n}\ncase $1 in\n    addslots)\n    echo \"add slots ...\"\n    addslots $2 $3 $4;\n  ;;\n  *)\n    echo \"Usage: inetpanel [addslots]\"\n  ;;\nesac\n```\n## 4.$0,$?,$!等的特殊用法\n变量|说明\n---|---\n$$|Shell本身的PID（ProcessID）\n$!|Shell最后运行的后台Process的PID\n$?|最后运行的命令的结束代码（返回值）\n$-|使用Set命令设定的Flag一览\n$*|所有参数列表。如\"$*\"用「\"」括起来的情况、以\"$1 $2 … $n\"的形式输出所有参数。\n$@|所有参数列表。如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。\n$#|添加到Shell的参数个数\n$0|Shell本身的文件名\n$1～$n|添加到Shell的各参数值。$1是第1参数、$2是第2参数…\n\n## 5.变量判断\n- -n\n可以使用-n来判断一个string不是NULL值\n```\nif [ ! -f \"/bin/redis-cli\" ]; then \n   ln -s $LOGSTASH_HOME/bin/redis-cli /bin/redis-cli\nfi \n```\n- -f\n可以使用-f来判断一个文件是否存在\n```\nPID=`cat logs/$confName.pid`\n    if [ -n \"$PID\" ];then\n       kill -9 $PID\n       rm -f logs/$confName.pid\n    fi\n```\n- 更多\n```\n[ -f \"$file\" ] 判断$file是否是一个文件\n\n[ $a -lt 3 ] 判断$a的值是否小于3，同样-gt和-le分别表示大于或小于等于\n\n[ -x \"$file\" ] 判断$file是否存在且有可执行权限，同样-r测试文件可读性\n\n[ -n \"$a\" ] 判断变量$a是否有值(空)，测试空串用-z\n\n[ -n $a ] 判断变量$a是否为null\n\n[ \"$a\" = \"$b\" ] 判断$a和$b的取值是否相等\n\n[ cond1 -a cond2 ] 判断cond1和cond2是否同时成立，-o表示cond1和cond2有一成立\n```\n## 6.写文件\n- 两种方式\n\n两种方式：>重写>>追加\n- 写log\n```\nshell上:\n0表示标准输入\n1表示标准输出\n2表示标准错误输出\n> 默认为标准输出重定向，与 1> 相同\n2>&1 意思是把 标准错误输出 重定向到 标准输出.\n&>file 意思是把 标准输出 和 标准错误输出 都重定向到文件file中\n```","slug":"Shell学习","published":1,"updated":"2016-10-24T13:07:39.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcd00468ceeanr8fu3p","content":"<h1 id=\"Shell学习\"><a href=\"#Shell学习\" class=\"headerlink\" title=\"Shell学习\"></a>Shell学习</h1><h2 id=\"1-流程控制\"><a href=\"#1-流程控制\" class=\"headerlink\" title=\"1.流程控制\"></a>1.流程控制</h2><ul>\n<li>for<br>使用格式如下：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for loop in 1 2 3 4 5</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo &quot;The value is: $loop&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>其中需要循环的数据用空格分离开</p>\n<ul>\n<li><p>while</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int=1</span><br><span class=\"line\">while(( $int&lt;=5 ))</span><br><span class=\"line\">do</span><br><span class=\"line\">        echo $int</span><br><span class=\"line\">        let &quot;int++&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>if/else<br>以下实例判断两个变量是否相等：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=10</span><br><span class=\"line\">b=20</span><br><span class=\"line\">if [ $a == $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 等于 b&quot;</span><br><span class=\"line\">elif [ $a -gt $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 大于 b&quot;</span><br><span class=\"line\">elif [ $a -lt $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 小于 b&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">   echo &quot;没有符合的条件&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"2-数组\"><a href=\"#2-数组\" class=\"headerlink\" title=\"2.数组\"></a>2.数组</h2><ul>\n<li>数组定义<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array_name=(value1 ... valuen)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>数组用括号来表示，元素用”空格”符号分割开</p>\n<ul>\n<li><p>读取数组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$&#123;array_name[index]&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>数组长度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$&#123;#my_array[*]&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>遍历数组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arr=(1 2 3 4 5)</span><br><span class=\"line\">for var in $&#123;arr[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $var</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>循环输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for j in $(seq 7000 7005)</span><br><span class=\"line\">do</span><br><span class=\"line\">  echo $j</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-函数\"><a href=\"#3-函数\" class=\"headerlink\" title=\"3.函数\"></a>3.函数</h2><ul>\n<li>函数传参<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#/bin/sh</span><br><span class=\"line\">function addslots()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">for i in `seq  $2 $3`;do</span><br><span class=\"line\"> src/redis-cli  -p $1 cluster addslots $i  </span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">    addslots)</span><br><span class=\"line\">    echo &quot;add slots ...&quot;</span><br><span class=\"line\">    addslots $2 $3 $4;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: inetpanel [addslots]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"4-0-等的特殊用法\"><a href=\"#4-0-等的特殊用法\" class=\"headerlink\" title=\"4.$0,$?,$!等的特殊用法\"></a>4.$0,$?,$!等的特殊用法</h2><table>\n<thead>\n<tr>\n<th>变量</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$$</td>\n<td>Shell本身的PID（ProcessID）</td>\n</tr>\n<tr>\n<td>$!</td>\n<td>Shell最后运行的后台Process的PID</td>\n</tr>\n<tr>\n<td>$?</td>\n<td>最后运行的命令的结束代码（返回值）</td>\n</tr>\n<tr>\n<td>$-</td>\n<td>使用Set命令设定的Flag一览</td>\n</tr>\n<tr>\n<td>$*</td>\n<td>所有参数列表。如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。</td>\n</tr>\n<tr>\n<td>$@</td>\n<td>所有参数列表。如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。</td>\n</tr>\n<tr>\n<td>$#</td>\n<td>添加到Shell的参数个数</td>\n</tr>\n<tr>\n<td>$0</td>\n<td>Shell本身的文件名</td>\n</tr>\n<tr>\n<td>$1～$n</td>\n<td>添加到Shell的各参数值。$1是第1参数、$2是第2参数…</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"5-变量判断\"><a href=\"#5-变量判断\" class=\"headerlink\" title=\"5.变量判断\"></a>5.变量判断</h2><ul>\n<li><p>-n<br>可以使用-n来判断一个string不是NULL值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if [ ! -f &quot;/bin/redis-cli&quot; ]; then </span><br><span class=\"line\">   ln -s $LOGSTASH_HOME/bin/redis-cli /bin/redis-cli</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>-f<br>可以使用-f来判断一个文件是否存在</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PID=`cat logs/$confName.pid`</span><br><span class=\"line\">    if [ -n &quot;$PID&quot; ];then</span><br><span class=\"line\">       kill -9 $PID</span><br><span class=\"line\">       rm -f logs/$confName.pid</span><br><span class=\"line\">    fi</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>更多</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ -f &quot;$file&quot; ] 判断$file是否是一个文件</span><br><span class=\"line\"></span><br><span class=\"line\">[ $a -lt 3 ] 判断$a的值是否小于3，同样-gt和-le分别表示大于或小于等于</span><br><span class=\"line\"></span><br><span class=\"line\">[ -x &quot;$file&quot; ] 判断$file是否存在且有可执行权限，同样-r测试文件可读性</span><br><span class=\"line\"></span><br><span class=\"line\">[ -n &quot;$a&quot; ] 判断变量$a是否有值(空)，测试空串用-z</span><br><span class=\"line\"></span><br><span class=\"line\">[ -n $a ] 判断变量$a是否为null</span><br><span class=\"line\"></span><br><span class=\"line\">[ &quot;$a&quot; = &quot;$b&quot; ] 判断$a和$b的取值是否相等</span><br><span class=\"line\"></span><br><span class=\"line\">[ cond1 -a cond2 ] 判断cond1和cond2是否同时成立，-o表示cond1和cond2有一成立</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"6-写文件\"><a href=\"#6-写文件\" class=\"headerlink\" title=\"6.写文件\"></a>6.写文件</h2><ul>\n<li>两种方式</li>\n</ul>\n<p>两种方式：&gt;重写&gt;&gt;追加</p>\n<ul>\n<li>写log<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shell上:</span><br><span class=\"line\">0表示标准输入</span><br><span class=\"line\">1表示标准输出</span><br><span class=\"line\">2表示标准错误输出</span><br><span class=\"line\">&gt; 默认为标准输出重定向，与 1&gt; 相同</span><br><span class=\"line\">2&gt;&amp;1 意思是把 标准错误输出 重定向到 标准输出.</span><br><span class=\"line\">&amp;&gt;file 意思是把 标准输出 和 标准错误输出 都重定向到文件file中</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Shell学习\"><a href=\"#Shell学习\" class=\"headerlink\" title=\"Shell学习\"></a>Shell学习</h1><h2 id=\"1-流程控制\"><a href=\"#1-流程控制\" class=\"headerlink\" title=\"1.流程控制\"></a>1.流程控制</h2><ul>\n<li>for<br>使用格式如下：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for loop in 1 2 3 4 5</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo &quot;The value is: $loop&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>其中需要循环的数据用空格分离开</p>\n<ul>\n<li><p>while</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">int=1</span><br><span class=\"line\">while(( $int&lt;=5 ))</span><br><span class=\"line\">do</span><br><span class=\"line\">        echo $int</span><br><span class=\"line\">        let &quot;int++&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>if/else<br>以下实例判断两个变量是否相等：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=10</span><br><span class=\"line\">b=20</span><br><span class=\"line\">if [ $a == $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 等于 b&quot;</span><br><span class=\"line\">elif [ $a -gt $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 大于 b&quot;</span><br><span class=\"line\">elif [ $a -lt $b ]</span><br><span class=\"line\">then</span><br><span class=\"line\">   echo &quot;a 小于 b&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">   echo &quot;没有符合的条件&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"2-数组\"><a href=\"#2-数组\" class=\"headerlink\" title=\"2.数组\"></a>2.数组</h2><ul>\n<li>数组定义<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array_name=(value1 ... valuen)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>数组用括号来表示，元素用”空格”符号分割开</p>\n<ul>\n<li><p>读取数组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$&#123;array_name[index]&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>数组长度</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$&#123;#my_array[*]&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>遍历数组</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">arr=(1 2 3 4 5)</span><br><span class=\"line\">for var in $&#123;arr[@]&#125;;</span><br><span class=\"line\">do</span><br><span class=\"line\">    echo $var</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>循环输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for j in $(seq 7000 7005)</span><br><span class=\"line\">do</span><br><span class=\"line\">  echo $j</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-函数\"><a href=\"#3-函数\" class=\"headerlink\" title=\"3.函数\"></a>3.函数</h2><ul>\n<li>函数传参<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#/bin/sh</span><br><span class=\"line\">function addslots()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">for i in `seq  $2 $3`;do</span><br><span class=\"line\"> src/redis-cli  -p $1 cluster addslots $i  </span><br><span class=\"line\">done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">    addslots)</span><br><span class=\"line\">    echo &quot;add slots ...&quot;</span><br><span class=\"line\">    addslots $2 $3 $4;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">  *)</span><br><span class=\"line\">    echo &quot;Usage: inetpanel [addslots]&quot;</span><br><span class=\"line\">  ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"4-0-等的特殊用法\"><a href=\"#4-0-等的特殊用法\" class=\"headerlink\" title=\"4.$0,$?,$!等的特殊用法\"></a>4.$0,$?,$!等的特殊用法</h2><table>\n<thead>\n<tr>\n<th>变量</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>$$</td>\n<td>Shell本身的PID（ProcessID）</td>\n</tr>\n<tr>\n<td>$!</td>\n<td>Shell最后运行的后台Process的PID</td>\n</tr>\n<tr>\n<td>$?</td>\n<td>最后运行的命令的结束代码（返回值）</td>\n</tr>\n<tr>\n<td>$-</td>\n<td>使用Set命令设定的Flag一览</td>\n</tr>\n<tr>\n<td>$*</td>\n<td>所有参数列表。如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。</td>\n</tr>\n<tr>\n<td>$@</td>\n<td>所有参数列表。如”$@”用「”」括起来的情况、以”$1” “$2” … “$n” 的形式输出所有参数。</td>\n</tr>\n<tr>\n<td>$#</td>\n<td>添加到Shell的参数个数</td>\n</tr>\n<tr>\n<td>$0</td>\n<td>Shell本身的文件名</td>\n</tr>\n<tr>\n<td>$1～$n</td>\n<td>添加到Shell的各参数值。$1是第1参数、$2是第2参数…</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"5-变量判断\"><a href=\"#5-变量判断\" class=\"headerlink\" title=\"5.变量判断\"></a>5.变量判断</h2><ul>\n<li><p>-n<br>可以使用-n来判断一个string不是NULL值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if [ ! -f &quot;/bin/redis-cli&quot; ]; then </span><br><span class=\"line\">   ln -s $LOGSTASH_HOME/bin/redis-cli /bin/redis-cli</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>-f<br>可以使用-f来判断一个文件是否存在</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PID=`cat logs/$confName.pid`</span><br><span class=\"line\">    if [ -n &quot;$PID&quot; ];then</span><br><span class=\"line\">       kill -9 $PID</span><br><span class=\"line\">       rm -f logs/$confName.pid</span><br><span class=\"line\">    fi</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>更多</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[ -f &quot;$file&quot; ] 判断$file是否是一个文件</span><br><span class=\"line\"></span><br><span class=\"line\">[ $a -lt 3 ] 判断$a的值是否小于3，同样-gt和-le分别表示大于或小于等于</span><br><span class=\"line\"></span><br><span class=\"line\">[ -x &quot;$file&quot; ] 判断$file是否存在且有可执行权限，同样-r测试文件可读性</span><br><span class=\"line\"></span><br><span class=\"line\">[ -n &quot;$a&quot; ] 判断变量$a是否有值(空)，测试空串用-z</span><br><span class=\"line\"></span><br><span class=\"line\">[ -n $a ] 判断变量$a是否为null</span><br><span class=\"line\"></span><br><span class=\"line\">[ &quot;$a&quot; = &quot;$b&quot; ] 判断$a和$b的取值是否相等</span><br><span class=\"line\"></span><br><span class=\"line\">[ cond1 -a cond2 ] 判断cond1和cond2是否同时成立，-o表示cond1和cond2有一成立</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"6-写文件\"><a href=\"#6-写文件\" class=\"headerlink\" title=\"6.写文件\"></a>6.写文件</h2><ul>\n<li>两种方式</li>\n</ul>\n<p>两种方式：&gt;重写&gt;&gt;追加</p>\n<ul>\n<li>写log<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shell上:</span><br><span class=\"line\">0表示标准输入</span><br><span class=\"line\">1表示标准输出</span><br><span class=\"line\">2表示标准错误输出</span><br><span class=\"line\">&gt; 默认为标准输出重定向，与 1&gt; 相同</span><br><span class=\"line\">2&gt;&amp;1 意思是把 标准错误输出 重定向到 标准输出.</span><br><span class=\"line\">&amp;&gt;file 意思是把 标准输出 和 标准错误输出 都重定向到文件file中</span><br></pre></td></tr></table></figure></li>\n</ul>\n"},{"title":"Sentinl-（Kibana Alert & Report App for Elasticsearch)","date":"2017-09-17T04:24:28.000Z","_content":"# Sentinl-(Kibana Alert&Report App for Elasticsearch)\n\n## 前言\n最近公司ES集群升级到5.5.1，新版增加了许多新的功能，也废弃了一些特性，同时整合一些插件，做了一个统一的封装，ES栈也越来越丰富，强大了。\n\n最近有增加[X-PACK](https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.html),X-Pack是一个Elastic栈扩展，将安全性，警报，监视，报告和图形功能捆绑到一个易于安装的软件包中。 X-Pack组件旨在无缝协同工作，您可以轻松地启用或禁用要使用的功能。\n\n然并卵，这个东西是要收费的，但是这个并不妨碍开源方案，[Sentinl](https://github.com/sirensolutions/sentinl)就是一个开源方案，作为kibana插件，集成在kibana中，主要提供了预警和报告功能，在架构设计上往X-PACK上靠拢，只提供了一些基本功能，但对于目前一些简单业务需求，完全可以满足需求，这个软件开源不久，期待更多完善。\n##  Sentinl简介\nSentinl 5扩展自Kibi / Kibana 5，具有警报和报告功能，可使用标准查询，可编程验证器和各种可配置操作来监控，通知和报告数据系列更改 - 将其视为一个独立的“观察者” “报告”功能（PNG / PDFs快照）。\n\nSENTINEL还旨在通过直接在Kibana UI中整合来简化在Kibi / Kibana中创建和管理警报和报告的过程。\n\n\n### 功能模块\n\n- Watchers\n- Alarms\n- Reports\n\n\nWatchers是Sentinl核心，主要由 input,Condition,Transform,Actions几大块组成，可以和X-Pack一一对应，部分文档可参考X-Pack，但需要注意的是它和X-Pack还有一些区别，主要体现在input只实现了search，其他并未实现，Actions也并未都实现\n\n##  Sentinl安装与配置\n\n**1. 安装**\n```\n/opt/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-5.5/sentinl-v5.5.1.zip\n```\n**2. 配置**\n在kibana.yml添加以下内容，可以根据具体需求删减\n```\nsentinl:\n  es:\n    timefield: '@timestamp'\n    default_index: watcher\n    type: watch\n    alarm_index: watcher_alarms\n  sentinl:\n    history: 20\n    results: 50\n  settings:\n    email:\n      active: false\n      user: username\n      password: password\n      host: smtp.server.com\n      ssl: true\n      timeout: 10000  # mail server connection timeout\n    slack:\n      active: false\n      username: username\n      hook: 'https://hooks.slack.com/services/<token>'\n      channel: '#channel'\n    report:\n      active: false\n      tmp_path: /tmp/\n    pushapps:\n      active: false\n      api_key: '<pushapps API Key>'  \n```\n##  使用案例\n\n\n**业务需求：**\n\n监控指定索引1小时内数量大于1w,控制台提醒\n\n**实践：**\n\n1. 配置General\n\n输入名称和监控频率\n2. 配置input\n\n```\n{\n  \"search\": {\n    \"request\": {\n      \"index\": [\n        \"shoppingcartapi-*\"\n      ],\n      \"body\": {\n        \"_source\": false,\n        \"query\": {\n          \"bool\": {\n            \"filter\": {\n              \"range\": {\n                \"post_date\": {\n                  \"from\": \"now-1h\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n其中shoppingcartapi-*为索引模糊名称，post_date为时间字段\n3. 配置Condition\n\n```\npayload.hits.total > 10\n```\n4. 配置Action\n\n配置一个console action message填写以下内容\n```\npayload.hits.total:{ {payload.hits.total} }\n```\nTip:通过```{   { } }```可以拿出search结果中的值,此处有空格是避免hexo问题，正常使用无需如此。","source":"_posts/Sentinl-（Kibana-Alert-Report-App-for-Elasticsearch.md","raw":"---\ntitle: Sentinl-（Kibana Alert & Report App for Elasticsearch)\ndate: 2017-09-17 12:24:28\ntags: research\ncategories:\n- elasticsearch\n---\n# Sentinl-(Kibana Alert&Report App for Elasticsearch)\n\n## 前言\n最近公司ES集群升级到5.5.1，新版增加了许多新的功能，也废弃了一些特性，同时整合一些插件，做了一个统一的封装，ES栈也越来越丰富，强大了。\n\n最近有增加[X-PACK](https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.html),X-Pack是一个Elastic栈扩展，将安全性，警报，监视，报告和图形功能捆绑到一个易于安装的软件包中。 X-Pack组件旨在无缝协同工作，您可以轻松地启用或禁用要使用的功能。\n\n然并卵，这个东西是要收费的，但是这个并不妨碍开源方案，[Sentinl](https://github.com/sirensolutions/sentinl)就是一个开源方案，作为kibana插件，集成在kibana中，主要提供了预警和报告功能，在架构设计上往X-PACK上靠拢，只提供了一些基本功能，但对于目前一些简单业务需求，完全可以满足需求，这个软件开源不久，期待更多完善。\n##  Sentinl简介\nSentinl 5扩展自Kibi / Kibana 5，具有警报和报告功能，可使用标准查询，可编程验证器和各种可配置操作来监控，通知和报告数据系列更改 - 将其视为一个独立的“观察者” “报告”功能（PNG / PDFs快照）。\n\nSENTINEL还旨在通过直接在Kibana UI中整合来简化在Kibi / Kibana中创建和管理警报和报告的过程。\n\n\n### 功能模块\n\n- Watchers\n- Alarms\n- Reports\n\n\nWatchers是Sentinl核心，主要由 input,Condition,Transform,Actions几大块组成，可以和X-Pack一一对应，部分文档可参考X-Pack，但需要注意的是它和X-Pack还有一些区别，主要体现在input只实现了search，其他并未实现，Actions也并未都实现\n\n##  Sentinl安装与配置\n\n**1. 安装**\n```\n/opt/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-5.5/sentinl-v5.5.1.zip\n```\n**2. 配置**\n在kibana.yml添加以下内容，可以根据具体需求删减\n```\nsentinl:\n  es:\n    timefield: '@timestamp'\n    default_index: watcher\n    type: watch\n    alarm_index: watcher_alarms\n  sentinl:\n    history: 20\n    results: 50\n  settings:\n    email:\n      active: false\n      user: username\n      password: password\n      host: smtp.server.com\n      ssl: true\n      timeout: 10000  # mail server connection timeout\n    slack:\n      active: false\n      username: username\n      hook: 'https://hooks.slack.com/services/<token>'\n      channel: '#channel'\n    report:\n      active: false\n      tmp_path: /tmp/\n    pushapps:\n      active: false\n      api_key: '<pushapps API Key>'  \n```\n##  使用案例\n\n\n**业务需求：**\n\n监控指定索引1小时内数量大于1w,控制台提醒\n\n**实践：**\n\n1. 配置General\n\n输入名称和监控频率\n2. 配置input\n\n```\n{\n  \"search\": {\n    \"request\": {\n      \"index\": [\n        \"shoppingcartapi-*\"\n      ],\n      \"body\": {\n        \"_source\": false,\n        \"query\": {\n          \"bool\": {\n            \"filter\": {\n              \"range\": {\n                \"post_date\": {\n                  \"from\": \"now-1h\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n其中shoppingcartapi-*为索引模糊名称，post_date为时间字段\n3. 配置Condition\n\n```\npayload.hits.total > 10\n```\n4. 配置Action\n\n配置一个console action message填写以下内容\n```\npayload.hits.total:{ {payload.hits.total} }\n```\nTip:通过```{   { } }```可以拿出search结果中的值,此处有空格是避免hexo问题，正常使用无需如此。","slug":"Sentinl-（Kibana-Alert-Report-App-for-Elasticsearch","published":1,"updated":"2017-09-17T04:44:09.235Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcg004a8ceebn1psif5","content":"<h1 id=\"Sentinl-Kibana-Alert-amp-Report-App-for-Elasticsearch\"><a href=\"#Sentinl-Kibana-Alert-amp-Report-App-for-Elasticsearch\" class=\"headerlink\" title=\"Sentinl-(Kibana Alert&amp;Report App for Elasticsearch)\"></a>Sentinl-(Kibana Alert&amp;Report App for Elasticsearch)</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近公司ES集群升级到5.5.1，新版增加了许多新的功能，也废弃了一些特性，同时整合一些插件，做了一个统一的封装，ES栈也越来越丰富，强大了。</p>\n<p>最近有增加<a href=\"https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.html\" target=\"_blank\" rel=\"noopener\">X-PACK</a>,X-Pack是一个Elastic栈扩展，将安全性，警报，监视，报告和图形功能捆绑到一个易于安装的软件包中。 X-Pack组件旨在无缝协同工作，您可以轻松地启用或禁用要使用的功能。</p>\n<p>然并卵，这个东西是要收费的，但是这个并不妨碍开源方案，<a href=\"https://github.com/sirensolutions/sentinl\" target=\"_blank\" rel=\"noopener\">Sentinl</a>就是一个开源方案，作为kibana插件，集成在kibana中，主要提供了预警和报告功能，在架构设计上往X-PACK上靠拢，只提供了一些基本功能，但对于目前一些简单业务需求，完全可以满足需求，这个软件开源不久，期待更多完善。</p>\n<h2 id=\"Sentinl简介\"><a href=\"#Sentinl简介\" class=\"headerlink\" title=\"Sentinl简介\"></a>Sentinl简介</h2><p>Sentinl 5扩展自Kibi / Kibana 5，具有警报和报告功能，可使用标准查询，可编程验证器和各种可配置操作来监控，通知和报告数据系列更改 - 将其视为一个独立的“观察者” “报告”功能（PNG / PDFs快照）。</p>\n<p>SENTINEL还旨在通过直接在Kibana UI中整合来简化在Kibi / Kibana中创建和管理警报和报告的过程。</p>\n<h3 id=\"功能模块\"><a href=\"#功能模块\" class=\"headerlink\" title=\"功能模块\"></a>功能模块</h3><ul>\n<li>Watchers</li>\n<li>Alarms</li>\n<li>Reports</li>\n</ul>\n<p>Watchers是Sentinl核心，主要由 input,Condition,Transform,Actions几大块组成，可以和X-Pack一一对应，部分文档可参考X-Pack，但需要注意的是它和X-Pack还有一些区别，主要体现在input只实现了search，其他并未实现，Actions也并未都实现</p>\n<h2 id=\"Sentinl安装与配置\"><a href=\"#Sentinl安装与配置\" class=\"headerlink\" title=\"Sentinl安装与配置\"></a>Sentinl安装与配置</h2><p><strong>1. 安装</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-5.5/sentinl-v5.5.1.zip</span><br></pre></td></tr></table></figure></p>\n<p><strong>2. 配置</strong><br>在kibana.yml添加以下内容，可以根据具体需求删减<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sentinl:</span><br><span class=\"line\">  es:</span><br><span class=\"line\">    timefield: &apos;@timestamp&apos;</span><br><span class=\"line\">    default_index: watcher</span><br><span class=\"line\">    type: watch</span><br><span class=\"line\">    alarm_index: watcher_alarms</span><br><span class=\"line\">  sentinl:</span><br><span class=\"line\">    history: 20</span><br><span class=\"line\">    results: 50</span><br><span class=\"line\">  settings:</span><br><span class=\"line\">    email:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      user: username</span><br><span class=\"line\">      password: password</span><br><span class=\"line\">      host: smtp.server.com</span><br><span class=\"line\">      ssl: true</span><br><span class=\"line\">      timeout: 10000  # mail server connection timeout</span><br><span class=\"line\">    slack:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      username: username</span><br><span class=\"line\">      hook: &apos;https://hooks.slack.com/services/&lt;token&gt;&apos;</span><br><span class=\"line\">      channel: &apos;#channel&apos;</span><br><span class=\"line\">    report:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      tmp_path: /tmp/</span><br><span class=\"line\">    pushapps:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      api_key: &apos;&lt;pushapps API Key&gt;&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"使用案例\"><a href=\"#使用案例\" class=\"headerlink\" title=\"使用案例\"></a>使用案例</h2><p><strong>业务需求：</strong></p>\n<p>监控指定索引1小时内数量大于1w,控制台提醒</p>\n<p><strong>实践：</strong></p>\n<ol>\n<li>配置General</li>\n</ol>\n<p>输入名称和监控频率</p>\n<ol start=\"2\">\n<li>配置input</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;search&quot;: &#123;</span><br><span class=\"line\">    &quot;request&quot;: &#123;</span><br><span class=\"line\">      &quot;index&quot;: [</span><br><span class=\"line\">        &quot;shoppingcartapi-*&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;body&quot;: &#123;</span><br><span class=\"line\">        &quot;_source&quot;: false,</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">          &quot;bool&quot;: &#123;</span><br><span class=\"line\">            &quot;filter&quot;: &#123;</span><br><span class=\"line\">              &quot;range&quot;: &#123;</span><br><span class=\"line\">                &quot;post_date&quot;: &#123;</span><br><span class=\"line\">                  &quot;from&quot;: &quot;now-1h&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中shoppingcartapi-*为索引模糊名称，post_date为时间字段</p>\n<ol start=\"3\">\n<li>配置Condition</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload.hits.total &gt; 10</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>配置Action</li>\n</ol>\n<p>配置一个console action message填写以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload.hits.total:&#123; &#123;payload.hits.total&#125; &#125;</span><br></pre></td></tr></table></figure></p>\n<p>Tip:通过<code>{   { } }</code>可以拿出search结果中的值,此处有空格是避免hexo问题，正常使用无需如此。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Sentinl-Kibana-Alert-amp-Report-App-for-Elasticsearch\"><a href=\"#Sentinl-Kibana-Alert-amp-Report-App-for-Elasticsearch\" class=\"headerlink\" title=\"Sentinl-(Kibana Alert&amp;Report App for Elasticsearch)\"></a>Sentinl-(Kibana Alert&amp;Report App for Elasticsearch)</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近公司ES集群升级到5.5.1，新版增加了许多新的功能，也废弃了一些特性，同时整合一些插件，做了一个统一的封装，ES栈也越来越丰富，强大了。</p>\n<p>最近有增加<a href=\"https://www.elastic.co/guide/en/x-pack/current/xpack-introduction.html\" target=\"_blank\" rel=\"noopener\">X-PACK</a>,X-Pack是一个Elastic栈扩展，将安全性，警报，监视，报告和图形功能捆绑到一个易于安装的软件包中。 X-Pack组件旨在无缝协同工作，您可以轻松地启用或禁用要使用的功能。</p>\n<p>然并卵，这个东西是要收费的，但是这个并不妨碍开源方案，<a href=\"https://github.com/sirensolutions/sentinl\" target=\"_blank\" rel=\"noopener\">Sentinl</a>就是一个开源方案，作为kibana插件，集成在kibana中，主要提供了预警和报告功能，在架构设计上往X-PACK上靠拢，只提供了一些基本功能，但对于目前一些简单业务需求，完全可以满足需求，这个软件开源不久，期待更多完善。</p>\n<h2 id=\"Sentinl简介\"><a href=\"#Sentinl简介\" class=\"headerlink\" title=\"Sentinl简介\"></a>Sentinl简介</h2><p>Sentinl 5扩展自Kibi / Kibana 5，具有警报和报告功能，可使用标准查询，可编程验证器和各种可配置操作来监控，通知和报告数据系列更改 - 将其视为一个独立的“观察者” “报告”功能（PNG / PDFs快照）。</p>\n<p>SENTINEL还旨在通过直接在Kibana UI中整合来简化在Kibi / Kibana中创建和管理警报和报告的过程。</p>\n<h3 id=\"功能模块\"><a href=\"#功能模块\" class=\"headerlink\" title=\"功能模块\"></a>功能模块</h3><ul>\n<li>Watchers</li>\n<li>Alarms</li>\n<li>Reports</li>\n</ul>\n<p>Watchers是Sentinl核心，主要由 input,Condition,Transform,Actions几大块组成，可以和X-Pack一一对应，部分文档可参考X-Pack，但需要注意的是它和X-Pack还有一些区别，主要体现在input只实现了search，其他并未实现，Actions也并未都实现</p>\n<h2 id=\"Sentinl安装与配置\"><a href=\"#Sentinl安装与配置\" class=\"headerlink\" title=\"Sentinl安装与配置\"></a>Sentinl安装与配置</h2><p><strong>1. 安装</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/opt/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-5.5/sentinl-v5.5.1.zip</span><br></pre></td></tr></table></figure></p>\n<p><strong>2. 配置</strong><br>在kibana.yml添加以下内容，可以根据具体需求删减<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sentinl:</span><br><span class=\"line\">  es:</span><br><span class=\"line\">    timefield: &apos;@timestamp&apos;</span><br><span class=\"line\">    default_index: watcher</span><br><span class=\"line\">    type: watch</span><br><span class=\"line\">    alarm_index: watcher_alarms</span><br><span class=\"line\">  sentinl:</span><br><span class=\"line\">    history: 20</span><br><span class=\"line\">    results: 50</span><br><span class=\"line\">  settings:</span><br><span class=\"line\">    email:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      user: username</span><br><span class=\"line\">      password: password</span><br><span class=\"line\">      host: smtp.server.com</span><br><span class=\"line\">      ssl: true</span><br><span class=\"line\">      timeout: 10000  # mail server connection timeout</span><br><span class=\"line\">    slack:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      username: username</span><br><span class=\"line\">      hook: &apos;https://hooks.slack.com/services/&lt;token&gt;&apos;</span><br><span class=\"line\">      channel: &apos;#channel&apos;</span><br><span class=\"line\">    report:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      tmp_path: /tmp/</span><br><span class=\"line\">    pushapps:</span><br><span class=\"line\">      active: false</span><br><span class=\"line\">      api_key: &apos;&lt;pushapps API Key&gt;&apos;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"使用案例\"><a href=\"#使用案例\" class=\"headerlink\" title=\"使用案例\"></a>使用案例</h2><p><strong>业务需求：</strong></p>\n<p>监控指定索引1小时内数量大于1w,控制台提醒</p>\n<p><strong>实践：</strong></p>\n<ol>\n<li>配置General</li>\n</ol>\n<p>输入名称和监控频率</p>\n<ol start=\"2\">\n<li>配置input</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;search&quot;: &#123;</span><br><span class=\"line\">    &quot;request&quot;: &#123;</span><br><span class=\"line\">      &quot;index&quot;: [</span><br><span class=\"line\">        &quot;shoppingcartapi-*&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;body&quot;: &#123;</span><br><span class=\"line\">        &quot;_source&quot;: false,</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">          &quot;bool&quot;: &#123;</span><br><span class=\"line\">            &quot;filter&quot;: &#123;</span><br><span class=\"line\">              &quot;range&quot;: &#123;</span><br><span class=\"line\">                &quot;post_date&quot;: &#123;</span><br><span class=\"line\">                  &quot;from&quot;: &quot;now-1h&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">              &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中shoppingcartapi-*为索引模糊名称，post_date为时间字段</p>\n<ol start=\"3\">\n<li>配置Condition</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload.hits.total &gt; 10</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>配置Action</li>\n</ol>\n<p>配置一个console action message填写以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload.hits.total:&#123; &#123;payload.hits.total&#125; &#125;</span><br></pre></td></tr></table></figure></p>\n<p>Tip:通过<code>{   { } }</code>可以拿出search结果中的值,此处有空格是避免hexo问题，正常使用无需如此。</p>\n"},{"title":"Thymeleaf布局","date":"2018-07-22T10:52:07.000Z","_content":"# Thymeleaf布局\n\n## 前言\n在web 开发过程中，经常会有一些公共页面，例如head,foot,menu等，利用layout可以极大的提高开发效率。通过使用Thymeleaf Layout Dialect,可以便捷使用布局，减少代码的重复。\n## 正文\n\n### 1.配置\n\n在pom 中增加依赖：\n```\n<dependency>\n\t<groupId>nz.net.ultraq.thymeleaf</groupId>\n\t<artifactId>thymeleaf-layout-dialect</artifactId>\n</dependency>\n```\n\nSpring Boot 2 java 初始化配置：\n```\n@Bean\npublic LayoutDialect layoutDialect() {\n    return new LayoutDialect();\n}\n```\n以上配置会使layout 命名空间可以引入五种属性：decorate, title-pattern, insert, replace, fragment\n\nThymeleaf Layout Dialect 默认可以合并layout与content中head信息，默认在layout 标签之后追加content中的标签。\n\n### 2.布局\nLayout.html\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\">\n<head>\n  <title>Layout page</title>\n  <script src=\"common-script.js\"></script>\n</head>\n<body>\n  <header>\n    <h1>My website</h1>\n  </header>\n  <section layout:fragment=\"content\">\n    <p>Page content goes here</p>\n  </section>\n  <footer>\n    <p>My footer</p>\n    <p layout:fragment=\"custom-footer\">Custom footer here</p>\n  </footer>  \n</body>\n</html>\n```\nContent1.html\n\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\"\n  layout:decorate=\"~{Layout}\">\n<head>\n  <title>Content page 1</title>\n  <script src=\"content-script.js\"></script>\n</head>\n<body>\n  <section layout:fragment=\"content\">\n    <p>This is a paragraph from content page 1</p>\n  </section>\n  <footer>\n    <p layout:fragment=\"custom-footer\">This is some footer content from content page 1</p>\n  </footer>\n</body>\n</html>\n```\n生成结果如下：\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <title>Content page 1</title>\n  <script src=\"common-script.js\"></script>\n  <script src=\"content-script.js\"></script>\n</head>\n<body>\n  <header>\n    <h1>My website</h1>\n  </header>\n  <section>\n    <p>This is a paragraph from content page 1</p>\n  </section>\n  <footer>\n    <p>My footer</p>\n    <p>This is some footer content from content page 1</p>\n  </footer>  \n</body>\n</html>\n```\n### 3.布局传递数据\nChild/content template:\n```\n<html layout:decorate=\"your-layout.html\" th:with=\"greeting='Hello!'\">\n```\nParent/layout template:\n```\n<html>\n  ...\n  <p th:text=\"${greeting}\"></p> <!-- You'll end up with \"Hello!\" in here -->\n```\n### 4.配置title\nLayout.html\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\">\n<head>\n  <title layout:title-pattern=\"$LAYOUT_TITLE - $CONTENT_TITLE\">My website</title>\n</head>\n...\n</html>\n```\n\nContent.html\n\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\"\n  layout:decorator=\"Layout\">\n<head>\n  <title>My blog</title>\n</head>\n...\n</html>\n```\n\n生成结果如下：\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <title>My website - My blog</title>\n</head>\n...\n</html>\n```\n\n## 参考\n1. [Thymeleaf Layout Dialect](https://ultraq.github.io/thymeleaf-layout-dialect/)","source":"_posts/Thymeleaf布局.md","raw":"---\ntitle: Thymeleaf布局\ndate: 2018-07-22 18:52:07\ntags: 开发笔记\ncategories:\n         - thymeleaf\n         - java\n         - 模板\n---\n# Thymeleaf布局\n\n## 前言\n在web 开发过程中，经常会有一些公共页面，例如head,foot,menu等，利用layout可以极大的提高开发效率。通过使用Thymeleaf Layout Dialect,可以便捷使用布局，减少代码的重复。\n## 正文\n\n### 1.配置\n\n在pom 中增加依赖：\n```\n<dependency>\n\t<groupId>nz.net.ultraq.thymeleaf</groupId>\n\t<artifactId>thymeleaf-layout-dialect</artifactId>\n</dependency>\n```\n\nSpring Boot 2 java 初始化配置：\n```\n@Bean\npublic LayoutDialect layoutDialect() {\n    return new LayoutDialect();\n}\n```\n以上配置会使layout 命名空间可以引入五种属性：decorate, title-pattern, insert, replace, fragment\n\nThymeleaf Layout Dialect 默认可以合并layout与content中head信息，默认在layout 标签之后追加content中的标签。\n\n### 2.布局\nLayout.html\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\">\n<head>\n  <title>Layout page</title>\n  <script src=\"common-script.js\"></script>\n</head>\n<body>\n  <header>\n    <h1>My website</h1>\n  </header>\n  <section layout:fragment=\"content\">\n    <p>Page content goes here</p>\n  </section>\n  <footer>\n    <p>My footer</p>\n    <p layout:fragment=\"custom-footer\">Custom footer here</p>\n  </footer>  \n</body>\n</html>\n```\nContent1.html\n\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\"\n  layout:decorate=\"~{Layout}\">\n<head>\n  <title>Content page 1</title>\n  <script src=\"content-script.js\"></script>\n</head>\n<body>\n  <section layout:fragment=\"content\">\n    <p>This is a paragraph from content page 1</p>\n  </section>\n  <footer>\n    <p layout:fragment=\"custom-footer\">This is some footer content from content page 1</p>\n  </footer>\n</body>\n</html>\n```\n生成结果如下：\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <title>Content page 1</title>\n  <script src=\"common-script.js\"></script>\n  <script src=\"content-script.js\"></script>\n</head>\n<body>\n  <header>\n    <h1>My website</h1>\n  </header>\n  <section>\n    <p>This is a paragraph from content page 1</p>\n  </section>\n  <footer>\n    <p>My footer</p>\n    <p>This is some footer content from content page 1</p>\n  </footer>  \n</body>\n</html>\n```\n### 3.布局传递数据\nChild/content template:\n```\n<html layout:decorate=\"your-layout.html\" th:with=\"greeting='Hello!'\">\n```\nParent/layout template:\n```\n<html>\n  ...\n  <p th:text=\"${greeting}\"></p> <!-- You'll end up with \"Hello!\" in here -->\n```\n### 4.配置title\nLayout.html\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\">\n<head>\n  <title layout:title-pattern=\"$LAYOUT_TITLE - $CONTENT_TITLE\">My website</title>\n</head>\n...\n</html>\n```\n\nContent.html\n\n```\n<!DOCTYPE html>\n<html xmlns:layout=\"http://www.ultraq.net.nz/thymeleaf/layout\"\n  layout:decorator=\"Layout\">\n<head>\n  <title>My blog</title>\n</head>\n...\n</html>\n```\n\n生成结果如下：\n\n```\n<!DOCTYPE html>\n<html>\n<head>\n  <title>My website - My blog</title>\n</head>\n...\n</html>\n```\n\n## 参考\n1. [Thymeleaf Layout Dialect](https://ultraq.github.io/thymeleaf-layout-dialect/)","slug":"Thymeleaf布局","published":1,"updated":"2018-07-22T11:35:04.963Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvci004d8ceencry8x8q","content":"<h1 id=\"Thymeleaf布局\"><a href=\"#Thymeleaf布局\" class=\"headerlink\" title=\"Thymeleaf布局\"></a>Thymeleaf布局</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在web 开发过程中，经常会有一些公共页面，例如head,foot,menu等，利用layout可以极大的提高开发效率。通过使用Thymeleaf Layout Dialect,可以便捷使用布局，减少代码的重复。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-配置\"><a href=\"#1-配置\" class=\"headerlink\" title=\"1.配置\"></a>1.配置</h3><p>在pom 中增加依赖：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;nz.net.ultraq.thymeleaf&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;thymeleaf-layout-dialect&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Spring Boot 2 java 初始化配置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Bean</span><br><span class=\"line\">public LayoutDialect layoutDialect() &#123;</span><br><span class=\"line\">    return new LayoutDialect();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>以上配置会使layout 命名空间可以引入五种属性：decorate, title-pattern, insert, replace, fragment</p>\n<p>Thymeleaf Layout Dialect 默认可以合并layout与content中head信息，默认在layout 标签之后追加content中的标签。</p>\n<h3 id=\"2-布局\"><a href=\"#2-布局\" class=\"headerlink\" title=\"2.布局\"></a>2.布局</h3><p>Layout.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Layout page&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;common-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;header&gt;</span><br><span class=\"line\">    &lt;h1&gt;My website&lt;/h1&gt;</span><br><span class=\"line\">  &lt;/header&gt;</span><br><span class=\"line\">  &lt;section layout:fragment=&quot;content&quot;&gt;</span><br><span class=\"line\">    &lt;p&gt;Page content goes here&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p&gt;My footer&lt;/p&gt;</span><br><span class=\"line\">    &lt;p layout:fragment=&quot;custom-footer&quot;&gt;Custom footer here&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;  </span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Content1.html</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;</span><br><span class=\"line\">  layout:decorate=&quot;~&#123;Layout&#125;&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Content page 1&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;content-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;section layout:fragment=&quot;content&quot;&gt;</span><br><span class=\"line\">    &lt;p&gt;This is a paragraph from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p layout:fragment=&quot;custom-footer&quot;&gt;This is some footer content from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>生成结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Content page 1&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;common-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;content-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;header&gt;</span><br><span class=\"line\">    &lt;h1&gt;My website&lt;/h1&gt;</span><br><span class=\"line\">  &lt;/header&gt;</span><br><span class=\"line\">  &lt;section&gt;</span><br><span class=\"line\">    &lt;p&gt;This is a paragraph from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p&gt;My footer&lt;/p&gt;</span><br><span class=\"line\">    &lt;p&gt;This is some footer content from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;  </span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-布局传递数据\"><a href=\"#3-布局传递数据\" class=\"headerlink\" title=\"3.布局传递数据\"></a>3.布局传递数据</h3><p>Child/content template:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html layout:decorate=&quot;your-layout.html&quot; th:with=&quot;greeting=&apos;Hello!&apos;&quot;&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Parent/layout template:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &lt;p th:text=&quot;$&#123;greeting&#125;&quot;&gt;&lt;/p&gt; &lt;!-- You&apos;ll end up with &quot;Hello!&quot; in here --&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-配置title\"><a href=\"#4-配置title\" class=\"headerlink\" title=\"4.配置title\"></a>4.配置title</h3><p>Layout.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title layout:title-pattern=&quot;$LAYOUT_TITLE - $CONTENT_TITLE&quot;&gt;My website&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Content.html</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;</span><br><span class=\"line\">  layout:decorator=&quot;Layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;My blog&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>生成结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;My website - My blog&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ultraq.github.io/thymeleaf-layout-dialect/\" target=\"_blank\" rel=\"noopener\">Thymeleaf Layout Dialect</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Thymeleaf布局\"><a href=\"#Thymeleaf布局\" class=\"headerlink\" title=\"Thymeleaf布局\"></a>Thymeleaf布局</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在web 开发过程中，经常会有一些公共页面，例如head,foot,menu等，利用layout可以极大的提高开发效率。通过使用Thymeleaf Layout Dialect,可以便捷使用布局，减少代码的重复。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-配置\"><a href=\"#1-配置\" class=\"headerlink\" title=\"1.配置\"></a>1.配置</h3><p>在pom 中增加依赖：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;nz.net.ultraq.thymeleaf&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;thymeleaf-layout-dialect&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Spring Boot 2 java 初始化配置：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Bean</span><br><span class=\"line\">public LayoutDialect layoutDialect() &#123;</span><br><span class=\"line\">    return new LayoutDialect();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>以上配置会使layout 命名空间可以引入五种属性：decorate, title-pattern, insert, replace, fragment</p>\n<p>Thymeleaf Layout Dialect 默认可以合并layout与content中head信息，默认在layout 标签之后追加content中的标签。</p>\n<h3 id=\"2-布局\"><a href=\"#2-布局\" class=\"headerlink\" title=\"2.布局\"></a>2.布局</h3><p>Layout.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Layout page&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;common-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;header&gt;</span><br><span class=\"line\">    &lt;h1&gt;My website&lt;/h1&gt;</span><br><span class=\"line\">  &lt;/header&gt;</span><br><span class=\"line\">  &lt;section layout:fragment=&quot;content&quot;&gt;</span><br><span class=\"line\">    &lt;p&gt;Page content goes here&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p&gt;My footer&lt;/p&gt;</span><br><span class=\"line\">    &lt;p layout:fragment=&quot;custom-footer&quot;&gt;Custom footer here&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;  </span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Content1.html</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;</span><br><span class=\"line\">  layout:decorate=&quot;~&#123;Layout&#125;&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Content page 1&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;content-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;section layout:fragment=&quot;content&quot;&gt;</span><br><span class=\"line\">    &lt;p&gt;This is a paragraph from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p layout:fragment=&quot;custom-footer&quot;&gt;This is some footer content from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>生成结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;Content page 1&lt;/title&gt;</span><br><span class=\"line\">  &lt;script src=&quot;common-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">  &lt;script src=&quot;content-script.js&quot;&gt;&lt;/script&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">  &lt;header&gt;</span><br><span class=\"line\">    &lt;h1&gt;My website&lt;/h1&gt;</span><br><span class=\"line\">  &lt;/header&gt;</span><br><span class=\"line\">  &lt;section&gt;</span><br><span class=\"line\">    &lt;p&gt;This is a paragraph from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/section&gt;</span><br><span class=\"line\">  &lt;footer&gt;</span><br><span class=\"line\">    &lt;p&gt;My footer&lt;/p&gt;</span><br><span class=\"line\">    &lt;p&gt;This is some footer content from content page 1&lt;/p&gt;</span><br><span class=\"line\">  &lt;/footer&gt;  </span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-布局传递数据\"><a href=\"#3-布局传递数据\" class=\"headerlink\" title=\"3.布局传递数据\"></a>3.布局传递数据</h3><p>Child/content template:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html layout:decorate=&quot;your-layout.html&quot; th:with=&quot;greeting=&apos;Hello!&apos;&quot;&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Parent/layout template:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  &lt;p th:text=&quot;$&#123;greeting&#125;&quot;&gt;&lt;/p&gt; &lt;!-- You&apos;ll end up with &quot;Hello!&quot; in here --&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-配置title\"><a href=\"#4-配置title\" class=\"headerlink\" title=\"4.配置title\"></a>4.配置title</h3><p>Layout.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title layout:title-pattern=&quot;$LAYOUT_TITLE - $CONTENT_TITLE&quot;&gt;My website&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n<p>Content.html</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html xmlns:layout=&quot;http://www.ultraq.net.nz/thymeleaf/layout&quot;</span><br><span class=\"line\">  layout:decorator=&quot;Layout&quot;&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;My blog&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<p>生成结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html&gt;</span><br><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">  &lt;title&gt;My website - My blog&lt;/title&gt;</span><br><span class=\"line\">&lt;/head&gt;</span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://ultraq.github.io/thymeleaf-layout-dialect/\" target=\"_blank\" rel=\"noopener\">Thymeleaf Layout Dialect</a></li>\n</ol>\n"},{"title":"beat开发环境搭建","date":"2018-10-15T09:21:09.000Z","_content":"# 前言\nbeats 是什么？Beats是您在服务器上作为代理安装的开源数据托运方，用于将操作数据发送到Elasticsearch。\n\nelastc 官方提供的beats 有以下方面：\n\nAudit data|Auditbeat\n---|---\nLog files|Filebeat\nAvailability|Heartbeat\nMetrics|Metricbeat\nNetwork traffic|Packetbeat\nWindows event logs|Winlogbeat\n\n![image](https://www.elastic.co/guide/en/beats/libbeat/current/images/beats-platform.png)\nbeats可以直接发送数据到Elasticsearch或通过Logstash，您可以在Kibana中进行可视化之前进一步处理和增强数据。\n\n# 安装软件\n1. 安装新版 [go](https://golang.org/dl/)\n\n   beats 是用go 语言写的，因此需要安装go  \n   (1).配置GOROOT 例如:<code>GOROOT=D:\\Go\\</code>\n   (2).配置Path 环境中新增<code>D:\\Go\\bin</code>\n   (3).配置GOPATH  GOPATH=<code>D:\\Go\\GOPATH</code>  \n2. 安装 Python 2  \n\n     (1).安装python  \n     (2).安装pip   \n     ```\n     wget https://bootstrap.pypa.io/get-pip.py\n     sudo python get-pip.py\n     ```\n     (3). virtualenv  \n          virtualenv 用来生成支持make update\n     ```\n     pip install virtualenv\n     ```\n# 源码配置\n    ```\n         mkdir -p ${GOPATH}/src/github.com/elastic\n         cd ${GOPATH}/src/github.com/elastic\n         git clone https://github.com/elastic/beats.git\n    ```\n   **切记上面配置地址路径，不然运行就会报错**。\n\n# 编译运行\n1. 编译\n   ```\n   make collect\n   make update\n   make\n   ```\n2. 运行\n   ```\n   ./{beat} -e -d \"*\"\n   ```\n   ```* ```代表选择输出的debug 日志，例如```./metricset -e -d \"kafka\"``` 输出kafka moduel 相关debug log\n# 注意事项\n\n 执行```make update```可以生成配置文件和文档，例如运行beat需要field.yml文件，即可由该你命令生成，但是切记，该命令也会重写{beat}.yml配置文件  \n \n# 参考\n1. [beats-contributing](https://www.elastic.co/guide/en/beats/devguide/current/beats-contributing.html)\n\n","source":"_posts/beat开发环境搭建.md","raw":"---\ntitle: beat开发环境搭建\ndate: 2018-10-15 17:21:09\ntags: 教程\ncategories:\n- elasticsearch\n---\n# 前言\nbeats 是什么？Beats是您在服务器上作为代理安装的开源数据托运方，用于将操作数据发送到Elasticsearch。\n\nelastc 官方提供的beats 有以下方面：\n\nAudit data|Auditbeat\n---|---\nLog files|Filebeat\nAvailability|Heartbeat\nMetrics|Metricbeat\nNetwork traffic|Packetbeat\nWindows event logs|Winlogbeat\n\n![image](https://www.elastic.co/guide/en/beats/libbeat/current/images/beats-platform.png)\nbeats可以直接发送数据到Elasticsearch或通过Logstash，您可以在Kibana中进行可视化之前进一步处理和增强数据。\n\n# 安装软件\n1. 安装新版 [go](https://golang.org/dl/)\n\n   beats 是用go 语言写的，因此需要安装go  \n   (1).配置GOROOT 例如:<code>GOROOT=D:\\Go\\</code>\n   (2).配置Path 环境中新增<code>D:\\Go\\bin</code>\n   (3).配置GOPATH  GOPATH=<code>D:\\Go\\GOPATH</code>  \n2. 安装 Python 2  \n\n     (1).安装python  \n     (2).安装pip   \n     ```\n     wget https://bootstrap.pypa.io/get-pip.py\n     sudo python get-pip.py\n     ```\n     (3). virtualenv  \n          virtualenv 用来生成支持make update\n     ```\n     pip install virtualenv\n     ```\n# 源码配置\n    ```\n         mkdir -p ${GOPATH}/src/github.com/elastic\n         cd ${GOPATH}/src/github.com/elastic\n         git clone https://github.com/elastic/beats.git\n    ```\n   **切记上面配置地址路径，不然运行就会报错**。\n\n# 编译运行\n1. 编译\n   ```\n   make collect\n   make update\n   make\n   ```\n2. 运行\n   ```\n   ./{beat} -e -d \"*\"\n   ```\n   ```* ```代表选择输出的debug 日志，例如```./metricset -e -d \"kafka\"``` 输出kafka moduel 相关debug log\n# 注意事项\n\n 执行```make update```可以生成配置文件和文档，例如运行beat需要field.yml文件，即可由该你命令生成，但是切记，该命令也会重写{beat}.yml配置文件  \n \n# 参考\n1. [beats-contributing](https://www.elastic.co/guide/en/beats/devguide/current/beats-contributing.html)\n\n","slug":"beat开发环境搭建","published":1,"updated":"2018-10-15T09:39:33.132Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcl004g8ceerj8e6sah","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>beats 是什么？Beats是您在服务器上作为代理安装的开源数据托运方，用于将操作数据发送到Elasticsearch。</p>\n<p>elastc 官方提供的beats 有以下方面：</p>\n<table>\n<thead>\n<tr>\n<th>Audit data</th>\n<th>Auditbeat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Log files</td>\n<td>Filebeat</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>Heartbeat</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Metricbeat</td>\n</tr>\n<tr>\n<td>Network traffic</td>\n<td>Packetbeat</td>\n</tr>\n<tr>\n<td>Windows event logs</td>\n<td>Winlogbeat</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"https://www.elastic.co/guide/en/beats/libbeat/current/images/beats-platform.png\" alt=\"image\"><br>beats可以直接发送数据到Elasticsearch或通过Logstash，您可以在Kibana中进行可视化之前进一步处理和增强数据。</p>\n<h1 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h1><ol>\n<li><p>安装新版 <a href=\"https://golang.org/dl/\" target=\"_blank\" rel=\"noopener\">go</a></p>\n<p>beats 是用go 语言写的，因此需要安装go<br>(1).配置GOROOT 例如:<code>GOROOT=D:\\Go\\</code><br>(2).配置Path 环境中新增<code>D:\\Go\\bin</code><br>(3).配置GOPATH  GOPATH=<code>D:\\Go\\GOPATH</code>  </p>\n</li>\n<li><p>安装 Python 2  </p>\n<p>  (1).安装python<br>  (2).安装pip   </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://bootstrap.pypa.io/get-pip.py</span><br><span class=\"line\">sudo python get-pip.py</span><br></pre></td></tr></table></figure>\n<p>  (3). virtualenv  </p>\n<pre><code>virtualenv 用来生成支持make update\n</code></pre>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install virtualenv</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"源码配置\"><a href=\"#源码配置\" class=\"headerlink\" title=\"源码配置\"></a>源码配置</h1><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p $&#123;GOPATH&#125;/src/github.com/elastic</span><br><span class=\"line\">cd $&#123;GOPATH&#125;/src/github.com/elastic</span><br><span class=\"line\">git clone https://github.com/elastic/beats.git</span><br></pre></td></tr></table></figure>\n</code></pre><p>   <strong>切记上面配置地址路径，不然运行就会报错</strong>。</p>\n<h1 id=\"编译运行\"><a href=\"#编译运行\" class=\"headerlink\" title=\"编译运行\"></a>编译运行</h1><ol>\n<li><p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make collect</span><br><span class=\"line\">make update</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./&#123;beat&#125; -e -d &quot;*&quot;</span><br></pre></td></tr></table></figure>\n<p><code>*</code>代表选择输出的debug 日志，例如<code>./metricset -e -d &quot;kafka&quot;</code> 输出kafka moduel 相关debug log</p>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><p>执行<code>make update</code>可以生成配置文件和文档，例如运行beat需要field.yml文件，即可由该你命令生成，但是切记，该命令也会重写{beat}.yml配置文件  </p>\n</li>\n</ol>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><a href=\"https://www.elastic.co/guide/en/beats/devguide/current/beats-contributing.html\" target=\"_blank\" rel=\"noopener\">beats-contributing</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>beats 是什么？Beats是您在服务器上作为代理安装的开源数据托运方，用于将操作数据发送到Elasticsearch。</p>\n<p>elastc 官方提供的beats 有以下方面：</p>\n<table>\n<thead>\n<tr>\n<th>Audit data</th>\n<th>Auditbeat</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Log files</td>\n<td>Filebeat</td>\n</tr>\n<tr>\n<td>Availability</td>\n<td>Heartbeat</td>\n</tr>\n<tr>\n<td>Metrics</td>\n<td>Metricbeat</td>\n</tr>\n<tr>\n<td>Network traffic</td>\n<td>Packetbeat</td>\n</tr>\n<tr>\n<td>Windows event logs</td>\n<td>Winlogbeat</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"https://www.elastic.co/guide/en/beats/libbeat/current/images/beats-platform.png\" alt=\"image\"><br>beats可以直接发送数据到Elasticsearch或通过Logstash，您可以在Kibana中进行可视化之前进一步处理和增强数据。</p>\n<h1 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h1><ol>\n<li><p>安装新版 <a href=\"https://golang.org/dl/\" target=\"_blank\" rel=\"noopener\">go</a></p>\n<p>beats 是用go 语言写的，因此需要安装go<br>(1).配置GOROOT 例如:<code>GOROOT=D:\\Go\\</code><br>(2).配置Path 环境中新增<code>D:\\Go\\bin</code><br>(3).配置GOPATH  GOPATH=<code>D:\\Go\\GOPATH</code>  </p>\n</li>\n<li><p>安装 Python 2  </p>\n<p>  (1).安装python<br>  (2).安装pip   </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://bootstrap.pypa.io/get-pip.py</span><br><span class=\"line\">sudo python get-pip.py</span><br></pre></td></tr></table></figure>\n<p>  (3). virtualenv  </p>\n<pre><code>virtualenv 用来生成支持make update\n</code></pre>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install virtualenv</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"源码配置\"><a href=\"#源码配置\" class=\"headerlink\" title=\"源码配置\"></a>源码配置</h1><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p $&#123;GOPATH&#125;/src/github.com/elastic</span><br><span class=\"line\">cd $&#123;GOPATH&#125;/src/github.com/elastic</span><br><span class=\"line\">git clone https://github.com/elastic/beats.git</span><br></pre></td></tr></table></figure>\n</code></pre><p>   <strong>切记上面配置地址路径，不然运行就会报错</strong>。</p>\n<h1 id=\"编译运行\"><a href=\"#编译运行\" class=\"headerlink\" title=\"编译运行\"></a>编译运行</h1><ol>\n<li><p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make collect</span><br><span class=\"line\">make update</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./&#123;beat&#125; -e -d &quot;*&quot;</span><br></pre></td></tr></table></figure>\n<p><code>*</code>代表选择输出的debug 日志，例如<code>./metricset -e -d &quot;kafka&quot;</code> 输出kafka moduel 相关debug log</p>\n<h1 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h1><p>执行<code>make update</code>可以生成配置文件和文档，例如运行beat需要field.yml文件，即可由该你命令生成，但是切记，该命令也会重写{beat}.yml配置文件  </p>\n</li>\n</ol>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><a href=\"https://www.elastic.co/guide/en/beats/devguide/current/beats-contributing.html\" target=\"_blank\" rel=\"noopener\">beats-contributing</a></li>\n</ol>\n"},{"title":"docker registry创建","date":"2016-10-24T13:26:53.000Z","_content":"centos6.X版本，docker version1.7.1版本创建过程如下：\n1. 运行容器\n```\n$ docker run -d -p 5000:5000 registry\n```\n2. 修改配置\n\n在docker1.3.X版本以后，与docker registry交互默认使用的是https，此处需要修改为http。在/etc/sysconfig/docker文件中添加以下内容即可：\n```\nother_args=\"$other_args --insecure-registry myregistry.example.com:5000 \"\n```\n在1.12最新版本中可以使用以下方式修改，原理是一致的。\n```\n Create or modify /etc/docker/daemon.json\n{ \"insecure-registries\":[\"myregistry.example.com:5000\"] }\n```\n然后重新启动\n```\n$ service docker restart\n```\n3. 测试新的registry\n- 镜像打标签\n```\n$ docker tag <img_id> myregistry.example.com:5000/truman/opentsdb\n```\n- 提交镜像\n```\n$ docker push  myregistry.example.com:5000/truman/opentsdb\n```\n然后通过docker images即可查看到push 的镜像\n\n在别的机器中就可以拉取镜像了命令如下：\n```\ndocker pull myregistry.example.com:5000/truman/opentsdb\n```","source":"_posts/docker-registry创建.md","raw":"---\ntitle: docker registry创建\ndate: 2016-10-24 21:26:53\ntags: 虚拟化\ncategories:\n- docker\n---\ncentos6.X版本，docker version1.7.1版本创建过程如下：\n1. 运行容器\n```\n$ docker run -d -p 5000:5000 registry\n```\n2. 修改配置\n\n在docker1.3.X版本以后，与docker registry交互默认使用的是https，此处需要修改为http。在/etc/sysconfig/docker文件中添加以下内容即可：\n```\nother_args=\"$other_args --insecure-registry myregistry.example.com:5000 \"\n```\n在1.12最新版本中可以使用以下方式修改，原理是一致的。\n```\n Create or modify /etc/docker/daemon.json\n{ \"insecure-registries\":[\"myregistry.example.com:5000\"] }\n```\n然后重新启动\n```\n$ service docker restart\n```\n3. 测试新的registry\n- 镜像打标签\n```\n$ docker tag <img_id> myregistry.example.com:5000/truman/opentsdb\n```\n- 提交镜像\n```\n$ docker push  myregistry.example.com:5000/truman/opentsdb\n```\n然后通过docker images即可查看到push 的镜像\n\n在别的机器中就可以拉取镜像了命令如下：\n```\ndocker pull myregistry.example.com:5000/truman/opentsdb\n```","slug":"docker-registry创建","published":1,"updated":"2016-10-24T13:27:36.912Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcn004k8ceemgtq3ieo","content":"<p>centos6.X版本，docker version1.7.1版本创建过程如下：</p>\n<ol>\n<li><p>运行容器</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 5000:5000 registry</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改配置</p>\n</li>\n</ol>\n<p>在docker1.3.X版本以后，与docker registry交互默认使用的是https，此处需要修改为http。在/etc/sysconfig/docker文件中添加以下内容即可：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">other_args=&quot;$other_args --insecure-registry myregistry.example.com:5000 &quot;</span><br></pre></td></tr></table></figure></p>\n<p>在1.12最新版本中可以使用以下方式修改，原理是一致的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> Create or modify /etc/docker/daemon.json</span><br><span class=\"line\">&#123; &quot;insecure-registries&quot;:[&quot;myregistry.example.com:5000&quot;] &#125;</span><br></pre></td></tr></table></figure></p>\n<p>然后重新启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ service docker restart</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>测试新的registry</li>\n</ol>\n<ul>\n<li><p>镜像打标签</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag &lt;img_id&gt; myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>提交镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push  myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>然后通过docker images即可查看到push 的镜像</p>\n<p>在别的机器中就可以拉取镜像了命令如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>centos6.X版本，docker version1.7.1版本创建过程如下：</p>\n<ol>\n<li><p>运行容器</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -p 5000:5000 registry</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改配置</p>\n</li>\n</ol>\n<p>在docker1.3.X版本以后，与docker registry交互默认使用的是https，此处需要修改为http。在/etc/sysconfig/docker文件中添加以下内容即可：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">other_args=&quot;$other_args --insecure-registry myregistry.example.com:5000 &quot;</span><br></pre></td></tr></table></figure></p>\n<p>在1.12最新版本中可以使用以下方式修改，原理是一致的。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> Create or modify /etc/docker/daemon.json</span><br><span class=\"line\">&#123; &quot;insecure-registries&quot;:[&quot;myregistry.example.com:5000&quot;] &#125;</span><br></pre></td></tr></table></figure></p>\n<p>然后重新启动<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ service docker restart</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>测试新的registry</li>\n</ol>\n<ul>\n<li><p>镜像打标签</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker tag &lt;img_id&gt; myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>提交镜像</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker push  myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>然后通过docker images即可查看到push 的镜像</p>\n<p>在别的机器中就可以拉取镜像了命令如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull myregistry.example.com:5000/truman/opentsdb</span><br></pre></td></tr></table></figure></p>\n"},{"title":"docker命令","date":"2016-10-24T13:09:11.000Z","_content":"1. docker version\n\n查看docker安装版本\n2. docker search \n\n查找opentsdb相关的镜像\n```\n$ docker search opentsdb\n```\n3. docker pull\n\n拉去镜像\n```\n$ docker pull **/**\n```\n4. docker ps/build\n\n查看当前机器运行的docker容器\n\n构建镜像\n```\ndocker build -t=truman/redis:3.0.6 .\n```\n5. docker run\n- **不带参数**\n\n```\n$ docker run ubuntu /bin/echo 'Hello world'\n```\n\n参数 |解释\n---|---\ndocker |告诉操作系统我们要使用docker应用\ndocker run|组合起来意思就是运行一个docker镜像\nubuntu|镜像(image)名称\n/bin/echo 'Hello world'|告诉docker我们要在容器中执行的操作\n之后我们就可以看到输出结果：Hello world\n- **带参**\n```\n$ docker run -t -i ubuntu /bin/bash\n$ docker run -d -p 127.0.0.1:80:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker\n```\n\n参数 |解释\n---|---\n-t|为这个容器分配一个虚拟的终端\n-i|保持对于容器的stdin为打开的状态（输入）。\n-d|让docker容器在后台中运行\n-p|将docker容器内部端口映射到我们的host上面，我们可以使用 docker port CONTAINER_ID 来查询容器的端口 映射情况\n一般情况下 -i 与 -t 参数都是结合在一起使用，这样交互会比较好一点。\n\n- **镜像运行传参**\n\n这个参数是在容器生成的时候传入的，例如：指定hosts\n```\ndocker run -d -p 4244:4242 --name opentsdb5 --add-host lab1:192.168.0.101 --add-host lab2:192.168.0.102 --add-host lab3:192.168.0.103 truman/opentsdb  \n```\n都是在镜像名字之前传入的，可以写多个\n6. docher start/stop/restart\n\n该命令可以操作容器\n7. docker rmi\n\n强制删除镜像\n```\n$ docker rmi -f <img_id>\n```\n8. docker logs\n\n在容器以守护进程运行的过程中，可以通过docker logs命令查看log日志，具体用法如下：\n```\n$ docker logs -ft <img_id>\n```\n以终端模式查看最新log。还有其他命令：docker logs --tail 10 <img_id>获取日志最后10行内容，也可以使用 docker logs --tail 0 -f <img_id>跟踪最新日志\n9. 更多命令\n```\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders from a container's filesystem to the host path\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Stream the contents of a container as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Create a new filesystem image from the contents of a tarball\n    info      Display system-wide information\n    inspect   Return low-level information on a container or image\n    kill      Kill a running container\n    load      Load an image from a tar archive\n    login     Register or log in to a Docker registry server\n    logout    Log out from a Docker registry server\n    logs      Fetch the logs of a container\n    pause     Pause all processes within a container\n    port      Lookup the public-facing port that is NAT-ed to PRIVATE_PORT\n    ps        List containers\n    pull      Pull an image or a repository from a Docker registry server\n    push      Push an image or a repository to a Docker registry server\n    rename    Rename an existing container\n    restart   Restart a running container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save an image to a tar archive\n    search    Search for an image on the Docker Hub\n    start     Start a stopped container\n    stats     Display a stream of a containers' resource usage statistics\n    stop      Stop a running container\n    tag       Tag an image into a repository\n    top       Lookup the running processes of a container\n    unpause   Unpause a paused container\n    version   Show the Docker version information\n    wait      Block until a container stops, then print its exit code\n```\n","source":"_posts/docker命令.md","raw":"---\ntitle: docker命令\ndate: 2016-10-24 21:09:11\ntags: 虚拟化\ncategories:\n- docker\n---\n1. docker version\n\n查看docker安装版本\n2. docker search \n\n查找opentsdb相关的镜像\n```\n$ docker search opentsdb\n```\n3. docker pull\n\n拉去镜像\n```\n$ docker pull **/**\n```\n4. docker ps/build\n\n查看当前机器运行的docker容器\n\n构建镜像\n```\ndocker build -t=truman/redis:3.0.6 .\n```\n5. docker run\n- **不带参数**\n\n```\n$ docker run ubuntu /bin/echo 'Hello world'\n```\n\n参数 |解释\n---|---\ndocker |告诉操作系统我们要使用docker应用\ndocker run|组合起来意思就是运行一个docker镜像\nubuntu|镜像(image)名称\n/bin/echo 'Hello world'|告诉docker我们要在容器中执行的操作\n之后我们就可以看到输出结果：Hello world\n- **带参**\n```\n$ docker run -t -i ubuntu /bin/bash\n$ docker run -d -p 127.0.0.1:80:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker\n```\n\n参数 |解释\n---|---\n-t|为这个容器分配一个虚拟的终端\n-i|保持对于容器的stdin为打开的状态（输入）。\n-d|让docker容器在后台中运行\n-p|将docker容器内部端口映射到我们的host上面，我们可以使用 docker port CONTAINER_ID 来查询容器的端口 映射情况\n一般情况下 -i 与 -t 参数都是结合在一起使用，这样交互会比较好一点。\n\n- **镜像运行传参**\n\n这个参数是在容器生成的时候传入的，例如：指定hosts\n```\ndocker run -d -p 4244:4242 --name opentsdb5 --add-host lab1:192.168.0.101 --add-host lab2:192.168.0.102 --add-host lab3:192.168.0.103 truman/opentsdb  \n```\n都是在镜像名字之前传入的，可以写多个\n6. docher start/stop/restart\n\n该命令可以操作容器\n7. docker rmi\n\n强制删除镜像\n```\n$ docker rmi -f <img_id>\n```\n8. docker logs\n\n在容器以守护进程运行的过程中，可以通过docker logs命令查看log日志，具体用法如下：\n```\n$ docker logs -ft <img_id>\n```\n以终端模式查看最新log。还有其他命令：docker logs --tail 10 <img_id>获取日志最后10行内容，也可以使用 docker logs --tail 0 -f <img_id>跟踪最新日志\n9. 更多命令\n```\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders from a container's filesystem to the host path\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Stream the contents of a container as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Create a new filesystem image from the contents of a tarball\n    info      Display system-wide information\n    inspect   Return low-level information on a container or image\n    kill      Kill a running container\n    load      Load an image from a tar archive\n    login     Register or log in to a Docker registry server\n    logout    Log out from a Docker registry server\n    logs      Fetch the logs of a container\n    pause     Pause all processes within a container\n    port      Lookup the public-facing port that is NAT-ed to PRIVATE_PORT\n    ps        List containers\n    pull      Pull an image or a repository from a Docker registry server\n    push      Push an image or a repository to a Docker registry server\n    rename    Rename an existing container\n    restart   Restart a running container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save an image to a tar archive\n    search    Search for an image on the Docker Hub\n    start     Start a stopped container\n    stats     Display a stream of a containers' resource usage statistics\n    stop      Stop a running container\n    tag       Tag an image into a repository\n    top       Lookup the running processes of a container\n    unpause   Unpause a paused container\n    version   Show the Docker version information\n    wait      Block until a container stops, then print its exit code\n```\n","slug":"docker命令","published":1,"updated":"2019-08-22T12:31:46.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcp004n8ceerxfm0crh","content":"<ol>\n<li>docker version</li>\n</ol>\n<p>查看docker安装版本</p>\n<ol start=\"2\">\n<li>docker search </li>\n</ol>\n<p>查找opentsdb相关的镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker search opentsdb</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>docker pull</li>\n</ol>\n<p>拉去镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull **/**</span><br></pre></td></tr></table></figure></p>\n<ol start=\"4\">\n<li>docker ps/build</li>\n</ol>\n<p>查看当前机器运行的docker容器</p>\n<p>构建镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build -t=truman/redis:3.0.6 .</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li>docker run</li>\n</ol>\n<ul>\n<li><strong>不带参数</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run ubuntu /bin/echo &apos;Hello world&apos;</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>docker</td>\n<td>告诉操作系统我们要使用docker应用</td>\n</tr>\n<tr>\n<td>docker run</td>\n<td>组合起来意思就是运行一个docker镜像</td>\n</tr>\n<tr>\n<td>ubuntu</td>\n<td>镜像(image)名称</td>\n</tr>\n<tr>\n<td>/bin/echo ‘Hello world’</td>\n<td>告诉docker我们要在容器中执行的操作</td>\n</tr>\n</tbody>\n</table>\n<p>之后我们就可以看到输出结果：Hello world</p>\n<ul>\n<li><strong>带参</strong><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -t -i ubuntu /bin/bash</span><br><span class=\"line\">$ docker run -d -p 127.0.0.1:80:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-t</td>\n<td>为这个容器分配一个虚拟的终端</td>\n</tr>\n<tr>\n<td>-i</td>\n<td>保持对于容器的stdin为打开的状态（输入）。</td>\n</tr>\n<tr>\n<td>-d</td>\n<td>让docker容器在后台中运行</td>\n</tr>\n<tr>\n<td>-p</td>\n<td>将docker容器内部端口映射到我们的host上面，我们可以使用 docker port CONTAINER_ID 来查询容器的端口 映射情况</td>\n</tr>\n</tbody>\n</table>\n<p>一般情况下 -i 与 -t 参数都是结合在一起使用，这样交互会比较好一点。</p>\n<ul>\n<li><strong>镜像运行传参</strong></li>\n</ul>\n<p>这个参数是在容器生成的时候传入的，例如：指定hosts<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -p 4244:4242 --name opentsdb5 --add-host lab1:192.168.0.101 --add-host lab2:192.168.0.102 --add-host lab3:192.168.0.103 truman/opentsdb</span><br></pre></td></tr></table></figure></p>\n<p>都是在镜像名字之前传入的，可以写多个</p>\n<ol start=\"6\">\n<li>docher start/stop/restart</li>\n</ol>\n<p>该命令可以操作容器</p>\n<ol start=\"7\">\n<li>docker rmi</li>\n</ol>\n<p>强制删除镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker rmi -f &lt;img_id&gt;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"8\">\n<li>docker logs</li>\n</ol>\n<p>在容器以守护进程运行的过程中，可以通过docker logs命令查看log日志，具体用法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker logs -ft &lt;img_id&gt;</span><br></pre></td></tr></table></figure></p>\n<p>以终端模式查看最新log。还有其他命令：docker logs –tail 10 &lt;img_id&gt;获取日志最后10行内容，也可以使用 docker logs –tail 0 -f &lt;img_id&gt;跟踪最新日志</p>\n<ol start=\"9\">\n<li>更多命令<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Commands:</span><br><span class=\"line\">    attach    Attach to a running container</span><br><span class=\"line\">    build     Build an image from a Dockerfile</span><br><span class=\"line\">    commit    Create a new image from a container&apos;s changes</span><br><span class=\"line\">    cp        Copy files/folders from a container&apos;s filesystem to the host path</span><br><span class=\"line\">    create    Create a new container</span><br><span class=\"line\">    diff      Inspect changes on a container&apos;s filesystem</span><br><span class=\"line\">    events    Get real time events from the server</span><br><span class=\"line\">    exec      Run a command in a running container</span><br><span class=\"line\">    export    Stream the contents of a container as a tar archive</span><br><span class=\"line\">    history   Show the history of an image</span><br><span class=\"line\">    images    List images</span><br><span class=\"line\">    import    Create a new filesystem image from the contents of a tarball</span><br><span class=\"line\">    info      Display system-wide information</span><br><span class=\"line\">    inspect   Return low-level information on a container or image</span><br><span class=\"line\">    kill      Kill a running container</span><br><span class=\"line\">    load      Load an image from a tar archive</span><br><span class=\"line\">    login     Register or log in to a Docker registry server</span><br><span class=\"line\">    logout    Log out from a Docker registry server</span><br><span class=\"line\">    logs      Fetch the logs of a container</span><br><span class=\"line\">    pause     Pause all processes within a container</span><br><span class=\"line\">    port      Lookup the public-facing port that is NAT-ed to PRIVATE_PORT</span><br><span class=\"line\">    ps        List containers</span><br><span class=\"line\">    pull      Pull an image or a repository from a Docker registry server</span><br><span class=\"line\">    push      Push an image or a repository to a Docker registry server</span><br><span class=\"line\">    rename    Rename an existing container</span><br><span class=\"line\">    restart   Restart a running container</span><br><span class=\"line\">    rm        Remove one or more containers</span><br><span class=\"line\">    rmi       Remove one or more images</span><br><span class=\"line\">    run       Run a command in a new container</span><br><span class=\"line\">    save      Save an image to a tar archive</span><br><span class=\"line\">    search    Search for an image on the Docker Hub</span><br><span class=\"line\">    start     Start a stopped container</span><br><span class=\"line\">    stats     Display a stream of a containers&apos; resource usage statistics</span><br><span class=\"line\">    stop      Stop a running container</span><br><span class=\"line\">    tag       Tag an image into a repository</span><br><span class=\"line\">    top       Lookup the running processes of a container</span><br><span class=\"line\">    unpause   Unpause a paused container</span><br><span class=\"line\">    version   Show the Docker version information</span><br><span class=\"line\">    wait      Block until a container stops, then print its exit code</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<ol>\n<li>docker version</li>\n</ol>\n<p>查看docker安装版本</p>\n<ol start=\"2\">\n<li>docker search </li>\n</ol>\n<p>查找opentsdb相关的镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker search opentsdb</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>docker pull</li>\n</ol>\n<p>拉去镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker pull **/**</span><br></pre></td></tr></table></figure></p>\n<ol start=\"4\">\n<li>docker ps/build</li>\n</ol>\n<p>查看当前机器运行的docker容器</p>\n<p>构建镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build -t=truman/redis:3.0.6 .</span><br></pre></td></tr></table></figure></p>\n<ol start=\"5\">\n<li>docker run</li>\n</ol>\n<ul>\n<li><strong>不带参数</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run ubuntu /bin/echo &apos;Hello world&apos;</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>docker</td>\n<td>告诉操作系统我们要使用docker应用</td>\n</tr>\n<tr>\n<td>docker run</td>\n<td>组合起来意思就是运行一个docker镜像</td>\n</tr>\n<tr>\n<td>ubuntu</td>\n<td>镜像(image)名称</td>\n</tr>\n<tr>\n<td>/bin/echo ‘Hello world’</td>\n<td>告诉docker我们要在容器中执行的操作</td>\n</tr>\n</tbody>\n</table>\n<p>之后我们就可以看到输出结果：Hello world</p>\n<ul>\n<li><strong>带参</strong><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -t -i ubuntu /bin/bash</span><br><span class=\"line\">$ docker run -d -p 127.0.0.1:80:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-t</td>\n<td>为这个容器分配一个虚拟的终端</td>\n</tr>\n<tr>\n<td>-i</td>\n<td>保持对于容器的stdin为打开的状态（输入）。</td>\n</tr>\n<tr>\n<td>-d</td>\n<td>让docker容器在后台中运行</td>\n</tr>\n<tr>\n<td>-p</td>\n<td>将docker容器内部端口映射到我们的host上面，我们可以使用 docker port CONTAINER_ID 来查询容器的端口 映射情况</td>\n</tr>\n</tbody>\n</table>\n<p>一般情况下 -i 与 -t 参数都是结合在一起使用，这样交互会比较好一点。</p>\n<ul>\n<li><strong>镜像运行传参</strong></li>\n</ul>\n<p>这个参数是在容器生成的时候传入的，例如：指定hosts<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -d -p 4244:4242 --name opentsdb5 --add-host lab1:192.168.0.101 --add-host lab2:192.168.0.102 --add-host lab3:192.168.0.103 truman/opentsdb</span><br></pre></td></tr></table></figure></p>\n<p>都是在镜像名字之前传入的，可以写多个</p>\n<ol start=\"6\">\n<li>docher start/stop/restart</li>\n</ol>\n<p>该命令可以操作容器</p>\n<ol start=\"7\">\n<li>docker rmi</li>\n</ol>\n<p>强制删除镜像<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker rmi -f &lt;img_id&gt;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"8\">\n<li>docker logs</li>\n</ol>\n<p>在容器以守护进程运行的过程中，可以通过docker logs命令查看log日志，具体用法如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker logs -ft &lt;img_id&gt;</span><br></pre></td></tr></table></figure></p>\n<p>以终端模式查看最新log。还有其他命令：docker logs –tail 10 &lt;img_id&gt;获取日志最后10行内容，也可以使用 docker logs –tail 0 -f &lt;img_id&gt;跟踪最新日志</p>\n<ol start=\"9\">\n<li>更多命令<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Commands:</span><br><span class=\"line\">    attach    Attach to a running container</span><br><span class=\"line\">    build     Build an image from a Dockerfile</span><br><span class=\"line\">    commit    Create a new image from a container&apos;s changes</span><br><span class=\"line\">    cp        Copy files/folders from a container&apos;s filesystem to the host path</span><br><span class=\"line\">    create    Create a new container</span><br><span class=\"line\">    diff      Inspect changes on a container&apos;s filesystem</span><br><span class=\"line\">    events    Get real time events from the server</span><br><span class=\"line\">    exec      Run a command in a running container</span><br><span class=\"line\">    export    Stream the contents of a container as a tar archive</span><br><span class=\"line\">    history   Show the history of an image</span><br><span class=\"line\">    images    List images</span><br><span class=\"line\">    import    Create a new filesystem image from the contents of a tarball</span><br><span class=\"line\">    info      Display system-wide information</span><br><span class=\"line\">    inspect   Return low-level information on a container or image</span><br><span class=\"line\">    kill      Kill a running container</span><br><span class=\"line\">    load      Load an image from a tar archive</span><br><span class=\"line\">    login     Register or log in to a Docker registry server</span><br><span class=\"line\">    logout    Log out from a Docker registry server</span><br><span class=\"line\">    logs      Fetch the logs of a container</span><br><span class=\"line\">    pause     Pause all processes within a container</span><br><span class=\"line\">    port      Lookup the public-facing port that is NAT-ed to PRIVATE_PORT</span><br><span class=\"line\">    ps        List containers</span><br><span class=\"line\">    pull      Pull an image or a repository from a Docker registry server</span><br><span class=\"line\">    push      Push an image or a repository to a Docker registry server</span><br><span class=\"line\">    rename    Rename an existing container</span><br><span class=\"line\">    restart   Restart a running container</span><br><span class=\"line\">    rm        Remove one or more containers</span><br><span class=\"line\">    rmi       Remove one or more images</span><br><span class=\"line\">    run       Run a command in a new container</span><br><span class=\"line\">    save      Save an image to a tar archive</span><br><span class=\"line\">    search    Search for an image on the Docker Hub</span><br><span class=\"line\">    start     Start a stopped container</span><br><span class=\"line\">    stats     Display a stream of a containers&apos; resource usage statistics</span><br><span class=\"line\">    stop      Stop a running container</span><br><span class=\"line\">    tag       Tag an image into a repository</span><br><span class=\"line\">    top       Lookup the running processes of a container</span><br><span class=\"line\">    unpause   Unpause a paused container</span><br><span class=\"line\">    version   Show the Docker version information</span><br><span class=\"line\">    wait      Block until a container stops, then print its exit code</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n"},{"title":"eclipse marketplace无法安装解决","date":"2018-01-15T11:21:04.000Z","_content":"# eclipse marketplace无法安装解决\n\n## 问题简介\n\n最近看到eclipse发布了oxygen,心里就痒痒了，想着升级一下新版本，体验一下新功能，下载以后想着安装一些生产力工具，但是发现竟然无法安装，要求我配置代理信息，并且报以下错误：\n```\nProxy Authentication Required Error\n```\n 心里一下子就有一千万个草泥马，再返回之前4.6版本，发现，这个版本竟然可以安装。。。。\n \n ## 解决方案\n \n 经过千辛万苦的google，别问我问啥没用baidu,因为没有卵用。\n \n 终于在程序员神器网站stackoverflow找到解决方案。\n 在eclipse.ini中增加以下内容：\n ```\n -Dorg.eclipse.ecf.provider.filetransfer.excludeContributors=org.eclipse.ecf.provider.filetransfer.httpclient4\n ```\n \n 然后就可以欢快的下载各种插件了。\n \n \n ## 参考\n \n [stackoverflow](https://stackoverflow.com/questions/39576290/eclipse-neon-http-proxy-authentication-required-error)","source":"_posts/eclipse-marketplace无法安装解决.md","raw":"---\ntitle: eclipse marketplace无法安装解决\ndate: 2018-01-15 19:21:04\ntags: 问题答疑\ncategories:\n- 开发工具\n---\n# eclipse marketplace无法安装解决\n\n## 问题简介\n\n最近看到eclipse发布了oxygen,心里就痒痒了，想着升级一下新版本，体验一下新功能，下载以后想着安装一些生产力工具，但是发现竟然无法安装，要求我配置代理信息，并且报以下错误：\n```\nProxy Authentication Required Error\n```\n 心里一下子就有一千万个草泥马，再返回之前4.6版本，发现，这个版本竟然可以安装。。。。\n \n ## 解决方案\n \n 经过千辛万苦的google，别问我问啥没用baidu,因为没有卵用。\n \n 终于在程序员神器网站stackoverflow找到解决方案。\n 在eclipse.ini中增加以下内容：\n ```\n -Dorg.eclipse.ecf.provider.filetransfer.excludeContributors=org.eclipse.ecf.provider.filetransfer.httpclient4\n ```\n \n 然后就可以欢快的下载各种插件了。\n \n \n ## 参考\n \n [stackoverflow](https://stackoverflow.com/questions/39576290/eclipse-neon-http-proxy-authentication-required-error)","slug":"eclipse-marketplace无法安装解决","published":1,"updated":"2018-01-15T11:22:41.032Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcr004r8cee83ie48du","content":"<h1 id=\"eclipse-marketplace无法安装解决\"><a href=\"#eclipse-marketplace无法安装解决\" class=\"headerlink\" title=\"eclipse marketplace无法安装解决\"></a>eclipse marketplace无法安装解决</h1><h2 id=\"问题简介\"><a href=\"#问题简介\" class=\"headerlink\" title=\"问题简介\"></a>问题简介</h2><p>最近看到eclipse发布了oxygen,心里就痒痒了，想着升级一下新版本，体验一下新功能，下载以后想着安装一些生产力工具，但是发现竟然无法安装，要求我配置代理信息，并且报以下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Proxy Authentication Required Error</span><br></pre></td></tr></table></figure></p>\n<p> 心里一下子就有一千万个草泥马，再返回之前4.6版本，发现，这个版本竟然可以安装。。。。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p> 经过千辛万苦的google，别问我问啥没用baidu,因为没有卵用。</p>\n<p> 终于在程序员神器网站stackoverflow找到解决方案。<br> 在eclipse.ini中增加以下内容：<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-Dorg.eclipse.ecf.provider.filetransfer.excludeContributors=org.eclipse.ecf.provider.filetransfer.httpclient4</span><br></pre></td></tr></table></figure></p>\n<p> 然后就可以欢快的下载各种插件了。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p> <a href=\"https://stackoverflow.com/questions/39576290/eclipse-neon-http-proxy-authentication-required-error\" target=\"_blank\" rel=\"noopener\">stackoverflow</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"eclipse-marketplace无法安装解决\"><a href=\"#eclipse-marketplace无法安装解决\" class=\"headerlink\" title=\"eclipse marketplace无法安装解决\"></a>eclipse marketplace无法安装解决</h1><h2 id=\"问题简介\"><a href=\"#问题简介\" class=\"headerlink\" title=\"问题简介\"></a>问题简介</h2><p>最近看到eclipse发布了oxygen,心里就痒痒了，想着升级一下新版本，体验一下新功能，下载以后想着安装一些生产力工具，但是发现竟然无法安装，要求我配置代理信息，并且报以下错误：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Proxy Authentication Required Error</span><br></pre></td></tr></table></figure></p>\n<p> 心里一下子就有一千万个草泥马，再返回之前4.6版本，发现，这个版本竟然可以安装。。。。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><p> 经过千辛万苦的google，别问我问啥没用baidu,因为没有卵用。</p>\n<p> 终于在程序员神器网站stackoverflow找到解决方案。<br> 在eclipse.ini中增加以下内容：<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-Dorg.eclipse.ecf.provider.filetransfer.excludeContributors=org.eclipse.ecf.provider.filetransfer.httpclient4</span><br></pre></td></tr></table></figure></p>\n<p> 然后就可以欢快的下载各种插件了。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p> <a href=\"https://stackoverflow.com/questions/39576290/eclipse-neon-http-proxy-authentication-required-error\" target=\"_blank\" rel=\"noopener\">stackoverflow</a></p>\n"},{"title":"eureka获取注册服务","date":"2018-07-22T10:47:55.000Z","_content":"# eureka获取注册服务\n\n## eureka client获取注册服务\n\n首先在启动类中增加```@EnableDiscoveryClient```\n\n其次在spring 项目中注入\n```\t\n@Autowired\n\tprivate DiscoveryClient discoveryClient;\n```\n\n然后通过以下方式进行获取\n```\nList<ServiceInstance> list = discoveryClient.getInstances(\"analyze\");\n```\nanalyze为注册服务名\n\n\n\n## eureka server获取注册服务\n1. 方法1\n\n首先在配置文件中增加\n```\neureka.client.fetch-registry=true\n```\n\n其次在spring 项目中注入\n```\t\n@Autowired\n\tprivate DiscoveryClient discoveryClient;\n```\n\n然后通过以下方式进行获取\n```\nList<ServiceInstance> list = discoveryClient.getInstances(\"analyze\");\n```\nanalyze为注册服务名\n\n这种方法有一定的延迟，原理和Client一样，如果需要及时更新那么需要配置一下其他参数，及时更新注册信息.\n\nserver\n```\n#清理时间间隔\neureka.server.eviction-interval-timer-in-ms=10000\n#关闭自我保护模式。自我保护模式是指，出现网络分区、eureka在短时间内丢失过多客户端时，会进入自我保护模式。\n#自我保护：一个服务长时间没有发送心跳包，eureka也不会将其删除，默认为true。\neureka.server.enable-self-preservation=false\n```\nclient\n```\n#发送时间间隔\neureka.instance.lease-renewal-interval-in-seconds=10\n#多长时间过期\neureka.instance.lease-expiration-duration-in-seconds=30\n```\n2. 方法2\n\n如果需要在server中获取，强烈建议使用这种方式，优点就是和dashboard状态保持一致。\n```\nPeerAwareInstanceRegistry registry = EurekaServerContextHolder.getInstance().getServerContext().getRegistry();\n    Applications applications = registry.getApplications();\n\n    applications.getRegisteredApplications().forEach((registeredApplication) -> {\n        registeredApplication.getInstances().forEach((instance) -> {\n            System.out.println(instance.getAppName() + \" (\" + instance.getInstanceId() + \") : \" + response);\n        });\n    });\n```\n\n## 参考\n1. [Spring Cloud中文文档](https://springcloud.cc/spring-cloud-dalston.html#netflix-eureka-server-starter)\n2. [Spring Cloud doc](https://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html)\n3. [Eureka Server - list all registered instances](https://stackoverflow.com/questions/42427492/eureka-server-list-all-registered-instances)","source":"_posts/eureka获取注册服务.md","raw":"---\ntitle: eureka获取注册服务\ndate: 2018-07-22 18:47:55\ntags: 开发笔记\ncategories:\n         - springcloud\n         - java\n         - 分布式框架\n---\n# eureka获取注册服务\n\n## eureka client获取注册服务\n\n首先在启动类中增加```@EnableDiscoveryClient```\n\n其次在spring 项目中注入\n```\t\n@Autowired\n\tprivate DiscoveryClient discoveryClient;\n```\n\n然后通过以下方式进行获取\n```\nList<ServiceInstance> list = discoveryClient.getInstances(\"analyze\");\n```\nanalyze为注册服务名\n\n\n\n## eureka server获取注册服务\n1. 方法1\n\n首先在配置文件中增加\n```\neureka.client.fetch-registry=true\n```\n\n其次在spring 项目中注入\n```\t\n@Autowired\n\tprivate DiscoveryClient discoveryClient;\n```\n\n然后通过以下方式进行获取\n```\nList<ServiceInstance> list = discoveryClient.getInstances(\"analyze\");\n```\nanalyze为注册服务名\n\n这种方法有一定的延迟，原理和Client一样，如果需要及时更新那么需要配置一下其他参数，及时更新注册信息.\n\nserver\n```\n#清理时间间隔\neureka.server.eviction-interval-timer-in-ms=10000\n#关闭自我保护模式。自我保护模式是指，出现网络分区、eureka在短时间内丢失过多客户端时，会进入自我保护模式。\n#自我保护：一个服务长时间没有发送心跳包，eureka也不会将其删除，默认为true。\neureka.server.enable-self-preservation=false\n```\nclient\n```\n#发送时间间隔\neureka.instance.lease-renewal-interval-in-seconds=10\n#多长时间过期\neureka.instance.lease-expiration-duration-in-seconds=30\n```\n2. 方法2\n\n如果需要在server中获取，强烈建议使用这种方式，优点就是和dashboard状态保持一致。\n```\nPeerAwareInstanceRegistry registry = EurekaServerContextHolder.getInstance().getServerContext().getRegistry();\n    Applications applications = registry.getApplications();\n\n    applications.getRegisteredApplications().forEach((registeredApplication) -> {\n        registeredApplication.getInstances().forEach((instance) -> {\n            System.out.println(instance.getAppName() + \" (\" + instance.getInstanceId() + \") : \" + response);\n        });\n    });\n```\n\n## 参考\n1. [Spring Cloud中文文档](https://springcloud.cc/spring-cloud-dalston.html#netflix-eureka-server-starter)\n2. [Spring Cloud doc](https://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html)\n3. [Eureka Server - list all registered instances](https://stackoverflow.com/questions/42427492/eureka-server-list-all-registered-instances)","slug":"eureka获取注册服务","published":1,"updated":"2018-07-22T11:33:45.275Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvct004u8ceen8figdj1","content":"<h1 id=\"eureka获取注册服务\"><a href=\"#eureka获取注册服务\" class=\"headerlink\" title=\"eureka获取注册服务\"></a>eureka获取注册服务</h1><h2 id=\"eureka-client获取注册服务\"><a href=\"#eureka-client获取注册服务\" class=\"headerlink\" title=\"eureka client获取注册服务\"></a>eureka client获取注册服务</h2><p>首先在启动类中增加<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">其次在spring 项目中注入</span><br><span class=\"line\">```\t</span><br><span class=\"line\">@Autowired</span><br><span class=\"line\">\tprivate DiscoveryClient discoveryClient;</span><br></pre></td></tr></table></figure></p>\n<p>然后通过以下方式进行获取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;analyze&quot;);</span><br></pre></td></tr></table></figure></p>\n<p>analyze为注册服务名</p>\n<h2 id=\"eureka-server获取注册服务\"><a href=\"#eureka-server获取注册服务\" class=\"headerlink\" title=\"eureka server获取注册服务\"></a>eureka server获取注册服务</h2><ol>\n<li>方法1</li>\n</ol>\n<p>首先在配置文件中增加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">eureka.client.fetch-registry=true</span><br></pre></td></tr></table></figure></p>\n<p>其次在spring 项目中注入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Autowired</span><br><span class=\"line\">\tprivate DiscoveryClient discoveryClient;</span><br></pre></td></tr></table></figure></p>\n<p>然后通过以下方式进行获取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;analyze&quot;);</span><br></pre></td></tr></table></figure></p>\n<p>analyze为注册服务名</p>\n<p>这种方法有一定的延迟，原理和Client一样，如果需要及时更新那么需要配置一下其他参数，及时更新注册信息.</p>\n<p>server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#清理时间间隔</span><br><span class=\"line\">eureka.server.eviction-interval-timer-in-ms=10000</span><br><span class=\"line\">#关闭自我保护模式。自我保护模式是指，出现网络分区、eureka在短时间内丢失过多客户端时，会进入自我保护模式。</span><br><span class=\"line\">#自我保护：一个服务长时间没有发送心跳包，eureka也不会将其删除，默认为true。</span><br><span class=\"line\">eureka.server.enable-self-preservation=false</span><br></pre></td></tr></table></figure></p>\n<p>client<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#发送时间间隔</span><br><span class=\"line\">eureka.instance.lease-renewal-interval-in-seconds=10</span><br><span class=\"line\">#多长时间过期</span><br><span class=\"line\">eureka.instance.lease-expiration-duration-in-seconds=30</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>方法2</li>\n</ol>\n<p>如果需要在server中获取，强烈建议使用这种方式，优点就是和dashboard状态保持一致。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PeerAwareInstanceRegistry registry = EurekaServerContextHolder.getInstance().getServerContext().getRegistry();</span><br><span class=\"line\">    Applications applications = registry.getApplications();</span><br><span class=\"line\"></span><br><span class=\"line\">    applications.getRegisteredApplications().forEach((registeredApplication) -&gt; &#123;</span><br><span class=\"line\">        registeredApplication.getInstances().forEach((instance) -&gt; &#123;</span><br><span class=\"line\">            System.out.println(instance.getAppName() + &quot; (&quot; + instance.getInstanceId() + &quot;) : &quot; + response);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://springcloud.cc/spring-cloud-dalston.html#netflix-eureka-server-starter\" target=\"_blank\" rel=\"noopener\">Spring Cloud中文文档</a></li>\n<li><a href=\"https://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud doc</a></li>\n<li><a href=\"https://stackoverflow.com/questions/42427492/eureka-server-list-all-registered-instances\" target=\"_blank\" rel=\"noopener\">Eureka Server - list all registered instances</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"eureka获取注册服务\"><a href=\"#eureka获取注册服务\" class=\"headerlink\" title=\"eureka获取注册服务\"></a>eureka获取注册服务</h1><h2 id=\"eureka-client获取注册服务\"><a href=\"#eureka-client获取注册服务\" class=\"headerlink\" title=\"eureka client获取注册服务\"></a>eureka client获取注册服务</h2><p>首先在启动类中增加<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">其次在spring 项目中注入</span><br><span class=\"line\">```\t</span><br><span class=\"line\">@Autowired</span><br><span class=\"line\">\tprivate DiscoveryClient discoveryClient;</span><br></pre></td></tr></table></figure></p>\n<p>然后通过以下方式进行获取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;analyze&quot;);</span><br></pre></td></tr></table></figure></p>\n<p>analyze为注册服务名</p>\n<h2 id=\"eureka-server获取注册服务\"><a href=\"#eureka-server获取注册服务\" class=\"headerlink\" title=\"eureka server获取注册服务\"></a>eureka server获取注册服务</h2><ol>\n<li>方法1</li>\n</ol>\n<p>首先在配置文件中增加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">eureka.client.fetch-registry=true</span><br></pre></td></tr></table></figure></p>\n<p>其次在spring 项目中注入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@Autowired</span><br><span class=\"line\">\tprivate DiscoveryClient discoveryClient;</span><br></pre></td></tr></table></figure></p>\n<p>然后通过以下方式进行获取<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;ServiceInstance&gt; list = discoveryClient.getInstances(&quot;analyze&quot;);</span><br></pre></td></tr></table></figure></p>\n<p>analyze为注册服务名</p>\n<p>这种方法有一定的延迟，原理和Client一样，如果需要及时更新那么需要配置一下其他参数，及时更新注册信息.</p>\n<p>server<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#清理时间间隔</span><br><span class=\"line\">eureka.server.eviction-interval-timer-in-ms=10000</span><br><span class=\"line\">#关闭自我保护模式。自我保护模式是指，出现网络分区、eureka在短时间内丢失过多客户端时，会进入自我保护模式。</span><br><span class=\"line\">#自我保护：一个服务长时间没有发送心跳包，eureka也不会将其删除，默认为true。</span><br><span class=\"line\">eureka.server.enable-self-preservation=false</span><br></pre></td></tr></table></figure></p>\n<p>client<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#发送时间间隔</span><br><span class=\"line\">eureka.instance.lease-renewal-interval-in-seconds=10</span><br><span class=\"line\">#多长时间过期</span><br><span class=\"line\">eureka.instance.lease-expiration-duration-in-seconds=30</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>方法2</li>\n</ol>\n<p>如果需要在server中获取，强烈建议使用这种方式，优点就是和dashboard状态保持一致。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PeerAwareInstanceRegistry registry = EurekaServerContextHolder.getInstance().getServerContext().getRegistry();</span><br><span class=\"line\">    Applications applications = registry.getApplications();</span><br><span class=\"line\"></span><br><span class=\"line\">    applications.getRegisteredApplications().forEach((registeredApplication) -&gt; &#123;</span><br><span class=\"line\">        registeredApplication.getInstances().forEach((instance) -&gt; &#123;</span><br><span class=\"line\">            System.out.println(instance.getAppName() + &quot; (&quot; + instance.getInstanceId() + &quot;) : &quot; + response);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://springcloud.cc/spring-cloud-dalston.html#netflix-eureka-server-starter\" target=\"_blank\" rel=\"noopener\">Spring Cloud中文文档</a></li>\n<li><a href=\"https://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud doc</a></li>\n<li><a href=\"https://stackoverflow.com/questions/42427492/eureka-server-list-all-registered-instances\" target=\"_blank\" rel=\"noopener\">Eureka Server - list all registered instances</a></li>\n</ol>\n"},{"title":"hbase单机搭建教程","date":"2016-03-19T11:58:41.000Z","_content":"# 搭建hbase单机版\n## 目的\n&emsp;本教程为了搭建一个hbas单机版本，搭建好可以便于练习使用hbase,选用的版本不是最新的，但搭建原理都是一样的。单机版数据存储在本地文件上，使用自带的zookeeper。\n## 准备\n1.[下载](http:http://www.gtlib.gatech.edu/pub/apache/hbase/hbase-0.94.27/)hbase-0.94.27 \n## 安装与部署\n- 安装\n<pre><code>\ntar -xvf hbase-0.94.27.tar.gz \n</code></pre>\n<!--more-->\n- 配置\n1.**编辑conf/hbase-env.sh**\n<pre><code>\nexport JAVA_HOME=/opt/jdk1.8.0_45\nexport HBASE_MANAGES_ZK=true\n</code></pre>\n2.**编辑conf/hbase-site.xml**\n<pre><code>\n<configuration>\n      <property>\n          <name>hbase.rootdir</name>\n          <value>file:///data/truman/hbasedata</value>\n       </property>\n       <property>\n           <name>hbase.tmp.dir</name>\n           <value>/data/truman/hbasedata/log</value>\n       </property>\n       <property>\n           <name>hbase.zookeeper.property.dataDir</name>\n           <value>/data/truman/hbasedata/logs/zookeeper</value>\n      </property>\n</configuration>\n</code></pre>\n## 启动\n\n1.启动\n\n<pre><code>\ncd hbase-0.94.27\nbin/start-hbase.sh\n</code></pre>\n\n2.连接测试\n\n测试成功结果如下：\n<pre><code>\n[root@LAB hbase-0.94.3]# bin/hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nType \"exit<RETURN>\" to leave the HBase Shell\nVersion 0.94.3, r1408904, Wed Nov 14 19:55:11 UTC 2012\nhbase(main):001:0> list\nTABLE\nemp\nt1\n2 row(s) in 1.0670 seconds\nhbase(main):002:0>\n</code></pre>\n\n","source":"_posts/hbase单机搭建教程.md","raw":"---\ntitle: hbase单机搭建教程\ndate: 2016-03-19 19:58:41\ntags: 大数据\ncategories:\n- hbase\n---\n# 搭建hbase单机版\n## 目的\n&emsp;本教程为了搭建一个hbas单机版本，搭建好可以便于练习使用hbase,选用的版本不是最新的，但搭建原理都是一样的。单机版数据存储在本地文件上，使用自带的zookeeper。\n## 准备\n1.[下载](http:http://www.gtlib.gatech.edu/pub/apache/hbase/hbase-0.94.27/)hbase-0.94.27 \n## 安装与部署\n- 安装\n<pre><code>\ntar -xvf hbase-0.94.27.tar.gz \n</code></pre>\n<!--more-->\n- 配置\n1.**编辑conf/hbase-env.sh**\n<pre><code>\nexport JAVA_HOME=/opt/jdk1.8.0_45\nexport HBASE_MANAGES_ZK=true\n</code></pre>\n2.**编辑conf/hbase-site.xml**\n<pre><code>\n<configuration>\n      <property>\n          <name>hbase.rootdir</name>\n          <value>file:///data/truman/hbasedata</value>\n       </property>\n       <property>\n           <name>hbase.tmp.dir</name>\n           <value>/data/truman/hbasedata/log</value>\n       </property>\n       <property>\n           <name>hbase.zookeeper.property.dataDir</name>\n           <value>/data/truman/hbasedata/logs/zookeeper</value>\n      </property>\n</configuration>\n</code></pre>\n## 启动\n\n1.启动\n\n<pre><code>\ncd hbase-0.94.27\nbin/start-hbase.sh\n</code></pre>\n\n2.连接测试\n\n测试成功结果如下：\n<pre><code>\n[root@LAB hbase-0.94.3]# bin/hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nType \"exit<RETURN>\" to leave the HBase Shell\nVersion 0.94.3, r1408904, Wed Nov 14 19:55:11 UTC 2012\nhbase(main):001:0> list\nTABLE\nemp\nt1\n2 row(s) in 1.0670 seconds\nhbase(main):002:0>\n</code></pre>\n\n","slug":"hbase单机搭建教程","published":1,"updated":"2019-08-22T12:37:53.050Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcw004y8cee56q7f8yq","content":"<h1 id=\"搭建hbase单机版\"><a href=\"#搭建hbase单机版\" class=\"headerlink\" title=\"搭建hbase单机版\"></a>搭建hbase单机版</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>&emsp;本教程为了搭建一个hbas单机版本，搭建好可以便于练习使用hbase,选用的版本不是最新的，但搭建原理都是一样的。单机版数据存储在本地文件上，使用自带的zookeeper。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>1.<a href=\"http:http://www.gtlib.gatech.edu/pub/apache/hbase/hbase-0.94.27/\" target=\"_blank\" rel=\"noopener\">下载</a>hbase-0.94.27 </p>\n<h2 id=\"安装与部署\"><a href=\"#安装与部署\" class=\"headerlink\" title=\"安装与部署\"></a>安装与部署</h2><ul>\n<li>安装<br><pre><code><br>tar -xvf hbase-0.94.27.tar.gz<br></code></pre><a id=\"more\"></a></li>\n<li>配置<br>1.<strong>编辑conf/hbase-env.sh</strong><br><pre><code><br>export JAVA_HOME=/opt/jdk1.8.0_45<br>export HBASE_MANAGES_ZK=true<br></code></pre><br>2.<strong>编辑conf/hbase-site.xml</strong><br><pre><code><br><configuration><pre><code>&lt;property&gt;\n    &lt;name&gt;hbase.rootdir&lt;/name&gt;\n    &lt;value&gt;file:///data/truman/hbasedata&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n     &lt;name&gt;hbase.tmp.dir&lt;/name&gt;\n     &lt;value&gt;/data/truman/hbasedata/log&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n     &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n     &lt;value&gt;/data/truman/hbasedata/logs/zookeeper&lt;/value&gt;\n&lt;/property&gt;\n</code></pre></configuration><br></code></pre><h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2></li>\n</ul>\n<p>1.启动</p>\n<pre><code>\ncd hbase-0.94.27\nbin/start-hbase.sh\n</code></pre>\n\n<p>2.连接测试</p>\n<p>测试成功结果如下：</p>\n<pre><code>\n[root@LAB hbase-0.94.3]# bin/hbase shell\nHBase Shell; enter 'help<return>' for list of supported commands.\nType \"exit<return>\" to leave the HBase Shell\nVersion 0.94.3, r1408904, Wed Nov 14 19:55:11 UTC 2012\nhbase(main):001:0> list\nTABLE\nemp\nt1\n2 row(s) in 1.0670 seconds\nhbase(main):002:0>\n</return></return></code></pre>\n\n","site":{"data":{}},"excerpt":"<h1 id=\"搭建hbase单机版\"><a href=\"#搭建hbase单机版\" class=\"headerlink\" title=\"搭建hbase单机版\"></a>搭建hbase单机版</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>&emsp;本教程为了搭建一个hbas单机版本，搭建好可以便于练习使用hbase,选用的版本不是最新的，但搭建原理都是一样的。单机版数据存储在本地文件上，使用自带的zookeeper。</p>\n<h2 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h2><p>1.<a href=\"http:http://www.gtlib.gatech.edu/pub/apache/hbase/hbase-0.94.27/\" target=\"_blank\" rel=\"noopener\">下载</a>hbase-0.94.27 </p>\n<h2 id=\"安装与部署\"><a href=\"#安装与部署\" class=\"headerlink\" title=\"安装与部署\"></a>安装与部署</h2><ul>\n<li>安装<br><pre><code><br>tar -xvf hbase-0.94.27.tar.gz<br></code></pre>","more":"</li>\n<li>配置<br>1.<strong>编辑conf/hbase-env.sh</strong><br><pre><code><br>export JAVA_HOME=/opt/jdk1.8.0_45<br>export HBASE_MANAGES_ZK=true<br></code></pre><br>2.<strong>编辑conf/hbase-site.xml</strong><br><pre><code><br><configuration><pre><code>&lt;property&gt;\n    &lt;name&gt;hbase.rootdir&lt;/name&gt;\n    &lt;value&gt;file:///data/truman/hbasedata&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n     &lt;name&gt;hbase.tmp.dir&lt;/name&gt;\n     &lt;value&gt;/data/truman/hbasedata/log&lt;/value&gt;\n &lt;/property&gt;\n &lt;property&gt;\n     &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;\n     &lt;value&gt;/data/truman/hbasedata/logs/zookeeper&lt;/value&gt;\n&lt;/property&gt;\n</code></pre></configuration><br></code></pre><h2 id=\"启动\"><a href=\"#启动\" class=\"headerlink\" title=\"启动\"></a>启动</h2></li>\n</ul>\n<p>1.启动</p>\n<pre><code>\ncd hbase-0.94.27\nbin/start-hbase.sh\n</code></pre>\n\n<p>2.连接测试</p>\n<p>测试成功结果如下：</p>\n<pre><code>\n[root@LAB hbase-0.94.3]# bin/hbase shell\nHBase Shell; enter 'help<return>' for list of supported commands.\nType \"exit<return>\" to leave the HBase Shell\nVersion 0.94.3, r1408904, Wed Nov 14 19:55:11 UTC 2012\nhbase(main):001:0> list\nTABLE\nemp\nt1\n2 row(s) in 1.0670 seconds\nhbase(main):002:0>\n</return></return></code></pre>"},{"title":"git reset、git checkout和git revert","date":"2018-07-22T10:42:36.000Z","_content":"\n### 命令场景总结\n\n命令|作用域\t|常用情景\n---|---|--\ngit reset|提交层面|在私有分支上舍弃一些没有提交的更改\ngit reset|文件层面|将文件从缓存区中移除\ngit checkout|提交层面|切换分支或查看旧版本\ngit checkout|文件层面|舍弃工作目录中的更改\ngit revert|\t提交层面|在公共分支上回滚更改\n\n### 详细解释\n\n1. git checkout\n> 一般有两种场景：切换分支，舍弃工作目录中的更改\n\n例如：\n```\ngit checkout brach2\ngit checkout -- src/cli/serve/serve.js\n```\n2. git revert\n>撤销一个提交的同时会创建一个新的提交。这是一个安全的方法，因为它不会重写提交历史。比如，下面的命令会找出倒数第二个提交，然后创建一个新的提交来撤销这些更改，然后把这个提交加入项目中。\n\n```\ngit checkout hotfix\ngit revert HEAD~2\n```\n3. git reset\n>如果你的更改还没有共享给别人，git reset是撤销这些更改的简单方法。当你开发一个功能的时候发现『糟糕，我做了什么？我应该重新来过！』时，reset就像是go-to命令一样。\n除了在当前分支上操作，\n\n\n- 提交层面\n\nreset将一个分支的末端指向另一个提交。这可以用来移除当前分支的一些提交。你传入HEAD以外的其他提交的时候要格外小心，因为reset操作会重写当前分支的历史\n```\ngit checkout hotfix\ngit reset HEAD~2\n```\n可以通过传入这些标记来修改你的缓存区或工作目录：\n\n--soft – 缓存区和工作目录都不会被改变\n\n--mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响\n\n--hard – 缓存区和工作目录都同步到你指定的提交\n- 文件层面\n\n```\ngit reset HEAD~2 foo.py\n```\n会将当前的foo.py从缓存区中移除出去，而不会影响工作目录中对foo.py的更改。--soft、--mixed和--hard对文件层面的git reset毫无作用，因为缓存区中的文件一定会变化，而工作目录中的文件一定不变。\n\n\n\n### 总结\n\n一般我们开发中常用以下两个恢复文件\n\n1. git checkout\n```\ngit checkout -- src/cli/serve/serve.js\n```\n这条命令把serve.js从HEAD中签出并且把它恢复成未修改时的样子\n2. git reset --hard HEAD\n```\ngit reset --hard HEAD\n```\n这条命令会把你工作目录中所有未提交的内容清空(当然这不包括未置于版控制下的文件 untracked files).让工作目录回到上次提交时的状态(last committed state)\n3. git revert\n撤消(revert)了前期修改的提交(commit)是很容易的; 只要把出错的提交(commit)的名字(reference)做为参数传给命令\n```\ngit revert HEAD\n\n```\n这条命令创建了一个撤消了上次提交(HEAD)的新提交\n","source":"_posts/git-reset、git-checkout和git-revert.md","raw":"---\ntitle: git reset、git checkout和git revert\ndate: 2018-07-22 18:42:36\ntags: 开发笔记\ncategories:\n- git\n---\n\n### 命令场景总结\n\n命令|作用域\t|常用情景\n---|---|--\ngit reset|提交层面|在私有分支上舍弃一些没有提交的更改\ngit reset|文件层面|将文件从缓存区中移除\ngit checkout|提交层面|切换分支或查看旧版本\ngit checkout|文件层面|舍弃工作目录中的更改\ngit revert|\t提交层面|在公共分支上回滚更改\n\n### 详细解释\n\n1. git checkout\n> 一般有两种场景：切换分支，舍弃工作目录中的更改\n\n例如：\n```\ngit checkout brach2\ngit checkout -- src/cli/serve/serve.js\n```\n2. git revert\n>撤销一个提交的同时会创建一个新的提交。这是一个安全的方法，因为它不会重写提交历史。比如，下面的命令会找出倒数第二个提交，然后创建一个新的提交来撤销这些更改，然后把这个提交加入项目中。\n\n```\ngit checkout hotfix\ngit revert HEAD~2\n```\n3. git reset\n>如果你的更改还没有共享给别人，git reset是撤销这些更改的简单方法。当你开发一个功能的时候发现『糟糕，我做了什么？我应该重新来过！』时，reset就像是go-to命令一样。\n除了在当前分支上操作，\n\n\n- 提交层面\n\nreset将一个分支的末端指向另一个提交。这可以用来移除当前分支的一些提交。你传入HEAD以外的其他提交的时候要格外小心，因为reset操作会重写当前分支的历史\n```\ngit checkout hotfix\ngit reset HEAD~2\n```\n可以通过传入这些标记来修改你的缓存区或工作目录：\n\n--soft – 缓存区和工作目录都不会被改变\n\n--mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响\n\n--hard – 缓存区和工作目录都同步到你指定的提交\n- 文件层面\n\n```\ngit reset HEAD~2 foo.py\n```\n会将当前的foo.py从缓存区中移除出去，而不会影响工作目录中对foo.py的更改。--soft、--mixed和--hard对文件层面的git reset毫无作用，因为缓存区中的文件一定会变化，而工作目录中的文件一定不变。\n\n\n\n### 总结\n\n一般我们开发中常用以下两个恢复文件\n\n1. git checkout\n```\ngit checkout -- src/cli/serve/serve.js\n```\n这条命令把serve.js从HEAD中签出并且把它恢复成未修改时的样子\n2. git reset --hard HEAD\n```\ngit reset --hard HEAD\n```\n这条命令会把你工作目录中所有未提交的内容清空(当然这不包括未置于版控制下的文件 untracked files).让工作目录回到上次提交时的状态(last committed state)\n3. git revert\n撤消(revert)了前期修改的提交(commit)是很容易的; 只要把出错的提交(commit)的名字(reference)做为参数传给命令\n```\ngit revert HEAD\n\n```\n这条命令创建了一个撤消了上次提交(HEAD)的新提交\n","slug":"git-reset、git-checkout和git-revert","published":1,"updated":"2018-07-22T11:54:56.012Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvcy00518ceeg4ycfxq2","content":"<h3 id=\"命令场景总结\"><a href=\"#命令场景总结\" class=\"headerlink\" title=\"命令场景总结\"></a>命令场景总结</h3><table>\n<thead>\n<tr>\n<th>命令</th>\n<th>作用域</th>\n<th>常用情景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>git reset</td>\n<td>提交层面</td>\n<td>在私有分支上舍弃一些没有提交的更改</td>\n</tr>\n<tr>\n<td>git reset</td>\n<td>文件层面</td>\n<td>将文件从缓存区中移除</td>\n</tr>\n<tr>\n<td>git checkout</td>\n<td>提交层面</td>\n<td>切换分支或查看旧版本</td>\n</tr>\n<tr>\n<td>git checkout</td>\n<td>文件层面</td>\n<td>舍弃工作目录中的更改</td>\n</tr>\n<tr>\n<td>git revert</td>\n<td>提交层面</td>\n<td>在公共分支上回滚更改</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"详细解释\"><a href=\"#详细解释\" class=\"headerlink\" title=\"详细解释\"></a>详细解释</h3><ol>\n<li>git checkout<blockquote>\n<p>一般有两种场景：切换分支，舍弃工作目录中的更改</p>\n</blockquote>\n</li>\n</ol>\n<p>例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout brach2</span><br><span class=\"line\">git checkout -- src/cli/serve/serve.js</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>git revert<blockquote>\n<p>撤销一个提交的同时会创建一个新的提交。这是一个安全的方法，因为它不会重写提交历史。比如，下面的命令会找出倒数第二个提交，然后创建一个新的提交来撤销这些更改，然后把这个提交加入项目中。</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout hotfix</span><br><span class=\"line\">git revert HEAD~2</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>git reset<blockquote>\n<p>如果你的更改还没有共享给别人，git reset是撤销这些更改的简单方法。当你开发一个功能的时候发现『糟糕，我做了什么？我应该重新来过！』时，reset就像是go-to命令一样。<br>除了在当前分支上操作，</p>\n</blockquote>\n</li>\n</ol>\n<ul>\n<li>提交层面</li>\n</ul>\n<p>reset将一个分支的末端指向另一个提交。这可以用来移除当前分支的一些提交。你传入HEAD以外的其他提交的时候要格外小心，因为reset操作会重写当前分支的历史<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout hotfix</span><br><span class=\"line\">git reset HEAD~2</span><br></pre></td></tr></table></figure></p>\n<p>可以通过传入这些标记来修改你的缓存区或工作目录：</p>\n<p>–soft – 缓存区和工作目录都不会被改变</p>\n<p>–mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响</p>\n<p>–hard – 缓存区和工作目录都同步到你指定的提交</p>\n<ul>\n<li>文件层面</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset HEAD~2 foo.py</span><br></pre></td></tr></table></figure>\n<p>会将当前的foo.py从缓存区中移除出去，而不会影响工作目录中对foo.py的更改。–soft、–mixed和–hard对文件层面的git reset毫无作用，因为缓存区中的文件一定会变化，而工作目录中的文件一定不变。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>一般我们开发中常用以下两个恢复文件</p>\n<ol>\n<li>git checkout<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -- src/cli/serve/serve.js</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令把serve.js从HEAD中签出并且把它恢复成未修改时的样子</p>\n<ol start=\"2\">\n<li>git reset –hard HEAD<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard HEAD</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令会把你工作目录中所有未提交的内容清空(当然这不包括未置于版控制下的文件 untracked files).让工作目录回到上次提交时的状态(last committed state)</p>\n<ol start=\"3\">\n<li>git revert<br>撤消(revert)了前期修改的提交(commit)是很容易的; 只要把出错的提交(commit)的名字(reference)做为参数传给命令<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git revert HEAD</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令创建了一个撤消了上次提交(HEAD)的新提交</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"命令场景总结\"><a href=\"#命令场景总结\" class=\"headerlink\" title=\"命令场景总结\"></a>命令场景总结</h3><table>\n<thead>\n<tr>\n<th>命令</th>\n<th>作用域</th>\n<th>常用情景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>git reset</td>\n<td>提交层面</td>\n<td>在私有分支上舍弃一些没有提交的更改</td>\n</tr>\n<tr>\n<td>git reset</td>\n<td>文件层面</td>\n<td>将文件从缓存区中移除</td>\n</tr>\n<tr>\n<td>git checkout</td>\n<td>提交层面</td>\n<td>切换分支或查看旧版本</td>\n</tr>\n<tr>\n<td>git checkout</td>\n<td>文件层面</td>\n<td>舍弃工作目录中的更改</td>\n</tr>\n<tr>\n<td>git revert</td>\n<td>提交层面</td>\n<td>在公共分支上回滚更改</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"详细解释\"><a href=\"#详细解释\" class=\"headerlink\" title=\"详细解释\"></a>详细解释</h3><ol>\n<li>git checkout<blockquote>\n<p>一般有两种场景：切换分支，舍弃工作目录中的更改</p>\n</blockquote>\n</li>\n</ol>\n<p>例如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout brach2</span><br><span class=\"line\">git checkout -- src/cli/serve/serve.js</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>git revert<blockquote>\n<p>撤销一个提交的同时会创建一个新的提交。这是一个安全的方法，因为它不会重写提交历史。比如，下面的命令会找出倒数第二个提交，然后创建一个新的提交来撤销这些更改，然后把这个提交加入项目中。</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout hotfix</span><br><span class=\"line\">git revert HEAD~2</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>git reset<blockquote>\n<p>如果你的更改还没有共享给别人，git reset是撤销这些更改的简单方法。当你开发一个功能的时候发现『糟糕，我做了什么？我应该重新来过！』时，reset就像是go-to命令一样。<br>除了在当前分支上操作，</p>\n</blockquote>\n</li>\n</ol>\n<ul>\n<li>提交层面</li>\n</ul>\n<p>reset将一个分支的末端指向另一个提交。这可以用来移除当前分支的一些提交。你传入HEAD以外的其他提交的时候要格外小心，因为reset操作会重写当前分支的历史<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout hotfix</span><br><span class=\"line\">git reset HEAD~2</span><br></pre></td></tr></table></figure></p>\n<p>可以通过传入这些标记来修改你的缓存区或工作目录：</p>\n<p>–soft – 缓存区和工作目录都不会被改变</p>\n<p>–mixed – 默认选项。缓存区和你指定的提交同步，但工作目录不受影响</p>\n<p>–hard – 缓存区和工作目录都同步到你指定的提交</p>\n<ul>\n<li>文件层面</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset HEAD~2 foo.py</span><br></pre></td></tr></table></figure>\n<p>会将当前的foo.py从缓存区中移除出去，而不会影响工作目录中对foo.py的更改。–soft、–mixed和–hard对文件层面的git reset毫无作用，因为缓存区中的文件一定会变化，而工作目录中的文件一定不变。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>一般我们开发中常用以下两个恢复文件</p>\n<ol>\n<li>git checkout<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout -- src/cli/serve/serve.js</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令把serve.js从HEAD中签出并且把它恢复成未修改时的样子</p>\n<ol start=\"2\">\n<li>git reset –hard HEAD<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard HEAD</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令会把你工作目录中所有未提交的内容清空(当然这不包括未置于版控制下的文件 untracked files).让工作目录回到上次提交时的状态(last committed state)</p>\n<ol start=\"3\">\n<li>git revert<br>撤消(revert)了前期修改的提交(commit)是很容易的; 只要把出错的提交(commit)的名字(reference)做为参数传给命令<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git revert HEAD</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>这条命令创建了一个撤消了上次提交(HEAD)的新提交</p>\n"},{"title":"java8系列专栏之Date Time API","originContent":"","toc":false,"date":"2019-04-28T11:03:07.000Z","_content":"\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Date Time API\n## 正文\n### Clock\n##### 详解\n可以取代`System.currentTimeMillis()`,时区敏感，带有时区信息\n##### 用法\n```\nClock clock = Clock.systemDefaultZone();\nlong millis = clock.millis();\n\nInstant instant = clock.instant();\nDate legacyDate = Date.from(instant);   // legacy java.util.Date\n```\n### ZoneId\n##### 详解\n新的时区类 `java.time.ZoneId` 是原有的 `java.util.TimeZone` 类的替代品。 ZoneId对象可以通过 `ZoneId.of()` 方法创建，也可以通过 `ZoneId.systemDefault()` 获取系统默认时区。\n##### 用法\n```\nSystem.out.println(ZoneId.getAvailableZoneIds());\n// prints all available timezone ids\n\nZoneId shanghaiZoneId = ZoneId.of(\"Asia/Shanghai\");\n// ZoneRules[currentStandardOffset=+08:00]\n```\n有了 ZoneId，我们就可以将一个 LocalDate、LocalTime 或 LocalDateTime 对象转化为 ZonedDateTime 对象\n```\nLocalDateTime localDateTime = LocalDateTime.now();\nZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, shanghaiZoneId);\n```\nZonedDateTime 对象由两部分构成，LocalDateTime 和 ZoneId，其中 2018-03-03T15:26:56.147 部分为 LocalDateTime，+08:00[Asia/Shanghai] 部分为ZoneId。\n### LocalTime\n##### 详解\nLocalTime类是Java 8中日期时间功能里表示一整天中某个时间点的类，它的时间是无时区属性的（早上10点等等）\n##### 用法\n```\nLocalTime now1 = LocalTime.now(zone1);\nLocalTime now2 = LocalTime.now(zone2);\n\nSystem.out.println(now1.isBefore(now2));  // false\n\nlong hoursBetween = ChronoUnit.HOURS.between(now1, now2);\nlong minutesBetween = ChronoUnit.MINUTES.between(now1, now2);\n\nSystem.out.println(hoursBetween);       // -3\nSystem.out.println(minutesBetween);     // -239\n```\n### LocalDate\n##### 详解\nLocalDate类是Java 8中日期时间功能里表示一个本地日期的类，它的日期是无时区属性的。 可以用来表示生日、节假日期等等。这个类用于表示一个确切的日期，而不是这个日期所在的时间\n##### 用法\n```\nLocalDate today = LocalDate.now();\nLocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);\nLocalDate yesterday = tomorrow.minusDays(2);\n\nLocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);\nDayOfWeek dayOfWeek = independenceDay.getDayOfWeek();\nSystem.out.println(dayOfWeek);    // FRIDAY\n```\n### LocalDateTime\n##### 详解\nLocalDateTime类是Java 8中日期时间功能里，用于表示当地的日期与时间的类，它的值是无时区属性的。你可以将其视为Java 8中LocalDate与LocalTime两个类的结合。\n##### 用法\n```\nLocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);\n\nDayOfWeek dayOfWeek = sylvester.getDayOfWeek();\nSystem.out.println(dayOfWeek);      // WEDNESDAY\n\nMonth month = sylvester.getMonth();\nSystem.out.println(month);          // DECEMBER\n\nlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);\nSystem.out.println(minuteOfDay);    // 1439\n```\n### ZonedDateTime\n##### 详解\nZonedDateTime类是Java 8中日期时间功能里，用于表示带时区的日期与时间信息的类。可以用于表示一个真实事件的开始时间，如某火箭升空时间等等。\n##### 用法\n```\nZonedDateTime dateTime = ZonedDateTime.now();\nZonedDateTime zonedDateTime = ZonedDateTime.of(LocalDateTime.now(), shanghaiZoneId);\n```\n### Duration\n##### 详解\n一个Duration对象表示两个Instant间的一段时间\n##### 用法\n```\nInstant first = Instant.now();\n// wait some time while something happens\nInstant second = Instant.now();\nDuration duration = Duration.between(first, second);\n```\n### DateTimeFormatter\n##### 详解\nDateTimeFormatter类是Java 8中日期时间功能里，线程安全。用于解析和格式化日期时间的类\n。类中包含如下预定义的实例：\n```\nBASIC_ISO_DATE\n\nISO_LOCAL_DATE\nISO_LOCAL_TIME\nISO_LOCAL_DATE_TIME\n\nISO_OFFSET_DATE\nISO_OFFSET_TIME\nISO_OFFSET_DATE_TIME\n\nISO_ZONED_DATE_TIME\n\nISO_INSTANT\n\nISO_DATE\nISO_TIME\nISO_DATE_TIME\n\nISO_ORDINAL_TIME\nISO_WEEK_DATE\n\nRFC_1123_DATE_TIME\n```\n##### 用法\n```\nDateTimeFormatter formatter = DateTimeFormatter.ISO_DATE_TIME;\n//DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\");\nString formattedDate = formatter.format(LocalDateTime.now());\n```\n## 参考\n1. [java8-tutorial#date-api](https://github.com/winterbe/java8-tutorial#date-api)\n2. [java8-datetime-api](https://github.com/biezhi/learn-java8/blob/master/java8-datetime-api/README.md)","source":"_posts/java8系列专栏之Date-Time-API.md","raw":"---\ntitle: java8系列专栏之Date Time API\ntags:\n  - 笔记\noriginContent: ''\ncategories:\n  - java\ntoc: false\ndate: 2019-04-28 19:03:07\n---\n\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Date Time API\n## 正文\n### Clock\n##### 详解\n可以取代`System.currentTimeMillis()`,时区敏感，带有时区信息\n##### 用法\n```\nClock clock = Clock.systemDefaultZone();\nlong millis = clock.millis();\n\nInstant instant = clock.instant();\nDate legacyDate = Date.from(instant);   // legacy java.util.Date\n```\n### ZoneId\n##### 详解\n新的时区类 `java.time.ZoneId` 是原有的 `java.util.TimeZone` 类的替代品。 ZoneId对象可以通过 `ZoneId.of()` 方法创建，也可以通过 `ZoneId.systemDefault()` 获取系统默认时区。\n##### 用法\n```\nSystem.out.println(ZoneId.getAvailableZoneIds());\n// prints all available timezone ids\n\nZoneId shanghaiZoneId = ZoneId.of(\"Asia/Shanghai\");\n// ZoneRules[currentStandardOffset=+08:00]\n```\n有了 ZoneId，我们就可以将一个 LocalDate、LocalTime 或 LocalDateTime 对象转化为 ZonedDateTime 对象\n```\nLocalDateTime localDateTime = LocalDateTime.now();\nZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, shanghaiZoneId);\n```\nZonedDateTime 对象由两部分构成，LocalDateTime 和 ZoneId，其中 2018-03-03T15:26:56.147 部分为 LocalDateTime，+08:00[Asia/Shanghai] 部分为ZoneId。\n### LocalTime\n##### 详解\nLocalTime类是Java 8中日期时间功能里表示一整天中某个时间点的类，它的时间是无时区属性的（早上10点等等）\n##### 用法\n```\nLocalTime now1 = LocalTime.now(zone1);\nLocalTime now2 = LocalTime.now(zone2);\n\nSystem.out.println(now1.isBefore(now2));  // false\n\nlong hoursBetween = ChronoUnit.HOURS.between(now1, now2);\nlong minutesBetween = ChronoUnit.MINUTES.between(now1, now2);\n\nSystem.out.println(hoursBetween);       // -3\nSystem.out.println(minutesBetween);     // -239\n```\n### LocalDate\n##### 详解\nLocalDate类是Java 8中日期时间功能里表示一个本地日期的类，它的日期是无时区属性的。 可以用来表示生日、节假日期等等。这个类用于表示一个确切的日期，而不是这个日期所在的时间\n##### 用法\n```\nLocalDate today = LocalDate.now();\nLocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);\nLocalDate yesterday = tomorrow.minusDays(2);\n\nLocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);\nDayOfWeek dayOfWeek = independenceDay.getDayOfWeek();\nSystem.out.println(dayOfWeek);    // FRIDAY\n```\n### LocalDateTime\n##### 详解\nLocalDateTime类是Java 8中日期时间功能里，用于表示当地的日期与时间的类，它的值是无时区属性的。你可以将其视为Java 8中LocalDate与LocalTime两个类的结合。\n##### 用法\n```\nLocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);\n\nDayOfWeek dayOfWeek = sylvester.getDayOfWeek();\nSystem.out.println(dayOfWeek);      // WEDNESDAY\n\nMonth month = sylvester.getMonth();\nSystem.out.println(month);          // DECEMBER\n\nlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);\nSystem.out.println(minuteOfDay);    // 1439\n```\n### ZonedDateTime\n##### 详解\nZonedDateTime类是Java 8中日期时间功能里，用于表示带时区的日期与时间信息的类。可以用于表示一个真实事件的开始时间，如某火箭升空时间等等。\n##### 用法\n```\nZonedDateTime dateTime = ZonedDateTime.now();\nZonedDateTime zonedDateTime = ZonedDateTime.of(LocalDateTime.now(), shanghaiZoneId);\n```\n### Duration\n##### 详解\n一个Duration对象表示两个Instant间的一段时间\n##### 用法\n```\nInstant first = Instant.now();\n// wait some time while something happens\nInstant second = Instant.now();\nDuration duration = Duration.between(first, second);\n```\n### DateTimeFormatter\n##### 详解\nDateTimeFormatter类是Java 8中日期时间功能里，线程安全。用于解析和格式化日期时间的类\n。类中包含如下预定义的实例：\n```\nBASIC_ISO_DATE\n\nISO_LOCAL_DATE\nISO_LOCAL_TIME\nISO_LOCAL_DATE_TIME\n\nISO_OFFSET_DATE\nISO_OFFSET_TIME\nISO_OFFSET_DATE_TIME\n\nISO_ZONED_DATE_TIME\n\nISO_INSTANT\n\nISO_DATE\nISO_TIME\nISO_DATE_TIME\n\nISO_ORDINAL_TIME\nISO_WEEK_DATE\n\nRFC_1123_DATE_TIME\n```\n##### 用法\n```\nDateTimeFormatter formatter = DateTimeFormatter.ISO_DATE_TIME;\n//DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\");\nString formattedDate = formatter.format(LocalDateTime.now());\n```\n## 参考\n1. [java8-tutorial#date-api](https://github.com/winterbe/java8-tutorial#date-api)\n2. [java8-datetime-api](https://github.com/biezhi/learn-java8/blob/master/java8-datetime-api/README.md)","slug":"java8系列专栏之Date-Time-API","published":1,"updated":"2019-04-28T11:03:07.955Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd000558ceeojck08yc","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Date Time API</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Clock\"><a href=\"#Clock\" class=\"headerlink\" title=\"Clock\"></a>Clock</h3><h5 id=\"详解\"><a href=\"#详解\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>可以取代<code>System.currentTimeMillis()</code>,时区敏感，带有时区信息</p>\n<h5 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Clock clock = Clock.systemDefaultZone();</span><br><span class=\"line\">long millis = clock.millis();</span><br><span class=\"line\"></span><br><span class=\"line\">Instant instant = clock.instant();</span><br><span class=\"line\">Date legacyDate = Date.from(instant);   // legacy java.util.Date</span><br></pre></td></tr></table></figure>\n<h3 id=\"ZoneId\"><a href=\"#ZoneId\" class=\"headerlink\" title=\"ZoneId\"></a>ZoneId</h3><h5 id=\"详解-1\"><a href=\"#详解-1\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>新的时区类 <code>java.time.ZoneId</code> 是原有的 <code>java.util.TimeZone</code> 类的替代品。 ZoneId对象可以通过 <code>ZoneId.of()</code> 方法创建，也可以通过 <code>ZoneId.systemDefault()</code> 获取系统默认时区。</p>\n<h5 id=\"用法-1\"><a href=\"#用法-1\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(ZoneId.getAvailableZoneIds());</span><br><span class=\"line\">// prints all available timezone ids</span><br><span class=\"line\"></span><br><span class=\"line\">ZoneId shanghaiZoneId = ZoneId.of(&quot;Asia/Shanghai&quot;);</span><br><span class=\"line\">// ZoneRules[currentStandardOffset=+08:00]</span><br></pre></td></tr></table></figure>\n<p>有了 ZoneId，我们就可以将一个 LocalDate、LocalTime 或 LocalDateTime 对象转化为 ZonedDateTime 对象<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDateTime localDateTime = LocalDateTime.now();</span><br><span class=\"line\">ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, shanghaiZoneId);</span><br></pre></td></tr></table></figure></p>\n<p>ZonedDateTime 对象由两部分构成，LocalDateTime 和 ZoneId，其中 2018-03-03T15:26:56.147 部分为 LocalDateTime，+08:00[Asia/Shanghai] 部分为ZoneId。</p>\n<h3 id=\"LocalTime\"><a href=\"#LocalTime\" class=\"headerlink\" title=\"LocalTime\"></a>LocalTime</h3><h5 id=\"详解-2\"><a href=\"#详解-2\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalTime类是Java 8中日期时间功能里表示一整天中某个时间点的类，它的时间是无时区属性的（早上10点等等）</p>\n<h5 id=\"用法-2\"><a href=\"#用法-2\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalTime now1 = LocalTime.now(zone1);</span><br><span class=\"line\">LocalTime now2 = LocalTime.now(zone2);</span><br><span class=\"line\"></span><br><span class=\"line\">System.out.println(now1.isBefore(now2));  // false</span><br><span class=\"line\"></span><br><span class=\"line\">long hoursBetween = ChronoUnit.HOURS.between(now1, now2);</span><br><span class=\"line\">long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);</span><br><span class=\"line\"></span><br><span class=\"line\">System.out.println(hoursBetween);       // -3</span><br><span class=\"line\">System.out.println(minutesBetween);     // -239</span><br></pre></td></tr></table></figure>\n<h3 id=\"LocalDate\"><a href=\"#LocalDate\" class=\"headerlink\" title=\"LocalDate\"></a>LocalDate</h3><h5 id=\"详解-3\"><a href=\"#详解-3\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalDate类是Java 8中日期时间功能里表示一个本地日期的类，它的日期是无时区属性的。 可以用来表示生日、节假日期等等。这个类用于表示一个确切的日期，而不是这个日期所在的时间</p>\n<h5 id=\"用法-3\"><a href=\"#用法-3\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDate today = LocalDate.now();</span><br><span class=\"line\">LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);</span><br><span class=\"line\">LocalDate yesterday = tomorrow.minusDays(2);</span><br><span class=\"line\"></span><br><span class=\"line\">LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);</span><br><span class=\"line\">DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();</span><br><span class=\"line\">System.out.println(dayOfWeek);    // FRIDAY</span><br></pre></td></tr></table></figure>\n<h3 id=\"LocalDateTime\"><a href=\"#LocalDateTime\" class=\"headerlink\" title=\"LocalDateTime\"></a>LocalDateTime</h3><h5 id=\"详解-4\"><a href=\"#详解-4\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalDateTime类是Java 8中日期时间功能里，用于表示当地的日期与时间的类，它的值是无时区属性的。你可以将其视为Java 8中LocalDate与LocalTime两个类的结合。</p>\n<h5 id=\"用法-4\"><a href=\"#用法-4\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);</span><br><span class=\"line\"></span><br><span class=\"line\">DayOfWeek dayOfWeek = sylvester.getDayOfWeek();</span><br><span class=\"line\">System.out.println(dayOfWeek);      // WEDNESDAY</span><br><span class=\"line\"></span><br><span class=\"line\">Month month = sylvester.getMonth();</span><br><span class=\"line\">System.out.println(month);          // DECEMBER</span><br><span class=\"line\"></span><br><span class=\"line\">long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);</span><br><span class=\"line\">System.out.println(minuteOfDay);    // 1439</span><br></pre></td></tr></table></figure>\n<h3 id=\"ZonedDateTime\"><a href=\"#ZonedDateTime\" class=\"headerlink\" title=\"ZonedDateTime\"></a>ZonedDateTime</h3><h5 id=\"详解-5\"><a href=\"#详解-5\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>ZonedDateTime类是Java 8中日期时间功能里，用于表示带时区的日期与时间信息的类。可以用于表示一个真实事件的开始时间，如某火箭升空时间等等。</p>\n<h5 id=\"用法-5\"><a href=\"#用法-5\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZonedDateTime dateTime = ZonedDateTime.now();</span><br><span class=\"line\">ZonedDateTime zonedDateTime = ZonedDateTime.of(LocalDateTime.now(), shanghaiZoneId);</span><br></pre></td></tr></table></figure>\n<h3 id=\"Duration\"><a href=\"#Duration\" class=\"headerlink\" title=\"Duration\"></a>Duration</h3><h5 id=\"详解-6\"><a href=\"#详解-6\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>一个Duration对象表示两个Instant间的一段时间</p>\n<h5 id=\"用法-6\"><a href=\"#用法-6\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Instant first = Instant.now();</span><br><span class=\"line\">// wait some time while something happens</span><br><span class=\"line\">Instant second = Instant.now();</span><br><span class=\"line\">Duration duration = Duration.between(first, second);</span><br></pre></td></tr></table></figure>\n<h3 id=\"DateTimeFormatter\"><a href=\"#DateTimeFormatter\" class=\"headerlink\" title=\"DateTimeFormatter\"></a>DateTimeFormatter</h3><h5 id=\"详解-7\"><a href=\"#详解-7\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>DateTimeFormatter类是Java 8中日期时间功能里，线程安全。用于解析和格式化日期时间的类<br>。类中包含如下预定义的实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BASIC_ISO_DATE</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_LOCAL_DATE</span><br><span class=\"line\">ISO_LOCAL_TIME</span><br><span class=\"line\">ISO_LOCAL_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_OFFSET_DATE</span><br><span class=\"line\">ISO_OFFSET_TIME</span><br><span class=\"line\">ISO_OFFSET_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_ZONED_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_INSTANT</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_DATE</span><br><span class=\"line\">ISO_TIME</span><br><span class=\"line\">ISO_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_ORDINAL_TIME</span><br><span class=\"line\">ISO_WEEK_DATE</span><br><span class=\"line\"></span><br><span class=\"line\">RFC_1123_DATE_TIME</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"用法-7\"><a href=\"#用法-7\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DateTimeFormatter formatter = DateTimeFormatter.ISO_DATE_TIME;</span><br><span class=\"line\">//DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class=\"line\">String formattedDate = formatter.format(LocalDateTime.now());</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/winterbe/java8-tutorial#date-api\" target=\"_blank\" rel=\"noopener\">java8-tutorial#date-api</a></li>\n<li><a href=\"https://github.com/biezhi/learn-java8/blob/master/java8-datetime-api/README.md\" target=\"_blank\" rel=\"noopener\">java8-datetime-api</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Date Time API</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Clock\"><a href=\"#Clock\" class=\"headerlink\" title=\"Clock\"></a>Clock</h3><h5 id=\"详解\"><a href=\"#详解\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>可以取代<code>System.currentTimeMillis()</code>,时区敏感，带有时区信息</p>\n<h5 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Clock clock = Clock.systemDefaultZone();</span><br><span class=\"line\">long millis = clock.millis();</span><br><span class=\"line\"></span><br><span class=\"line\">Instant instant = clock.instant();</span><br><span class=\"line\">Date legacyDate = Date.from(instant);   // legacy java.util.Date</span><br></pre></td></tr></table></figure>\n<h3 id=\"ZoneId\"><a href=\"#ZoneId\" class=\"headerlink\" title=\"ZoneId\"></a>ZoneId</h3><h5 id=\"详解-1\"><a href=\"#详解-1\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>新的时区类 <code>java.time.ZoneId</code> 是原有的 <code>java.util.TimeZone</code> 类的替代品。 ZoneId对象可以通过 <code>ZoneId.of()</code> 方法创建，也可以通过 <code>ZoneId.systemDefault()</code> 获取系统默认时区。</p>\n<h5 id=\"用法-1\"><a href=\"#用法-1\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(ZoneId.getAvailableZoneIds());</span><br><span class=\"line\">// prints all available timezone ids</span><br><span class=\"line\"></span><br><span class=\"line\">ZoneId shanghaiZoneId = ZoneId.of(&quot;Asia/Shanghai&quot;);</span><br><span class=\"line\">// ZoneRules[currentStandardOffset=+08:00]</span><br></pre></td></tr></table></figure>\n<p>有了 ZoneId，我们就可以将一个 LocalDate、LocalTime 或 LocalDateTime 对象转化为 ZonedDateTime 对象<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDateTime localDateTime = LocalDateTime.now();</span><br><span class=\"line\">ZonedDateTime zonedDateTime = ZonedDateTime.of(localDateTime, shanghaiZoneId);</span><br></pre></td></tr></table></figure></p>\n<p>ZonedDateTime 对象由两部分构成，LocalDateTime 和 ZoneId，其中 2018-03-03T15:26:56.147 部分为 LocalDateTime，+08:00[Asia/Shanghai] 部分为ZoneId。</p>\n<h3 id=\"LocalTime\"><a href=\"#LocalTime\" class=\"headerlink\" title=\"LocalTime\"></a>LocalTime</h3><h5 id=\"详解-2\"><a href=\"#详解-2\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalTime类是Java 8中日期时间功能里表示一整天中某个时间点的类，它的时间是无时区属性的（早上10点等等）</p>\n<h5 id=\"用法-2\"><a href=\"#用法-2\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalTime now1 = LocalTime.now(zone1);</span><br><span class=\"line\">LocalTime now2 = LocalTime.now(zone2);</span><br><span class=\"line\"></span><br><span class=\"line\">System.out.println(now1.isBefore(now2));  // false</span><br><span class=\"line\"></span><br><span class=\"line\">long hoursBetween = ChronoUnit.HOURS.between(now1, now2);</span><br><span class=\"line\">long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);</span><br><span class=\"line\"></span><br><span class=\"line\">System.out.println(hoursBetween);       // -3</span><br><span class=\"line\">System.out.println(minutesBetween);     // -239</span><br></pre></td></tr></table></figure>\n<h3 id=\"LocalDate\"><a href=\"#LocalDate\" class=\"headerlink\" title=\"LocalDate\"></a>LocalDate</h3><h5 id=\"详解-3\"><a href=\"#详解-3\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalDate类是Java 8中日期时间功能里表示一个本地日期的类，它的日期是无时区属性的。 可以用来表示生日、节假日期等等。这个类用于表示一个确切的日期，而不是这个日期所在的时间</p>\n<h5 id=\"用法-3\"><a href=\"#用法-3\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDate today = LocalDate.now();</span><br><span class=\"line\">LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);</span><br><span class=\"line\">LocalDate yesterday = tomorrow.minusDays(2);</span><br><span class=\"line\"></span><br><span class=\"line\">LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);</span><br><span class=\"line\">DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();</span><br><span class=\"line\">System.out.println(dayOfWeek);    // FRIDAY</span><br></pre></td></tr></table></figure>\n<h3 id=\"LocalDateTime\"><a href=\"#LocalDateTime\" class=\"headerlink\" title=\"LocalDateTime\"></a>LocalDateTime</h3><h5 id=\"详解-4\"><a href=\"#详解-4\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>LocalDateTime类是Java 8中日期时间功能里，用于表示当地的日期与时间的类，它的值是无时区属性的。你可以将其视为Java 8中LocalDate与LocalTime两个类的结合。</p>\n<h5 id=\"用法-4\"><a href=\"#用法-4\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);</span><br><span class=\"line\"></span><br><span class=\"line\">DayOfWeek dayOfWeek = sylvester.getDayOfWeek();</span><br><span class=\"line\">System.out.println(dayOfWeek);      // WEDNESDAY</span><br><span class=\"line\"></span><br><span class=\"line\">Month month = sylvester.getMonth();</span><br><span class=\"line\">System.out.println(month);          // DECEMBER</span><br><span class=\"line\"></span><br><span class=\"line\">long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);</span><br><span class=\"line\">System.out.println(minuteOfDay);    // 1439</span><br></pre></td></tr></table></figure>\n<h3 id=\"ZonedDateTime\"><a href=\"#ZonedDateTime\" class=\"headerlink\" title=\"ZonedDateTime\"></a>ZonedDateTime</h3><h5 id=\"详解-5\"><a href=\"#详解-5\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>ZonedDateTime类是Java 8中日期时间功能里，用于表示带时区的日期与时间信息的类。可以用于表示一个真实事件的开始时间，如某火箭升空时间等等。</p>\n<h5 id=\"用法-5\"><a href=\"#用法-5\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ZonedDateTime dateTime = ZonedDateTime.now();</span><br><span class=\"line\">ZonedDateTime zonedDateTime = ZonedDateTime.of(LocalDateTime.now(), shanghaiZoneId);</span><br></pre></td></tr></table></figure>\n<h3 id=\"Duration\"><a href=\"#Duration\" class=\"headerlink\" title=\"Duration\"></a>Duration</h3><h5 id=\"详解-6\"><a href=\"#详解-6\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>一个Duration对象表示两个Instant间的一段时间</p>\n<h5 id=\"用法-6\"><a href=\"#用法-6\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Instant first = Instant.now();</span><br><span class=\"line\">// wait some time while something happens</span><br><span class=\"line\">Instant second = Instant.now();</span><br><span class=\"line\">Duration duration = Duration.between(first, second);</span><br></pre></td></tr></table></figure>\n<h3 id=\"DateTimeFormatter\"><a href=\"#DateTimeFormatter\" class=\"headerlink\" title=\"DateTimeFormatter\"></a>DateTimeFormatter</h3><h5 id=\"详解-7\"><a href=\"#详解-7\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>DateTimeFormatter类是Java 8中日期时间功能里，线程安全。用于解析和格式化日期时间的类<br>。类中包含如下预定义的实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BASIC_ISO_DATE</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_LOCAL_DATE</span><br><span class=\"line\">ISO_LOCAL_TIME</span><br><span class=\"line\">ISO_LOCAL_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_OFFSET_DATE</span><br><span class=\"line\">ISO_OFFSET_TIME</span><br><span class=\"line\">ISO_OFFSET_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_ZONED_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_INSTANT</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_DATE</span><br><span class=\"line\">ISO_TIME</span><br><span class=\"line\">ISO_DATE_TIME</span><br><span class=\"line\"></span><br><span class=\"line\">ISO_ORDINAL_TIME</span><br><span class=\"line\">ISO_WEEK_DATE</span><br><span class=\"line\"></span><br><span class=\"line\">RFC_1123_DATE_TIME</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"用法-7\"><a href=\"#用法-7\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DateTimeFormatter formatter = DateTimeFormatter.ISO_DATE_TIME;</span><br><span class=\"line\">//DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class=\"line\">String formattedDate = formatter.format(LocalDateTime.now());</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/winterbe/java8-tutorial#date-api\" target=\"_blank\" rel=\"noopener\">java8-tutorial#date-api</a></li>\n<li><a href=\"https://github.com/biezhi/learn-java8/blob/master/java8-datetime-api/README.md\" target=\"_blank\" rel=\"noopener\">java8-datetime-api</a></li>\n</ol>\n"},{"title":"java8系列专栏之Lambda与Stream API","originContent":"","toc":false,"date":"2019-04-29T12:14:42.000Z","_content":"\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Lambda与Stream API\n## 正文\n### Lambda\n#### 什么是Lambda\n\n```\nList<String>list = Arrays.asList(\"a\",\"c\",\"b\");\nCollections.sort(list, (o1,o2)->o1.compareTo(o2));\n```\n以下方式都是常用的lambda使用方式\n```\nstr->str.toLowerCase()\n(o1,o2)->o1.compareTo(o2)\n(o1,o2)->{return o1.compareTo(o2)}\n(String o1,String o2)->{return o1.compareTo(o2)}\n```\n#### 怎么用，哪里用\n函数接口声明既可使用。例如Runnable，Comparator都是函数接口。用@FunctionalInterface声明的都是函数接口\n#### 实现原理\n首先需要明确说明的是lambda没有使用匿名内部类去实现。\n\nJava 8设计人员决定使用在Java 7中添加的`invokedynamic`指令来推迟在运行时的翻译策略。当javac编译代码时，它捕获lambda表达式并生成`invokedynamic`调用站点(称为lambda工厂)。调用`invokedynamic`调用站点时，返回一个函数接口实例，lambda将被转换到这个函数接口\n\n\n使用invokedynamic指令，运行时调用LambdaMetafactory.metafactory动态的生成内部类，实现了接口，内部类里的调用方法块并不是动态生成的，只是在原class里已经编译生成了一个静态的方法，内部类只需要调用该静态方法\n\n[参考1](https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#using-invokedynamic)\n[参考2](https://blog.csdn.net/raintungli/article/details/54910152)\n#### 内置函数接口\n\n| 函数式接口 | 函数描述符 | 原始类型特化 |\n|:-----:|:--------|:-------|\n| `Predicate<T>` | `T->boolean` | `IntPredicate,LongPredicate, DoublePredicate` |\n| `Consumer<T>` | `T->void` | `IntConsumer,LongConsumer, DoubleConsumer` |\n| `Function<T,R>` | `T->R` | `IntFunction<R>, IntToDoubleFunction,` <br/> `IntToLongFunction, LongFunction<R>,` <br/> `LongToDoubleFunction, LongToIntFunction, ` <br/> `DoubleFunction<R>, ToIntFunction<T>, ` <br/> `ToDoubleFunction<T>, ToLongFunction<T>` |\n| `Supplier<T>` | `()->T` | `BooleanSupplier,IntSupplier, LongSupplier, DoubleSupplier` |\n| `UnaryOperator<T>` | `T->T` | `IntUnaryOperator, LongUnaryOperator, DoubleUnaryOperator` |\n| `BinaryOperator<T>` | `(T,T)->T` | `IntBinaryOperator, LongBinaryOperator, DoubleBinaryOperator` |\n| `BiPredicate<L,R>` | `(L,R)->boolean` |  |\n| `BiConsumer<T,U>` | `(T,U)->void` | `ObjIntConsumer<T>, ObjLongConsumer<T>, ObjDoubleConsumer<T>` |\n| `BiFunction<T,U,R>` | `(T,U)->R` | `ToIntBiFunction<T,U>, ToLongBiFunction<T,U>, ToDoubleBiFunction<T,U>` |\n\n### Stream API\n\n| 操作 | 类型 | 返回类型 | 使用的类型/函数式接口 | 函数描述符 |\n|:-----:|:--------|:-------|:-------|:-------|\n| `filter` | 中间 | `Stream<T>` | `Predicate<T>` | `T -> boolean` |\n| `distinct` | 中间 | `Stream<T>` |  |  |\n| `skip` | 中间 | `Stream<T>` | long |  |\n| `map` | 中间 | `Stream<R>` | `Function<T, R>` | `T -> R` |\n| `flatMap` | 中间 | `Stream<R>` | `Function<T, Stream<R>>` | `T -> Stream<R>` |\n| `limit` | 中间 | `Stream<T>` | long  | |\n| `sorted` | 中间 | `Stream<T>` | `Comparator<T>` | `(T, T) -> int` |\n| `anyMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `noneMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `allMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `findAny` | 终端 | `Optional<T>` | |  |\n| `findFirst` | 终端 | `Optional<T>` | |  |\n| `forEach` | 终端 | `void` | `Consumer<T>` | `T -> void` |\n| `collect` | 终端 | `R` | `Collector<T, A, R>` |  |\n| `reduce` | 终端 | `Optional<T>` | `BinaryOperator<T>` | `(T, T) -> T` |\n| `count` | 终端 | `long` | | |\n\n### Collector 收集\n**Collectors 类的静态工厂方法**\n\n| 工厂方法 | 返回类型 | 用途 | 示例 |\n|:-----:|:--------|:-------|:-------|\n| `toList` | `List<T>` | 把流中所有项目收集到一个 List | `List<Project> projects = projectStream.collect(toList());` |\n| `toSet` | `Set<T>` | 把流中所有项目收集到一个 Set，删除重复项 | `Set<Project> projects = projectStream.collect(toSet());` |\n| `toCollection` | `Collection<T>` | 把流中所有项目收集到给定的供应源创建的集合 | `Collection<Project> projects = projectStream.collect(toCollection(), ArrayList::new);` |\n| `counting` | `Long` | 计算流中元素的个数 | `long howManyProjects = projectStream.collect(counting());` |\n| `summingInt` | `Integer` | 对流中项目的一个整数属性求和 | `int totalStars = projectStream.collect(summingInt(Project::getStars));` |\n| `averagingInt` | `Double` | 计算流中项目 Integer 属性的平均值 | `double avgStars = projectStream.collect(averagingInt(Project::getStars));` |\n| `summarizingInt` | `IntSummaryStatistics` | 收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值 | `IntSummaryStatistics projectStatistics = projectStream.collect(summarizingInt(Project::getStars));` |\n| `joining` | `String` | 连接对流中每个项目调用 toString 方法所生成的字符串 | `String shortProject = projectStream.map(Project::getName).collect(joining(\", \"));` |\n| `maxBy` | `Optional<T>` | 按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty() | `Optional<Project> fattest = projectStream.collect(maxBy(comparingInt(Project::getStars)));` |\n| `minBy` | `Optional<T>` | 按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty() | `Optional<Project> fattest = projectStream.collect(minBy(comparingInt(Project::getStars)));` |\n| `reducing` | 归约操作产生的类型 | 从一个作为累加器的初始值开始，利用 BinaryOperator 与流中的元素逐个结合，从而将流归约为单个值 | `int totalStars = projectStream.collect(reducing(0, Project::getStars, Integer::sum));` |\n| `collectingAndThen` | 转换函数返回的类型 | 包含另一个收集器，对其结果应用转换函数 | `int howManyProjects = projectStream.collect(collectingAndThen(toList(), List::size));` |\n| `groupingBy` | `Map<K, List<T>>` | 根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键 | `Map<String,List<Project>> projectByLanguage = projectStream.collect(groupingBy(Project::getLanguage));` |\n| `partitioningBy` | `Map<Boolean,List<T>>` | 根据对流中每个项目应用断言的结果来对项目进行分区 | `Map<Boolean,List<Project>> vegetarianDishes = projectStream.collect(partitioningBy(Project::isVegetarian));` |\n## 参考\n1. [Lambdas](https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#lambdas-)\n2. [Lambda expressions](https://github.com/winterbe/java8-tutorial#lambda-expressions)\n3. [Java 8 动态类型语言Lambda表达式实现原理分析](https://blog.csdn.net/raintungli/article/details/54910152)\n4. [Stream](https://github.com/biezhi/learn-java8/blob/master/java8-stream/README.md#stream)","source":"_posts/java8系列专栏之Lambda与Stream-API.md","raw":"---\ntitle: java8系列专栏之Lambda与Stream API\ntags:\n  - 笔记\noriginContent: ''\ncategories:\n  - java\ntoc: false\ndate: 2019-04-29 20:14:42\n---\n\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Lambda与Stream API\n## 正文\n### Lambda\n#### 什么是Lambda\n\n```\nList<String>list = Arrays.asList(\"a\",\"c\",\"b\");\nCollections.sort(list, (o1,o2)->o1.compareTo(o2));\n```\n以下方式都是常用的lambda使用方式\n```\nstr->str.toLowerCase()\n(o1,o2)->o1.compareTo(o2)\n(o1,o2)->{return o1.compareTo(o2)}\n(String o1,String o2)->{return o1.compareTo(o2)}\n```\n#### 怎么用，哪里用\n函数接口声明既可使用。例如Runnable，Comparator都是函数接口。用@FunctionalInterface声明的都是函数接口\n#### 实现原理\n首先需要明确说明的是lambda没有使用匿名内部类去实现。\n\nJava 8设计人员决定使用在Java 7中添加的`invokedynamic`指令来推迟在运行时的翻译策略。当javac编译代码时，它捕获lambda表达式并生成`invokedynamic`调用站点(称为lambda工厂)。调用`invokedynamic`调用站点时，返回一个函数接口实例，lambda将被转换到这个函数接口\n\n\n使用invokedynamic指令，运行时调用LambdaMetafactory.metafactory动态的生成内部类，实现了接口，内部类里的调用方法块并不是动态生成的，只是在原class里已经编译生成了一个静态的方法，内部类只需要调用该静态方法\n\n[参考1](https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#using-invokedynamic)\n[参考2](https://blog.csdn.net/raintungli/article/details/54910152)\n#### 内置函数接口\n\n| 函数式接口 | 函数描述符 | 原始类型特化 |\n|:-----:|:--------|:-------|\n| `Predicate<T>` | `T->boolean` | `IntPredicate,LongPredicate, DoublePredicate` |\n| `Consumer<T>` | `T->void` | `IntConsumer,LongConsumer, DoubleConsumer` |\n| `Function<T,R>` | `T->R` | `IntFunction<R>, IntToDoubleFunction,` <br/> `IntToLongFunction, LongFunction<R>,` <br/> `LongToDoubleFunction, LongToIntFunction, ` <br/> `DoubleFunction<R>, ToIntFunction<T>, ` <br/> `ToDoubleFunction<T>, ToLongFunction<T>` |\n| `Supplier<T>` | `()->T` | `BooleanSupplier,IntSupplier, LongSupplier, DoubleSupplier` |\n| `UnaryOperator<T>` | `T->T` | `IntUnaryOperator, LongUnaryOperator, DoubleUnaryOperator` |\n| `BinaryOperator<T>` | `(T,T)->T` | `IntBinaryOperator, LongBinaryOperator, DoubleBinaryOperator` |\n| `BiPredicate<L,R>` | `(L,R)->boolean` |  |\n| `BiConsumer<T,U>` | `(T,U)->void` | `ObjIntConsumer<T>, ObjLongConsumer<T>, ObjDoubleConsumer<T>` |\n| `BiFunction<T,U,R>` | `(T,U)->R` | `ToIntBiFunction<T,U>, ToLongBiFunction<T,U>, ToDoubleBiFunction<T,U>` |\n\n### Stream API\n\n| 操作 | 类型 | 返回类型 | 使用的类型/函数式接口 | 函数描述符 |\n|:-----:|:--------|:-------|:-------|:-------|\n| `filter` | 中间 | `Stream<T>` | `Predicate<T>` | `T -> boolean` |\n| `distinct` | 中间 | `Stream<T>` |  |  |\n| `skip` | 中间 | `Stream<T>` | long |  |\n| `map` | 中间 | `Stream<R>` | `Function<T, R>` | `T -> R` |\n| `flatMap` | 中间 | `Stream<R>` | `Function<T, Stream<R>>` | `T -> Stream<R>` |\n| `limit` | 中间 | `Stream<T>` | long  | |\n| `sorted` | 中间 | `Stream<T>` | `Comparator<T>` | `(T, T) -> int` |\n| `anyMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `noneMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `allMatch` | 终端 | `boolean` | `Predicate<T>` | `T -> boolean` |\n| `findAny` | 终端 | `Optional<T>` | |  |\n| `findFirst` | 终端 | `Optional<T>` | |  |\n| `forEach` | 终端 | `void` | `Consumer<T>` | `T -> void` |\n| `collect` | 终端 | `R` | `Collector<T, A, R>` |  |\n| `reduce` | 终端 | `Optional<T>` | `BinaryOperator<T>` | `(T, T) -> T` |\n| `count` | 终端 | `long` | | |\n\n### Collector 收集\n**Collectors 类的静态工厂方法**\n\n| 工厂方法 | 返回类型 | 用途 | 示例 |\n|:-----:|:--------|:-------|:-------|\n| `toList` | `List<T>` | 把流中所有项目收集到一个 List | `List<Project> projects = projectStream.collect(toList());` |\n| `toSet` | `Set<T>` | 把流中所有项目收集到一个 Set，删除重复项 | `Set<Project> projects = projectStream.collect(toSet());` |\n| `toCollection` | `Collection<T>` | 把流中所有项目收集到给定的供应源创建的集合 | `Collection<Project> projects = projectStream.collect(toCollection(), ArrayList::new);` |\n| `counting` | `Long` | 计算流中元素的个数 | `long howManyProjects = projectStream.collect(counting());` |\n| `summingInt` | `Integer` | 对流中项目的一个整数属性求和 | `int totalStars = projectStream.collect(summingInt(Project::getStars));` |\n| `averagingInt` | `Double` | 计算流中项目 Integer 属性的平均值 | `double avgStars = projectStream.collect(averagingInt(Project::getStars));` |\n| `summarizingInt` | `IntSummaryStatistics` | 收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值 | `IntSummaryStatistics projectStatistics = projectStream.collect(summarizingInt(Project::getStars));` |\n| `joining` | `String` | 连接对流中每个项目调用 toString 方法所生成的字符串 | `String shortProject = projectStream.map(Project::getName).collect(joining(\", \"));` |\n| `maxBy` | `Optional<T>` | 按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty() | `Optional<Project> fattest = projectStream.collect(maxBy(comparingInt(Project::getStars)));` |\n| `minBy` | `Optional<T>` | 按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty() | `Optional<Project> fattest = projectStream.collect(minBy(comparingInt(Project::getStars)));` |\n| `reducing` | 归约操作产生的类型 | 从一个作为累加器的初始值开始，利用 BinaryOperator 与流中的元素逐个结合，从而将流归约为单个值 | `int totalStars = projectStream.collect(reducing(0, Project::getStars, Integer::sum));` |\n| `collectingAndThen` | 转换函数返回的类型 | 包含另一个收集器，对其结果应用转换函数 | `int howManyProjects = projectStream.collect(collectingAndThen(toList(), List::size));` |\n| `groupingBy` | `Map<K, List<T>>` | 根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键 | `Map<String,List<Project>> projectByLanguage = projectStream.collect(groupingBy(Project::getLanguage));` |\n| `partitioningBy` | `Map<Boolean,List<T>>` | 根据对流中每个项目应用断言的结果来对项目进行分区 | `Map<Boolean,List<Project>> vegetarianDishes = projectStream.collect(partitioningBy(Project::isVegetarian));` |\n## 参考\n1. [Lambdas](https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#lambdas-)\n2. [Lambda expressions](https://github.com/winterbe/java8-tutorial#lambda-expressions)\n3. [Java 8 动态类型语言Lambda表达式实现原理分析](https://blog.csdn.net/raintungli/article/details/54910152)\n4. [Stream](https://github.com/biezhi/learn-java8/blob/master/java8-stream/README.md#stream)","slug":"java8系列专栏之Lambda与Stream-API","published":1,"updated":"2019-04-29T12:14:42.299Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd100588cee8o8h4xyp","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Lambda与Stream API</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h3><h4 id=\"什么是Lambda\"><a href=\"#什么是Lambda\" class=\"headerlink\" title=\"什么是Lambda\"></a>什么是Lambda</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt;list = Arrays.asList(&quot;a&quot;,&quot;c&quot;,&quot;b&quot;);</span><br><span class=\"line\">Collections.sort(list, (o1,o2)-&gt;o1.compareTo(o2));</span><br></pre></td></tr></table></figure>\n<p>以下方式都是常用的lambda使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">str-&gt;str.toLowerCase()</span><br><span class=\"line\">(o1,o2)-&gt;o1.compareTo(o2)</span><br><span class=\"line\">(o1,o2)-&gt;&#123;return o1.compareTo(o2)&#125;</span><br><span class=\"line\">(String o1,String o2)-&gt;&#123;return o1.compareTo(o2)&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"怎么用，哪里用\"><a href=\"#怎么用，哪里用\" class=\"headerlink\" title=\"怎么用，哪里用\"></a>怎么用，哪里用</h4><p>函数接口声明既可使用。例如Runnable，Comparator都是函数接口。用@FunctionalInterface声明的都是函数接口</p>\n<h4 id=\"实现原理\"><a href=\"#实现原理\" class=\"headerlink\" title=\"实现原理\"></a>实现原理</h4><p>首先需要明确说明的是lambda没有使用匿名内部类去实现。</p>\n<p>Java 8设计人员决定使用在Java 7中添加的<code>invokedynamic</code>指令来推迟在运行时的翻译策略。当javac编译代码时，它捕获lambda表达式并生成<code>invokedynamic</code>调用站点(称为lambda工厂)。调用<code>invokedynamic</code>调用站点时，返回一个函数接口实例，lambda将被转换到这个函数接口</p>\n<p>使用invokedynamic指令，运行时调用LambdaMetafactory.metafactory动态的生成内部类，实现了接口，内部类里的调用方法块并不是动态生成的，只是在原class里已经编译生成了一个静态的方法，内部类只需要调用该静态方法</p>\n<p><a href=\"https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#using-invokedynamic\" target=\"_blank\" rel=\"noopener\">参考1</a><br><a href=\"https://blog.csdn.net/raintungli/article/details/54910152\" target=\"_blank\" rel=\"noopener\">参考2</a></p>\n<h4 id=\"内置函数接口\"><a href=\"#内置函数接口\" class=\"headerlink\" title=\"内置函数接口\"></a>内置函数接口</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">函数式接口</th>\n<th style=\"text-align:left\">函数描述符</th>\n<th style=\"text-align:left\">原始类型特化</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;boolean</code></td>\n<td style=\"text-align:left\"><code>IntPredicate,LongPredicate, DoublePredicate</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Consumer&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;void</code></td>\n<td style=\"text-align:left\"><code>IntConsumer,LongConsumer, DoubleConsumer</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Function&lt;T,R&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;R</code></td>\n<td style=\"text-align:left\"><code>IntFunction&lt;R&gt;, IntToDoubleFunction,</code> <br> <code>IntToLongFunction, LongFunction&lt;R&gt;,</code> <br> <code>LongToDoubleFunction, LongToIntFunction,</code> <br> <code>DoubleFunction&lt;R&gt;, ToIntFunction&lt;T&gt;,</code> <br> <code>ToDoubleFunction&lt;T&gt;, ToLongFunction&lt;T&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Supplier&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>()-&gt;T</code></td>\n<td style=\"text-align:left\"><code>BooleanSupplier,IntSupplier, LongSupplier, DoubleSupplier</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>UnaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;T</code></td>\n<td style=\"text-align:left\"><code>IntUnaryOperator, LongUnaryOperator, DoubleUnaryOperator</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BinaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,T)-&gt;T</code></td>\n<td style=\"text-align:left\"><code>IntBinaryOperator, LongBinaryOperator, DoubleBinaryOperator</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiPredicate&lt;L,R&gt;</code></td>\n<td style=\"text-align:left\"><code>(L,R)-&gt;boolean</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiConsumer&lt;T,U&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,U)-&gt;void</code></td>\n<td style=\"text-align:left\"><code>ObjIntConsumer&lt;T&gt;, ObjLongConsumer&lt;T&gt;, ObjDoubleConsumer&lt;T&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiFunction&lt;T,U,R&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,U)-&gt;R</code></td>\n<td style=\"text-align:left\"><code>ToIntBiFunction&lt;T,U&gt;, ToLongBiFunction&lt;T,U&gt;, ToDoubleBiFunction&lt;T,U&gt;</code></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"Stream-API\"><a href=\"#Stream-API\" class=\"headerlink\" title=\"Stream API\"></a>Stream API</h3><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">操作</th>\n<th style=\"text-align:left\">类型</th>\n<th style=\"text-align:left\">返回类型</th>\n<th style=\"text-align:left\">使用的类型/函数式接口</th>\n<th style=\"text-align:left\">函数描述符</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>filter</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>distinct</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>skip</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\">long</td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>map</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;R&gt;</code></td>\n<td style=\"text-align:left\"><code>Function&lt;T, R&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; R</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>flatMap</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;R&gt;</code></td>\n<td style=\"text-align:left\"><code>Function&lt;T, Stream&lt;R&gt;&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; Stream&lt;R&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>limit</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\">long</td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>sorted</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>Comparator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T, T) -&gt; int</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>anyMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>noneMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>allMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>findAny</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>findFirst</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>forEach</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>void</code></td>\n<td style=\"text-align:left\"><code>Consumer&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; void</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>collect</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>R</code></td>\n<td style=\"text-align:left\"><code>Collector&lt;T, A, R&gt;</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>reduce</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>BinaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T, T) -&gt; T</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>count</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>long</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"Collector-收集\"><a href=\"#Collector-收集\" class=\"headerlink\" title=\"Collector 收集\"></a>Collector 收集</h3><p><strong>Collectors 类的静态工厂方法</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">工厂方法</th>\n<th style=\"text-align:left\">返回类型</th>\n<th style=\"text-align:left\">用途</th>\n<th style=\"text-align:left\">示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>toList</code></td>\n<td style=\"text-align:left\"><code>List&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到一个 List</td>\n<td style=\"text-align:left\"><code>List&lt;Project&gt; projects = projectStream.collect(toList());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>toSet</code></td>\n<td style=\"text-align:left\"><code>Set&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到一个 Set，删除重复项</td>\n<td style=\"text-align:left\"><code>Set&lt;Project&gt; projects = projectStream.collect(toSet());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>toCollection</code></td>\n<td style=\"text-align:left\"><code>Collection&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到给定的供应源创建的集合</td>\n<td style=\"text-align:left\"><code>Collection&lt;Project&gt; projects = projectStream.collect(toCollection(), ArrayList::new);</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>counting</code></td>\n<td style=\"text-align:left\"><code>Long</code></td>\n<td style=\"text-align:left\">计算流中元素的个数</td>\n<td style=\"text-align:left\"><code>long howManyProjects = projectStream.collect(counting());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>summingInt</code></td>\n<td style=\"text-align:left\"><code>Integer</code></td>\n<td style=\"text-align:left\">对流中项目的一个整数属性求和</td>\n<td style=\"text-align:left\"><code>int totalStars = projectStream.collect(summingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>averagingInt</code></td>\n<td style=\"text-align:left\"><code>Double</code></td>\n<td style=\"text-align:left\">计算流中项目 Integer 属性的平均值</td>\n<td style=\"text-align:left\"><code>double avgStars = projectStream.collect(averagingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>summarizingInt</code></td>\n<td style=\"text-align:left\"><code>IntSummaryStatistics</code></td>\n<td style=\"text-align:left\">收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值</td>\n<td style=\"text-align:left\"><code>IntSummaryStatistics projectStatistics = projectStream.collect(summarizingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>joining</code></td>\n<td style=\"text-align:left\"><code>String</code></td>\n<td style=\"text-align:left\">连接对流中每个项目调用 toString 方法所生成的字符串</td>\n<td style=\"text-align:left\"><code>String shortProject = projectStream.map(Project::getName).collect(joining(&quot;, &quot;));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>maxBy</code></td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\">按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty()</td>\n<td style=\"text-align:left\"><code>Optional&lt;Project&gt; fattest = projectStream.collect(maxBy(comparingInt(Project::getStars)));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>minBy</code></td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\">按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty()</td>\n<td style=\"text-align:left\"><code>Optional&lt;Project&gt; fattest = projectStream.collect(minBy(comparingInt(Project::getStars)));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>reducing</code></td>\n<td style=\"text-align:left\">归约操作产生的类型</td>\n<td style=\"text-align:left\">从一个作为累加器的初始值开始，利用 BinaryOperator 与流中的元素逐个结合，从而将流归约为单个值</td>\n<td style=\"text-align:left\"><code>int totalStars = projectStream.collect(reducing(0, Project::getStars, Integer::sum));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>collectingAndThen</code></td>\n<td style=\"text-align:left\">转换函数返回的类型</td>\n<td style=\"text-align:left\">包含另一个收集器，对其结果应用转换函数</td>\n<td style=\"text-align:left\"><code>int howManyProjects = projectStream.collect(collectingAndThen(toList(), List::size));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>groupingBy</code></td>\n<td style=\"text-align:left\"><code>Map&lt;K, List&lt;T&gt;&gt;</code></td>\n<td style=\"text-align:left\">根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键</td>\n<td style=\"text-align:left\"><code>Map&lt;String,List&lt;Project&gt;&gt; projectByLanguage = projectStream.collect(groupingBy(Project::getLanguage));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>partitioningBy</code></td>\n<td style=\"text-align:left\"><code>Map&lt;Boolean,List&lt;T&gt;&gt;</code></td>\n<td style=\"text-align:left\">根据对流中每个项目应用断言的结果来对项目进行分区</td>\n<td style=\"text-align:left\"><code>Map&lt;Boolean,List&lt;Project&gt;&gt; vegetarianDishes = projectStream.collect(partitioningBy(Project::isVegetarian));</code></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#lambdas-\" target=\"_blank\" rel=\"noopener\">Lambdas</a></li>\n<li><a href=\"https://github.com/winterbe/java8-tutorial#lambda-expressions\" target=\"_blank\" rel=\"noopener\">Lambda expressions</a></li>\n<li><a href=\"https://blog.csdn.net/raintungli/article/details/54910152\" target=\"_blank\" rel=\"noopener\">Java 8 动态类型语言Lambda表达式实现原理分析</a></li>\n<li><a href=\"https://github.com/biezhi/learn-java8/blob/master/java8-stream/README.md#stream\" target=\"_blank\" rel=\"noopener\">Stream</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之Lambda与Stream API</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h3><h4 id=\"什么是Lambda\"><a href=\"#什么是Lambda\" class=\"headerlink\" title=\"什么是Lambda\"></a>什么是Lambda</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt;list = Arrays.asList(&quot;a&quot;,&quot;c&quot;,&quot;b&quot;);</span><br><span class=\"line\">Collections.sort(list, (o1,o2)-&gt;o1.compareTo(o2));</span><br></pre></td></tr></table></figure>\n<p>以下方式都是常用的lambda使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">str-&gt;str.toLowerCase()</span><br><span class=\"line\">(o1,o2)-&gt;o1.compareTo(o2)</span><br><span class=\"line\">(o1,o2)-&gt;&#123;return o1.compareTo(o2)&#125;</span><br><span class=\"line\">(String o1,String o2)-&gt;&#123;return o1.compareTo(o2)&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"怎么用，哪里用\"><a href=\"#怎么用，哪里用\" class=\"headerlink\" title=\"怎么用，哪里用\"></a>怎么用，哪里用</h4><p>函数接口声明既可使用。例如Runnable，Comparator都是函数接口。用@FunctionalInterface声明的都是函数接口</p>\n<h4 id=\"实现原理\"><a href=\"#实现原理\" class=\"headerlink\" title=\"实现原理\"></a>实现原理</h4><p>首先需要明确说明的是lambda没有使用匿名内部类去实现。</p>\n<p>Java 8设计人员决定使用在Java 7中添加的<code>invokedynamic</code>指令来推迟在运行时的翻译策略。当javac编译代码时，它捕获lambda表达式并生成<code>invokedynamic</code>调用站点(称为lambda工厂)。调用<code>invokedynamic</code>调用站点时，返回一个函数接口实例，lambda将被转换到这个函数接口</p>\n<p>使用invokedynamic指令，运行时调用LambdaMetafactory.metafactory动态的生成内部类，实现了接口，内部类里的调用方法块并不是动态生成的，只是在原class里已经编译生成了一个静态的方法，内部类只需要调用该静态方法</p>\n<p><a href=\"https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#using-invokedynamic\" target=\"_blank\" rel=\"noopener\">参考1</a><br><a href=\"https://blog.csdn.net/raintungli/article/details/54910152\" target=\"_blank\" rel=\"noopener\">参考2</a></p>\n<h4 id=\"内置函数接口\"><a href=\"#内置函数接口\" class=\"headerlink\" title=\"内置函数接口\"></a>内置函数接口</h4><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">函数式接口</th>\n<th style=\"text-align:left\">函数描述符</th>\n<th style=\"text-align:left\">原始类型特化</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;boolean</code></td>\n<td style=\"text-align:left\"><code>IntPredicate,LongPredicate, DoublePredicate</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Consumer&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;void</code></td>\n<td style=\"text-align:left\"><code>IntConsumer,LongConsumer, DoubleConsumer</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Function&lt;T,R&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;R</code></td>\n<td style=\"text-align:left\"><code>IntFunction&lt;R&gt;, IntToDoubleFunction,</code> <br> <code>IntToLongFunction, LongFunction&lt;R&gt;,</code> <br> <code>LongToDoubleFunction, LongToIntFunction,</code> <br> <code>DoubleFunction&lt;R&gt;, ToIntFunction&lt;T&gt;,</code> <br> <code>ToDoubleFunction&lt;T&gt;, ToLongFunction&lt;T&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>Supplier&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>()-&gt;T</code></td>\n<td style=\"text-align:left\"><code>BooleanSupplier,IntSupplier, LongSupplier, DoubleSupplier</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>UnaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T-&gt;T</code></td>\n<td style=\"text-align:left\"><code>IntUnaryOperator, LongUnaryOperator, DoubleUnaryOperator</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BinaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,T)-&gt;T</code></td>\n<td style=\"text-align:left\"><code>IntBinaryOperator, LongBinaryOperator, DoubleBinaryOperator</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiPredicate&lt;L,R&gt;</code></td>\n<td style=\"text-align:left\"><code>(L,R)-&gt;boolean</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiConsumer&lt;T,U&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,U)-&gt;void</code></td>\n<td style=\"text-align:left\"><code>ObjIntConsumer&lt;T&gt;, ObjLongConsumer&lt;T&gt;, ObjDoubleConsumer&lt;T&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>BiFunction&lt;T,U,R&gt;</code></td>\n<td style=\"text-align:left\"><code>(T,U)-&gt;R</code></td>\n<td style=\"text-align:left\"><code>ToIntBiFunction&lt;T,U&gt;, ToLongBiFunction&lt;T,U&gt;, ToDoubleBiFunction&lt;T,U&gt;</code></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"Stream-API\"><a href=\"#Stream-API\" class=\"headerlink\" title=\"Stream API\"></a>Stream API</h3><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">操作</th>\n<th style=\"text-align:left\">类型</th>\n<th style=\"text-align:left\">返回类型</th>\n<th style=\"text-align:left\">使用的类型/函数式接口</th>\n<th style=\"text-align:left\">函数描述符</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>filter</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>distinct</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>skip</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\">long</td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>map</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;R&gt;</code></td>\n<td style=\"text-align:left\"><code>Function&lt;T, R&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; R</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>flatMap</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;R&gt;</code></td>\n<td style=\"text-align:left\"><code>Function&lt;T, Stream&lt;R&gt;&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; Stream&lt;R&gt;</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>limit</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\">long</td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>sorted</code></td>\n<td style=\"text-align:left\">中间</td>\n<td style=\"text-align:left\"><code>Stream&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>Comparator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T, T) -&gt; int</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>anyMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>noneMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>allMatch</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>boolean</code></td>\n<td style=\"text-align:left\"><code>Predicate&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; boolean</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>findAny</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>findFirst</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>forEach</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>void</code></td>\n<td style=\"text-align:left\"><code>Consumer&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>T -&gt; void</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>collect</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>R</code></td>\n<td style=\"text-align:left\"><code>Collector&lt;T, A, R&gt;</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>reduce</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>BinaryOperator&lt;T&gt;</code></td>\n<td style=\"text-align:left\"><code>(T, T) -&gt; T</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>count</code></td>\n<td style=\"text-align:left\">终端</td>\n<td style=\"text-align:left\"><code>long</code></td>\n<td style=\"text-align:left\"></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"Collector-收集\"><a href=\"#Collector-收集\" class=\"headerlink\" title=\"Collector 收集\"></a>Collector 收集</h3><p><strong>Collectors 类的静态工厂方法</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">工厂方法</th>\n<th style=\"text-align:left\">返回类型</th>\n<th style=\"text-align:left\">用途</th>\n<th style=\"text-align:left\">示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>toList</code></td>\n<td style=\"text-align:left\"><code>List&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到一个 List</td>\n<td style=\"text-align:left\"><code>List&lt;Project&gt; projects = projectStream.collect(toList());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>toSet</code></td>\n<td style=\"text-align:left\"><code>Set&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到一个 Set，删除重复项</td>\n<td style=\"text-align:left\"><code>Set&lt;Project&gt; projects = projectStream.collect(toSet());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>toCollection</code></td>\n<td style=\"text-align:left\"><code>Collection&lt;T&gt;</code></td>\n<td style=\"text-align:left\">把流中所有项目收集到给定的供应源创建的集合</td>\n<td style=\"text-align:left\"><code>Collection&lt;Project&gt; projects = projectStream.collect(toCollection(), ArrayList::new);</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>counting</code></td>\n<td style=\"text-align:left\"><code>Long</code></td>\n<td style=\"text-align:left\">计算流中元素的个数</td>\n<td style=\"text-align:left\"><code>long howManyProjects = projectStream.collect(counting());</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>summingInt</code></td>\n<td style=\"text-align:left\"><code>Integer</code></td>\n<td style=\"text-align:left\">对流中项目的一个整数属性求和</td>\n<td style=\"text-align:left\"><code>int totalStars = projectStream.collect(summingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>averagingInt</code></td>\n<td style=\"text-align:left\"><code>Double</code></td>\n<td style=\"text-align:left\">计算流中项目 Integer 属性的平均值</td>\n<td style=\"text-align:left\"><code>double avgStars = projectStream.collect(averagingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>summarizingInt</code></td>\n<td style=\"text-align:left\"><code>IntSummaryStatistics</code></td>\n<td style=\"text-align:left\">收集关于流中项目 Integer 属性的统计值，例如最大、最小、 总和与平均值</td>\n<td style=\"text-align:left\"><code>IntSummaryStatistics projectStatistics = projectStream.collect(summarizingInt(Project::getStars));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>joining</code></td>\n<td style=\"text-align:left\"><code>String</code></td>\n<td style=\"text-align:left\">连接对流中每个项目调用 toString 方法所生成的字符串</td>\n<td style=\"text-align:left\"><code>String shortProject = projectStream.map(Project::getName).collect(joining(&quot;, &quot;));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>maxBy</code></td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\">按照给定比较器选出的最大元素的 Optional， 或如果流为空则为 Optional.empty()</td>\n<td style=\"text-align:left\"><code>Optional&lt;Project&gt; fattest = projectStream.collect(maxBy(comparingInt(Project::getStars)));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>minBy</code></td>\n<td style=\"text-align:left\"><code>Optional&lt;T&gt;</code></td>\n<td style=\"text-align:left\">按照给定比较器选出的最小元素的 Optional， 或如果流为空则为 Optional.empty()</td>\n<td style=\"text-align:left\"><code>Optional&lt;Project&gt; fattest = projectStream.collect(minBy(comparingInt(Project::getStars)));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>reducing</code></td>\n<td style=\"text-align:left\">归约操作产生的类型</td>\n<td style=\"text-align:left\">从一个作为累加器的初始值开始，利用 BinaryOperator 与流中的元素逐个结合，从而将流归约为单个值</td>\n<td style=\"text-align:left\"><code>int totalStars = projectStream.collect(reducing(0, Project::getStars, Integer::sum));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>collectingAndThen</code></td>\n<td style=\"text-align:left\">转换函数返回的类型</td>\n<td style=\"text-align:left\">包含另一个收集器，对其结果应用转换函数</td>\n<td style=\"text-align:left\"><code>int howManyProjects = projectStream.collect(collectingAndThen(toList(), List::size));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>groupingBy</code></td>\n<td style=\"text-align:left\"><code>Map&lt;K, List&lt;T&gt;&gt;</code></td>\n<td style=\"text-align:left\">根据项目的一个属性的值对流中的项目作问组，并将属性值作 为结果 Map 的键</td>\n<td style=\"text-align:left\"><code>Map&lt;String,List&lt;Project&gt;&gt; projectByLanguage = projectStream.collect(groupingBy(Project::getLanguage));</code></td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>partitioningBy</code></td>\n<td style=\"text-align:left\"><code>Map&lt;Boolean,List&lt;T&gt;&gt;</code></td>\n<td style=\"text-align:left\">根据对流中每个项目应用断言的结果来对项目进行分区</td>\n<td style=\"text-align:left\"><code>Map&lt;Boolean,List&lt;Project&gt;&gt; vegetarianDishes = projectStream.collect(partitioningBy(Project::isVegetarian));</code></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/shekhargulati/java8-the-missing-tutorial/blob/master/02-lambdas.md#lambdas-\" target=\"_blank\" rel=\"noopener\">Lambdas</a></li>\n<li><a href=\"https://github.com/winterbe/java8-tutorial#lambda-expressions\" target=\"_blank\" rel=\"noopener\">Lambda expressions</a></li>\n<li><a href=\"https://blog.csdn.net/raintungli/article/details/54910152\" target=\"_blank\" rel=\"noopener\">Java 8 动态类型语言Lambda表达式实现原理分析</a></li>\n<li><a href=\"https://github.com/biezhi/learn-java8/blob/master/java8-stream/README.md#stream\" target=\"_blank\" rel=\"noopener\">Stream</a></li>\n</ol>\n"},{"title":"java8系列专栏之Optinal","originContent":"","toc":false,"date":"2019-04-28T11:54:10.000Z","_content":"\n## Optional类的方法\n\n| 方法 | 描述 |\n|:-----:|:-------|\n| `empty` | 返回一个空的 Optional 实例 |\n| `filter` | 如果值存在并且满足提供的断言， 就返回包含该值的 Optional 对象；否则返回一个空的 Optional 对象 |\n| `map` | 如果值存在，就对该值执行提供的 mapping 函数调用 |\n| `flatMap` | 如果值存在，就对该值执行提供的 mapping 函数调用，返回一个 Optional 类型的值，否则就返 回一个空的 Optional 对象 |\n| `get` | 如果该值存在，将该值用 Optional 封装返回，否则抛出一个 NoSuchElementException 异常 |\n| `ifPresent` | 如果值存在，就执行使用该值的方法调用，否则什么也不做 |\n| `isPresent` | 如果值存在就返回 true，否则返回 false |\n| `of` | 将指定值用 Optional 封装之后返回，如果该值为 null，则抛出一个 NullPointerException 异常 |\n| `ofNullable` | 将指定值用 Optional 封装之后返回，如果该值为 null，则返回一个空的 Optional 对象 |\n| `orElse` | 如果有值则将其返回，否则返回一个默认值 |\n| `orElseGet` | 如果有值则将其返回，否则返回一个由指定的 Supplier 接口生成的值 |\n| `orElseThrow` | 如果有值则将其返回，否则抛出一个由指定的 Supplier 接口生成的异常 |","source":"_posts/java8系列专栏之Optinal.md","raw":"---\ntitle: java8系列专栏之Optinal\ntags:\n  - 笔记\noriginContent: ''\ncategories:\n  - java\ntoc: false\ndate: 2019-04-28 19:54:10\n---\n\n## Optional类的方法\n\n| 方法 | 描述 |\n|:-----:|:-------|\n| `empty` | 返回一个空的 Optional 实例 |\n| `filter` | 如果值存在并且满足提供的断言， 就返回包含该值的 Optional 对象；否则返回一个空的 Optional 对象 |\n| `map` | 如果值存在，就对该值执行提供的 mapping 函数调用 |\n| `flatMap` | 如果值存在，就对该值执行提供的 mapping 函数调用，返回一个 Optional 类型的值，否则就返 回一个空的 Optional 对象 |\n| `get` | 如果该值存在，将该值用 Optional 封装返回，否则抛出一个 NoSuchElementException 异常 |\n| `ifPresent` | 如果值存在，就执行使用该值的方法调用，否则什么也不做 |\n| `isPresent` | 如果值存在就返回 true，否则返回 false |\n| `of` | 将指定值用 Optional 封装之后返回，如果该值为 null，则抛出一个 NullPointerException 异常 |\n| `ofNullable` | 将指定值用 Optional 封装之后返回，如果该值为 null，则返回一个空的 Optional 对象 |\n| `orElse` | 如果有值则将其返回，否则返回一个默认值 |\n| `orElseGet` | 如果有值则将其返回，否则返回一个由指定的 Supplier 接口生成的值 |\n| `orElseThrow` | 如果有值则将其返回，否则抛出一个由指定的 Supplier 接口生成的异常 |","slug":"java8系列专栏之Optinal","published":1,"updated":"2019-04-28T11:54:10.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd3005c8ceekqlkko6w","content":"<h2 id=\"Optional类的方法\"><a href=\"#Optional类的方法\" class=\"headerlink\" title=\"Optional类的方法\"></a>Optional类的方法</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:left\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>empty</code></td>\n<td style=\"text-align:left\">返回一个空的 Optional 实例</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>filter</code></td>\n<td style=\"text-align:left\">如果值存在并且满足提供的断言， 就返回包含该值的 Optional 对象；否则返回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>map</code></td>\n<td style=\"text-align:left\">如果值存在，就对该值执行提供的 mapping 函数调用</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>flatMap</code></td>\n<td style=\"text-align:left\">如果值存在，就对该值执行提供的 mapping 函数调用，返回一个 Optional 类型的值，否则就返 回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>get</code></td>\n<td style=\"text-align:left\">如果该值存在，将该值用 Optional 封装返回，否则抛出一个 NoSuchElementException 异常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>ifPresent</code></td>\n<td style=\"text-align:left\">如果值存在，就执行使用该值的方法调用，否则什么也不做</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>isPresent</code></td>\n<td style=\"text-align:left\">如果值存在就返回 true，否则返回 false</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>of</code></td>\n<td style=\"text-align:left\">将指定值用 Optional 封装之后返回，如果该值为 null，则抛出一个 NullPointerException 异常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>ofNullable</code></td>\n<td style=\"text-align:left\">将指定值用 Optional 封装之后返回，如果该值为 null，则返回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElse</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则返回一个默认值</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElseGet</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则返回一个由指定的 Supplier 接口生成的值</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElseThrow</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则抛出一个由指定的 Supplier 接口生成的异常</td>\n</tr>\n</tbody>\n</table>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Optional类的方法\"><a href=\"#Optional类的方法\" class=\"headerlink\" title=\"Optional类的方法\"></a>Optional类的方法</h2><table>\n<thead>\n<tr>\n<th style=\"text-align:center\">方法</th>\n<th style=\"text-align:left\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\"><code>empty</code></td>\n<td style=\"text-align:left\">返回一个空的 Optional 实例</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>filter</code></td>\n<td style=\"text-align:left\">如果值存在并且满足提供的断言， 就返回包含该值的 Optional 对象；否则返回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>map</code></td>\n<td style=\"text-align:left\">如果值存在，就对该值执行提供的 mapping 函数调用</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>flatMap</code></td>\n<td style=\"text-align:left\">如果值存在，就对该值执行提供的 mapping 函数调用，返回一个 Optional 类型的值，否则就返 回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>get</code></td>\n<td style=\"text-align:left\">如果该值存在，将该值用 Optional 封装返回，否则抛出一个 NoSuchElementException 异常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>ifPresent</code></td>\n<td style=\"text-align:left\">如果值存在，就执行使用该值的方法调用，否则什么也不做</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>isPresent</code></td>\n<td style=\"text-align:left\">如果值存在就返回 true，否则返回 false</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>of</code></td>\n<td style=\"text-align:left\">将指定值用 Optional 封装之后返回，如果该值为 null，则抛出一个 NullPointerException 异常</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>ofNullable</code></td>\n<td style=\"text-align:left\">将指定值用 Optional 封装之后返回，如果该值为 null，则返回一个空的 Optional 对象</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElse</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则返回一个默认值</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElseGet</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则返回一个由指定的 Supplier 接口生成的值</td>\n</tr>\n<tr>\n<td style=\"text-align:center\"><code>orElseThrow</code></td>\n<td style=\"text-align:left\">如果有值则将其返回，否则抛出一个由指定的 Supplier 接口生成的异常</td>\n</tr>\n</tbody>\n</table>\n"},{"title":"java8系列专栏之字符串","originContent":"","toc":false,"date":"2019-04-28T11:07:00.000Z","_content":"\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之字符串\n## 正文\n### StringJoiner\n##### 详解\n拼接字符串\n##### 用法\n```\n//不指定前缀和后缀\nStringJoiner stringJoiner = new StringJoiner(\",\");\n//指定前缀和后缀\n//StringJoiner stringJoiner = new StringJoiner(\",\",\"{\",\"}\");\nList<String> list = Arrays.asList(\"a\",\"b\",\"c\");\nlist.forEach(str->stringJoiner.add(str));\n```\n### String.join\n##### 详解\n拼接字符串，缺点是无法指定前缀和后缀\n##### 用法\n```\nList<String> list = Arrays.asList(\"a\",\"b\",\"c\");\nSystem.out.println(String.join(\",\", list));\n```\n## 参考\n1. [Java8（2）：JDK 对字符串连接的改进](https://www.jianshu.com/p/0654d0b69eef)","source":"_posts/java8系列专栏之字符串.md","raw":"---\ntitle: java8系列专栏之字符串\ntags:\n  - 笔记\noriginContent: ''\ncategories:\n  - java\ntoc: false\ndate: 2019-04-28 19:07:00\n---\n\n## 前言\n写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之字符串\n## 正文\n### StringJoiner\n##### 详解\n拼接字符串\n##### 用法\n```\n//不指定前缀和后缀\nStringJoiner stringJoiner = new StringJoiner(\",\");\n//指定前缀和后缀\n//StringJoiner stringJoiner = new StringJoiner(\",\",\"{\",\"}\");\nList<String> list = Arrays.asList(\"a\",\"b\",\"c\");\nlist.forEach(str->stringJoiner.add(str));\n```\n### String.join\n##### 详解\n拼接字符串，缺点是无法指定前缀和后缀\n##### 用法\n```\nList<String> list = Arrays.asList(\"a\",\"b\",\"c\");\nSystem.out.println(String.join(\",\", list));\n```\n## 参考\n1. [Java8（2）：JDK 对字符串连接的改进](https://www.jianshu.com/p/0654d0b69eef)","slug":"java8系列专栏之字符串","published":1,"updated":"2019-04-28T11:07:00.989Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd5005f8ceevp7rye92","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之字符串</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"StringJoiner\"><a href=\"#StringJoiner\" class=\"headerlink\" title=\"StringJoiner\"></a>StringJoiner</h3><h5 id=\"详解\"><a href=\"#详解\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>拼接字符串</p>\n<h5 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//不指定前缀和后缀</span><br><span class=\"line\">StringJoiner stringJoiner = new StringJoiner(&quot;,&quot;);</span><br><span class=\"line\">//指定前缀和后缀</span><br><span class=\"line\">//StringJoiner stringJoiner = new StringJoiner(&quot;,&quot;,&quot;&#123;&quot;,&quot;&#125;&quot;);</span><br><span class=\"line\">List&lt;String&gt; list = Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);</span><br><span class=\"line\">list.forEach(str-&gt;stringJoiner.add(str));</span><br></pre></td></tr></table></figure>\n<h3 id=\"String-join\"><a href=\"#String-join\" class=\"headerlink\" title=\"String.join\"></a>String.join</h3><h5 id=\"详解-1\"><a href=\"#详解-1\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>拼接字符串，缺点是无法指定前缀和后缀</p>\n<h5 id=\"用法-1\"><a href=\"#用法-1\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt; list = Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);</span><br><span class=\"line\">System.out.println(String.join(&quot;,&quot;, list));</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.jianshu.com/p/0654d0b69eef\" target=\"_blank\" rel=\"noopener\">Java8（2）：JDK 对字符串连接的改进</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>写这篇文章的目的是为了记录一下学习笔记，其次为了能够在复习的时候快速掌握相关知识。本篇记录java8系列专栏之字符串</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"StringJoiner\"><a href=\"#StringJoiner\" class=\"headerlink\" title=\"StringJoiner\"></a>StringJoiner</h3><h5 id=\"详解\"><a href=\"#详解\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>拼接字符串</p>\n<h5 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//不指定前缀和后缀</span><br><span class=\"line\">StringJoiner stringJoiner = new StringJoiner(&quot;,&quot;);</span><br><span class=\"line\">//指定前缀和后缀</span><br><span class=\"line\">//StringJoiner stringJoiner = new StringJoiner(&quot;,&quot;,&quot;&#123;&quot;,&quot;&#125;&quot;);</span><br><span class=\"line\">List&lt;String&gt; list = Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);</span><br><span class=\"line\">list.forEach(str-&gt;stringJoiner.add(str));</span><br></pre></td></tr></table></figure>\n<h3 id=\"String-join\"><a href=\"#String-join\" class=\"headerlink\" title=\"String.join\"></a>String.join</h3><h5 id=\"详解-1\"><a href=\"#详解-1\" class=\"headerlink\" title=\"详解\"></a>详解</h5><p>拼接字符串，缺点是无法指定前缀和后缀</p>\n<h5 id=\"用法-1\"><a href=\"#用法-1\" class=\"headerlink\" title=\"用法\"></a>用法</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;String&gt; list = Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;);</span><br><span class=\"line\">System.out.println(String.join(&quot;,&quot;, list));</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.jianshu.com/p/0654d0b69eef\" target=\"_blank\" rel=\"noopener\">Java8（2）：JDK 对字符串连接的改进</a></li>\n</ol>\n"},{"title":"jvm 在docker中内存占用问题探索","date":"2018-07-22T10:35:47.000Z","_content":"# jvm 在docker中内存占用问题探索\n\n## 问题背景\n\n最近有个项目在PRD上部署，因为涉及到读取大量数据，会出现内存占用。为了避免因为该项目影响线上其他服务，所以设置了-m=2048，结果发现运行会超过这个值，docker 进程即将该container killed. \n\n随后设置了好几个级别，直到-m=6048,依然无法避免container 被干掉。但是在本地测试和在同事机器上测试，不会出现内存飙升。同样的数据，同样的容器，唯一不同的就是机器物理配置的不同。\n\n\n## 问题原因\n\n线上机器是128g内存，目前制作的jre image 是1.8版本，未设置堆栈等jvm 配置，那么jvm 会字节分配一个默认堆栈大小，这个大小是根据物理机配置分配的。这样就会造成越高的配置，默认分配（**使用1/4的物理内存**）的堆内存就越大，而docker设置限制内存大小，jvm却无法感知，不知道自己在容器中运行。目前存在该问题的不止jvm,一些linux 命令也是如此，例如：top,free,ps等。\n\n因此就会出现container 被docker killed情况。这是个惨痛教训。。。\n\n\n## 问题复现\n\n略...\n\n有个不错的[文章](http://www.linux-ren.org/thread/89699.html)，可以查看一下。\n\n## 解决方案\n\n- **Dockerfile增加jvm参数**\n\n在调用java 可以增加jvm 参数，控制堆栈大小。\n```\nCMD java  $JAVA_OPTIONS -jar java-container.jar\n```\n\n```\n$ docker run -d --name mycontainer8g -p 8080:8080 -m 800M -e JAVA_OPTIONS='-Xmx300m' rafabene/java-container:openjdk-env\n```\n- **选用Fabric8 docker image**\n\n镜像fabric8/java-jboss-openjdk8-jdk使用了脚本来计算容器的内存限制，并且使用50%的内存作为上限。也就是有50%的内存可以写入。你也可以使用这个镜像来开/关调试、诊断或者其他更多的事情\n","source":"_posts/jvm-在docker中内存占用问题探索.md","raw":"---\ntitle: jvm 在docker中内存占用问题探索\ndate: 2018-07-22 18:35:47\ntags: 开发笔记\ncategories:\n         - java\n         - docker\n---\n# jvm 在docker中内存占用问题探索\n\n## 问题背景\n\n最近有个项目在PRD上部署，因为涉及到读取大量数据，会出现内存占用。为了避免因为该项目影响线上其他服务，所以设置了-m=2048，结果发现运行会超过这个值，docker 进程即将该container killed. \n\n随后设置了好几个级别，直到-m=6048,依然无法避免container 被干掉。但是在本地测试和在同事机器上测试，不会出现内存飙升。同样的数据，同样的容器，唯一不同的就是机器物理配置的不同。\n\n\n## 问题原因\n\n线上机器是128g内存，目前制作的jre image 是1.8版本，未设置堆栈等jvm 配置，那么jvm 会字节分配一个默认堆栈大小，这个大小是根据物理机配置分配的。这样就会造成越高的配置，默认分配（**使用1/4的物理内存**）的堆内存就越大，而docker设置限制内存大小，jvm却无法感知，不知道自己在容器中运行。目前存在该问题的不止jvm,一些linux 命令也是如此，例如：top,free,ps等。\n\n因此就会出现container 被docker killed情况。这是个惨痛教训。。。\n\n\n## 问题复现\n\n略...\n\n有个不错的[文章](http://www.linux-ren.org/thread/89699.html)，可以查看一下。\n\n## 解决方案\n\n- **Dockerfile增加jvm参数**\n\n在调用java 可以增加jvm 参数，控制堆栈大小。\n```\nCMD java  $JAVA_OPTIONS -jar java-container.jar\n```\n\n```\n$ docker run -d --name mycontainer8g -p 8080:8080 -m 800M -e JAVA_OPTIONS='-Xmx300m' rafabene/java-container:openjdk-env\n```\n- **选用Fabric8 docker image**\n\n镜像fabric8/java-jboss-openjdk8-jdk使用了脚本来计算容器的内存限制，并且使用50%的内存作为上限。也就是有50%的内存可以写入。你也可以使用这个镜像来开/关调试、诊断或者其他更多的事情\n","slug":"jvm-在docker中内存占用问题探索","published":1,"updated":"2018-07-22T11:34:39.306Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd7005j8ceengkhyyp5","content":"<h1 id=\"jvm-在docker中内存占用问题探索\"><a href=\"#jvm-在docker中内存占用问题探索\" class=\"headerlink\" title=\"jvm 在docker中内存占用问题探索\"></a>jvm 在docker中内存占用问题探索</h1><h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>最近有个项目在PRD上部署，因为涉及到读取大量数据，会出现内存占用。为了避免因为该项目影响线上其他服务，所以设置了-m=2048，结果发现运行会超过这个值，docker 进程即将该container killed. </p>\n<p>随后设置了好几个级别，直到-m=6048,依然无法避免container 被干掉。但是在本地测试和在同事机器上测试，不会出现内存飙升。同样的数据，同样的容器，唯一不同的就是机器物理配置的不同。</p>\n<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>线上机器是128g内存，目前制作的jre image 是1.8版本，未设置堆栈等jvm 配置，那么jvm 会字节分配一个默认堆栈大小，这个大小是根据物理机配置分配的。这样就会造成越高的配置，默认分配（<strong>使用1/4的物理内存</strong>）的堆内存就越大，而docker设置限制内存大小，jvm却无法感知，不知道自己在容器中运行。目前存在该问题的不止jvm,一些linux 命令也是如此，例如：top,free,ps等。</p>\n<p>因此就会出现container 被docker killed情况。这是个惨痛教训。。。</p>\n<h2 id=\"问题复现\"><a href=\"#问题复现\" class=\"headerlink\" title=\"问题复现\"></a>问题复现</h2><p>略…</p>\n<p>有个不错的<a href=\"http://www.linux-ren.org/thread/89699.html\" target=\"_blank\" rel=\"noopener\">文章</a>，可以查看一下。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><ul>\n<li><strong>Dockerfile增加jvm参数</strong></li>\n</ul>\n<p>在调用java 可以增加jvm 参数，控制堆栈大小。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD java  $JAVA_OPTIONS -jar java-container.jar</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d --name mycontainer8g -p 8080:8080 -m 800M -e JAVA_OPTIONS=&apos;-Xmx300m&apos; rafabene/java-container:openjdk-env</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>选用Fabric8 docker image</strong></li>\n</ul>\n<p>镜像fabric8/java-jboss-openjdk8-jdk使用了脚本来计算容器的内存限制，并且使用50%的内存作为上限。也就是有50%的内存可以写入。你也可以使用这个镜像来开/关调试、诊断或者其他更多的事情</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"jvm-在docker中内存占用问题探索\"><a href=\"#jvm-在docker中内存占用问题探索\" class=\"headerlink\" title=\"jvm 在docker中内存占用问题探索\"></a>jvm 在docker中内存占用问题探索</h1><h2 id=\"问题背景\"><a href=\"#问题背景\" class=\"headerlink\" title=\"问题背景\"></a>问题背景</h2><p>最近有个项目在PRD上部署，因为涉及到读取大量数据，会出现内存占用。为了避免因为该项目影响线上其他服务，所以设置了-m=2048，结果发现运行会超过这个值，docker 进程即将该container killed. </p>\n<p>随后设置了好几个级别，直到-m=6048,依然无法避免container 被干掉。但是在本地测试和在同事机器上测试，不会出现内存飙升。同样的数据，同样的容器，唯一不同的就是机器物理配置的不同。</p>\n<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>线上机器是128g内存，目前制作的jre image 是1.8版本，未设置堆栈等jvm 配置，那么jvm 会字节分配一个默认堆栈大小，这个大小是根据物理机配置分配的。这样就会造成越高的配置，默认分配（<strong>使用1/4的物理内存</strong>）的堆内存就越大，而docker设置限制内存大小，jvm却无法感知，不知道自己在容器中运行。目前存在该问题的不止jvm,一些linux 命令也是如此，例如：top,free,ps等。</p>\n<p>因此就会出现container 被docker killed情况。这是个惨痛教训。。。</p>\n<h2 id=\"问题复现\"><a href=\"#问题复现\" class=\"headerlink\" title=\"问题复现\"></a>问题复现</h2><p>略…</p>\n<p>有个不错的<a href=\"http://www.linux-ren.org/thread/89699.html\" target=\"_blank\" rel=\"noopener\">文章</a>，可以查看一下。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><ul>\n<li><strong>Dockerfile增加jvm参数</strong></li>\n</ul>\n<p>在调用java 可以增加jvm 参数，控制堆栈大小。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMD java  $JAVA_OPTIONS -jar java-container.jar</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d --name mycontainer8g -p 8080:8080 -m 800M -e JAVA_OPTIONS=&apos;-Xmx300m&apos; rafabene/java-container:openjdk-env</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>选用Fabric8 docker image</strong></li>\n</ul>\n<p>镜像fabric8/java-jboss-openjdk8-jdk使用了脚本来计算容器的内存限制，并且使用50%的内存作为上限。也就是有50%的内存可以写入。你也可以使用这个镜像来开/关调试、诊断或者其他更多的事情</p>\n"},{"title":"kafka consumer 源码分析（一）Consumer处理流程","originContent":"","toc":false,"date":"2019-06-27T15:03:24.000Z","_content":"\n# kafka consumer 源码分析（一）Consumer处理流程\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n1. consumer处理流程\n2. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\n\n   \n\n\n## 正文\n\n\n\n### Consumer处理流程\n\n#### 核心组件\n\n**ConsumerCoordinator**： 消费者的协调者， 管理消费者的协调过程 \n\n- 维持coordinator节点信息(也就是对consumer进行assignment的节点)\n- 维持当前consumerGroup的信息, 当前consumer已进入consumerGroup\n\n**Fetcher**: 数据请求类\n\n**ConsumerNetworkClient**： 消费者的网络客户端，负责网络传输的流程\n\n**SubscriptionState**： 订阅状态类\n\n**Metadata**： 集群的元数据管理类，使用租约机制\n\n#### 工作流程\n\n![](http://blogstatic.aibibang.com/kafka%20consumer%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89Consumer%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\n### 消费者提交消费位移源码追究\n\n查看[官方API文档](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html)，在描述如何手动提交offset,代码如下：\n\n```java\ntry {\n      while(running) {\n        ConsumerRecords<String, String> records = consumer.poll(Long.MAX_VALUE);\n          for (TopicPartition partition : records.partitions()) {\n              List<ConsumerRecord<String, String>> partitionRecords = records.records(partition);\n            for (ConsumerRecord<String, String> record : partitionRecords) {\n                System.out.println(record.offset() + \": \" + record.value());\n             }\n          long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();\n          //此处为offset+1\n          consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));\n             }\n         }\n  } finally {\n      consumer.close();\n  }\n \n```\n\n> **The committed offset should always be the offset of the next message that your application will read.** Thus, when calling [`commitSync(offsets)`](https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html) you should add one to the offset of the last message processed\n\n意思是在调用`commitSync(offsets)`必须是当前offset+1。\n\n---\n\n其实我很想知道，自动提交offset方式(包含不指定offset,例如：`commitSync()`)，具体源代码在哪里**+1**。\n\n经过孜孜不倦的翻阅代码，感觉好像看懂了一点点，就把自己看懂的那点写出来，如果不对的话，欢迎看到的同学帮忙纠正一下，当不胜感激！\n\n这里就只探究自动提交offset的情形，因为涉及的代码较多，这里只给出相应的关键代码。\n\n```java\n  public void commitSync(Duration timeout) {\n        try {\n            //subscriptions.allConsumed()获取需要提交的offset\n            if (!coordinator.commitOffsetsSync(subscriptions.allConsumed(), time.timer(timeout))) {\n             ...略\n            }\n        } finally {\n            release();\n        }\n    }\n```\n\n```java\n//可以看出allConsumed是从state.value().position中获取相应partition的offset\npublic Map<TopicPartition, OffsetAndMetadata> allConsumed() {\n    Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>();\n    for (PartitionStates.PartitionState<TopicPartitionState> state : assignment.partitionStates()) {\n         if (state.value().hasValidPosition())\n            allConsumed.put(state.topicPartition(), new OffsetAndMetadata(state.value().position));\n        }\n        return allConsumed;\n    }\n```\n\n下来只用查看到什么时候更新state中信息，即可知道提交的offset是如何计算的，首先我们要知道一个知识点，consumer非多线程处理逻辑，因此每次提交offset都是在poll中处理的，因此我们需要查看poll中的逻辑，接着往下看。\n\n```java\nfinal Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);\n                                   |\n                                   V\nfinal Map<TopicPartition, List<ConsumerRecord<K, V>>> records = fetcher.fetchedRecords();\n                                   |\n                                   V\nList<ConsumerRecord<K, V>> records = fetchRecords(nextInLineRecords, recordsRemaining);\n                                   |\n                                   V\n long nextOffset = partitionRecords.nextFetchOffset;\n log.trace(\"Returning fetched records at offset {} for assigned partition {} and update \" +\n                        \"position to {}\", position, partitionRecords.partition, nextOffset);\n//这里就是所谓的offset+1,也就是开头问题的答案！\n subscriptions.position(partitionRecords.partition, nextOffset);\n```\n\n这里其实已经看到答案，但是可能有同学还会问，不是说更新state吗？这里更新的是 `subscriptions.position`，接着往下看\n\n```java \npublic void position(TopicPartition tp, long offset) {\n        assignedState(tp).position(offset);\n}\n\nprivate TopicPartitionState assignedState(TopicPartition tp) {\n        TopicPartitionState state = this.assignment.stateValue(tp);\n        if (state == null)\n            throw new IllegalStateException(\"No current assignment for partition \" + tp);\n        return state;\n}\n```\n\n终于一切真相大白！看了两天源码，累的要死！Enjoy!\n\n## 参考\n\n1. [ The committed offset should always be the offset of the next message that your application will read](https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html)\n2. [KafkaConsumer 流程解析](https://www.jianshu.com/p/7aaab7aeb33b)","source":"_posts/kafka-consumer-源码分析（一）Consumer处理流程.md","raw":"---\ntitle: kafka consumer 源码分析（一）Consumer处理流程\ntags:\n  - kafka\n  - 专栏\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-06-27 23:03:24\n---\n\n# kafka consumer 源码分析（一）Consumer处理流程\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n1. consumer处理流程\n2. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\n\n   \n\n\n## 正文\n\n\n\n### Consumer处理流程\n\n#### 核心组件\n\n**ConsumerCoordinator**： 消费者的协调者， 管理消费者的协调过程 \n\n- 维持coordinator节点信息(也就是对consumer进行assignment的节点)\n- 维持当前consumerGroup的信息, 当前consumer已进入consumerGroup\n\n**Fetcher**: 数据请求类\n\n**ConsumerNetworkClient**： 消费者的网络客户端，负责网络传输的流程\n\n**SubscriptionState**： 订阅状态类\n\n**Metadata**： 集群的元数据管理类，使用租约机制\n\n#### 工作流程\n\n![](http://blogstatic.aibibang.com/kafka%20consumer%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89Consumer%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png)\n\n### 消费者提交消费位移源码追究\n\n查看[官方API文档](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html)，在描述如何手动提交offset,代码如下：\n\n```java\ntry {\n      while(running) {\n        ConsumerRecords<String, String> records = consumer.poll(Long.MAX_VALUE);\n          for (TopicPartition partition : records.partitions()) {\n              List<ConsumerRecord<String, String>> partitionRecords = records.records(partition);\n            for (ConsumerRecord<String, String> record : partitionRecords) {\n                System.out.println(record.offset() + \": \" + record.value());\n             }\n          long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();\n          //此处为offset+1\n          consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset + 1)));\n             }\n         }\n  } finally {\n      consumer.close();\n  }\n \n```\n\n> **The committed offset should always be the offset of the next message that your application will read.** Thus, when calling [`commitSync(offsets)`](https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html) you should add one to the offset of the last message processed\n\n意思是在调用`commitSync(offsets)`必须是当前offset+1。\n\n---\n\n其实我很想知道，自动提交offset方式(包含不指定offset,例如：`commitSync()`)，具体源代码在哪里**+1**。\n\n经过孜孜不倦的翻阅代码，感觉好像看懂了一点点，就把自己看懂的那点写出来，如果不对的话，欢迎看到的同学帮忙纠正一下，当不胜感激！\n\n这里就只探究自动提交offset的情形，因为涉及的代码较多，这里只给出相应的关键代码。\n\n```java\n  public void commitSync(Duration timeout) {\n        try {\n            //subscriptions.allConsumed()获取需要提交的offset\n            if (!coordinator.commitOffsetsSync(subscriptions.allConsumed(), time.timer(timeout))) {\n             ...略\n            }\n        } finally {\n            release();\n        }\n    }\n```\n\n```java\n//可以看出allConsumed是从state.value().position中获取相应partition的offset\npublic Map<TopicPartition, OffsetAndMetadata> allConsumed() {\n    Map<TopicPartition, OffsetAndMetadata> allConsumed = new HashMap<>();\n    for (PartitionStates.PartitionState<TopicPartitionState> state : assignment.partitionStates()) {\n         if (state.value().hasValidPosition())\n            allConsumed.put(state.topicPartition(), new OffsetAndMetadata(state.value().position));\n        }\n        return allConsumed;\n    }\n```\n\n下来只用查看到什么时候更新state中信息，即可知道提交的offset是如何计算的，首先我们要知道一个知识点，consumer非多线程处理逻辑，因此每次提交offset都是在poll中处理的，因此我们需要查看poll中的逻辑，接着往下看。\n\n```java\nfinal Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);\n                                   |\n                                   V\nfinal Map<TopicPartition, List<ConsumerRecord<K, V>>> records = fetcher.fetchedRecords();\n                                   |\n                                   V\nList<ConsumerRecord<K, V>> records = fetchRecords(nextInLineRecords, recordsRemaining);\n                                   |\n                                   V\n long nextOffset = partitionRecords.nextFetchOffset;\n log.trace(\"Returning fetched records at offset {} for assigned partition {} and update \" +\n                        \"position to {}\", position, partitionRecords.partition, nextOffset);\n//这里就是所谓的offset+1,也就是开头问题的答案！\n subscriptions.position(partitionRecords.partition, nextOffset);\n```\n\n这里其实已经看到答案，但是可能有同学还会问，不是说更新state吗？这里更新的是 `subscriptions.position`，接着往下看\n\n```java \npublic void position(TopicPartition tp, long offset) {\n        assignedState(tp).position(offset);\n}\n\nprivate TopicPartitionState assignedState(TopicPartition tp) {\n        TopicPartitionState state = this.assignment.stateValue(tp);\n        if (state == null)\n            throw new IllegalStateException(\"No current assignment for partition \" + tp);\n        return state;\n}\n```\n\n终于一切真相大白！看了两天源码，累的要死！Enjoy!\n\n## 参考\n\n1. [ The committed offset should always be the offset of the next message that your application will read](https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html)\n2. [KafkaConsumer 流程解析](https://www.jianshu.com/p/7aaab7aeb33b)","slug":"kafka-consumer-源码分析（一）Consumer处理流程","published":1,"updated":"2019-06-27T15:03:24.178Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvd9005m8cee2d5dmuhf","content":"<h1 id=\"kafka-consumer-源码分析（一）Consumer处理流程\"><a href=\"#kafka-consumer-源码分析（一）Consumer处理流程\" class=\"headerlink\" title=\"kafka consumer 源码分析（一）Consumer处理流程\"></a>kafka consumer 源码分析（一）Consumer处理流程</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<ol>\n<li>consumer处理流程</li>\n<li>消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Consumer处理流程\"><a href=\"#Consumer处理流程\" class=\"headerlink\" title=\"Consumer处理流程\"></a>Consumer处理流程</h3><h4 id=\"核心组件\"><a href=\"#核心组件\" class=\"headerlink\" title=\"核心组件\"></a>核心组件</h4><p><strong>ConsumerCoordinator</strong>： 消费者的协调者， 管理消费者的协调过程 </p>\n<ul>\n<li>维持coordinator节点信息(也就是对consumer进行assignment的节点)</li>\n<li>维持当前consumerGroup的信息, 当前consumer已进入consumerGroup</li>\n</ul>\n<p><strong>Fetcher</strong>: 数据请求类</p>\n<p><strong>ConsumerNetworkClient</strong>： 消费者的网络客户端，负责网络传输的流程</p>\n<p><strong>SubscriptionState</strong>： 订阅状态类</p>\n<p><strong>Metadata</strong>： 集群的元数据管理类，使用租约机制</p>\n<h4 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h4><p><img src=\"http://blogstatic.aibibang.com/kafka%20consumer%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89Consumer%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h3 id=\"消费者提交消费位移源码追究\"><a href=\"#消费者提交消费位移源码追究\" class=\"headerlink\" title=\"消费者提交消费位移源码追究\"></a>消费者提交消费位移源码追究</h3><p>查看<a href=\"https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\">官方API文档</a>，在描述如何手动提交offset,代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span>(running) &#123;</span><br><span class=\"line\">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE);</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class=\"line\">              List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class=\"line\">                System.out.println(record.offset() + <span class=\"string\">\": \"</span> + record.value());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">          <span class=\"keyword\">long</span> lastOffset = partitionRecords.get(partitionRecords.size() - <span class=\"number\">1</span>).offset();</span><br><span class=\"line\">          <span class=\"comment\">//此处为offset+1</span></span><br><span class=\"line\">          consumer.commitSync(Collections.singletonMap(partition, <span class=\"keyword\">new</span> OffsetAndMetadata(lastOffset + <span class=\"number\">1</span>)));</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      consumer.close();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><strong>The committed offset should always be the offset of the next message that your application will read.</strong> Thus, when calling <a href=\"https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\"><code>commitSync(offsets)</code></a> you should add one to the offset of the last message processed</p>\n</blockquote>\n<p>意思是在调用<code>commitSync(offsets)</code>必须是当前offset+1。</p>\n<hr>\n<p>其实我很想知道，自动提交offset方式(包含不指定offset,例如：<code>commitSync()</code>)，具体源代码在哪里<strong>+1</strong>。</p>\n<p>经过孜孜不倦的翻阅代码，感觉好像看懂了一点点，就把自己看懂的那点写出来，如果不对的话，欢迎看到的同学帮忙纠正一下，当不胜感激！</p>\n<p>这里就只探究自动提交offset的情形，因为涉及的代码较多，这里只给出相应的关键代码。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">commitSync</span><span class=\"params\">(Duration timeout)</span> </span>&#123;</span><br><span class=\"line\">      <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">//subscriptions.allConsumed()获取需要提交的offset</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!coordinator.commitOffsetsSync(subscriptions.allConsumed(), time.timer(timeout))) &#123;</span><br><span class=\"line\">           ...略</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">          release();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//可以看出allConsumed是从state.value().position中获取相应partition的offset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; <span class=\"title\">allConsumed</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Map&lt;TopicPartition, OffsetAndMetadata&gt; allConsumed = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (PartitionStates.PartitionState&lt;TopicPartitionState&gt; state : assignment.partitionStates()) &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (state.value().hasValidPosition())</span><br><span class=\"line\">            allConsumed.put(state.topicPartition(), <span class=\"keyword\">new</span> OffsetAndMetadata(state.value().position));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> allConsumed;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>下来只用查看到什么时候更新state中信息，即可知道提交的offset是如何计算的，首先我们要知道一个知识点，consumer非多线程处理逻辑，因此每次提交offset都是在poll中处理的，因此我们需要查看poll中的逻辑，接着往下看。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\"><span class=\"keyword\">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\">List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining);</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\"> <span class=\"keyword\">long</span> nextOffset = partitionRecords.nextFetchOffset;</span><br><span class=\"line\"> log.trace(<span class=\"string\">\"Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update \"</span> +</span><br><span class=\"line\">                        <span class=\"string\">\"position to &#123;&#125;\"</span>, position, partitionRecords.partition, nextOffset);</span><br><span class=\"line\"><span class=\"comment\">//这里就是所谓的offset+1,也就是开头问题的答案！</span></span><br><span class=\"line\"> subscriptions.position(partitionRecords.partition, nextOffset);</span><br></pre></td></tr></table></figure>\n<p>这里其实已经看到答案，但是可能有同学还会问，不是说更新state吗？这里更新的是 <code>subscriptions.position</code>，接着往下看</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">position</span><span class=\"params\">(TopicPartition tp, <span class=\"keyword\">long</span> offset)</span> </span>&#123;</span><br><span class=\"line\">        assignedState(tp).position(offset);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> TopicPartitionState <span class=\"title\">assignedState</span><span class=\"params\">(TopicPartition tp)</span> </span>&#123;</span><br><span class=\"line\">        TopicPartitionState state = <span class=\"keyword\">this</span>.assignment.stateValue(tp);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (state == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException(<span class=\"string\">\"No current assignment for partition \"</span> + tp);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>终于一切真相大白！看了两天源码，累的要死！Enjoy!</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\"> The committed offset should always be the offset of the next message that your application will read</a></li>\n<li><a href=\"https://www.jianshu.com/p/7aaab7aeb33b\" target=\"_blank\" rel=\"noopener\">KafkaConsumer 流程解析</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka-consumer-源码分析（一）Consumer处理流程\"><a href=\"#kafka-consumer-源码分析（一）Consumer处理流程\" class=\"headerlink\" title=\"kafka consumer 源码分析（一）Consumer处理流程\"></a>kafka consumer 源码分析（一）Consumer处理流程</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<ol>\n<li>consumer处理流程</li>\n<li>消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Consumer处理流程\"><a href=\"#Consumer处理流程\" class=\"headerlink\" title=\"Consumer处理流程\"></a>Consumer处理流程</h3><h4 id=\"核心组件\"><a href=\"#核心组件\" class=\"headerlink\" title=\"核心组件\"></a>核心组件</h4><p><strong>ConsumerCoordinator</strong>： 消费者的协调者， 管理消费者的协调过程 </p>\n<ul>\n<li>维持coordinator节点信息(也就是对consumer进行assignment的节点)</li>\n<li>维持当前consumerGroup的信息, 当前consumer已进入consumerGroup</li>\n</ul>\n<p><strong>Fetcher</strong>: 数据请求类</p>\n<p><strong>ConsumerNetworkClient</strong>： 消费者的网络客户端，负责网络传输的流程</p>\n<p><strong>SubscriptionState</strong>： 订阅状态类</p>\n<p><strong>Metadata</strong>： 集群的元数据管理类，使用租约机制</p>\n<h4 id=\"工作流程\"><a href=\"#工作流程\" class=\"headerlink\" title=\"工作流程\"></a>工作流程</h4><p><img src=\"http://blogstatic.aibibang.com/kafka%20consumer%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89Consumer%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png\" alt=\"\"></p>\n<h3 id=\"消费者提交消费位移源码追究\"><a href=\"#消费者提交消费位移源码追究\" class=\"headerlink\" title=\"消费者提交消费位移源码追究\"></a>消费者提交消费位移源码追究</h3><p>查看<a href=\"https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\">官方API文档</a>，在描述如何手动提交offset,代码如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span>(running) &#123;</span><br><span class=\"line\">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Long.MAX_VALUE);</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class=\"line\">              List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class=\"line\">                System.out.println(record.offset() + <span class=\"string\">\": \"</span> + record.value());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">          <span class=\"keyword\">long</span> lastOffset = partitionRecords.get(partitionRecords.size() - <span class=\"number\">1</span>).offset();</span><br><span class=\"line\">          <span class=\"comment\">//此处为offset+1</span></span><br><span class=\"line\">          consumer.commitSync(Collections.singletonMap(partition, <span class=\"keyword\">new</span> OffsetAndMetadata(lastOffset + <span class=\"number\">1</span>)));</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      consumer.close();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><strong>The committed offset should always be the offset of the next message that your application will read.</strong> Thus, when calling <a href=\"https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\"><code>commitSync(offsets)</code></a> you should add one to the offset of the last message processed</p>\n</blockquote>\n<p>意思是在调用<code>commitSync(offsets)</code>必须是当前offset+1。</p>\n<hr>\n<p>其实我很想知道，自动提交offset方式(包含不指定offset,例如：<code>commitSync()</code>)，具体源代码在哪里<strong>+1</strong>。</p>\n<p>经过孜孜不倦的翻阅代码，感觉好像看懂了一点点，就把自己看懂的那点写出来，如果不对的话，欢迎看到的同学帮忙纠正一下，当不胜感激！</p>\n<p>这里就只探究自动提交offset的情形，因为涉及的代码较多，这里只给出相应的关键代码。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">commitSync</span><span class=\"params\">(Duration timeout)</span> </span>&#123;</span><br><span class=\"line\">      <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">//subscriptions.allConsumed()获取需要提交的offset</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!coordinator.commitOffsetsSync(subscriptions.allConsumed(), time.timer(timeout))) &#123;</span><br><span class=\"line\">           ...略</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">      &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">          release();</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//可以看出allConsumed是从state.value().position中获取相应partition的offset</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; <span class=\"title\">allConsumed</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Map&lt;TopicPartition, OffsetAndMetadata&gt; allConsumed = <span class=\"keyword\">new</span> HashMap&lt;&gt;();</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (PartitionStates.PartitionState&lt;TopicPartitionState&gt; state : assignment.partitionStates()) &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (state.value().hasValidPosition())</span><br><span class=\"line\">            allConsumed.put(state.topicPartition(), <span class=\"keyword\">new</span> OffsetAndMetadata(state.value().position));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> allConsumed;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>下来只用查看到什么时候更新state中信息，即可知道提交的offset是如何计算的，首先我们要知道一个知识点，consumer非多线程处理逻辑，因此每次提交offset都是在poll中处理的，因此我们需要查看poll中的逻辑，接着往下看。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\"><span class=\"keyword\">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\">List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineRecords, recordsRemaining);</span><br><span class=\"line\">                                   |</span><br><span class=\"line\">                                   V</span><br><span class=\"line\"> <span class=\"keyword\">long</span> nextOffset = partitionRecords.nextFetchOffset;</span><br><span class=\"line\"> log.trace(<span class=\"string\">\"Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update \"</span> +</span><br><span class=\"line\">                        <span class=\"string\">\"position to &#123;&#125;\"</span>, position, partitionRecords.partition, nextOffset);</span><br><span class=\"line\"><span class=\"comment\">//这里就是所谓的offset+1,也就是开头问题的答案！</span></span><br><span class=\"line\"> subscriptions.position(partitionRecords.partition, nextOffset);</span><br></pre></td></tr></table></figure>\n<p>这里其实已经看到答案，但是可能有同学还会问，不是说更新state吗？这里更新的是 <code>subscriptions.position</code>，接着往下看</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">position</span><span class=\"params\">(TopicPartition tp, <span class=\"keyword\">long</span> offset)</span> </span>&#123;</span><br><span class=\"line\">        assignedState(tp).position(offset);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> TopicPartitionState <span class=\"title\">assignedState</span><span class=\"params\">(TopicPartition tp)</span> </span>&#123;</span><br><span class=\"line\">        TopicPartitionState state = <span class=\"keyword\">this</span>.assignment.stateValue(tp);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (state == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalStateException(<span class=\"string\">\"No current assignment for partition \"</span> + tp);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> state;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>终于一切真相大白！看了两天源码，累的要死！Enjoy!</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://kafka.apache.org/22/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\" target=\"_blank\" rel=\"noopener\"> The committed offset should always be the offset of the next message that your application will read</a></li>\n<li><a href=\"https://www.jianshu.com/p/7aaab7aeb33b\" target=\"_blank\" rel=\"noopener\">KafkaConsumer 流程解析</a></li>\n</ol>\n"},{"title":"kafka consumer 源码分析（三）Consumer消费再均衡原理探究","originContent":"","toc":false,"date":"2019-06-30T12:26:34.000Z","_content":"\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n消费再均衡的原理是什么？\n\n## 正文\n\n### 消费再均衡的原理\n\n主要分为四步\n\n**1.FIND_COORDINATOR**\n\n根据`hash(group_id)%consumerOffsetPartitionNum`查找出对应的partition,再查找出该partitiom对应的leader所在的broker,即可获得GroupCoordinator\n\n**2.JOIN_GROUP**\n\n在这一步主要完成消费组leader选举（获取第一个加入的组为leader，如果没有,选择map中的第一个node）和分区分配策略\n\n**3.SYNC_GROUP**\n\n客户端向GroupCoordinator发起同步请求，获取步骤2的分区分配方案。\n\n**4.HEARTBEAT**\n\nGroupCoordinator通过心跳来确定从属关系。\n\n## 参考\n\n略\n","source":"_posts/kafka-consumer-源码分析（三）Consumer消费再均衡原理探究.md","raw":"---\ntitle: kafka consumer 源码分析（三）Consumer消费再均衡原理探究\ntags:\n  - kafka\n  - 专栏\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-06-30 20:26:34\n---\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n消费再均衡的原理是什么？\n\n## 正文\n\n### 消费再均衡的原理\n\n主要分为四步\n\n**1.FIND_COORDINATOR**\n\n根据`hash(group_id)%consumerOffsetPartitionNum`查找出对应的partition,再查找出该partitiom对应的leader所在的broker,即可获得GroupCoordinator\n\n**2.JOIN_GROUP**\n\n在这一步主要完成消费组leader选举（获取第一个加入的组为leader，如果没有,选择map中的第一个node）和分区分配策略\n\n**3.SYNC_GROUP**\n\n客户端向GroupCoordinator发起同步请求，获取步骤2的分区分配方案。\n\n**4.HEARTBEAT**\n\nGroupCoordinator通过心跳来确定从属关系。\n\n## 参考\n\n略\n","slug":"kafka-consumer-源码分析（三）Consumer消费再均衡原理探究","published":1,"updated":"2019-06-30T12:26:34.832Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdb005q8ceea98du10a","content":"<h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<p>消费再均衡的原理是什么？</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"消费再均衡的原理\"><a href=\"#消费再均衡的原理\" class=\"headerlink\" title=\"消费再均衡的原理\"></a>消费再均衡的原理</h3><p>主要分为四步</p>\n<p><strong>1.FIND_COORDINATOR</strong></p>\n<p>根据<code>hash(group_id)%consumerOffsetPartitionNum</code>查找出对应的partition,再查找出该partitiom对应的leader所在的broker,即可获得GroupCoordinator</p>\n<p><strong>2.JOIN_GROUP</strong></p>\n<p>在这一步主要完成消费组leader选举（获取第一个加入的组为leader，如果没有,选择map中的第一个node）和分区分配策略</p>\n<p><strong>3.SYNC_GROUP</strong></p>\n<p>客户端向GroupCoordinator发起同步请求，获取步骤2的分区分配方案。</p>\n<p><strong>4.HEARTBEAT</strong></p>\n<p>GroupCoordinator通过心跳来确定从属关系。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>略</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<p>消费再均衡的原理是什么？</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"消费再均衡的原理\"><a href=\"#消费再均衡的原理\" class=\"headerlink\" title=\"消费再均衡的原理\"></a>消费再均衡的原理</h3><p>主要分为四步</p>\n<p><strong>1.FIND_COORDINATOR</strong></p>\n<p>根据<code>hash(group_id)%consumerOffsetPartitionNum</code>查找出对应的partition,再查找出该partitiom对应的leader所在的broker,即可获得GroupCoordinator</p>\n<p><strong>2.JOIN_GROUP</strong></p>\n<p>在这一步主要完成消费组leader选举（获取第一个加入的组为leader，如果没有,选择map中的第一个node）和分区分配策略</p>\n<p><strong>3.SYNC_GROUP</strong></p>\n<p>客户端向GroupCoordinator发起同步请求，获取步骤2的分区分配方案。</p>\n<p><strong>4.HEARTBEAT</strong></p>\n<p>GroupCoordinator通过心跳来确定从属关系。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>略</p>\n"},{"title":"kafka consumer 源码分析（二）分区分配策略","toc":false,"date":"2019-06-27T15:04:18.000Z","_content":"\n# kafka consumer 源码分析（二）分区分配策略\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n分区分配策略是什么样的？\n\n## 正文\n\n### 分区分配策略\n\n这里的分区分配策略仅仅只讨论consumer,关于producer端的分区不在本次探讨范围内。\n\nconsumer端分区分配策略官方有三种：`RangeAssignor`（默认值）,`RoundRobinAssignor`,`StickyAssignor`\n\n通常是通过`partition.assignment.strategy`配置生效。\n\n#### RangeAssignor\n\n```java\norg.apache.kafka.clients.consumer.RangeAssignor#assign\n\nCollections.sort(consumersForTopic);//consumer按字典排序\nint numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();\nint consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();\nList<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic,numPartitionsForTopic);\nint i = 0;\nfor(int n = consumersForTopic.size(); i < n; ++i) {\n     int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);\n     int length = numPartitionsPerConsumer + (i + 1 > consumersWithExtraPartition ? 0 : 1);\n     ((List)assignment.get(consumersForTopic.get(i))).addAll(partitions.subList(start, start + length));\n}\n```\n\n源码太多，这里只贴出核心代码，需要的按照以上目录去找。\n\n**例如**：\n\n有一个topic:**truman** partition:**10** consumer:**consumer0,consumer1,consumer2**。那么numPartitionsPerConsumer=10/3=3，consumersWithExtraPartition=10%3=1。分配逻辑为：当i=0,按字典顺序，先分配consumer0，start=0，length=4，即consumer1分配tp0,tp1,tp2,tp3。同理最终方案为：\n\n```\nconsumer0->tp0,tp1,tp2,tp3\nconsumer1->tp4,tp5,tp6\nconsumer2->tp7,tp8,tp9\n```\n\n\n\n从源码我们能得出如何分配的，还能得出一个结论，consumer分配partition和consumerId有关系，字典顺序靠前的，有可能会多分配partition。\n\n为什么这么说？上面的源码其实没有给出，但能看出来consumersForTopic集合是按字典排序的，那么只用知道consumersForTopic是什么样的集合就能知道了，同样在该源码中可以看到`put(res, topic, consumerId);`,说明该集合是consumerId集合。因此可以得出该结论！\n\n**一句话总结该分配策略，就是尽可能的均分**\n\n#### RoundRobinAssignor\n\n```java\norg.apache.kafka.clients.consumer.RoundRobinAssignor\n//assigner存放了所有的consumer\nCircularIterator<String> assigner = new CircularIterator(Utils.sorted(subscriptions.keySet()));\n//所有的consumer订阅的所有的TopicPartition的List\nIterator var9 = this.allPartitionsSorted(partitionsPerTopic, subscriptions).iterator();\n\nwhile(var9.hasNext()) {\n TopicPartition partition = (TopicPartition)var9.next();\n String topic = partition.topic();\n // 如果当前这个assigner(consumer)没有订阅这个topic，直接跳过\n while(!((Subscription)subscriptions.get(assigner.peek())).topics().contains(topic)) {\n assigner.next();\n}\n//跳出循环，表示终于找到了订阅过这个TopicPartition对应的topic的assigner\n//将这个partition分派给对应的assigner\n((List)assignment.get(assigner.next())).add(partition);\n}\n```\n\nCircularIterator是一个封装类，让一个有限大小的list变成一个RoundRobin方式的无限遍历\n\nRoundRobinAssignor与RangeAssignor最大的区别，是进行分区分配的时候不再逐个topic进行，即不是为某个topic完成了分区分派以后，再进行下一个topic的分区分派，而是首先将这个group中的所有consumer订阅的所有的topic-partition按顺序展开，然后，依次对于每一个topic-partition，在consumer进行round robin，为这个topic-partition选择一个consumer。\n\n**例如**：\n\n有两个topic :t0和t1，每个topic都有3个分区，分区编号从0开始，因此展开以后得到的TopicPartition的List是：[t0p0,t0p1,t0p2,t1p0,t1p1,t1p2]\n\n我们有两个consumer C0,C1,他们都同时订阅了这两个topic。\n\n然后，在这个展开的TopicParititon的List开始进行分派：\n\nt0p0 分配给C0\nt0p1 分配给C1\nt0p2 分配给C0\nt1p0 分配给C1\nt1p1 分配给C0\nt1p1 分配给C1\n从以上分配流程，我们可以很清楚地看到RoundRobinAssignor的两个基本特征：\n\n1. 对所有topic的topic partition求并集\n2. 基于consumer进行RoundRobin轮询\n\n最终分配方案：\n\n```\nC0->[t0p0, t0p2, t1p1]\nC1->[t0p1, t1p0, t1p2]\n```\n\n**RoundRobinAssignor与RangeAssignor的重大区别，就是RoundRobinAssignor是在Group中所有consumer所关注的全体topic中进行分派，而RangeAssignor则是依次对每个topic进行分派。**\n\n那么什么时候选择RoundRobinAssignor，我们再看如下案例：\n\n假如现在有两个Topic:Ta和Tb ，每个topic都有两个partition，同时，有两个消费者Ca和Cb与Cc，这三个消费者全部订阅了这两个主题，那么，通过两种不同的分区分派算法得到的结果将是：\n\nRangeAssignor:\n\n先对Ta进行处理：会将TaP0分派给Ca，将TaP1分派给Cb\n\n在对Tb进行处理：会将TbP0分派给Ca，将TbP1分派给Cb\n\n最终结果是：\n\n```\nCa:[TaP0,TbP0]\nCb:[TaP1,TbP1]\nCc:[]\n```\n\nRoundRobinAssignor：\n\n将所有TopicPartition展开，变成:[TaP0,TaP1,TbP0,TbP1]\n\n将TaP0分派给Ca，将TaP1分派给Cb，将TbP0分派给Cc，将TbP1分派给Ca\n\n最终分派结果是：\n\n```\nCa:[TaP0,TbP1]\nCb:[TaP1]\nCc:[TbP0]\n```\n\n#### StickyAssignor\n\n这个分配器源码较多，实现也要优于RoundRobinAssignor与RangeAssignor\n\n它的主要特点是分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，均衡优先于上一次分配结果。鉴于这两个目标。\n\n\n\n## 参考\n\n1. [Kafka为Consumer分派分区：RangeAssignor和RoundRobinAssignor](https://blog.csdn.net/zhanyuanlin/article/details/76021614)\n2. [Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor](https://blog.csdn.net/u013256816/article/details/81123625)","source":"_posts/kafka-consumer-源码分析（二）分区分配策略.md","raw":"---\ntitle: kafka consumer 源码分析（二）分区分配策略\ntags:\n  - kafka\n  - 专栏\ncategories:\n  - kafka\ntoc: false\ndate: 2019-06-27 23:04:18\n---\n\n# kafka consumer 源码分析（二）分区分配策略\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：\n\n分区分配策略是什么样的？\n\n## 正文\n\n### 分区分配策略\n\n这里的分区分配策略仅仅只讨论consumer,关于producer端的分区不在本次探讨范围内。\n\nconsumer端分区分配策略官方有三种：`RangeAssignor`（默认值）,`RoundRobinAssignor`,`StickyAssignor`\n\n通常是通过`partition.assignment.strategy`配置生效。\n\n#### RangeAssignor\n\n```java\norg.apache.kafka.clients.consumer.RangeAssignor#assign\n\nCollections.sort(consumersForTopic);//consumer按字典排序\nint numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();\nint consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();\nList<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic,numPartitionsForTopic);\nint i = 0;\nfor(int n = consumersForTopic.size(); i < n; ++i) {\n     int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);\n     int length = numPartitionsPerConsumer + (i + 1 > consumersWithExtraPartition ? 0 : 1);\n     ((List)assignment.get(consumersForTopic.get(i))).addAll(partitions.subList(start, start + length));\n}\n```\n\n源码太多，这里只贴出核心代码，需要的按照以上目录去找。\n\n**例如**：\n\n有一个topic:**truman** partition:**10** consumer:**consumer0,consumer1,consumer2**。那么numPartitionsPerConsumer=10/3=3，consumersWithExtraPartition=10%3=1。分配逻辑为：当i=0,按字典顺序，先分配consumer0，start=0，length=4，即consumer1分配tp0,tp1,tp2,tp3。同理最终方案为：\n\n```\nconsumer0->tp0,tp1,tp2,tp3\nconsumer1->tp4,tp5,tp6\nconsumer2->tp7,tp8,tp9\n```\n\n\n\n从源码我们能得出如何分配的，还能得出一个结论，consumer分配partition和consumerId有关系，字典顺序靠前的，有可能会多分配partition。\n\n为什么这么说？上面的源码其实没有给出，但能看出来consumersForTopic集合是按字典排序的，那么只用知道consumersForTopic是什么样的集合就能知道了，同样在该源码中可以看到`put(res, topic, consumerId);`,说明该集合是consumerId集合。因此可以得出该结论！\n\n**一句话总结该分配策略，就是尽可能的均分**\n\n#### RoundRobinAssignor\n\n```java\norg.apache.kafka.clients.consumer.RoundRobinAssignor\n//assigner存放了所有的consumer\nCircularIterator<String> assigner = new CircularIterator(Utils.sorted(subscriptions.keySet()));\n//所有的consumer订阅的所有的TopicPartition的List\nIterator var9 = this.allPartitionsSorted(partitionsPerTopic, subscriptions).iterator();\n\nwhile(var9.hasNext()) {\n TopicPartition partition = (TopicPartition)var9.next();\n String topic = partition.topic();\n // 如果当前这个assigner(consumer)没有订阅这个topic，直接跳过\n while(!((Subscription)subscriptions.get(assigner.peek())).topics().contains(topic)) {\n assigner.next();\n}\n//跳出循环，表示终于找到了订阅过这个TopicPartition对应的topic的assigner\n//将这个partition分派给对应的assigner\n((List)assignment.get(assigner.next())).add(partition);\n}\n```\n\nCircularIterator是一个封装类，让一个有限大小的list变成一个RoundRobin方式的无限遍历\n\nRoundRobinAssignor与RangeAssignor最大的区别，是进行分区分配的时候不再逐个topic进行，即不是为某个topic完成了分区分派以后，再进行下一个topic的分区分派，而是首先将这个group中的所有consumer订阅的所有的topic-partition按顺序展开，然后，依次对于每一个topic-partition，在consumer进行round robin，为这个topic-partition选择一个consumer。\n\n**例如**：\n\n有两个topic :t0和t1，每个topic都有3个分区，分区编号从0开始，因此展开以后得到的TopicPartition的List是：[t0p0,t0p1,t0p2,t1p0,t1p1,t1p2]\n\n我们有两个consumer C0,C1,他们都同时订阅了这两个topic。\n\n然后，在这个展开的TopicParititon的List开始进行分派：\n\nt0p0 分配给C0\nt0p1 分配给C1\nt0p2 分配给C0\nt1p0 分配给C1\nt1p1 分配给C0\nt1p1 分配给C1\n从以上分配流程，我们可以很清楚地看到RoundRobinAssignor的两个基本特征：\n\n1. 对所有topic的topic partition求并集\n2. 基于consumer进行RoundRobin轮询\n\n最终分配方案：\n\n```\nC0->[t0p0, t0p2, t1p1]\nC1->[t0p1, t1p0, t1p2]\n```\n\n**RoundRobinAssignor与RangeAssignor的重大区别，就是RoundRobinAssignor是在Group中所有consumer所关注的全体topic中进行分派，而RangeAssignor则是依次对每个topic进行分派。**\n\n那么什么时候选择RoundRobinAssignor，我们再看如下案例：\n\n假如现在有两个Topic:Ta和Tb ，每个topic都有两个partition，同时，有两个消费者Ca和Cb与Cc，这三个消费者全部订阅了这两个主题，那么，通过两种不同的分区分派算法得到的结果将是：\n\nRangeAssignor:\n\n先对Ta进行处理：会将TaP0分派给Ca，将TaP1分派给Cb\n\n在对Tb进行处理：会将TbP0分派给Ca，将TbP1分派给Cb\n\n最终结果是：\n\n```\nCa:[TaP0,TbP0]\nCb:[TaP1,TbP1]\nCc:[]\n```\n\nRoundRobinAssignor：\n\n将所有TopicPartition展开，变成:[TaP0,TaP1,TbP0,TbP1]\n\n将TaP0分派给Ca，将TaP1分派给Cb，将TbP0分派给Cc，将TbP1分派给Ca\n\n最终分派结果是：\n\n```\nCa:[TaP0,TbP1]\nCb:[TaP1]\nCc:[TbP0]\n```\n\n#### StickyAssignor\n\n这个分配器源码较多，实现也要优于RoundRobinAssignor与RangeAssignor\n\n它的主要特点是分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，均衡优先于上一次分配结果。鉴于这两个目标。\n\n\n\n## 参考\n\n1. [Kafka为Consumer分派分区：RangeAssignor和RoundRobinAssignor](https://blog.csdn.net/zhanyuanlin/article/details/76021614)\n2. [Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor](https://blog.csdn.net/u013256816/article/details/81123625)","slug":"kafka-consumer-源码分析（二）分区分配策略","published":1,"updated":"2019-06-27T15:12:01.195Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdd005t8ceea0ovbc87","content":"<h1 id=\"kafka-consumer-源码分析（二）分区分配策略\"><a href=\"#kafka-consumer-源码分析（二）分区分配策略\" class=\"headerlink\" title=\"kafka consumer 源码分析（二）分区分配策略\"></a>kafka consumer 源码分析（二）分区分配策略</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<p>分区分配策略是什么样的？</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"分区分配策略\"><a href=\"#分区分配策略\" class=\"headerlink\" title=\"分区分配策略\"></a>分区分配策略</h3><p>这里的分区分配策略仅仅只讨论consumer,关于producer端的分区不在本次探讨范围内。</p>\n<p>consumer端分区分配策略官方有三种：<code>RangeAssignor</code>（默认值）,<code>RoundRobinAssignor</code>,<code>StickyAssignor</code></p>\n<p>通常是通过<code>partition.assignment.strategy</code>配置生效。</p>\n<h4 id=\"RangeAssignor\"><a href=\"#RangeAssignor\" class=\"headerlink\" title=\"RangeAssignor\"></a>RangeAssignor</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.kafka.clients.consumer.RangeAssignor#assign</span><br><span class=\"line\"></span><br><span class=\"line\">Collections.sort(consumersForTopic);<span class=\"comment\">//consumer按字典排序</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();</span><br><span class=\"line\"><span class=\"keyword\">int</span> consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();</span><br><span class=\"line\">List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic,numPartitionsForTopic);</span><br><span class=\"line\"><span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> n = consumersForTopic.size(); i &lt; n; ++i) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</span><br><span class=\"line\">     <span class=\"keyword\">int</span> length = numPartitionsPerConsumer + (i + <span class=\"number\">1</span> &gt; consumersWithExtraPartition ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</span><br><span class=\"line\">     ((List)assignment.get(consumersForTopic.get(i))).addAll(partitions.subList(start, start + length));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>源码太多，这里只贴出核心代码，需要的按照以上目录去找。</p>\n<p><strong>例如</strong>：</p>\n<p>有一个topic:<strong>truman</strong> partition:<strong>10</strong> consumer:<strong>consumer0,consumer1,consumer2</strong>。那么numPartitionsPerConsumer=10/3=3，consumersWithExtraPartition=10%3=1。分配逻辑为：当i=0,按字典顺序，先分配consumer0，start=0，length=4，即consumer1分配tp0,tp1,tp2,tp3。同理最终方案为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">consumer0-&gt;tp0,tp1,tp2,tp3</span><br><span class=\"line\">consumer1-&gt;tp4,tp5,tp6</span><br><span class=\"line\">consumer2-&gt;tp7,tp8,tp9</span><br></pre></td></tr></table></figure>\n<p>从源码我们能得出如何分配的，还能得出一个结论，consumer分配partition和consumerId有关系，字典顺序靠前的，有可能会多分配partition。</p>\n<p>为什么这么说？上面的源码其实没有给出，但能看出来consumersForTopic集合是按字典排序的，那么只用知道consumersForTopic是什么样的集合就能知道了，同样在该源码中可以看到<code>put(res, topic, consumerId);</code>,说明该集合是consumerId集合。因此可以得出该结论！</p>\n<p><strong>一句话总结该分配策略，就是尽可能的均分</strong></p>\n<h4 id=\"RoundRobinAssignor\"><a href=\"#RoundRobinAssignor\" class=\"headerlink\" title=\"RoundRobinAssignor\"></a>RoundRobinAssignor</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.kafka.clients.consumer.RoundRobinAssignor</span><br><span class=\"line\"><span class=\"comment\">//assigner存放了所有的consumer</span></span><br><span class=\"line\">CircularIterator&lt;String&gt; assigner = <span class=\"keyword\">new</span> CircularIterator(Utils.sorted(subscriptions.keySet()));</span><br><span class=\"line\"><span class=\"comment\">//所有的consumer订阅的所有的TopicPartition的List</span></span><br><span class=\"line\">Iterator var9 = <span class=\"keyword\">this</span>.allPartitionsSorted(partitionsPerTopic, subscriptions).iterator();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span>(var9.hasNext()) &#123;</span><br><span class=\"line\"> TopicPartition partition = (TopicPartition)var9.next();</span><br><span class=\"line\"> String topic = partition.topic();</span><br><span class=\"line\"> <span class=\"comment\">// 如果当前这个assigner(consumer)没有订阅这个topic，直接跳过</span></span><br><span class=\"line\"> <span class=\"keyword\">while</span>(!((Subscription)subscriptions.get(assigner.peek())).topics().contains(topic)) &#123;</span><br><span class=\"line\"> assigner.next();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//跳出循环，表示终于找到了订阅过这个TopicPartition对应的topic的assigner</span></span><br><span class=\"line\"><span class=\"comment\">//将这个partition分派给对应的assigner</span></span><br><span class=\"line\">((List)assignment.get(assigner.next())).add(partition);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>CircularIterator是一个封装类，让一个有限大小的list变成一个RoundRobin方式的无限遍历</p>\n<p>RoundRobinAssignor与RangeAssignor最大的区别，是进行分区分配的时候不再逐个topic进行，即不是为某个topic完成了分区分派以后，再进行下一个topic的分区分派，而是首先将这个group中的所有consumer订阅的所有的topic-partition按顺序展开，然后，依次对于每一个topic-partition，在consumer进行round robin，为这个topic-partition选择一个consumer。</p>\n<p><strong>例如</strong>：</p>\n<p>有两个topic :t0和t1，每个topic都有3个分区，分区编号从0开始，因此展开以后得到的TopicPartition的List是：[t0p0,t0p1,t0p2,t1p0,t1p1,t1p2]</p>\n<p>我们有两个consumer C0,C1,他们都同时订阅了这两个topic。</p>\n<p>然后，在这个展开的TopicParititon的List开始进行分派：</p>\n<p>t0p0 分配给C0<br>t0p1 分配给C1<br>t0p2 分配给C0<br>t1p0 分配给C1<br>t1p1 分配给C0<br>t1p1 分配给C1<br>从以上分配流程，我们可以很清楚地看到RoundRobinAssignor的两个基本特征：</p>\n<ol>\n<li>对所有topic的topic partition求并集</li>\n<li>基于consumer进行RoundRobin轮询</li>\n</ol>\n<p>最终分配方案：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C0-&gt;[t0p0, t0p2, t1p1]</span><br><span class=\"line\">C1-&gt;[t0p1, t1p0, t1p2]</span><br></pre></td></tr></table></figure>\n<p><strong>RoundRobinAssignor与RangeAssignor的重大区别，就是RoundRobinAssignor是在Group中所有consumer所关注的全体topic中进行分派，而RangeAssignor则是依次对每个topic进行分派。</strong></p>\n<p>那么什么时候选择RoundRobinAssignor，我们再看如下案例：</p>\n<p>假如现在有两个Topic:Ta和Tb ，每个topic都有两个partition，同时，有两个消费者Ca和Cb与Cc，这三个消费者全部订阅了这两个主题，那么，通过两种不同的分区分派算法得到的结果将是：</p>\n<p>RangeAssignor:</p>\n<p>先对Ta进行处理：会将TaP0分派给Ca，将TaP1分派给Cb</p>\n<p>在对Tb进行处理：会将TbP0分派给Ca，将TbP1分派给Cb</p>\n<p>最终结果是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ca:[TaP0,TbP0]</span><br><span class=\"line\">Cb:[TaP1,TbP1]</span><br><span class=\"line\">Cc:[]</span><br></pre></td></tr></table></figure>\n<p>RoundRobinAssignor：</p>\n<p>将所有TopicPartition展开，变成:[TaP0,TaP1,TbP0,TbP1]</p>\n<p>将TaP0分派给Ca，将TaP1分派给Cb，将TbP0分派给Cc，将TbP1分派给Ca</p>\n<p>最终分派结果是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ca:[TaP0,TbP1]</span><br><span class=\"line\">Cb:[TaP1]</span><br><span class=\"line\">Cc:[TbP0]</span><br></pre></td></tr></table></figure>\n<h4 id=\"StickyAssignor\"><a href=\"#StickyAssignor\" class=\"headerlink\" title=\"StickyAssignor\"></a>StickyAssignor</h4><p>这个分配器源码较多，实现也要优于RoundRobinAssignor与RangeAssignor</p>\n<p>它的主要特点是分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，均衡优先于上一次分配结果。鉴于这两个目标。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/zhanyuanlin/article/details/76021614\" target=\"_blank\" rel=\"noopener\">Kafka为Consumer分派分区：RangeAssignor和RoundRobinAssignor</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/81123625\" target=\"_blank\" rel=\"noopener\">Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka-consumer-源码分析（二）分区分配策略\"><a href=\"#kafka-consumer-源码分析（二）分区分配策略\" class=\"headerlink\" title=\"kafka consumer 源码分析（二）分区分配策略\"></a>kafka consumer 源码分析（二）分区分配策略</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章主要通过研究consumer源码解决如下问题：</p>\n<p>分区分配策略是什么样的？</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"分区分配策略\"><a href=\"#分区分配策略\" class=\"headerlink\" title=\"分区分配策略\"></a>分区分配策略</h3><p>这里的分区分配策略仅仅只讨论consumer,关于producer端的分区不在本次探讨范围内。</p>\n<p>consumer端分区分配策略官方有三种：<code>RangeAssignor</code>（默认值）,<code>RoundRobinAssignor</code>,<code>StickyAssignor</code></p>\n<p>通常是通过<code>partition.assignment.strategy</code>配置生效。</p>\n<h4 id=\"RangeAssignor\"><a href=\"#RangeAssignor\" class=\"headerlink\" title=\"RangeAssignor\"></a>RangeAssignor</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.kafka.clients.consumer.RangeAssignor#assign</span><br><span class=\"line\"></span><br><span class=\"line\">Collections.sort(consumersForTopic);<span class=\"comment\">//consumer按字典排序</span></span><br><span class=\"line\"><span class=\"keyword\">int</span> numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();</span><br><span class=\"line\"><span class=\"keyword\">int</span> consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();</span><br><span class=\"line\">List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic,numPartitionsForTopic);</span><br><span class=\"line\"><span class=\"keyword\">int</span> i = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> n = consumersForTopic.size(); i &lt; n; ++i) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</span><br><span class=\"line\">     <span class=\"keyword\">int</span> length = numPartitionsPerConsumer + (i + <span class=\"number\">1</span> &gt; consumersWithExtraPartition ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</span><br><span class=\"line\">     ((List)assignment.get(consumersForTopic.get(i))).addAll(partitions.subList(start, start + length));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>源码太多，这里只贴出核心代码，需要的按照以上目录去找。</p>\n<p><strong>例如</strong>：</p>\n<p>有一个topic:<strong>truman</strong> partition:<strong>10</strong> consumer:<strong>consumer0,consumer1,consumer2</strong>。那么numPartitionsPerConsumer=10/3=3，consumersWithExtraPartition=10%3=1。分配逻辑为：当i=0,按字典顺序，先分配consumer0，start=0，length=4，即consumer1分配tp0,tp1,tp2,tp3。同理最终方案为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">consumer0-&gt;tp0,tp1,tp2,tp3</span><br><span class=\"line\">consumer1-&gt;tp4,tp5,tp6</span><br><span class=\"line\">consumer2-&gt;tp7,tp8,tp9</span><br></pre></td></tr></table></figure>\n<p>从源码我们能得出如何分配的，还能得出一个结论，consumer分配partition和consumerId有关系，字典顺序靠前的，有可能会多分配partition。</p>\n<p>为什么这么说？上面的源码其实没有给出，但能看出来consumersForTopic集合是按字典排序的，那么只用知道consumersForTopic是什么样的集合就能知道了，同样在该源码中可以看到<code>put(res, topic, consumerId);</code>,说明该集合是consumerId集合。因此可以得出该结论！</p>\n<p><strong>一句话总结该分配策略，就是尽可能的均分</strong></p>\n<h4 id=\"RoundRobinAssignor\"><a href=\"#RoundRobinAssignor\" class=\"headerlink\" title=\"RoundRobinAssignor\"></a>RoundRobinAssignor</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">org.apache.kafka.clients.consumer.RoundRobinAssignor</span><br><span class=\"line\"><span class=\"comment\">//assigner存放了所有的consumer</span></span><br><span class=\"line\">CircularIterator&lt;String&gt; assigner = <span class=\"keyword\">new</span> CircularIterator(Utils.sorted(subscriptions.keySet()));</span><br><span class=\"line\"><span class=\"comment\">//所有的consumer订阅的所有的TopicPartition的List</span></span><br><span class=\"line\">Iterator var9 = <span class=\"keyword\">this</span>.allPartitionsSorted(partitionsPerTopic, subscriptions).iterator();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span>(var9.hasNext()) &#123;</span><br><span class=\"line\"> TopicPartition partition = (TopicPartition)var9.next();</span><br><span class=\"line\"> String topic = partition.topic();</span><br><span class=\"line\"> <span class=\"comment\">// 如果当前这个assigner(consumer)没有订阅这个topic，直接跳过</span></span><br><span class=\"line\"> <span class=\"keyword\">while</span>(!((Subscription)subscriptions.get(assigner.peek())).topics().contains(topic)) &#123;</span><br><span class=\"line\"> assigner.next();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//跳出循环，表示终于找到了订阅过这个TopicPartition对应的topic的assigner</span></span><br><span class=\"line\"><span class=\"comment\">//将这个partition分派给对应的assigner</span></span><br><span class=\"line\">((List)assignment.get(assigner.next())).add(partition);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>CircularIterator是一个封装类，让一个有限大小的list变成一个RoundRobin方式的无限遍历</p>\n<p>RoundRobinAssignor与RangeAssignor最大的区别，是进行分区分配的时候不再逐个topic进行，即不是为某个topic完成了分区分派以后，再进行下一个topic的分区分派，而是首先将这个group中的所有consumer订阅的所有的topic-partition按顺序展开，然后，依次对于每一个topic-partition，在consumer进行round robin，为这个topic-partition选择一个consumer。</p>\n<p><strong>例如</strong>：</p>\n<p>有两个topic :t0和t1，每个topic都有3个分区，分区编号从0开始，因此展开以后得到的TopicPartition的List是：[t0p0,t0p1,t0p2,t1p0,t1p1,t1p2]</p>\n<p>我们有两个consumer C0,C1,他们都同时订阅了这两个topic。</p>\n<p>然后，在这个展开的TopicParititon的List开始进行分派：</p>\n<p>t0p0 分配给C0<br>t0p1 分配给C1<br>t0p2 分配给C0<br>t1p0 分配给C1<br>t1p1 分配给C0<br>t1p1 分配给C1<br>从以上分配流程，我们可以很清楚地看到RoundRobinAssignor的两个基本特征：</p>\n<ol>\n<li>对所有topic的topic partition求并集</li>\n<li>基于consumer进行RoundRobin轮询</li>\n</ol>\n<p>最终分配方案：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C0-&gt;[t0p0, t0p2, t1p1]</span><br><span class=\"line\">C1-&gt;[t0p1, t1p0, t1p2]</span><br></pre></td></tr></table></figure>\n<p><strong>RoundRobinAssignor与RangeAssignor的重大区别，就是RoundRobinAssignor是在Group中所有consumer所关注的全体topic中进行分派，而RangeAssignor则是依次对每个topic进行分派。</strong></p>\n<p>那么什么时候选择RoundRobinAssignor，我们再看如下案例：</p>\n<p>假如现在有两个Topic:Ta和Tb ，每个topic都有两个partition，同时，有两个消费者Ca和Cb与Cc，这三个消费者全部订阅了这两个主题，那么，通过两种不同的分区分派算法得到的结果将是：</p>\n<p>RangeAssignor:</p>\n<p>先对Ta进行处理：会将TaP0分派给Ca，将TaP1分派给Cb</p>\n<p>在对Tb进行处理：会将TbP0分派给Ca，将TbP1分派给Cb</p>\n<p>最终结果是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ca:[TaP0,TbP0]</span><br><span class=\"line\">Cb:[TaP1,TbP1]</span><br><span class=\"line\">Cc:[]</span><br></pre></td></tr></table></figure>\n<p>RoundRobinAssignor：</p>\n<p>将所有TopicPartition展开，变成:[TaP0,TaP1,TbP0,TbP1]</p>\n<p>将TaP0分派给Ca，将TaP1分派给Cb，将TbP0分派给Cc，将TbP1分派给Ca</p>\n<p>最终分派结果是：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ca:[TaP0,TbP1]</span><br><span class=\"line\">Cb:[TaP1]</span><br><span class=\"line\">Cc:[TbP0]</span><br></pre></td></tr></table></figure>\n<h4 id=\"StickyAssignor\"><a href=\"#StickyAssignor\" class=\"headerlink\" title=\"StickyAssignor\"></a>StickyAssignor</h4><p>这个分配器源码较多，实现也要优于RoundRobinAssignor与RangeAssignor</p>\n<p>它的主要特点是分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，均衡优先于上一次分配结果。鉴于这两个目标。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/zhanyuanlin/article/details/76021614\" target=\"_blank\" rel=\"noopener\">Kafka为Consumer分派分区：RangeAssignor和RoundRobinAssignor</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/81123625\" target=\"_blank\" rel=\"noopener\">Kafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor</a></li>\n</ol>\n"},{"title":"kafka patition计算","date":"2017-03-05T12:50:42.000Z","_content":"# kafka patition计算\nkafka生产信息的时候，可以指定该信息存储在具体的patition上，如果不指定会使用默认的算法计算要落的patition.以下是针对此做一个探讨和理解\n\n## 消息路由\n1. 指定了 patition，则直接使用；\n2. 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition\n3. patition 和 key 都未指定，使用轮询选出一个 patition。\n\n## Kafka Client计算patition\n详细代码如下：\n```java\n//创建消息实例\npublic ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value) {\n     if (topic == null)\n          throw new IllegalArgumentException(\"Topic cannot be null\");\n     if (timestamp != null && timestamp < 0)\n          throw new IllegalArgumentException(\"Invalid timestamp \" + timestamp);\n     this.topic = topic;\n     this.partition = partition;\n     this.key = key;\n     this.value = value;\n     this.timestamp = timestamp;\n}\n\n//计算 patition，如果指定了 patition 则直接使用，否则使用 key 计算\nprivate int partition(ProducerRecord<K, V> record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) {\n     Integer partition = record.partition();\n     if (partition != null) {\n          List<PartitionInfo> partitions = cluster.partitionsForTopic(record.topic());\n          int lastPartition = partitions.size() - 1;\n          if (partition < 0 || partition > lastPartition) {\n               throw new IllegalArgumentException(String.format(\"Invalid partition given with record: %d is not in the range [0...%d].\", partition, lastPartition));\n          }\n          return partition;\n     }\n     return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);\n}\n\n// 使用 key 选取 patition\npublic int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n     List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n     int numPartitions = partitions.size();\n     if (keyBytes == null) {\n          int nextValue = counter.getAndIncrement();\n          List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);\n          if (availablePartitions.size() > 0) {\n               int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size();\n               return availablePartitions.get(part).partition();\n          } else {\n               return DefaultPartitioner.toPositive(nextValue) % numPartitions;\n          }\n     } else {\n          //对 keyBytes 进行 hash 选出一个 patition\n          return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n     }\n}\n```\n","source":"_posts/kafka-patition计算.md","raw":"---\ntitle: kafka patition计算\ndate: 2017-03-05 20:50:42\ntags: 大数据\ncategories:\n- kafka\n---\n# kafka patition计算\nkafka生产信息的时候，可以指定该信息存储在具体的patition上，如果不指定会使用默认的算法计算要落的patition.以下是针对此做一个探讨和理解\n\n## 消息路由\n1. 指定了 patition，则直接使用；\n2. 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition\n3. patition 和 key 都未指定，使用轮询选出一个 patition。\n\n## Kafka Client计算patition\n详细代码如下：\n```java\n//创建消息实例\npublic ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value) {\n     if (topic == null)\n          throw new IllegalArgumentException(\"Topic cannot be null\");\n     if (timestamp != null && timestamp < 0)\n          throw new IllegalArgumentException(\"Invalid timestamp \" + timestamp);\n     this.topic = topic;\n     this.partition = partition;\n     this.key = key;\n     this.value = value;\n     this.timestamp = timestamp;\n}\n\n//计算 patition，如果指定了 patition 则直接使用，否则使用 key 计算\nprivate int partition(ProducerRecord<K, V> record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) {\n     Integer partition = record.partition();\n     if (partition != null) {\n          List<PartitionInfo> partitions = cluster.partitionsForTopic(record.topic());\n          int lastPartition = partitions.size() - 1;\n          if (partition < 0 || partition > lastPartition) {\n               throw new IllegalArgumentException(String.format(\"Invalid partition given with record: %d is not in the range [0...%d].\", partition, lastPartition));\n          }\n          return partition;\n     }\n     return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);\n}\n\n// 使用 key 选取 patition\npublic int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n     List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n     int numPartitions = partitions.size();\n     if (keyBytes == null) {\n          int nextValue = counter.getAndIncrement();\n          List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);\n          if (availablePartitions.size() > 0) {\n               int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size();\n               return availablePartitions.get(part).partition();\n          } else {\n               return DefaultPartitioner.toPositive(nextValue) % numPartitions;\n          }\n     } else {\n          //对 keyBytes 进行 hash 选出一个 patition\n          return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n     }\n}\n```\n","slug":"kafka-patition计算","published":1,"updated":"2017-03-05T12:51:26.092Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdf005w8ceeoj7afcr8","content":"<h1 id=\"kafka-patition计算\"><a href=\"#kafka-patition计算\" class=\"headerlink\" title=\"kafka patition计算\"></a>kafka patition计算</h1><p>kafka生产信息的时候，可以指定该信息存储在具体的patition上，如果不指定会使用默认的算法计算要落的patition.以下是针对此做一个探讨和理解</p>\n<h2 id=\"消息路由\"><a href=\"#消息路由\" class=\"headerlink\" title=\"消息路由\"></a>消息路由</h2><ol>\n<li>指定了 patition，则直接使用；</li>\n<li>未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition</li>\n<li>patition 和 key 都未指定，使用轮询选出一个 patition。</li>\n</ol>\n<h2 id=\"Kafka-Client计算patition\"><a href=\"#Kafka-Client计算patition\" class=\"headerlink\" title=\"Kafka Client计算patition\"></a>Kafka Client计算patition</h2><p>详细代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建消息实例</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ProducerRecord</span><span class=\"params\">(String topic, Integer partition, Long timestamp, K key, V value)</span> </span>&#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (topic == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"Topic cannot be null\"</span>);</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (timestamp != <span class=\"keyword\">null</span> &amp;&amp; timestamp &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"Invalid timestamp \"</span> + timestamp);</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.topic = topic;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.partition = partition;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.key = key;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.value = value;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.timestamp = timestamp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//计算 patition，如果指定了 patition 则直接使用，否则使用 key 计算</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(ProducerRecord&lt;K, V&gt; record, <span class=\"keyword\">byte</span>[] serializedKey , <span class=\"keyword\">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">     Integer partition = record.partition();</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (partition != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">          List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic());</span><br><span class=\"line\">          <span class=\"keyword\">int</span> lastPartition = partitions.size() - <span class=\"number\">1</span>;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (partition &lt; <span class=\"number\">0</span> || partition &gt; lastPartition) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(String.format(<span class=\"string\">\"Invalid partition given with record: %d is not in the range [0...%d].\"</span>, partition, lastPartition));</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> partition;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用 key 选取 patition</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(String topic, Object key, <span class=\"keyword\">byte</span>[] keyBytes, Object value, <span class=\"keyword\">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">     List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class=\"line\">     <span class=\"keyword\">int</span> numPartitions = partitions.size();</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (keyBytes == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">int</span> nextValue = counter.getAndIncrement();</span><br><span class=\"line\">          List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (availablePartitions.size() &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size();</span><br><span class=\"line\">               <span class=\"keyword\">return</span> availablePartitions.get(part).partition();</span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">               <span class=\"keyword\">return</span> DefaultPartitioner.toPositive(nextValue) % numPartitions;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">//对 keyBytes 进行 hash 选出一个 patition</span></span><br><span class=\"line\">          <span class=\"keyword\">return</span> DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka-patition计算\"><a href=\"#kafka-patition计算\" class=\"headerlink\" title=\"kafka patition计算\"></a>kafka patition计算</h1><p>kafka生产信息的时候，可以指定该信息存储在具体的patition上，如果不指定会使用默认的算法计算要落的patition.以下是针对此做一个探讨和理解</p>\n<h2 id=\"消息路由\"><a href=\"#消息路由\" class=\"headerlink\" title=\"消息路由\"></a>消息路由</h2><ol>\n<li>指定了 patition，则直接使用；</li>\n<li>未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition</li>\n<li>patition 和 key 都未指定，使用轮询选出一个 patition。</li>\n</ol>\n<h2 id=\"Kafka-Client计算patition\"><a href=\"#Kafka-Client计算patition\" class=\"headerlink\" title=\"Kafka Client计算patition\"></a>Kafka Client计算patition</h2><p>详细代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建消息实例</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">ProducerRecord</span><span class=\"params\">(String topic, Integer partition, Long timestamp, K key, V value)</span> </span>&#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (topic == <span class=\"keyword\">null</span>)</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"Topic cannot be null\"</span>);</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (timestamp != <span class=\"keyword\">null</span> &amp;&amp; timestamp &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"Invalid timestamp \"</span> + timestamp);</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.topic = topic;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.partition = partition;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.key = key;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.value = value;</span><br><span class=\"line\">     <span class=\"keyword\">this</span>.timestamp = timestamp;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//计算 patition，如果指定了 patition 则直接使用，否则使用 key 计算</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(ProducerRecord&lt;K, V&gt; record, <span class=\"keyword\">byte</span>[] serializedKey , <span class=\"keyword\">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">     Integer partition = record.partition();</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (partition != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">          List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic());</span><br><span class=\"line\">          <span class=\"keyword\">int</span> lastPartition = partitions.size() - <span class=\"number\">1</span>;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (partition &lt; <span class=\"number\">0</span> || partition &gt; lastPartition) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(String.format(<span class=\"string\">\"Invalid partition given with record: %d is not in the range [0...%d].\"</span>, partition, lastPartition));</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> partition;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 使用 key 选取 patition</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">partition</span><span class=\"params\">(String topic, Object key, <span class=\"keyword\">byte</span>[] keyBytes, Object value, <span class=\"keyword\">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class=\"line\">     List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class=\"line\">     <span class=\"keyword\">int</span> numPartitions = partitions.size();</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (keyBytes == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">int</span> nextValue = counter.getAndIncrement();</span><br><span class=\"line\">          List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (availablePartitions.size() &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">int</span> part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size();</span><br><span class=\"line\">               <span class=\"keyword\">return</span> availablePartitions.get(part).partition();</span><br><span class=\"line\">          &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">               <span class=\"keyword\">return</span> DefaultPartitioner.toPositive(nextValue) % numPartitions;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          <span class=\"comment\">//对 keyBytes 进行 hash 选出一个 patition</span></span><br><span class=\"line\">          <span class=\"keyword\">return</span> DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"kafka 源码 idea 开发环境搭建","toc":false,"date":"2019-06-14T13:45:03.000Z","_content":"\n## 前言\n\n为了深入研究kafka运行原理和架构，很有必要搭建开发环境，这样我们就很方便的debug 服务。\n## 前期准备\n\n1. Gradle 5.0+\n2. jdk8\n3. Scala 2.12\n4. [idea scale plugin](https://plugins.jetbrains.com/plugin/1347-scala)\n\n## 配置/运行\n\n- 首先执行在源码目录下执行`gradle`\n- 然后build `./gradlew jar`\n- 最后生成idea工程`./gradlew idea`\n\n### 运行MirrorMaker\n\n![](http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_2.png)\n### 运行kafka server\n\n因为kafka依赖zookeeper,在开始之前请先启动一个zookeeper服务，本文章略。\n\n![](http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_1.png)\n\n## 错误总结\n### 无log\n下载[log4j](https://mvnrepository.com/artifact/log4j/log4j/1.2.17)和\n[slf4j-log4j12](https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.26)\n\n然后在modules中找到core/main,将这两个依赖导入进来即可\n\n最后将config目录下的log4j.properties复制到core/main/scala目录下（如果还是无法看到log,可以将该文件复制到out/production/classes目录下）\n### 执行报错，无法找到相关类\n执行`./gradlew jar`即可解决。\n## Core源码模块解读\n<table>\n<tr><td width=\"110px\" text-align=\"center\">模块名程</td><td>说明</td></tr>\n<tr><td>admin</td><td>kafka的管理员模块，操作和管理其topic，partition相关，包含创建，删除topic，或者拓展分区等。</td></tr>\n<tr><td>api</td><td>主要负责数据交互，客户端与服务端交互数据的编码与解码。</td></tr>\n<tr><td>cluster</td><td>这里包含多个实体类，有Broker，Cluster，Partition，Replica。其中一个Cluster由多个Broker组成，一个Broker包含多个Partition，一个Topic的所有Partition分布在不同的Broker中，一个Replica包含都个Partition。</td></tr>\n<tr><td>common</td><td>这是一个通用模块，其只包含各种异常类以及错误验证。</td></tr>\n<tr><td>consumer</td><td>消费者处理模块，负责所有的客户端消费者数据和逻辑处理。</td></tr>\n<tr><td>controller</td><td>此模块负责中央控制器的选举，分区的Leader选举，Replica的分配或其重新分配，分区和副本的扩容等。</td></tr>\n<tr><td>coordinator</td><td>负责管理部分consumer group和他们的offset。</td></tr>\n<tr><td>log</td><td>这是一个负责Kafka文件存储模块，负责读写所有的Kafka的Topic消息数据。</td></tr>\n<tr><td>message</td><td>封装多条数据组成一个数据集或者压缩数据集。</td></tr>\n<tr><td>metrics</td><td>负责内部状态的监控模块。</td></tr>\n<tr><td>network</td><td>该模块负责处理和接收客户端连接，处理网络时间模块。</td></tr>\n<tr><td>security</td><td>负责Kafka的安全验证和管理模块。</td></tr>\n<tr><td>serializer</td><td>序列化和反序列化当前消息内容。</td></tr>\n<tr><td>server</td><td>\t该模块涉及的内容较多，有Leader和Offset的checkpoint，动态配置，延时创建和删除Topic，Leader的选举，Admin和Replica的管理，以及各种元数据的缓存等内容。</td></tr>\n<tr><td>tools</td><td>阅读该模块，就是一个工具模块，涉及的内容也比较多。有导出对应consumer的offset值；导出LogSegments信息，以及当前Topic的log写的Location信息；导出Zookeeper上的offset值等内容。</td></tr>\n\n<tr><td>utils</td><td>各种工具类，比如Json，ZkUtils，线程池工具类，KafkaScheduler公共调度器类，Mx4jLoader监控加载器，ReplicationUtils复制集工具类，CommandLineUtils命令行工具类，以及公共日志类等内容。</td></tr>\n</table>\n\n## 参考\n1. [Apache Kafka](https://github.com/apache/kafka)\n2. [在windows系统下使用IDEA对kafka源码进行编译环境搭建以及配置](https://blog.csdn.net/u012606565/article/details/83022630#_kafkaIDEA_33)","source":"_posts/kafka-源码-idea-开发环境搭建.md","raw":"---\ntitle: kafka 源码 idea 开发环境搭建\ntags:\n  - kafka\n  - 专栏\ncategories:\n  - kafka\ntoc: false\ndate: 2019-06-14 21:45:03\n---\n\n## 前言\n\n为了深入研究kafka运行原理和架构，很有必要搭建开发环境，这样我们就很方便的debug 服务。\n## 前期准备\n\n1. Gradle 5.0+\n2. jdk8\n3. Scala 2.12\n4. [idea scale plugin](https://plugins.jetbrains.com/plugin/1347-scala)\n\n## 配置/运行\n\n- 首先执行在源码目录下执行`gradle`\n- 然后build `./gradlew jar`\n- 最后生成idea工程`./gradlew idea`\n\n### 运行MirrorMaker\n\n![](http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_2.png)\n### 运行kafka server\n\n因为kafka依赖zookeeper,在开始之前请先启动一个zookeeper服务，本文章略。\n\n![](http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_1.png)\n\n## 错误总结\n### 无log\n下载[log4j](https://mvnrepository.com/artifact/log4j/log4j/1.2.17)和\n[slf4j-log4j12](https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.26)\n\n然后在modules中找到core/main,将这两个依赖导入进来即可\n\n最后将config目录下的log4j.properties复制到core/main/scala目录下（如果还是无法看到log,可以将该文件复制到out/production/classes目录下）\n### 执行报错，无法找到相关类\n执行`./gradlew jar`即可解决。\n## Core源码模块解读\n<table>\n<tr><td width=\"110px\" text-align=\"center\">模块名程</td><td>说明</td></tr>\n<tr><td>admin</td><td>kafka的管理员模块，操作和管理其topic，partition相关，包含创建，删除topic，或者拓展分区等。</td></tr>\n<tr><td>api</td><td>主要负责数据交互，客户端与服务端交互数据的编码与解码。</td></tr>\n<tr><td>cluster</td><td>这里包含多个实体类，有Broker，Cluster，Partition，Replica。其中一个Cluster由多个Broker组成，一个Broker包含多个Partition，一个Topic的所有Partition分布在不同的Broker中，一个Replica包含都个Partition。</td></tr>\n<tr><td>common</td><td>这是一个通用模块，其只包含各种异常类以及错误验证。</td></tr>\n<tr><td>consumer</td><td>消费者处理模块，负责所有的客户端消费者数据和逻辑处理。</td></tr>\n<tr><td>controller</td><td>此模块负责中央控制器的选举，分区的Leader选举，Replica的分配或其重新分配，分区和副本的扩容等。</td></tr>\n<tr><td>coordinator</td><td>负责管理部分consumer group和他们的offset。</td></tr>\n<tr><td>log</td><td>这是一个负责Kafka文件存储模块，负责读写所有的Kafka的Topic消息数据。</td></tr>\n<tr><td>message</td><td>封装多条数据组成一个数据集或者压缩数据集。</td></tr>\n<tr><td>metrics</td><td>负责内部状态的监控模块。</td></tr>\n<tr><td>network</td><td>该模块负责处理和接收客户端连接，处理网络时间模块。</td></tr>\n<tr><td>security</td><td>负责Kafka的安全验证和管理模块。</td></tr>\n<tr><td>serializer</td><td>序列化和反序列化当前消息内容。</td></tr>\n<tr><td>server</td><td>\t该模块涉及的内容较多，有Leader和Offset的checkpoint，动态配置，延时创建和删除Topic，Leader的选举，Admin和Replica的管理，以及各种元数据的缓存等内容。</td></tr>\n<tr><td>tools</td><td>阅读该模块，就是一个工具模块，涉及的内容也比较多。有导出对应consumer的offset值；导出LogSegments信息，以及当前Topic的log写的Location信息；导出Zookeeper上的offset值等内容。</td></tr>\n\n<tr><td>utils</td><td>各种工具类，比如Json，ZkUtils，线程池工具类，KafkaScheduler公共调度器类，Mx4jLoader监控加载器，ReplicationUtils复制集工具类，CommandLineUtils命令行工具类，以及公共日志类等内容。</td></tr>\n</table>\n\n## 参考\n1. [Apache Kafka](https://github.com/apache/kafka)\n2. [在windows系统下使用IDEA对kafka源码进行编译环境搭建以及配置](https://blog.csdn.net/u012606565/article/details/83022630#_kafkaIDEA_33)","slug":"kafka-源码-idea-开发环境搭建","published":1,"updated":"2019-06-14T13:57:07.368Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdh00608ceehjkfs34s","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>为了深入研究kafka运行原理和架构，很有必要搭建开发环境，这样我们就很方便的debug 服务。</p>\n<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2><ol>\n<li>Gradle 5.0+</li>\n<li>jdk8</li>\n<li>Scala 2.12</li>\n<li><a href=\"https://plugins.jetbrains.com/plugin/1347-scala\" target=\"_blank\" rel=\"noopener\">idea scale plugin</a></li>\n</ol>\n<h2 id=\"配置-运行\"><a href=\"#配置-运行\" class=\"headerlink\" title=\"配置/运行\"></a>配置/运行</h2><ul>\n<li>首先执行在源码目录下执行<code>gradle</code></li>\n<li>然后build <code>./gradlew jar</code></li>\n<li>最后生成idea工程<code>./gradlew idea</code></li>\n</ul>\n<h3 id=\"运行MirrorMaker\"><a href=\"#运行MirrorMaker\" class=\"headerlink\" title=\"运行MirrorMaker\"></a>运行MirrorMaker</h3><p><img src=\"http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_2.png\" alt=\"\"></p>\n<h3 id=\"运行kafka-server\"><a href=\"#运行kafka-server\" class=\"headerlink\" title=\"运行kafka server\"></a>运行kafka server</h3><p>因为kafka依赖zookeeper,在开始之前请先启动一个zookeeper服务，本文章略。</p>\n<p><img src=\"http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_1.png\" alt=\"\"></p>\n<h2 id=\"错误总结\"><a href=\"#错误总结\" class=\"headerlink\" title=\"错误总结\"></a>错误总结</h2><h3 id=\"无log\"><a href=\"#无log\" class=\"headerlink\" title=\"无log\"></a>无log</h3><p>下载<a href=\"https://mvnrepository.com/artifact/log4j/log4j/1.2.17\" target=\"_blank\" rel=\"noopener\">log4j</a>和<br><a href=\"https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.26\" target=\"_blank\" rel=\"noopener\">slf4j-log4j12</a></p>\n<p>然后在modules中找到core/main,将这两个依赖导入进来即可</p>\n<p>最后将config目录下的log4j.properties复制到core/main/scala目录下（如果还是无法看到log,可以将该文件复制到out/production/classes目录下）</p>\n<h3 id=\"执行报错，无法找到相关类\"><a href=\"#执行报错，无法找到相关类\" class=\"headerlink\" title=\"执行报错，无法找到相关类\"></a>执行报错，无法找到相关类</h3><p>执行<code>./gradlew jar</code>即可解决。</p>\n<h2 id=\"Core源码模块解读\"><a href=\"#Core源码模块解读\" class=\"headerlink\" title=\"Core源码模块解读\"></a>Core源码模块解读</h2><table><br><tr><td width=\"110px\" text-align=\"center\">模块名程</td><td>说明</td></tr><br><tr><td>admin</td><td>kafka的管理员模块，操作和管理其topic，partition相关，包含创建，删除topic，或者拓展分区等。</td></tr><br><tr><td>api</td><td>主要负责数据交互，客户端与服务端交互数据的编码与解码。</td></tr><br><tr><td>cluster</td><td>这里包含多个实体类，有Broker，Cluster，Partition，Replica。其中一个Cluster由多个Broker组成，一个Broker包含多个Partition，一个Topic的所有Partition分布在不同的Broker中，一个Replica包含都个Partition。</td></tr><br><tr><td>common</td><td>这是一个通用模块，其只包含各种异常类以及错误验证。</td></tr><br><tr><td>consumer</td><td>消费者处理模块，负责所有的客户端消费者数据和逻辑处理。</td></tr><br><tr><td>controller</td><td>此模块负责中央控制器的选举，分区的Leader选举，Replica的分配或其重新分配，分区和副本的扩容等。</td></tr><br><tr><td>coordinator</td><td>负责管理部分consumer group和他们的offset。</td></tr><br><tr><td>log</td><td>这是一个负责Kafka文件存储模块，负责读写所有的Kafka的Topic消息数据。</td></tr><br><tr><td>message</td><td>封装多条数据组成一个数据集或者压缩数据集。</td></tr><br><tr><td>metrics</td><td>负责内部状态的监控模块。</td></tr><br><tr><td>network</td><td>该模块负责处理和接收客户端连接，处理网络时间模块。</td></tr><br><tr><td>security</td><td>负责Kafka的安全验证和管理模块。</td></tr><br><tr><td>serializer</td><td>序列化和反序列化当前消息内容。</td></tr><br><tr><td>server</td><td>    该模块涉及的内容较多，有Leader和Offset的checkpoint，动态配置，延时创建和删除Topic，Leader的选举，Admin和Replica的管理，以及各种元数据的缓存等内容。</td></tr><br><tr><td>tools</td><td>阅读该模块，就是一个工具模块，涉及的内容也比较多。有导出对应consumer的offset值；导出LogSegments信息，以及当前Topic的log写的Location信息；导出Zookeeper上的offset值等内容。</td></tr><br><br><tr><td>utils</td><td>各种工具类，比如Json，ZkUtils，线程池工具类，KafkaScheduler公共调度器类，Mx4jLoader监控加载器，ReplicationUtils复制集工具类，CommandLineUtils命令行工具类，以及公共日志类等内容。</td></tr><br></table>\n\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/apache/kafka\" target=\"_blank\" rel=\"noopener\">Apache Kafka</a></li>\n<li><a href=\"https://blog.csdn.net/u012606565/article/details/83022630#_kafkaIDEA_33\" target=\"_blank\" rel=\"noopener\">在windows系统下使用IDEA对kafka源码进行编译环境搭建以及配置</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>为了深入研究kafka运行原理和架构，很有必要搭建开发环境，这样我们就很方便的debug 服务。</p>\n<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2><ol>\n<li>Gradle 5.0+</li>\n<li>jdk8</li>\n<li>Scala 2.12</li>\n<li><a href=\"https://plugins.jetbrains.com/plugin/1347-scala\" target=\"_blank\" rel=\"noopener\">idea scale plugin</a></li>\n</ol>\n<h2 id=\"配置-运行\"><a href=\"#配置-运行\" class=\"headerlink\" title=\"配置/运行\"></a>配置/运行</h2><ul>\n<li>首先执行在源码目录下执行<code>gradle</code></li>\n<li>然后build <code>./gradlew jar</code></li>\n<li>最后生成idea工程<code>./gradlew idea</code></li>\n</ul>\n<h3 id=\"运行MirrorMaker\"><a href=\"#运行MirrorMaker\" class=\"headerlink\" title=\"运行MirrorMaker\"></a>运行MirrorMaker</h3><p><img src=\"http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_2.png\" alt=\"\"></p>\n<h3 id=\"运行kafka-server\"><a href=\"#运行kafka-server\" class=\"headerlink\" title=\"运行kafka server\"></a>运行kafka server</h3><p>因为kafka依赖zookeeper,在开始之前请先启动一个zookeeper服务，本文章略。</p>\n<p><img src=\"http://blogstatic.aibibang.com/kafka%20%E6%BA%90%E7%A0%81%20idea%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA_1.png\" alt=\"\"></p>\n<h2 id=\"错误总结\"><a href=\"#错误总结\" class=\"headerlink\" title=\"错误总结\"></a>错误总结</h2><h3 id=\"无log\"><a href=\"#无log\" class=\"headerlink\" title=\"无log\"></a>无log</h3><p>下载<a href=\"https://mvnrepository.com/artifact/log4j/log4j/1.2.17\" target=\"_blank\" rel=\"noopener\">log4j</a>和<br><a href=\"https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.7.26\" target=\"_blank\" rel=\"noopener\">slf4j-log4j12</a></p>\n<p>然后在modules中找到core/main,将这两个依赖导入进来即可</p>\n<p>最后将config目录下的log4j.properties复制到core/main/scala目录下（如果还是无法看到log,可以将该文件复制到out/production/classes目录下）</p>\n<h3 id=\"执行报错，无法找到相关类\"><a href=\"#执行报错，无法找到相关类\" class=\"headerlink\" title=\"执行报错，无法找到相关类\"></a>执行报错，无法找到相关类</h3><p>执行<code>./gradlew jar</code>即可解决。</p>\n<h2 id=\"Core源码模块解读\"><a href=\"#Core源码模块解读\" class=\"headerlink\" title=\"Core源码模块解读\"></a>Core源码模块解读</h2><table><br><tr><td width=\"110px\" text-align=\"center\">模块名程</td><td>说明</td></tr><br><tr><td>admin</td><td>kafka的管理员模块，操作和管理其topic，partition相关，包含创建，删除topic，或者拓展分区等。</td></tr><br><tr><td>api</td><td>主要负责数据交互，客户端与服务端交互数据的编码与解码。</td></tr><br><tr><td>cluster</td><td>这里包含多个实体类，有Broker，Cluster，Partition，Replica。其中一个Cluster由多个Broker组成，一个Broker包含多个Partition，一个Topic的所有Partition分布在不同的Broker中，一个Replica包含都个Partition。</td></tr><br><tr><td>common</td><td>这是一个通用模块，其只包含各种异常类以及错误验证。</td></tr><br><tr><td>consumer</td><td>消费者处理模块，负责所有的客户端消费者数据和逻辑处理。</td></tr><br><tr><td>controller</td><td>此模块负责中央控制器的选举，分区的Leader选举，Replica的分配或其重新分配，分区和副本的扩容等。</td></tr><br><tr><td>coordinator</td><td>负责管理部分consumer group和他们的offset。</td></tr><br><tr><td>log</td><td>这是一个负责Kafka文件存储模块，负责读写所有的Kafka的Topic消息数据。</td></tr><br><tr><td>message</td><td>封装多条数据组成一个数据集或者压缩数据集。</td></tr><br><tr><td>metrics</td><td>负责内部状态的监控模块。</td></tr><br><tr><td>network</td><td>该模块负责处理和接收客户端连接，处理网络时间模块。</td></tr><br><tr><td>security</td><td>负责Kafka的安全验证和管理模块。</td></tr><br><tr><td>serializer</td><td>序列化和反序列化当前消息内容。</td></tr><br><tr><td>server</td><td>    该模块涉及的内容较多，有Leader和Offset的checkpoint，动态配置，延时创建和删除Topic，Leader的选举，Admin和Replica的管理，以及各种元数据的缓存等内容。</td></tr><br><tr><td>tools</td><td>阅读该模块，就是一个工具模块，涉及的内容也比较多。有导出对应consumer的offset值；导出LogSegments信息，以及当前Topic的log写的Location信息；导出Zookeeper上的offset值等内容。</td></tr><br><br><tr><td>utils</td><td>各种工具类，比如Json，ZkUtils，线程池工具类，KafkaScheduler公共调度器类，Mx4jLoader监控加载器，ReplicationUtils复制集工具类，CommandLineUtils命令行工具类，以及公共日志类等内容。</td></tr><br></table>\n\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://github.com/apache/kafka\" target=\"_blank\" rel=\"noopener\">Apache Kafka</a></li>\n<li><a href=\"https://blog.csdn.net/u012606565/article/details/83022630#_kafkaIDEA_33\" target=\"_blank\" rel=\"noopener\">在windows系统下使用IDEA对kafka源码进行编译环境搭建以及配置</a></li>\n</ol>\n"},{"title":"kafka 逻辑架构设计","originContent":"","toc":false,"date":"2019-07-08T08:30:16.000Z","_content":"\n# Kafka 逻辑架构设计\n\n## 开篇\n\n本文主要探讨如下问题；\n\n1. Kafka架构设计\n2. Kafka的日志目录结构\n\n## 正文\n\n### Kafka架构设计\n\nkafka为分布式消息系统，由多个broker组成。消息是通过topic来分类的，一个topic下存在多个partition,每个partition又由多个segment构成。\n\n#### 发布订阅者模式\n\n![](http://blogstatic.aibibang.com/%E6%B6%88%E8%B4%B9%E8%AE%A2%E9%98%85%E8%80%85%E6%A8%A1%E5%BC%8F.jpg)\n\n### kafka集群架构\n\n![](http://blogstatic.aibibang.com/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.jpg)\n\n### 主题逻辑结构\n\n![](http://blogstatic.aibibang.com/Topic%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.jpg)\n\n## Kafka的日志目录结构\n\n![](http://blogstatic.aibibang.com/kfaka%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.jpg)\n\n","source":"_posts/kafka-逻辑架构设计.md","raw":"---\ntitle: kafka 逻辑架构设计\ntags:\n  - 专刊\n  - kafka\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-07-08 16:30:16\n---\n\n# Kafka 逻辑架构设计\n\n## 开篇\n\n本文主要探讨如下问题；\n\n1. Kafka架构设计\n2. Kafka的日志目录结构\n\n## 正文\n\n### Kafka架构设计\n\nkafka为分布式消息系统，由多个broker组成。消息是通过topic来分类的，一个topic下存在多个partition,每个partition又由多个segment构成。\n\n#### 发布订阅者模式\n\n![](http://blogstatic.aibibang.com/%E6%B6%88%E8%B4%B9%E8%AE%A2%E9%98%85%E8%80%85%E6%A8%A1%E5%BC%8F.jpg)\n\n### kafka集群架构\n\n![](http://blogstatic.aibibang.com/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.jpg)\n\n### 主题逻辑结构\n\n![](http://blogstatic.aibibang.com/Topic%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.jpg)\n\n## Kafka的日志目录结构\n\n![](http://blogstatic.aibibang.com/kfaka%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.jpg)\n\n","slug":"kafka-逻辑架构设计","published":1,"updated":"2019-07-08T08:30:16.949Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdi00638ceelpv06h1u","content":"<h1 id=\"Kafka-逻辑架构设计\"><a href=\"#Kafka-逻辑架构设计\" class=\"headerlink\" title=\"Kafka 逻辑架构设计\"></a>Kafka 逻辑架构设计</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>本文主要探讨如下问题；</p>\n<ol>\n<li>Kafka架构设计</li>\n<li>Kafka的日志目录结构</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Kafka架构设计\"><a href=\"#Kafka架构设计\" class=\"headerlink\" title=\"Kafka架构设计\"></a>Kafka架构设计</h3><p>kafka为分布式消息系统，由多个broker组成。消息是通过topic来分类的，一个topic下存在多个partition,每个partition又由多个segment构成。</p>\n<h4 id=\"发布订阅者模式\"><a href=\"#发布订阅者模式\" class=\"headerlink\" title=\"发布订阅者模式\"></a>发布订阅者模式</h4><p><img src=\"http://blogstatic.aibibang.com/%E6%B6%88%E8%B4%B9%E8%AE%A2%E9%98%85%E8%80%85%E6%A8%A1%E5%BC%8F.jpg\" alt=\"\"></p>\n<h3 id=\"kafka集群架构\"><a href=\"#kafka集群架构\" class=\"headerlink\" title=\"kafka集群架构\"></a>kafka集群架构</h3><p><img src=\"http://blogstatic.aibibang.com/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.jpg\" alt=\"\"></p>\n<h3 id=\"主题逻辑结构\"><a href=\"#主题逻辑结构\" class=\"headerlink\" title=\"主题逻辑结构\"></a>主题逻辑结构</h3><p><img src=\"http://blogstatic.aibibang.com/Topic%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.jpg\" alt=\"\"></p>\n<h2 id=\"Kafka的日志目录结构\"><a href=\"#Kafka的日志目录结构\" class=\"headerlink\" title=\"Kafka的日志目录结构\"></a>Kafka的日志目录结构</h2><p><img src=\"http://blogstatic.aibibang.com/kfaka%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.jpg\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Kafka-逻辑架构设计\"><a href=\"#Kafka-逻辑架构设计\" class=\"headerlink\" title=\"Kafka 逻辑架构设计\"></a>Kafka 逻辑架构设计</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>本文主要探讨如下问题；</p>\n<ol>\n<li>Kafka架构设计</li>\n<li>Kafka的日志目录结构</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Kafka架构设计\"><a href=\"#Kafka架构设计\" class=\"headerlink\" title=\"Kafka架构设计\"></a>Kafka架构设计</h3><p>kafka为分布式消息系统，由多个broker组成。消息是通过topic来分类的，一个topic下存在多个partition,每个partition又由多个segment构成。</p>\n<h4 id=\"发布订阅者模式\"><a href=\"#发布订阅者模式\" class=\"headerlink\" title=\"发布订阅者模式\"></a>发布订阅者模式</h4><p><img src=\"http://blogstatic.aibibang.com/%E6%B6%88%E8%B4%B9%E8%AE%A2%E9%98%85%E8%80%85%E6%A8%A1%E5%BC%8F.jpg\" alt=\"\"></p>\n<h3 id=\"kafka集群架构\"><a href=\"#kafka集群架构\" class=\"headerlink\" title=\"kafka集群架构\"></a>kafka集群架构</h3><p><img src=\"http://blogstatic.aibibang.com/kafka%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84.jpg\" alt=\"\"></p>\n<h3 id=\"主题逻辑结构\"><a href=\"#主题逻辑结构\" class=\"headerlink\" title=\"主题逻辑结构\"></a>主题逻辑结构</h3><p><img src=\"http://blogstatic.aibibang.com/Topic%E9%80%BB%E8%BE%91%E7%BB%93%E6%9E%84.jpg\" alt=\"\"></p>\n<h2 id=\"Kafka的日志目录结构\"><a href=\"#Kafka的日志目录结构\" class=\"headerlink\" title=\"Kafka的日志目录结构\"></a>Kafka的日志目录结构</h2><p><img src=\"http://blogstatic.aibibang.com/kfaka%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.jpg\" alt=\"\"></p>\n"},{"title":"kafka使用教程","date":"2017-03-05T12:38:06.000Z","_content":"# kafka使用教程\n\n## shell操作\n\n**1.创建topic**\n\n--replication-factor 2：备份因子为2\n\n--partitions 10：partitions数目为10\n ```\nbin/kafka-topics.sh --create --zookeeper 192.168.0.101:2181 --replication-factor 2 --partitions 10 --topic upgrade-kafka_test\n ```\n**2.查询topic**\n\n列出所有的topic\n  ```\n  bin/kafka-topics.sh --list --zookeeper localhost:2181\n  ```\n**3.删除topic**\n\n首先需要确认集群是否配置delete.topic.enable=true，配置后即可删除，确保topic没有被使用。\n```\nbin/kafka-topics.sh --delete --zookeeper 192.168.0.101:2181 --topic upgrade-kafka_test\n\n```\n\n## 程序操作\n\n\n\n\n\n\n## 参考\n1. [kafka official website](https://kafka.apache.org/documentation/)\n","source":"_posts/kafka使用教程.md","raw":"---\ntitle: kafka使用教程\ndate: 2017-03-05 20:38:06\ntags: 大数据\ncategories:\n- kafka\n---\n# kafka使用教程\n\n## shell操作\n\n**1.创建topic**\n\n--replication-factor 2：备份因子为2\n\n--partitions 10：partitions数目为10\n ```\nbin/kafka-topics.sh --create --zookeeper 192.168.0.101:2181 --replication-factor 2 --partitions 10 --topic upgrade-kafka_test\n ```\n**2.查询topic**\n\n列出所有的topic\n  ```\n  bin/kafka-topics.sh --list --zookeeper localhost:2181\n  ```\n**3.删除topic**\n\n首先需要确认集群是否配置delete.topic.enable=true，配置后即可删除，确保topic没有被使用。\n```\nbin/kafka-topics.sh --delete --zookeeper 192.168.0.101:2181 --topic upgrade-kafka_test\n\n```\n\n## 程序操作\n\n\n\n\n\n\n## 参考\n1. [kafka official website](https://kafka.apache.org/documentation/)\n","slug":"kafka使用教程","published":1,"updated":"2019-08-22T12:31:46.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdl00678ceexh4cllr7","content":"<h1 id=\"kafka使用教程\"><a href=\"#kafka使用教程\" class=\"headerlink\" title=\"kafka使用教程\"></a>kafka使用教程</h1><h2 id=\"shell操作\"><a href=\"#shell操作\" class=\"headerlink\" title=\"shell操作\"></a>shell操作</h2><p><strong>1.创建topic</strong></p>\n<p>–replication-factor 2：备份因子为2</p>\n<p>–partitions 10：partitions数目为10<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --create --zookeeper 192.168.0.101:2181 --replication-factor 2 --partitions 10 --topic upgrade-kafka_test</span><br></pre></td></tr></table></figure></p>\n<p><strong>2.查询topic</strong></p>\n<p>列出所有的topic<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure></p>\n<p><strong>3.删除topic</strong></p>\n<p>首先需要确认集群是否配置delete.topic.enable=true，配置后即可删除，确保topic没有被使用。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --delete --zookeeper 192.168.0.101:2181 --topic upgrade-kafka_test</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"程序操作\"><a href=\"#程序操作\" class=\"headerlink\" title=\"程序操作\"></a>程序操作</h2><h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://kafka.apache.org/documentation/\" target=\"_blank\" rel=\"noopener\">kafka official website</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka使用教程\"><a href=\"#kafka使用教程\" class=\"headerlink\" title=\"kafka使用教程\"></a>kafka使用教程</h1><h2 id=\"shell操作\"><a href=\"#shell操作\" class=\"headerlink\" title=\"shell操作\"></a>shell操作</h2><p><strong>1.创建topic</strong></p>\n<p>–replication-factor 2：备份因子为2</p>\n<p>–partitions 10：partitions数目为10<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --create --zookeeper 192.168.0.101:2181 --replication-factor 2 --partitions 10 --topic upgrade-kafka_test</span><br></pre></td></tr></table></figure></p>\n<p><strong>2.查询topic</strong></p>\n<p>列出所有的topic<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure></p>\n<p><strong>3.删除topic</strong></p>\n<p>首先需要确认集群是否配置delete.topic.enable=true，配置后即可删除，确保topic没有被使用。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/kafka-topics.sh --delete --zookeeper 192.168.0.101:2181 --topic upgrade-kafka_test</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"程序操作\"><a href=\"#程序操作\" class=\"headerlink\" title=\"程序操作\"></a>程序操作</h2><h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://kafka.apache.org/documentation/\" target=\"_blank\" rel=\"noopener\">kafka official website</a></li>\n</ol>\n"},{"title":"linux shell问题记录","date":"2017-08-21T09:15:16.000Z","_content":"最近做docker image,编写shell 脚本，遇到以下问题，做个记录\n\n1.问题一：\n```\nif [ \"${1:0:1}\" = '-' ]; then\n\tset -- elasticsearch \"$@\"\nfi\n```\n**解释**：以上if 条件是指第一个参数的第一个字符为`-`,则符合条件\n\n2.问题二：\n```\nexec \"$@\"\n```\n**解释**：exec执行命令\n\n3.问题三：\n```\nset -- elasticsearch \"$@\"\n```\n**解释**：set设置环境，这句话的意思其实是将elasticsearch 作为第一个参数补充到\"$@\"中\n例如：\ndemo.sh\n```\n#!/bin/bash\nset -- elasticsearch \"$@\"\necho \"$@\"\n```\n执行脚本\n```\nbash demo.sh hello truman\n```\n输出结果为：elasticsearch hello truman\n\n4.问题四：\n```\n\"$(id -u)\"\n```\n**解释**：输出当前shell 环境UID(用户ID)\n","source":"_posts/linux-shell问题记录.md","raw":"---\ntitle: linux shell问题记录\ndate: 2017-08-21 17:15:16\ntags: 开发笔记\ncategories:\n- shell\n---\n最近做docker image,编写shell 脚本，遇到以下问题，做个记录\n\n1.问题一：\n```\nif [ \"${1:0:1}\" = '-' ]; then\n\tset -- elasticsearch \"$@\"\nfi\n```\n**解释**：以上if 条件是指第一个参数的第一个字符为`-`,则符合条件\n\n2.问题二：\n```\nexec \"$@\"\n```\n**解释**：exec执行命令\n\n3.问题三：\n```\nset -- elasticsearch \"$@\"\n```\n**解释**：set设置环境，这句话的意思其实是将elasticsearch 作为第一个参数补充到\"$@\"中\n例如：\ndemo.sh\n```\n#!/bin/bash\nset -- elasticsearch \"$@\"\necho \"$@\"\n```\n执行脚本\n```\nbash demo.sh hello truman\n```\n输出结果为：elasticsearch hello truman\n\n4.问题四：\n```\n\"$(id -u)\"\n```\n**解释**：输出当前shell 环境UID(用户ID)\n","slug":"linux-shell问题记录","published":1,"updated":"2017-08-21T09:27:39.222Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdn006b8cee8npkks9y","content":"<p>最近做docker image,编写shell 脚本，遇到以下问题，做个记录</p>\n<p>1.问题一：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if [ &quot;$&#123;1:0:1&#125;&quot; = &apos;-&apos; ]; then</span><br><span class=\"line\">\tset -- elasticsearch &quot;$@&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：以上if 条件是指第一个参数的第一个字符为<code>-</code>,则符合条件</p>\n<p>2.问题二：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exec &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：exec执行命令</p>\n<p>3.问题三：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set -- elasticsearch &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：set设置环境，这句话的意思其实是将elasticsearch 作为第一个参数补充到”$@”中<br>例如：<br>demo.sh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">set -- elasticsearch &quot;$@&quot;</span><br><span class=\"line\">echo &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p>执行脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash demo.sh hello truman</span><br></pre></td></tr></table></figure></p>\n<p>输出结果为：elasticsearch hello truman</p>\n<p>4.问题四：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;$(id -u)&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：输出当前shell 环境UID(用户ID)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>最近做docker image,编写shell 脚本，遇到以下问题，做个记录</p>\n<p>1.问题一：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if [ &quot;$&#123;1:0:1&#125;&quot; = &apos;-&apos; ]; then</span><br><span class=\"line\">\tset -- elasticsearch &quot;$@&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：以上if 条件是指第一个参数的第一个字符为<code>-</code>,则符合条件</p>\n<p>2.问题二：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exec &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：exec执行命令</p>\n<p>3.问题三：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set -- elasticsearch &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：set设置环境，这句话的意思其实是将elasticsearch 作为第一个参数补充到”$@”中<br>例如：<br>demo.sh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\">set -- elasticsearch &quot;$@&quot;</span><br><span class=\"line\">echo &quot;$@&quot;</span><br></pre></td></tr></table></figure></p>\n<p>执行脚本<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash demo.sh hello truman</span><br></pre></td></tr></table></figure></p>\n<p>输出结果为：elasticsearch hello truman</p>\n<p>4.问题四：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;$(id -u)&quot;</span><br></pre></td></tr></table></figure></p>\n<p><strong>解释</strong>：输出当前shell 环境UID(用户ID)</p>\n"},{"title":"linux性能优化笔记之CPU篇","date":"2019-03-02T14:25:36.000Z","_content":"# linux性能优化笔记之CPU篇\n\ncpu优化用到的命令\n\n- mpstat\n进程相关统计工具，cpu/io可以同时对比\n- vmstat\n内存分析工具\n- pidstat\n进程分析工具\n- perf 使用perf record -g -p < pid>和perf report就足够了\n\n\n## CPU篇\n\n### 根据指标找工具\n\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E6%8C%87%E6%A0%87%E6%89%BE%E5%B7%A5%E5%85%B7(CPU%E6%80%A7%E8%83%BD).png?raw=true)\n\n### 根据工具查指标\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E5%B7%A5%E5%85%B7%E6%9F%A5%E6%8C%87%E6%A0%87.png?raw=true)\n\n### 指标工具关联\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E5%85%B3%E8%81%94.png?raw=true)\n\n### 相关命令\n查看cpu核数：\n\n```\n$ grep 'model name' /proc/cpuinfo | wc -l\n```\nmpstat 查看 CPU 使用率的变化情况\n\n```\n# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据\n$ mpstat -P ALL 5\n```\npidstat查看具体进程cpu使用率\n\n```\n# 间隔 5 秒后输出一组数据\n\n$ pidstat -u 5 1\n# 查看进程IO读写情况\n$ pidstat -d \n# w可以查看上下文切换情况(进程)  cswch/s 自愿切换（资源不够） nvcswch/s被动切换（时间片到期等中断）\n# wt是可以查看到线程\n$ pidstat -wt 5\n```\nvmstat内存分析工具，可以分析上下文切换情况\n\n```\n# in中断 sc上下文切换\n$ vmstat 1\n```\n\nperl系能分析工具，可以追查引起性能问题的函数\n\n```\n$ sudo perf top -g -p 30377\n```\n### 名词解释\n\n1. 平均负载\n>简单来说，平均负载是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数，也就是**平均活跃进程数**，它和 CPU 使用率并没有直接关系。可运行状态顾名思义只得是正在运行的任务，不可中断状态例如等待cpu,等待IO。\n\n2. cpu上下文\n>CPU寄存器和程序计数器为任务运行必备依赖环境，即为cpu上下文\n\n>cpu上下文切换包含哪些？\n>(1)进程上下文切换(2)线程上下文切换(3)中断上下文切换\n\n### 知识点\n\n1. cpu使用率和平均负载一致吗？\n\n不一定，当平均负载高的时候，任务主要是等待IO型，IO密集型任务，CPU使用率就不一定高。\n\n\n### 注意\n\n1. pidstat中缺少%wait\n\ncentos中版本较低是，安装新版本即有\n2. pidstat中%wait与top wa区别\n\npidstat 中， %wait 表示进程等待 CPU 的时间百分。\n\ntop 中 ，iowait%(简写wa) 则表示等待 I/O 的 CPU 时间百分。\n","source":"_posts/linux性能优化笔记之CPU篇.md","raw":"---\ntitle: linux性能优化笔记之CPU篇\ndate: 2019-03-02 22:25:36\ntags: 笔记\ncategories:\n- linux\n---\n# linux性能优化笔记之CPU篇\n\ncpu优化用到的命令\n\n- mpstat\n进程相关统计工具，cpu/io可以同时对比\n- vmstat\n内存分析工具\n- pidstat\n进程分析工具\n- perf 使用perf record -g -p < pid>和perf report就足够了\n\n\n## CPU篇\n\n### 根据指标找工具\n\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E6%8C%87%E6%A0%87%E6%89%BE%E5%B7%A5%E5%85%B7(CPU%E6%80%A7%E8%83%BD).png?raw=true)\n\n### 根据工具查指标\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E5%B7%A5%E5%85%B7%E6%9F%A5%E6%8C%87%E6%A0%87.png?raw=true)\n\n### 指标工具关联\n![image](https://github.com/TrumanDu/pic_repository/blob/master/%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E5%85%B3%E8%81%94.png?raw=true)\n\n### 相关命令\n查看cpu核数：\n\n```\n$ grep 'model name' /proc/cpuinfo | wc -l\n```\nmpstat 查看 CPU 使用率的变化情况\n\n```\n# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据\n$ mpstat -P ALL 5\n```\npidstat查看具体进程cpu使用率\n\n```\n# 间隔 5 秒后输出一组数据\n\n$ pidstat -u 5 1\n# 查看进程IO读写情况\n$ pidstat -d \n# w可以查看上下文切换情况(进程)  cswch/s 自愿切换（资源不够） nvcswch/s被动切换（时间片到期等中断）\n# wt是可以查看到线程\n$ pidstat -wt 5\n```\nvmstat内存分析工具，可以分析上下文切换情况\n\n```\n# in中断 sc上下文切换\n$ vmstat 1\n```\n\nperl系能分析工具，可以追查引起性能问题的函数\n\n```\n$ sudo perf top -g -p 30377\n```\n### 名词解释\n\n1. 平均负载\n>简单来说，平均负载是指单位时间内，系统处于**可运行状态**和**不可中断状态**的平均进程数，也就是**平均活跃进程数**，它和 CPU 使用率并没有直接关系。可运行状态顾名思义只得是正在运行的任务，不可中断状态例如等待cpu,等待IO。\n\n2. cpu上下文\n>CPU寄存器和程序计数器为任务运行必备依赖环境，即为cpu上下文\n\n>cpu上下文切换包含哪些？\n>(1)进程上下文切换(2)线程上下文切换(3)中断上下文切换\n\n### 知识点\n\n1. cpu使用率和平均负载一致吗？\n\n不一定，当平均负载高的时候，任务主要是等待IO型，IO密集型任务，CPU使用率就不一定高。\n\n\n### 注意\n\n1. pidstat中缺少%wait\n\ncentos中版本较低是，安装新版本即有\n2. pidstat中%wait与top wa区别\n\npidstat 中， %wait 表示进程等待 CPU 的时间百分。\n\ntop 中 ，iowait%(简写wa) 则表示等待 I/O 的 CPU 时间百分。\n","slug":"linux性能优化笔记之CPU篇","published":1,"updated":"2019-03-02T14:26:21.871Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdq006f8ceefkl0y6tt","content":"<h1 id=\"linux性能优化笔记之CPU篇\"><a href=\"#linux性能优化笔记之CPU篇\" class=\"headerlink\" title=\"linux性能优化笔记之CPU篇\"></a>linux性能优化笔记之CPU篇</h1><p>cpu优化用到的命令</p>\n<ul>\n<li>mpstat<br>进程相关统计工具，cpu/io可以同时对比</li>\n<li>vmstat<br>内存分析工具</li>\n<li>pidstat<br>进程分析工具</li>\n<li>perf 使用perf record -g -p &lt; pid&gt;和perf report就足够了</li>\n</ul>\n<h2 id=\"CPU篇\"><a href=\"#CPU篇\" class=\"headerlink\" title=\"CPU篇\"></a>CPU篇</h2><h3 id=\"根据指标找工具\"><a href=\"#根据指标找工具\" class=\"headerlink\" title=\"根据指标找工具\"></a>根据指标找工具</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E6%8C%87%E6%A0%87%E6%89%BE%E5%B7%A5%E5%85%B7(CPU%E6%80%A7%E8%83%BD\" alt=\"image\">.png?raw=true)</p>\n<h3 id=\"根据工具查指标\"><a href=\"#根据工具查指标\" class=\"headerlink\" title=\"根据工具查指标\"></a>根据工具查指标</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E5%B7%A5%E5%85%B7%E6%9F%A5%E6%8C%87%E6%A0%87.png?raw=true\" alt=\"image\"></p>\n<h3 id=\"指标工具关联\"><a href=\"#指标工具关联\" class=\"headerlink\" title=\"指标工具关联\"></a>指标工具关联</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E5%85%B3%E8%81%94.png?raw=true\" alt=\"image\"></p>\n<h3 id=\"相关命令\"><a href=\"#相关命令\" class=\"headerlink\" title=\"相关命令\"></a>相关命令</h3><p>查看cpu核数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ grep &apos;model name&apos; /proc/cpuinfo | wc -l</span><br></pre></td></tr></table></figure>\n<p>mpstat 查看 CPU 使用率的变化情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据</span><br><span class=\"line\">$ mpstat -P ALL 5</span><br></pre></td></tr></table></figure>\n<p>pidstat查看具体进程cpu使用率</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 间隔 5 秒后输出一组数据</span><br><span class=\"line\"></span><br><span class=\"line\">$ pidstat -u 5 1</span><br><span class=\"line\"># 查看进程IO读写情况</span><br><span class=\"line\">$ pidstat -d </span><br><span class=\"line\"># w可以查看上下文切换情况(进程)  cswch/s 自愿切换（资源不够） nvcswch/s被动切换（时间片到期等中断）</span><br><span class=\"line\"># wt是可以查看到线程</span><br><span class=\"line\">$ pidstat -wt 5</span><br></pre></td></tr></table></figure>\n<p>vmstat内存分析工具，可以分析上下文切换情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># in中断 sc上下文切换</span><br><span class=\"line\">$ vmstat 1</span><br></pre></td></tr></table></figure>\n<p>perl系能分析工具，可以追查引起性能问题的函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo perf top -g -p 30377</span><br></pre></td></tr></table></figure>\n<h3 id=\"名词解释\"><a href=\"#名词解释\" class=\"headerlink\" title=\"名词解释\"></a>名词解释</h3><ol>\n<li><p>平均负载</p>\n<blockquote>\n<p>简单来说，平均负载是指单位时间内，系统处于<strong>可运行状态</strong>和<strong>不可中断状态</strong>的平均进程数，也就是<strong>平均活跃进程数</strong>，它和 CPU 使用率并没有直接关系。可运行状态顾名思义只得是正在运行的任务，不可中断状态例如等待cpu,等待IO。</p>\n</blockquote>\n</li>\n<li><p>cpu上下文</p>\n<blockquote>\n<p>CPU寄存器和程序计数器为任务运行必备依赖环境，即为cpu上下文</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>cpu上下文切换包含哪些？<br>(1)进程上下文切换(2)线程上下文切换(3)中断上下文切换</p>\n</blockquote>\n<h3 id=\"知识点\"><a href=\"#知识点\" class=\"headerlink\" title=\"知识点\"></a>知识点</h3><ol>\n<li>cpu使用率和平均负载一致吗？</li>\n</ol>\n<p>不一定，当平均负载高的时候，任务主要是等待IO型，IO密集型任务，CPU使用率就不一定高。</p>\n<h3 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h3><ol>\n<li>pidstat中缺少%wait</li>\n</ol>\n<p>centos中版本较低是，安装新版本即有</p>\n<ol start=\"2\">\n<li>pidstat中%wait与top wa区别</li>\n</ol>\n<p>pidstat 中， %wait 表示进程等待 CPU 的时间百分。</p>\n<p>top 中 ，iowait%(简写wa) 则表示等待 I/O 的 CPU 时间百分。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"linux性能优化笔记之CPU篇\"><a href=\"#linux性能优化笔记之CPU篇\" class=\"headerlink\" title=\"linux性能优化笔记之CPU篇\"></a>linux性能优化笔记之CPU篇</h1><p>cpu优化用到的命令</p>\n<ul>\n<li>mpstat<br>进程相关统计工具，cpu/io可以同时对比</li>\n<li>vmstat<br>内存分析工具</li>\n<li>pidstat<br>进程分析工具</li>\n<li>perf 使用perf record -g -p &lt; pid&gt;和perf report就足够了</li>\n</ul>\n<h2 id=\"CPU篇\"><a href=\"#CPU篇\" class=\"headerlink\" title=\"CPU篇\"></a>CPU篇</h2><h3 id=\"根据指标找工具\"><a href=\"#根据指标找工具\" class=\"headerlink\" title=\"根据指标找工具\"></a>根据指标找工具</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E6%8C%87%E6%A0%87%E6%89%BE%E5%B7%A5%E5%85%B7(CPU%E6%80%A7%E8%83%BD\" alt=\"image\">.png?raw=true)</p>\n<h3 id=\"根据工具查指标\"><a href=\"#根据工具查指标\" class=\"headerlink\" title=\"根据工具查指标\"></a>根据工具查指标</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%A0%B9%E6%8D%AE%E5%B7%A5%E5%85%B7%E6%9F%A5%E6%8C%87%E6%A0%87.png?raw=true\" alt=\"image\"></p>\n<h3 id=\"指标工具关联\"><a href=\"#指标工具关联\" class=\"headerlink\" title=\"指标工具关联\"></a>指标工具关联</h3><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E5%85%B3%E8%81%94.png?raw=true\" alt=\"image\"></p>\n<h3 id=\"相关命令\"><a href=\"#相关命令\" class=\"headerlink\" title=\"相关命令\"></a>相关命令</h3><p>查看cpu核数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ grep &apos;model name&apos; /proc/cpuinfo | wc -l</span><br></pre></td></tr></table></figure>\n<p>mpstat 查看 CPU 使用率的变化情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据</span><br><span class=\"line\">$ mpstat -P ALL 5</span><br></pre></td></tr></table></figure>\n<p>pidstat查看具体进程cpu使用率</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 间隔 5 秒后输出一组数据</span><br><span class=\"line\"></span><br><span class=\"line\">$ pidstat -u 5 1</span><br><span class=\"line\"># 查看进程IO读写情况</span><br><span class=\"line\">$ pidstat -d </span><br><span class=\"line\"># w可以查看上下文切换情况(进程)  cswch/s 自愿切换（资源不够） nvcswch/s被动切换（时间片到期等中断）</span><br><span class=\"line\"># wt是可以查看到线程</span><br><span class=\"line\">$ pidstat -wt 5</span><br></pre></td></tr></table></figure>\n<p>vmstat内存分析工具，可以分析上下文切换情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># in中断 sc上下文切换</span><br><span class=\"line\">$ vmstat 1</span><br></pre></td></tr></table></figure>\n<p>perl系能分析工具，可以追查引起性能问题的函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo perf top -g -p 30377</span><br></pre></td></tr></table></figure>\n<h3 id=\"名词解释\"><a href=\"#名词解释\" class=\"headerlink\" title=\"名词解释\"></a>名词解释</h3><ol>\n<li><p>平均负载</p>\n<blockquote>\n<p>简单来说，平均负载是指单位时间内，系统处于<strong>可运行状态</strong>和<strong>不可中断状态</strong>的平均进程数，也就是<strong>平均活跃进程数</strong>，它和 CPU 使用率并没有直接关系。可运行状态顾名思义只得是正在运行的任务，不可中断状态例如等待cpu,等待IO。</p>\n</blockquote>\n</li>\n<li><p>cpu上下文</p>\n<blockquote>\n<p>CPU寄存器和程序计数器为任务运行必备依赖环境，即为cpu上下文</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>cpu上下文切换包含哪些？<br>(1)进程上下文切换(2)线程上下文切换(3)中断上下文切换</p>\n</blockquote>\n<h3 id=\"知识点\"><a href=\"#知识点\" class=\"headerlink\" title=\"知识点\"></a>知识点</h3><ol>\n<li>cpu使用率和平均负载一致吗？</li>\n</ol>\n<p>不一定，当平均负载高的时候，任务主要是等待IO型，IO密集型任务，CPU使用率就不一定高。</p>\n<h3 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h3><ol>\n<li>pidstat中缺少%wait</li>\n</ol>\n<p>centos中版本较低是，安装新版本即有</p>\n<ol start=\"2\">\n<li>pidstat中%wait与top wa区别</li>\n</ol>\n<p>pidstat 中， %wait 表示进程等待 CPU 的时间百分。</p>\n<p>top 中 ，iowait%(简写wa) 则表示等待 I/O 的 CPU 时间百分。</p>\n"},{"title":"linux 问题记录","date":"2016-04-13T12:32:48.000Z","_content":"## 一、hostname 修改详解\n   1. 暂时性修改使用命令：\n   ``` \n   hostname ******\n   ```\n   此命令不用重启，重新打开一个终端，即可看到修改。但是重启后失效（本人red hat5竟然不会）\n   2. 永久性修改\n\n    在1之后，修改 /etc/sysconfig/network\n***\n## 二、配置ssh\n**本节参考：[网址](http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)**\n1. 生成公钥\n```\n$ssh-keygen \n```\n2. 公钥分发到远程主机\n```    　\n$ ssh-copy-id user@host\n```\n***\n```\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that the RSA host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\ndf:a8:ff:b2:0d:f8:1c:a9:d9:f9:fa:39:29:9d:6f:42.\nPlease contact your system administrator.\nAdd correct host key in /root/.ssh/known_hosts to get rid of this message.\nOffending key in /root/.ssh/known_hosts:13\nRSA host key for lab3 has changed and you have requested strict checking.\nHost key verification failed.\n```\n问题解决：\n删除/root/.ssh/known_hosts:13行中的信息即可\n\n***","source":"_posts/linux-问题记录.md","raw":"---\ntitle: linux 问题记录\ndate: 2016-04-13 20:32:48\ntags: 大数据\ncategories:\n- linux\n---\n## 一、hostname 修改详解\n   1. 暂时性修改使用命令：\n   ``` \n   hostname ******\n   ```\n   此命令不用重启，重新打开一个终端，即可看到修改。但是重启后失效（本人red hat5竟然不会）\n   2. 永久性修改\n\n    在1之后，修改 /etc/sysconfig/network\n***\n## 二、配置ssh\n**本节参考：[网址](http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html)**\n1. 生成公钥\n```\n$ssh-keygen \n```\n2. 公钥分发到远程主机\n```    　\n$ ssh-copy-id user@host\n```\n***\n```\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that the RSA host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\ndf:a8:ff:b2:0d:f8:1c:a9:d9:f9:fa:39:29:9d:6f:42.\nPlease contact your system administrator.\nAdd correct host key in /root/.ssh/known_hosts to get rid of this message.\nOffending key in /root/.ssh/known_hosts:13\nRSA host key for lab3 has changed and you have requested strict checking.\nHost key verification failed.\n```\n问题解决：\n删除/root/.ssh/known_hosts:13行中的信息即可\n\n***","slug":"linux-问题记录","published":1,"updated":"2019-08-22T12:38:17.124Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdr006i8ceezo6hr0fk","content":"<h2 id=\"一、hostname-修改详解\"><a href=\"#一、hostname-修改详解\" class=\"headerlink\" title=\"一、hostname 修改详解\"></a>一、hostname 修改详解</h2><ol>\n<li><p>暂时性修改使用命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname ******</span><br></pre></td></tr></table></figure>\n<p>此命令不用重启，重新打开一个终端，即可看到修改。但是重启后失效（本人red hat5竟然不会）</p>\n</li>\n<li><p>永久性修改</p>\n<p>在1之后，修改 /etc/sysconfig/network</p>\n</li>\n</ol>\n<hr>\n<h2 id=\"二、配置ssh\"><a href=\"#二、配置ssh\" class=\"headerlink\" title=\"二、配置ssh\"></a>二、配置ssh</h2><p><strong>本节参考：<a href=\"http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html\" target=\"_blank\" rel=\"noopener\">网址</a></strong></p>\n<ol>\n<li><p>生成公钥</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ssh-keygen</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>公钥分发到远程主机</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ssh-copy-id user@host</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class=\"line\">@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @</span><br><span class=\"line\">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class=\"line\">IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!</span><br><span class=\"line\">Someone could be eavesdropping on you right now (man-in-the-middle attack)!</span><br><span class=\"line\">It is also possible that the RSA host key has just been changed.</span><br><span class=\"line\">The fingerprint for the RSA key sent by the remote host is</span><br><span class=\"line\">df:a8:ff:b2:0d:f8:1c:a9:d9:f9:fa:39:29:9d:6f:42.</span><br><span class=\"line\">Please contact your system administrator.</span><br><span class=\"line\">Add correct host key in /root/.ssh/known_hosts to get rid of this message.</span><br><span class=\"line\">Offending key in /root/.ssh/known_hosts:13</span><br><span class=\"line\">RSA host key for lab3 has changed and you have requested strict checking.</span><br><span class=\"line\">Host key verification failed.</span><br></pre></td></tr></table></figure>\n<p>问题解决：<br>删除/root/.ssh/known_hosts:13行中的信息即可</p>\n<hr>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、hostname-修改详解\"><a href=\"#一、hostname-修改详解\" class=\"headerlink\" title=\"一、hostname 修改详解\"></a>一、hostname 修改详解</h2><ol>\n<li><p>暂时性修改使用命令：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hostname ******</span><br></pre></td></tr></table></figure>\n<p>此命令不用重启，重新打开一个终端，即可看到修改。但是重启后失效（本人red hat5竟然不会）</p>\n</li>\n<li><p>永久性修改</p>\n<p>在1之后，修改 /etc/sysconfig/network</p>\n</li>\n</ol>\n<hr>\n<h2 id=\"二、配置ssh\"><a href=\"#二、配置ssh\" class=\"headerlink\" title=\"二、配置ssh\"></a>二、配置ssh</h2><p><strong>本节参考：<a href=\"http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html\" target=\"_blank\" rel=\"noopener\">网址</a></strong></p>\n<ol>\n<li><p>生成公钥</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ssh-keygen</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>公钥分发到远程主机</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ssh-copy-id user@host</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class=\"line\">@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @</span><br><span class=\"line\">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class=\"line\">IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!</span><br><span class=\"line\">Someone could be eavesdropping on you right now (man-in-the-middle attack)!</span><br><span class=\"line\">It is also possible that the RSA host key has just been changed.</span><br><span class=\"line\">The fingerprint for the RSA key sent by the remote host is</span><br><span class=\"line\">df:a8:ff:b2:0d:f8:1c:a9:d9:f9:fa:39:29:9d:6f:42.</span><br><span class=\"line\">Please contact your system administrator.</span><br><span class=\"line\">Add correct host key in /root/.ssh/known_hosts to get rid of this message.</span><br><span class=\"line\">Offending key in /root/.ssh/known_hosts:13</span><br><span class=\"line\">RSA host key for lab3 has changed and you have requested strict checking.</span><br><span class=\"line\">Host key verification failed.</span><br></pre></td></tr></table></figure>\n<p>问题解决：<br>删除/root/.ssh/known_hosts:13行中的信息即可</p>\n<hr>\n"},{"title":"linux环境jdk安装及配置","date":"2016-04-15T12:09:44.000Z","_content":"1.下载jkd（ http://www.oracle.com/technetwork/java/javase/downloads/index.html）\n\n- 对于32位的系统可以下载以下两个Linux x86版本（uname -a 查看系统版本）\n\n\n\n- 264位系统下载Linux x64版本\n\n\n2.安装jdk（这里以.tar.gz版本，32位系统为例）\n\n安装方法参考http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html \n\n- 选择要安装java的位置，如/usr/目录下，新建文件夹java(mkdir java)\n\n- 将文件jdk-7u40-linux-i586.tar.gz移动到/usr/java\n\n- 解压：tar -zxvf jdk-7u40-linux-i586.tar.gz\n\n- 删除jdk-7u40-linux-i586.tar.gz（为了节省空间）\n\n至此，jkd安装完毕，下面配置环境变量\n\n3.打开\n```\n/etc/profile（vim /etc/profile）\n```\n在最后面添加如下内容：\n```\nJAVA_HOME=/usr/java/jdk1.7.0_40\nCLASSPATH=.:$JAVA_HOME/lib.tools.jar\nPATH=$JAVA_HOME/bin:$PATH\nexport JAVA_HOME CLASSPATH PATH\n```\n4.生效\n```\nsource /etc/profile\n```\n\n5.验证是否安装成功：java -version","source":"_posts/linux环境jdk安装及配置.md","raw":"---\ntitle: linux环境jdk安装及配置\ndate: 2016-04-15 20:09:44\ntags: 大数据\ncategories:\n- linux\n---\n1.下载jkd（ http://www.oracle.com/technetwork/java/javase/downloads/index.html）\n\n- 对于32位的系统可以下载以下两个Linux x86版本（uname -a 查看系统版本）\n\n\n\n- 264位系统下载Linux x64版本\n\n\n2.安装jdk（这里以.tar.gz版本，32位系统为例）\n\n安装方法参考http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html \n\n- 选择要安装java的位置，如/usr/目录下，新建文件夹java(mkdir java)\n\n- 将文件jdk-7u40-linux-i586.tar.gz移动到/usr/java\n\n- 解压：tar -zxvf jdk-7u40-linux-i586.tar.gz\n\n- 删除jdk-7u40-linux-i586.tar.gz（为了节省空间）\n\n至此，jkd安装完毕，下面配置环境变量\n\n3.打开\n```\n/etc/profile（vim /etc/profile）\n```\n在最后面添加如下内容：\n```\nJAVA_HOME=/usr/java/jdk1.7.0_40\nCLASSPATH=.:$JAVA_HOME/lib.tools.jar\nPATH=$JAVA_HOME/bin:$PATH\nexport JAVA_HOME CLASSPATH PATH\n```\n4.生效\n```\nsource /etc/profile\n```\n\n5.验证是否安装成功：java -version","slug":"linux环境jdk安装及配置","published":1,"updated":"2016-04-15T12:16:45.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdu006m8cee2xctp73z","content":"<p>1.下载jkd（ <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html）\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/index.html）</a></p>\n<ul>\n<li>对于32位的系统可以下载以下两个Linux x86版本（uname -a 查看系统版本）</li>\n</ul>\n<ul>\n<li>264位系统下载Linux x64版本</li>\n</ul>\n<p>2.安装jdk（这里以.tar.gz版本，32位系统为例）</p>\n<p>安装方法参考<a href=\"http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html\" target=\"_blank\" rel=\"noopener\">http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html</a> </p>\n<ul>\n<li><p>选择要安装java的位置，如/usr/目录下，新建文件夹java(mkdir java)</p>\n</li>\n<li><p>将文件jdk-7u40-linux-i586.tar.gz移动到/usr/java</p>\n</li>\n<li><p>解压：tar -zxvf jdk-7u40-linux-i586.tar.gz</p>\n</li>\n<li><p>删除jdk-7u40-linux-i586.tar.gz（为了节省空间）</p>\n</li>\n</ul>\n<p>至此，jkd安装完毕，下面配置环境变量</p>\n<p>3.打开<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/etc/profile（vim /etc/profile）</span><br></pre></td></tr></table></figure></p>\n<p>在最后面添加如下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JAVA_HOME=/usr/java/jdk1.7.0_40</span><br><span class=\"line\">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class=\"line\">PATH=$JAVA_HOME/bin:$PATH</span><br><span class=\"line\">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure></p>\n<p>4.生效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>5.验证是否安装成功：java -version</p>\n","site":{"data":{}},"excerpt":"","more":"<p>1.下载jkd（ <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html）\" target=\"_blank\" rel=\"noopener\">http://www.oracle.com/technetwork/java/javase/downloads/index.html）</a></p>\n<ul>\n<li>对于32位的系统可以下载以下两个Linux x86版本（uname -a 查看系统版本）</li>\n</ul>\n<ul>\n<li>264位系统下载Linux x64版本</li>\n</ul>\n<p>2.安装jdk（这里以.tar.gz版本，32位系统为例）</p>\n<p>安装方法参考<a href=\"http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html\" target=\"_blank\" rel=\"noopener\">http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html</a> </p>\n<ul>\n<li><p>选择要安装java的位置，如/usr/目录下，新建文件夹java(mkdir java)</p>\n</li>\n<li><p>将文件jdk-7u40-linux-i586.tar.gz移动到/usr/java</p>\n</li>\n<li><p>解压：tar -zxvf jdk-7u40-linux-i586.tar.gz</p>\n</li>\n<li><p>删除jdk-7u40-linux-i586.tar.gz（为了节省空间）</p>\n</li>\n</ul>\n<p>至此，jkd安装完毕，下面配置环境变量</p>\n<p>3.打开<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/etc/profile（vim /etc/profile）</span><br></pre></td></tr></table></figure></p>\n<p>在最后面添加如下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JAVA_HOME=/usr/java/jdk1.7.0_40</span><br><span class=\"line\">CLASSPATH=.:$JAVA_HOME/lib.tools.jar</span><br><span class=\"line\">PATH=$JAVA_HOME/bin:$PATH</span><br><span class=\"line\">export JAVA_HOME CLASSPATH PATH</span><br></pre></td></tr></table></figure></p>\n<p>4.生效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>5.验证是否安装成功：java -version</p>\n"},{"title":"linux运维问题","date":"2016-07-18T13:16:01.000Z","_content":"# linux 运维\n## 1.后台执行任务\n问题描述：\n\n在linux中执行一些服务，我们通常会使用sudo命令，以root权限运行，再以nohup后台运行，但是有的情况下需要交互操作。\n\n解决方案：\n\n1.首先输入命令，\n```\nsudo nohup commandline\n```\n2.使用以下快捷键中断\n```\nctrl+z\n```\n3.在命令台中输入\n```\nbg\n```\n## 2.监控linux磁盘IO\n可以使用**iostat**命令,iostat还有一个比较常用的选项-x，该选项将用于显示和io相关的扩展数据。（可以结合grep一起使用便于过滤具体磁盘）\n\n**命令解释：**\n参数 -d 表示，显示设备（磁盘）使用状态；-k某些使用block为单位的列强制使用Kilobytes为单位(同样可以用m表示兆)；2表示，数据显示每隔2秒刷新一次。\n```\niostat -d -k 2\n```\n输出信息解释\n```\ntps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。\"一次传输\"意思是\"一次I/O请求\"。多个逻辑请求可能会被合并为\"一次I/O请求\"。\"一次传输\"请求的大小是未知的。\n\nkB_read/s：每秒从设备（drive expressed）读取的数据量；\nkB_wrtn/s：每秒向设备（drive expressed）写入的数据量；\nkB_read：读取的总数据量；\nkB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。\n```\n","source":"_posts/linux运维问题.md","raw":"---\ntitle: linux运维问题\ndate: 2016-07-18 21:16:01\ntags: 运维\ncategories:\n- linux\n---\n# linux 运维\n## 1.后台执行任务\n问题描述：\n\n在linux中执行一些服务，我们通常会使用sudo命令，以root权限运行，再以nohup后台运行，但是有的情况下需要交互操作。\n\n解决方案：\n\n1.首先输入命令，\n```\nsudo nohup commandline\n```\n2.使用以下快捷键中断\n```\nctrl+z\n```\n3.在命令台中输入\n```\nbg\n```\n## 2.监控linux磁盘IO\n可以使用**iostat**命令,iostat还有一个比较常用的选项-x，该选项将用于显示和io相关的扩展数据。（可以结合grep一起使用便于过滤具体磁盘）\n\n**命令解释：**\n参数 -d 表示，显示设备（磁盘）使用状态；-k某些使用block为单位的列强制使用Kilobytes为单位(同样可以用m表示兆)；2表示，数据显示每隔2秒刷新一次。\n```\niostat -d -k 2\n```\n输出信息解释\n```\ntps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。\"一次传输\"意思是\"一次I/O请求\"。多个逻辑请求可能会被合并为\"一次I/O请求\"。\"一次传输\"请求的大小是未知的。\n\nkB_read/s：每秒从设备（drive expressed）读取的数据量；\nkB_wrtn/s：每秒向设备（drive expressed）写入的数据量；\nkB_read：读取的总数据量；\nkB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。\n```\n","slug":"linux运维问题","published":1,"updated":"2016-08-21T12:45:22.581Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdx006p8cee9h5cftd9","content":"<h1 id=\"linux-运维\"><a href=\"#linux-运维\" class=\"headerlink\" title=\"linux 运维\"></a>linux 运维</h1><h2 id=\"1-后台执行任务\"><a href=\"#1-后台执行任务\" class=\"headerlink\" title=\"1.后台执行任务\"></a>1.后台执行任务</h2><p>问题描述：</p>\n<p>在linux中执行一些服务，我们通常会使用sudo命令，以root权限运行，再以nohup后台运行，但是有的情况下需要交互操作。</p>\n<p>解决方案：</p>\n<p>1.首先输入命令，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nohup commandline</span><br></pre></td></tr></table></figure></p>\n<p>2.使用以下快捷键中断<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ctrl+z</span><br></pre></td></tr></table></figure></p>\n<p>3.在命令台中输入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bg</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-监控linux磁盘IO\"><a href=\"#2-监控linux磁盘IO\" class=\"headerlink\" title=\"2.监控linux磁盘IO\"></a>2.监控linux磁盘IO</h2><p>可以使用<strong>iostat</strong>命令,iostat还有一个比较常用的选项-x，该选项将用于显示和io相关的扩展数据。（可以结合grep一起使用便于过滤具体磁盘）</p>\n<p><strong>命令解释：</strong><br>参数 -d 表示，显示设备（磁盘）使用状态；-k某些使用block为单位的列强制使用Kilobytes为单位(同样可以用m表示兆)；2表示，数据显示每隔2秒刷新一次。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat -d -k 2</span><br></pre></td></tr></table></figure></p>\n<p>输出信息解释<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。&quot;一次传输&quot;意思是&quot;一次I/O请求&quot;。多个逻辑请求可能会被合并为&quot;一次I/O请求&quot;。&quot;一次传输&quot;请求的大小是未知的。</span><br><span class=\"line\"></span><br><span class=\"line\">kB_read/s：每秒从设备（drive expressed）读取的数据量；</span><br><span class=\"line\">kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；</span><br><span class=\"line\">kB_read：读取的总数据量；</span><br><span class=\"line\">kB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"linux-运维\"><a href=\"#linux-运维\" class=\"headerlink\" title=\"linux 运维\"></a>linux 运维</h1><h2 id=\"1-后台执行任务\"><a href=\"#1-后台执行任务\" class=\"headerlink\" title=\"1.后台执行任务\"></a>1.后台执行任务</h2><p>问题描述：</p>\n<p>在linux中执行一些服务，我们通常会使用sudo命令，以root权限运行，再以nohup后台运行，但是有的情况下需要交互操作。</p>\n<p>解决方案：</p>\n<p>1.首先输入命令，<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nohup commandline</span><br></pre></td></tr></table></figure></p>\n<p>2.使用以下快捷键中断<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ctrl+z</span><br></pre></td></tr></table></figure></p>\n<p>3.在命令台中输入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bg</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-监控linux磁盘IO\"><a href=\"#2-监控linux磁盘IO\" class=\"headerlink\" title=\"2.监控linux磁盘IO\"></a>2.监控linux磁盘IO</h2><p>可以使用<strong>iostat</strong>命令,iostat还有一个比较常用的选项-x，该选项将用于显示和io相关的扩展数据。（可以结合grep一起使用便于过滤具体磁盘）</p>\n<p><strong>命令解释：</strong><br>参数 -d 表示，显示设备（磁盘）使用状态；-k某些使用block为单位的列强制使用Kilobytes为单位(同样可以用m表示兆)；2表示，数据显示每隔2秒刷新一次。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">iostat -d -k 2</span><br></pre></td></tr></table></figure></p>\n<p>输出信息解释<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。&quot;一次传输&quot;意思是&quot;一次I/O请求&quot;。多个逻辑请求可能会被合并为&quot;一次I/O请求&quot;。&quot;一次传输&quot;请求的大小是未知的。</span><br><span class=\"line\"></span><br><span class=\"line\">kB_read/s：每秒从设备（drive expressed）读取的数据量；</span><br><span class=\"line\">kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；</span><br><span class=\"line\">kB_read：读取的总数据量；</span><br><span class=\"line\">kB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。</span><br></pre></td></tr></table></figure></p>\n"},{"title":"logstash使用教程","date":"2016-10-24T13:26:00.000Z","_content":"## 简介\nlogstash是一个实时流水式开源数据收集引擎。具有强大的plugin。可以根据自己的业务场景选择不同的input filter output。绝大多数情况下都是结合ElasticSearch Kibana一起使用的，俗称ELK。\n## 模块介绍\nLogstash使用管道方式进行日志的搜集处理和输出。有点类似*NIX系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。\n\n在logstash中，包括了三个阶段:\n\n输入input --> 处理filter（不是必须的） --> 输出output\n### 配置文件说明\n前面介绍过logstash基本上由三部分组成，input、output以及用户需要才添加的filter，因此标准的配置文件格式如下：\n```\ninput {\n   \n}\nfilter {\n    \n}\noutput {\n    \n}\n```\n### 执行说明\n```\nbin/logstash -f demo.conf\n```\n## 使用Demo\n### Output plugins ElasticSearch\n案例使用如下：\n```\noutput {\n#stdout {  codec => rubydebug }\nelasticsearch {\nhosts => [\"127.0.0.1:9200\"]\ntemplate_overwrite => true\nindex => \"rediscluster-%{+YYYY.MM.dd}\"\nworkers => 5\n}\n}\n```\n### Output plugins opentsdb\n使用logstash收集数据，并发送到opentsdb中。分为三部分：Input,Filter,Output\n\n输入数据时，输入一条数据，回车。以下为三条测试数据：\n```\nthreads.ThreadCount 1352279077 67 host=server1 port=1006\ngc.PSScavenge.CollectionTime 1352279137 1360 host=server2 port=1010\nmemorypool.CodeCache.Usage_used 1352279137 11625472 host=server1 port=1009\n```\n\n- Input采用命令行输入数据\n```\ninput {\n   stdin{\n   }\n}\n```\n\n- Filter过滤组织数据\n\n采用的是grok插件，可以使用其他插件完成相同的目的\n```\nfilter {\n    grok { \n        match => { \"message\" => \"%{DATA:metricName} %{NUMBER:unixtime} %{NUMBER:data} host=%{DATA:metricHost} port=%{NUMBER:port}\" }  \n        remove_field => [ \"host\" ]\n    }\n    \n} \n```\n**备忘**：\n\n1. logstash输入数据自带host,@timestamp等自带，为了避免干扰存入opentsdb数据，此处特将隐含的host字段去掉。\n2. DATA/NUMBER等实为grok自带的正则规则。\n- Output输出数据\n\n此处输出数据到opentsdb中，官方文档有误，详见[源码](https://github.com/logstash-plugins/logstash-output-opentsdb/blob/master/lib/logstash/outputs/opentsdb.rb)\n```\noutput {\n    stdout {  codec => rubydebug }#此处是为了将filter结果输出到控制台中\n    opentsdb {\n        host => '***.***.***.***'\n        port => 4242\n        metrics => [\n            \"%{metricName}\",\n            \"%{data}\",\n            \"host\",\n            \"%{metricHost}\",\n            \"port\",\n            \"%{port}\"\n        ]\n    }\n}\n```\n**备忘**：\n\nopentsdb输入信息格式为：put metric timestamp value tagname=tagvalue tag2=value2，在logstash-output-opentsdb插件metrics配置中默认已经输入timestamp，因此metrics需要配置的第一个参数为metricName，第二个参数为 value 之后依次为tagname,tagValue。\n\n## 参考\n1. https://www.elastic.co/guide/en/logstash/current/index.html","source":"_posts/logstash使用教程.md","raw":"---\ntitle: logstash使用教程\ndate: 2016-10-24 21:26:00\ntags: research\ncategories:\n- elasticsearch\n---\n## 简介\nlogstash是一个实时流水式开源数据收集引擎。具有强大的plugin。可以根据自己的业务场景选择不同的input filter output。绝大多数情况下都是结合ElasticSearch Kibana一起使用的，俗称ELK。\n## 模块介绍\nLogstash使用管道方式进行日志的搜集处理和输出。有点类似*NIX系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。\n\n在logstash中，包括了三个阶段:\n\n输入input --> 处理filter（不是必须的） --> 输出output\n### 配置文件说明\n前面介绍过logstash基本上由三部分组成，input、output以及用户需要才添加的filter，因此标准的配置文件格式如下：\n```\ninput {\n   \n}\nfilter {\n    \n}\noutput {\n    \n}\n```\n### 执行说明\n```\nbin/logstash -f demo.conf\n```\n## 使用Demo\n### Output plugins ElasticSearch\n案例使用如下：\n```\noutput {\n#stdout {  codec => rubydebug }\nelasticsearch {\nhosts => [\"127.0.0.1:9200\"]\ntemplate_overwrite => true\nindex => \"rediscluster-%{+YYYY.MM.dd}\"\nworkers => 5\n}\n}\n```\n### Output plugins opentsdb\n使用logstash收集数据，并发送到opentsdb中。分为三部分：Input,Filter,Output\n\n输入数据时，输入一条数据，回车。以下为三条测试数据：\n```\nthreads.ThreadCount 1352279077 67 host=server1 port=1006\ngc.PSScavenge.CollectionTime 1352279137 1360 host=server2 port=1010\nmemorypool.CodeCache.Usage_used 1352279137 11625472 host=server1 port=1009\n```\n\n- Input采用命令行输入数据\n```\ninput {\n   stdin{\n   }\n}\n```\n\n- Filter过滤组织数据\n\n采用的是grok插件，可以使用其他插件完成相同的目的\n```\nfilter {\n    grok { \n        match => { \"message\" => \"%{DATA:metricName} %{NUMBER:unixtime} %{NUMBER:data} host=%{DATA:metricHost} port=%{NUMBER:port}\" }  \n        remove_field => [ \"host\" ]\n    }\n    \n} \n```\n**备忘**：\n\n1. logstash输入数据自带host,@timestamp等自带，为了避免干扰存入opentsdb数据，此处特将隐含的host字段去掉。\n2. DATA/NUMBER等实为grok自带的正则规则。\n- Output输出数据\n\n此处输出数据到opentsdb中，官方文档有误，详见[源码](https://github.com/logstash-plugins/logstash-output-opentsdb/blob/master/lib/logstash/outputs/opentsdb.rb)\n```\noutput {\n    stdout {  codec => rubydebug }#此处是为了将filter结果输出到控制台中\n    opentsdb {\n        host => '***.***.***.***'\n        port => 4242\n        metrics => [\n            \"%{metricName}\",\n            \"%{data}\",\n            \"host\",\n            \"%{metricHost}\",\n            \"port\",\n            \"%{port}\"\n        ]\n    }\n}\n```\n**备忘**：\n\nopentsdb输入信息格式为：put metric timestamp value tagname=tagvalue tag2=value2，在logstash-output-opentsdb插件metrics配置中默认已经输入timestamp，因此metrics需要配置的第一个参数为metricName，第二个参数为 value 之后依次为tagname,tagValue。\n\n## 参考\n1. https://www.elastic.co/guide/en/logstash/current/index.html","slug":"logstash使用教程","published":1,"updated":"2016-10-24T13:26:34.415Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvdz006s8ceef73wwfiu","content":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>logstash是一个实时流水式开源数据收集引擎。具有强大的plugin。可以根据自己的业务场景选择不同的input filter output。绝大多数情况下都是结合ElasticSearch Kibana一起使用的，俗称ELK。</p>\n<h2 id=\"模块介绍\"><a href=\"#模块介绍\" class=\"headerlink\" title=\"模块介绍\"></a>模块介绍</h2><p>Logstash使用管道方式进行日志的搜集处理和输出。有点类似*NIX系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。</p>\n<p>在logstash中，包括了三个阶段:</p>\n<p>输入input –&gt; 处理filter（不是必须的） –&gt; 输出output</p>\n<h3 id=\"配置文件说明\"><a href=\"#配置文件说明\" class=\"headerlink\" title=\"配置文件说明\"></a>配置文件说明</h3><p>前面介绍过logstash基本上由三部分组成，input、output以及用户需要才添加的filter，因此标准的配置文件格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"执行说明\"><a href=\"#执行说明\" class=\"headerlink\" title=\"执行说明\"></a>执行说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash -f demo.conf</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用Demo\"><a href=\"#使用Demo\" class=\"headerlink\" title=\"使用Demo\"></a>使用Demo</h2><h3 id=\"Output-plugins-ElasticSearch\"><a href=\"#Output-plugins-ElasticSearch\" class=\"headerlink\" title=\"Output plugins ElasticSearch\"></a>Output plugins ElasticSearch</h3><p>案例使用如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output &#123;</span><br><span class=\"line\">#stdout &#123;  codec =&gt; rubydebug &#125;</span><br><span class=\"line\">elasticsearch &#123;</span><br><span class=\"line\">hosts =&gt; [&quot;127.0.0.1:9200&quot;]</span><br><span class=\"line\">template_overwrite =&gt; true</span><br><span class=\"line\">index =&gt; &quot;rediscluster-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class=\"line\">workers =&gt; 5</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Output-plugins-opentsdb\"><a href=\"#Output-plugins-opentsdb\" class=\"headerlink\" title=\"Output plugins opentsdb\"></a>Output plugins opentsdb</h3><p>使用logstash收集数据，并发送到opentsdb中。分为三部分：Input,Filter,Output</p>\n<p>输入数据时，输入一条数据，回车。以下为三条测试数据：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">threads.ThreadCount 1352279077 67 host=server1 port=1006</span><br><span class=\"line\">gc.PSScavenge.CollectionTime 1352279137 1360 host=server2 port=1010</span><br><span class=\"line\">memorypool.CodeCache.Usage_used 1352279137 11625472 host=server1 port=1009</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><p>Input采用命令行输入数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">   stdin&#123;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Filter过滤组织数据</p>\n</li>\n</ul>\n<p>采用的是grok插件，可以使用其他插件完成相同的目的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filter &#123;</span><br><span class=\"line\">    grok &#123; </span><br><span class=\"line\">        match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;DATA:metricName&#125; %&#123;NUMBER:unixtime&#125; %&#123;NUMBER:data&#125; host=%&#123;DATA:metricHost&#125; port=%&#123;NUMBER:port&#125;&quot; &#125;  </span><br><span class=\"line\">        remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>备忘</strong>：</p>\n<ol>\n<li>logstash输入数据自带host,@timestamp等自带，为了避免干扰存入opentsdb数据，此处特将隐含的host字段去掉。</li>\n<li>DATA/NUMBER等实为grok自带的正则规则。</li>\n</ol>\n<ul>\n<li>Output输出数据</li>\n</ul>\n<p>此处输出数据到opentsdb中，官方文档有误，详见<a href=\"https://github.com/logstash-plugins/logstash-output-opentsdb/blob/master/lib/logstash/outputs/opentsdb.rb\" target=\"_blank\" rel=\"noopener\">源码</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output &#123;</span><br><span class=\"line\">    stdout &#123;  codec =&gt; rubydebug &#125;#此处是为了将filter结果输出到控制台中</span><br><span class=\"line\">    opentsdb &#123;</span><br><span class=\"line\">        host =&gt; &apos;***.***.***.***&apos;</span><br><span class=\"line\">        port =&gt; 4242</span><br><span class=\"line\">        metrics =&gt; [</span><br><span class=\"line\">            &quot;%&#123;metricName&#125;&quot;,</span><br><span class=\"line\">            &quot;%&#123;data&#125;&quot;,</span><br><span class=\"line\">            &quot;host&quot;,</span><br><span class=\"line\">            &quot;%&#123;metricHost&#125;&quot;,</span><br><span class=\"line\">            &quot;port&quot;,</span><br><span class=\"line\">            &quot;%&#123;port&#125;&quot;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>备忘</strong>：</p>\n<p>opentsdb输入信息格式为：put metric timestamp value tagname=tagvalue tag2=value2，在logstash-output-opentsdb插件metrics配置中默认已经输入timestamp，因此metrics需要配置的第一个参数为metricName，第二个参数为 value 之后依次为tagname,tagValue。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/index.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>logstash是一个实时流水式开源数据收集引擎。具有强大的plugin。可以根据自己的业务场景选择不同的input filter output。绝大多数情况下都是结合ElasticSearch Kibana一起使用的，俗称ELK。</p>\n<h2 id=\"模块介绍\"><a href=\"#模块介绍\" class=\"headerlink\" title=\"模块介绍\"></a>模块介绍</h2><p>Logstash使用管道方式进行日志的搜集处理和输出。有点类似*NIX系统的管道命令 xxx | ccc | ddd，xxx执行完了会执行ccc，然后执行ddd。</p>\n<p>在logstash中，包括了三个阶段:</p>\n<p>输入input –&gt; 处理filter（不是必须的） –&gt; 输出output</p>\n<h3 id=\"配置文件说明\"><a href=\"#配置文件说明\" class=\"headerlink\" title=\"配置文件说明\"></a>配置文件说明</h3><p>前面介绍过logstash基本上由三部分组成，input、output以及用户需要才添加的filter，因此标准的配置文件格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output &#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"执行说明\"><a href=\"#执行说明\" class=\"headerlink\" title=\"执行说明\"></a>执行说明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash -f demo.conf</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用Demo\"><a href=\"#使用Demo\" class=\"headerlink\" title=\"使用Demo\"></a>使用Demo</h2><h3 id=\"Output-plugins-ElasticSearch\"><a href=\"#Output-plugins-ElasticSearch\" class=\"headerlink\" title=\"Output plugins ElasticSearch\"></a>Output plugins ElasticSearch</h3><p>案例使用如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output &#123;</span><br><span class=\"line\">#stdout &#123;  codec =&gt; rubydebug &#125;</span><br><span class=\"line\">elasticsearch &#123;</span><br><span class=\"line\">hosts =&gt; [&quot;127.0.0.1:9200&quot;]</span><br><span class=\"line\">template_overwrite =&gt; true</span><br><span class=\"line\">index =&gt; &quot;rediscluster-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class=\"line\">workers =&gt; 5</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Output-plugins-opentsdb\"><a href=\"#Output-plugins-opentsdb\" class=\"headerlink\" title=\"Output plugins opentsdb\"></a>Output plugins opentsdb</h3><p>使用logstash收集数据，并发送到opentsdb中。分为三部分：Input,Filter,Output</p>\n<p>输入数据时，输入一条数据，回车。以下为三条测试数据：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">threads.ThreadCount 1352279077 67 host=server1 port=1006</span><br><span class=\"line\">gc.PSScavenge.CollectionTime 1352279137 1360 host=server2 port=1010</span><br><span class=\"line\">memorypool.CodeCache.Usage_used 1352279137 11625472 host=server1 port=1009</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><p>Input采用命令行输入数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">   stdin&#123;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Filter过滤组织数据</p>\n</li>\n</ul>\n<p>采用的是grok插件，可以使用其他插件完成相同的目的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filter &#123;</span><br><span class=\"line\">    grok &#123; </span><br><span class=\"line\">        match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;DATA:metricName&#125; %&#123;NUMBER:unixtime&#125; %&#123;NUMBER:data&#125; host=%&#123;DATA:metricHost&#125; port=%&#123;NUMBER:port&#125;&quot; &#125;  </span><br><span class=\"line\">        remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>备忘</strong>：</p>\n<ol>\n<li>logstash输入数据自带host,@timestamp等自带，为了避免干扰存入opentsdb数据，此处特将隐含的host字段去掉。</li>\n<li>DATA/NUMBER等实为grok自带的正则规则。</li>\n</ol>\n<ul>\n<li>Output输出数据</li>\n</ul>\n<p>此处输出数据到opentsdb中，官方文档有误，详见<a href=\"https://github.com/logstash-plugins/logstash-output-opentsdb/blob/master/lib/logstash/outputs/opentsdb.rb\" target=\"_blank\" rel=\"noopener\">源码</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">output &#123;</span><br><span class=\"line\">    stdout &#123;  codec =&gt; rubydebug &#125;#此处是为了将filter结果输出到控制台中</span><br><span class=\"line\">    opentsdb &#123;</span><br><span class=\"line\">        host =&gt; &apos;***.***.***.***&apos;</span><br><span class=\"line\">        port =&gt; 4242</span><br><span class=\"line\">        metrics =&gt; [</span><br><span class=\"line\">            &quot;%&#123;metricName&#125;&quot;,</span><br><span class=\"line\">            &quot;%&#123;data&#125;&quot;,</span><br><span class=\"line\">            &quot;host&quot;,</span><br><span class=\"line\">            &quot;%&#123;metricHost&#125;&quot;,</span><br><span class=\"line\">            &quot;port&quot;,</span><br><span class=\"line\">            &quot;%&#123;port&#125;&quot;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>备忘</strong>：</p>\n<p>opentsdb输入信息格式为：put metric timestamp value tagname=tagvalue tag2=value2，在logstash-output-opentsdb插件metrics配置中默认已经输入timestamp，因此metrics需要配置的第一个参数为metricName，第二个参数为 value 之后依次为tagname,tagValue。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/index.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/en/logstash/current/index.html</a></li>\n</ol>\n"},{"title":"Filter plugins","date":"2016-10-24T13:24:37.000Z","_content":"# Filter plugins\n1. grok\n```\ngrok {\nmatch => {\"command\" => \"redis-cli -c -h %{IP:node:} -p %{NUMBER:port}%{DATA:data}\" }\nremove_field => [ \"host\" ]\n}\n```\n2. ruby\n\n功能描述：将redis info 信息格式化按字段输出\n```\n ruby {\n        code => \"fields = event['message'].split(/\\r\\n|\\n/)\n        length = fields.length-1\n        for i in 1..length do \n          if fields[i].include?':' then\n            field = fields[i].split(':')\n            event[field[0]] = field[1].to_f\n          end\n        end\n        \"\n        remove_field => [ \"message\" ]\n    }\n```\n3. mutate\n\n功能描述：字段类型指定\n```\nfilter {\n        mutate {\n            convert => {\"latestResponse\" => \"integer\"}\n            convert => {\"cacheHit\" => \"string\"}\n            convert => {\"cacheRate\" => \"float\"}\n        }\n        \n}\n```\n# Output plugins","source":"_posts/logstash插件使用.md","raw":"---\ntitle: Filter plugins\ndate: 2016-10-24 21:24:37\ntags: research\ncategories:\n- elasticsearch\n---\n# Filter plugins\n1. grok\n```\ngrok {\nmatch => {\"command\" => \"redis-cli -c -h %{IP:node:} -p %{NUMBER:port}%{DATA:data}\" }\nremove_field => [ \"host\" ]\n}\n```\n2. ruby\n\n功能描述：将redis info 信息格式化按字段输出\n```\n ruby {\n        code => \"fields = event['message'].split(/\\r\\n|\\n/)\n        length = fields.length-1\n        for i in 1..length do \n          if fields[i].include?':' then\n            field = fields[i].split(':')\n            event[field[0]] = field[1].to_f\n          end\n        end\n        \"\n        remove_field => [ \"message\" ]\n    }\n```\n3. mutate\n\n功能描述：字段类型指定\n```\nfilter {\n        mutate {\n            convert => {\"latestResponse\" => \"integer\"}\n            convert => {\"cacheHit\" => \"string\"}\n            convert => {\"cacheRate\" => \"float\"}\n        }\n        \n}\n```\n# Output plugins","slug":"logstash插件使用","published":1,"updated":"2016-10-24T13:25:33.992Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobve1006x8ceewv58z16v","content":"<h1 id=\"Filter-plugins\"><a href=\"#Filter-plugins\" class=\"headerlink\" title=\"Filter plugins\"></a>Filter plugins</h1><ol>\n<li><p>grok</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grok &#123;</span><br><span class=\"line\">match =&gt; &#123;&quot;command&quot; =&gt; &quot;redis-cli -c -h %&#123;IP:node:&#125; -p %&#123;NUMBER:port&#125;%&#123;DATA:data&#125;&quot; &#125;</span><br><span class=\"line\">remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>ruby</p>\n</li>\n</ol>\n<p>功能描述：将redis info 信息格式化按字段输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ruby &#123;</span><br><span class=\"line\">       code =&gt; &quot;fields = event[&apos;message&apos;].split(/\\r\\n|\\n/)</span><br><span class=\"line\">       length = fields.length-1</span><br><span class=\"line\">       for i in 1..length do </span><br><span class=\"line\">         if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">           field = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">           event[field[0]] = field[1].to_f</span><br><span class=\"line\">         end</span><br><span class=\"line\">       end</span><br><span class=\"line\">       &quot;</span><br><span class=\"line\">       remove_field =&gt; [ &quot;message&quot; ]</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>mutate</li>\n</ol>\n<p>功能描述：字段类型指定<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filter &#123;</span><br><span class=\"line\">        mutate &#123;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;latestResponse&quot; =&gt; &quot;integer&quot;&#125;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;cacheHit&quot; =&gt; &quot;string&quot;&#125;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;cacheRate&quot; =&gt; &quot;float&quot;&#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"Output-plugins\"><a href=\"#Output-plugins\" class=\"headerlink\" title=\"Output plugins\"></a>Output plugins</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Filter-plugins\"><a href=\"#Filter-plugins\" class=\"headerlink\" title=\"Filter plugins\"></a>Filter plugins</h1><ol>\n<li><p>grok</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grok &#123;</span><br><span class=\"line\">match =&gt; &#123;&quot;command&quot; =&gt; &quot;redis-cli -c -h %&#123;IP:node:&#125; -p %&#123;NUMBER:port&#125;%&#123;DATA:data&#125;&quot; &#125;</span><br><span class=\"line\">remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>ruby</p>\n</li>\n</ol>\n<p>功能描述：将redis info 信息格式化按字段输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ruby &#123;</span><br><span class=\"line\">       code =&gt; &quot;fields = event[&apos;message&apos;].split(/\\r\\n|\\n/)</span><br><span class=\"line\">       length = fields.length-1</span><br><span class=\"line\">       for i in 1..length do </span><br><span class=\"line\">         if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">           field = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">           event[field[0]] = field[1].to_f</span><br><span class=\"line\">         end</span><br><span class=\"line\">       end</span><br><span class=\"line\">       &quot;</span><br><span class=\"line\">       remove_field =&gt; [ &quot;message&quot; ]</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li>mutate</li>\n</ol>\n<p>功能描述：字段类型指定<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">filter &#123;</span><br><span class=\"line\">        mutate &#123;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;latestResponse&quot; =&gt; &quot;integer&quot;&#125;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;cacheHit&quot; =&gt; &quot;string&quot;&#125;</span><br><span class=\"line\">            convert =&gt; &#123;&quot;cacheRate&quot; =&gt; &quot;float&quot;&#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"Output-plugins\"><a href=\"#Output-plugins\" class=\"headerlink\" title=\"Output plugins\"></a>Output plugins</h1>"},{"title":"maven打包案例","date":"2016-10-24T13:20:13.000Z","_content":"## 一、spring-boot-maven-plugin\n使用案例如下：\n```\n<plugin>\n\t\t<groupId>org.springframework.boot</groupId>\n\t\t<artifactId>spring-boot-maven-plugin</artifactId>\n\t\t<executions>\n\t\t\t<execution>\n\t\t\t\t<goals>\n\t\t\t\t\t<goal>repackage</goal>\n\t\t\t\t</goals>\n\t\t\t</execution>\n\t\t</executions>\n\t\t<configuration>\n\t\t\t<mainClass>com.aibibang.AppMain</mainClass>\n\t\t\t<addResources>true</addResources>\n\t\t\t<excludes>\n\t\t\t\t<exclude>\n\t\t\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t\t\t<artifactId>spring-boot-devtools</artifactId>\n\t\t\t\t</exclude>\n\t\t\t</excludes>\n\t\t</configuration>\n</plugin>\n```\n## 二、自定义打包\n在开发过程中经常需要将依赖包与代码包分离，配置文件与代码包分离，这样更便于部署与修改参数。依次案例基于以上场景展开，借助于maven-jar-plugin与maven-assembly-plugin。\n\npom内容如下：\n```\n<build>\n\t\t<plugins>\n\t\t\t<plugin>  \n                <groupId>org.apache.maven.plugins</groupId>  \n                <artifactId>maven-jar-plugin</artifactId>  \n                <configuration>  \n                    <archive>    \n                        <manifest>  \n                            <mainClass>com.aibibang.AppMain</mainClass>  \n                            <addClasspath>true</addClasspath>  \n                            <classpathPrefix>lib/</classpathPrefix>  \n                        </manifest>\n                    </archive>  \n                    <excludes>  \n                       <exclude>*.properties</exclude>\n                       <exclude>*.xml</exclude>\n                    </excludes>  \n                </configuration>  \n            </plugin> \n\t\t\t<plugin>\n\t\t\t\t<groupId>org.apache.maven.plugins</groupId>\n\t\t\t\t<artifactId>maven-assembly-plugin</artifactId>\n\t\t\t\t<configuration>\n\t\t\t\t\t<descriptors>\n\t\t\t\t\t\t<descriptor>package.xml</descriptor>\n\t\t\t\t\t</descriptors>\n\t\t\t\t</configuration>\n\t\t\t\t<executions>\n\t\t\t\t\t<execution>\n\t\t\t\t\t\t<id>make-assembly</id>\n\t\t\t\t\t\t<phase>package</phase>\n\t\t\t\t\t\t<goals>\n\t\t\t\t\t\t\t<goal>single</goal>\n\t\t\t\t\t\t</goals>\n\t\t\t\t\t</execution>\n\t\t\t\t</executions>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n```\npackage.xml内容：\n```\n<assembly>\n    <id>bin</id>\n    <!-- 最终打包成一个用于发布的zip文件 -->\n    <formats>\n        <format>tar.gz</format>\n    </formats>\n\n    <fileSets>\n    \t<!-- 打包jar文件 -->\n        <fileSet>\n            <directory>${project.build.directory}</directory>\n            <outputDirectory></outputDirectory>\n            <includes>\n                <include>*.jar</include>\n            </includes>\n        </fileSet>\n        <!-- 把项目相关的启动脚本，打包进zip文件的bin目录 -->\n        <fileSet>\n            <directory>${project.basedir}/src/main/bash</directory>\n            <outputDirectory>/</outputDirectory>\n            <includes>\n                <include>*</include>\n            </includes>\n        </fileSet>\n        \n        <!-- 把项目的配置文件，打包进zip文件的config目录 -->\n        <fileSet>\n            <directory>${project.build.directory}/classes</directory>\n            <outputDirectory>conf</outputDirectory>\n            <includes>\n                <include>*.properties</include>\n                <include>*.xml</include>\n            </includes>\n        </fileSet>\n    </fileSets>\n    <!-- 把项目的依赖的jar打包到lib目录下 -->\n     <dependencySets>  \n        <dependencySet>  \n            <outputDirectory>lib</outputDirectory>  \n            <scope>runtime</scope>  \n            <excludes>  \n                <exclude>${groupId}:${artifactId}</exclude>  \n            </excludes>  \n        </dependencySet>  \n    </dependencySets>  \n</assembly>\n```\nmaven-jar-plugin只是讲代码打成一个jar,而对部署包的构建是由assembly插件完成的。\n\n结合启动、停止脚本即可高效便捷的部署一个项目。\n\nstart.sh:\n```\n#!/bin/sh\nexport JAVA_HOME=$JAVA_HOME\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nPIDFILE=service.pid\nROOT_DIR=\"$(cd $(dirname $0) && pwd)\"\nCLASSPATH=./*:$ROOT_DIR/lib/*:$ROOT_DIR/conf/\nJAVA_OPTS=\"-Xms512m -Xmx1024m -XX:+UseParallelGC\"\nMAIN_CLASS=com.aibibang.AppMain\n\n\nif [ ! -d \"logs\" ]; then\n   mkdir logs\nfi\n\nif [ -f \"$PIDFILE\" ]; then\n    echo \"Service is already start ...\"\nelse\n    echo \"Service  start ...\"\n    nohup java $JAVA_OPTS -cp $CLASSPATH $MAIN_CLASS 1> logs/server.out 2>&1  &\n    printf '%d' $! > $PIDFILE\nfi\n\n```\nstop.sh:\n```\n#!/bin/sh\nPIDFILE=service.pid\n\nif [ -f \"$PIDFILE\" ]; then\n     kill -9 `cat $PIDFILE`\n     rm -rf $PIDFILE\nelse\n    echo \"Service is already stop ...\"\nfi\n```","source":"_posts/maven打包案例.md","raw":"---\ntitle: maven打包案例\ndate: 2016-10-24 21:20:13\ntags: 开发笔记\ncategories:\n- java\n---\n## 一、spring-boot-maven-plugin\n使用案例如下：\n```\n<plugin>\n\t\t<groupId>org.springframework.boot</groupId>\n\t\t<artifactId>spring-boot-maven-plugin</artifactId>\n\t\t<executions>\n\t\t\t<execution>\n\t\t\t\t<goals>\n\t\t\t\t\t<goal>repackage</goal>\n\t\t\t\t</goals>\n\t\t\t</execution>\n\t\t</executions>\n\t\t<configuration>\n\t\t\t<mainClass>com.aibibang.AppMain</mainClass>\n\t\t\t<addResources>true</addResources>\n\t\t\t<excludes>\n\t\t\t\t<exclude>\n\t\t\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t\t\t<artifactId>spring-boot-devtools</artifactId>\n\t\t\t\t</exclude>\n\t\t\t</excludes>\n\t\t</configuration>\n</plugin>\n```\n## 二、自定义打包\n在开发过程中经常需要将依赖包与代码包分离，配置文件与代码包分离，这样更便于部署与修改参数。依次案例基于以上场景展开，借助于maven-jar-plugin与maven-assembly-plugin。\n\npom内容如下：\n```\n<build>\n\t\t<plugins>\n\t\t\t<plugin>  \n                <groupId>org.apache.maven.plugins</groupId>  \n                <artifactId>maven-jar-plugin</artifactId>  \n                <configuration>  \n                    <archive>    \n                        <manifest>  \n                            <mainClass>com.aibibang.AppMain</mainClass>  \n                            <addClasspath>true</addClasspath>  \n                            <classpathPrefix>lib/</classpathPrefix>  \n                        </manifest>\n                    </archive>  \n                    <excludes>  \n                       <exclude>*.properties</exclude>\n                       <exclude>*.xml</exclude>\n                    </excludes>  \n                </configuration>  \n            </plugin> \n\t\t\t<plugin>\n\t\t\t\t<groupId>org.apache.maven.plugins</groupId>\n\t\t\t\t<artifactId>maven-assembly-plugin</artifactId>\n\t\t\t\t<configuration>\n\t\t\t\t\t<descriptors>\n\t\t\t\t\t\t<descriptor>package.xml</descriptor>\n\t\t\t\t\t</descriptors>\n\t\t\t\t</configuration>\n\t\t\t\t<executions>\n\t\t\t\t\t<execution>\n\t\t\t\t\t\t<id>make-assembly</id>\n\t\t\t\t\t\t<phase>package</phase>\n\t\t\t\t\t\t<goals>\n\t\t\t\t\t\t\t<goal>single</goal>\n\t\t\t\t\t\t</goals>\n\t\t\t\t\t</execution>\n\t\t\t\t</executions>\n\t\t\t</plugin>\n\t\t</plugins>\n\t</build>\n```\npackage.xml内容：\n```\n<assembly>\n    <id>bin</id>\n    <!-- 最终打包成一个用于发布的zip文件 -->\n    <formats>\n        <format>tar.gz</format>\n    </formats>\n\n    <fileSets>\n    \t<!-- 打包jar文件 -->\n        <fileSet>\n            <directory>${project.build.directory}</directory>\n            <outputDirectory></outputDirectory>\n            <includes>\n                <include>*.jar</include>\n            </includes>\n        </fileSet>\n        <!-- 把项目相关的启动脚本，打包进zip文件的bin目录 -->\n        <fileSet>\n            <directory>${project.basedir}/src/main/bash</directory>\n            <outputDirectory>/</outputDirectory>\n            <includes>\n                <include>*</include>\n            </includes>\n        </fileSet>\n        \n        <!-- 把项目的配置文件，打包进zip文件的config目录 -->\n        <fileSet>\n            <directory>${project.build.directory}/classes</directory>\n            <outputDirectory>conf</outputDirectory>\n            <includes>\n                <include>*.properties</include>\n                <include>*.xml</include>\n            </includes>\n        </fileSet>\n    </fileSets>\n    <!-- 把项目的依赖的jar打包到lib目录下 -->\n     <dependencySets>  \n        <dependencySet>  \n            <outputDirectory>lib</outputDirectory>  \n            <scope>runtime</scope>  \n            <excludes>  \n                <exclude>${groupId}:${artifactId}</exclude>  \n            </excludes>  \n        </dependencySet>  \n    </dependencySets>  \n</assembly>\n```\nmaven-jar-plugin只是讲代码打成一个jar,而对部署包的构建是由assembly插件完成的。\n\n结合启动、停止脚本即可高效便捷的部署一个项目。\n\nstart.sh:\n```\n#!/bin/sh\nexport JAVA_HOME=$JAVA_HOME\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nPIDFILE=service.pid\nROOT_DIR=\"$(cd $(dirname $0) && pwd)\"\nCLASSPATH=./*:$ROOT_DIR/lib/*:$ROOT_DIR/conf/\nJAVA_OPTS=\"-Xms512m -Xmx1024m -XX:+UseParallelGC\"\nMAIN_CLASS=com.aibibang.AppMain\n\n\nif [ ! -d \"logs\" ]; then\n   mkdir logs\nfi\n\nif [ -f \"$PIDFILE\" ]; then\n    echo \"Service is already start ...\"\nelse\n    echo \"Service  start ...\"\n    nohup java $JAVA_OPTS -cp $CLASSPATH $MAIN_CLASS 1> logs/server.out 2>&1  &\n    printf '%d' $! > $PIDFILE\nfi\n\n```\nstop.sh:\n```\n#!/bin/sh\nPIDFILE=service.pid\n\nif [ -f \"$PIDFILE\" ]; then\n     kill -9 `cat $PIDFILE`\n     rm -rf $PIDFILE\nelse\n    echo \"Service is already stop ...\"\nfi\n```","slug":"maven打包案例","published":1,"updated":"2019-08-22T12:49:04.139Z","_id":"cjzmobve300708ceercnsk2yb","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"一、spring-boot-maven-plugin\"><a href=\"#一、spring-boot-maven-plugin\" class=\"headerlink\" title=\"一、spring-boot-maven-plugin\"></a>一、spring-boot-maven-plugin</h2><p>使用案例如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t&lt;executions&gt;</span><br><span class=\"line\">\t\t\t&lt;execution&gt;</span><br><span class=\"line\">\t\t\t\t&lt;goals&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;goal&gt;repackage&lt;/goal&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/goals&gt;</span><br><span class=\"line\">\t\t\t&lt;/execution&gt;</span><br><span class=\"line\">\t\t&lt;/executions&gt;</span><br><span class=\"line\">\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t&lt;mainClass&gt;com.aibibang.AppMain&lt;/mainClass&gt;</span><br><span class=\"line\">\t\t\t&lt;addResources&gt;true&lt;/addResources&gt;</span><br><span class=\"line\">\t\t\t&lt;excludes&gt;</span><br><span class=\"line\">\t\t\t\t&lt;exclude&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/exclude&gt;</span><br><span class=\"line\">\t\t\t&lt;/excludes&gt;</span><br><span class=\"line\">\t\t&lt;/configuration&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、自定义打包\"><a href=\"#二、自定义打包\" class=\"headerlink\" title=\"二、自定义打包\"></a>二、自定义打包</h2><p>在开发过程中经常需要将依赖包与代码包分离，配置文件与代码包分离，这样更便于部署与修改参数。依次案例基于以上场景展开，借助于maven-jar-plugin与maven-assembly-plugin。</p>\n<p>pom内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;</span><br><span class=\"line\">\t\t&lt;plugins&gt;</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;  </span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  </span><br><span class=\"line\">                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;  </span><br><span class=\"line\">                &lt;configuration&gt;  </span><br><span class=\"line\">                    &lt;archive&gt;    </span><br><span class=\"line\">                        &lt;manifest&gt;  </span><br><span class=\"line\">                            &lt;mainClass&gt;com.aibibang.AppMain&lt;/mainClass&gt;  </span><br><span class=\"line\">                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;  </span><br><span class=\"line\">                            &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;  </span><br><span class=\"line\">                        &lt;/manifest&gt;</span><br><span class=\"line\">                    &lt;/archive&gt;  </span><br><span class=\"line\">                    &lt;excludes&gt;  </span><br><span class=\"line\">                       &lt;exclude&gt;*.properties&lt;/exclude&gt;</span><br><span class=\"line\">                       &lt;exclude&gt;*.xml&lt;/exclude&gt;</span><br><span class=\"line\">                    &lt;/excludes&gt;  </span><br><span class=\"line\">                &lt;/configuration&gt;  </span><br><span class=\"line\">            &lt;/plugin&gt; </span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;descriptors&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;descriptor&gt;package.xml&lt;/descriptor&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;/descriptors&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/configuration&gt;</span><br><span class=\"line\">\t\t\t\t&lt;executions&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;execution&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;goals&gt;</span><br><span class=\"line\">\t\t\t\t\t\t\t&lt;goal&gt;single&lt;/goal&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;/goals&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;/execution&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/executions&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t\t&lt;/plugins&gt;</span><br><span class=\"line\">\t&lt;/build&gt;</span><br></pre></td></tr></table></figure></p>\n<p>package.xml内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;assembly&gt;</span><br><span class=\"line\">    &lt;id&gt;bin&lt;/id&gt;</span><br><span class=\"line\">    &lt;!-- 最终打包成一个用于发布的zip文件 --&gt;</span><br><span class=\"line\">    &lt;formats&gt;</span><br><span class=\"line\">        &lt;format&gt;tar.gz&lt;/format&gt;</span><br><span class=\"line\">    &lt;/formats&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;fileSets&gt;</span><br><span class=\"line\">    \t&lt;!-- 打包jar文件 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*.jar&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">        &lt;!-- 把项目相关的启动脚本，打包进zip文件的bin目录 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.basedir&#125;/src/main/bash&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &lt;!-- 把项目的配置文件，打包进zip文件的config目录 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.build.directory&#125;/classes&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;conf&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*.properties&lt;/include&gt;</span><br><span class=\"line\">                &lt;include&gt;*.xml&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">    &lt;/fileSets&gt;</span><br><span class=\"line\">    &lt;!-- 把项目的依赖的jar打包到lib目录下 --&gt;</span><br><span class=\"line\">     &lt;dependencySets&gt;  </span><br><span class=\"line\">        &lt;dependencySet&gt;  </span><br><span class=\"line\">            &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt;  </span><br><span class=\"line\">            &lt;scope&gt;runtime&lt;/scope&gt;  </span><br><span class=\"line\">            &lt;excludes&gt;  </span><br><span class=\"line\">                &lt;exclude&gt;$&#123;groupId&#125;:$&#123;artifactId&#125;&lt;/exclude&gt;  </span><br><span class=\"line\">            &lt;/excludes&gt;  </span><br><span class=\"line\">        &lt;/dependencySet&gt;  </span><br><span class=\"line\">    &lt;/dependencySets&gt;  </span><br><span class=\"line\">&lt;/assembly&gt;</span><br></pre></td></tr></table></figure></p>\n<p>maven-jar-plugin只是讲代码打成一个jar,而对部署包的构建是由assembly插件完成的。</p>\n<p>结合启动、停止脚本即可高效便捷的部署一个项目。</p>\n<p>start.sh:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">export JAVA_HOME=$JAVA_HOME</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class=\"line\">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class=\"line\">PIDFILE=service.pid</span><br><span class=\"line\">ROOT_DIR=&quot;$(cd $(dirname $0) &amp;&amp; pwd)&quot;</span><br><span class=\"line\">CLASSPATH=./*:$ROOT_DIR/lib/*:$ROOT_DIR/conf/</span><br><span class=\"line\">JAVA_OPTS=&quot;-Xms512m -Xmx1024m -XX:+UseParallelGC&quot;</span><br><span class=\"line\">MAIN_CLASS=com.aibibang.AppMain</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if [ ! -d &quot;logs&quot; ]; then</span><br><span class=\"line\">   mkdir logs</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">if [ -f &quot;$PIDFILE&quot; ]; then</span><br><span class=\"line\">    echo &quot;Service is already start ...&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo &quot;Service  start ...&quot;</span><br><span class=\"line\">    nohup java $JAVA_OPTS -cp $CLASSPATH $MAIN_CLASS 1&gt; logs/server.out 2&gt;&amp;1  &amp;</span><br><span class=\"line\">    printf &apos;%d&apos; $! &gt; $PIDFILE</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n<p>stop.sh:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">PIDFILE=service.pid</span><br><span class=\"line\"></span><br><span class=\"line\">if [ -f &quot;$PIDFILE&quot; ]; then</span><br><span class=\"line\">     kill -9 `cat $PIDFILE`</span><br><span class=\"line\">     rm -rf $PIDFILE</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo &quot;Service is already stop ...&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、spring-boot-maven-plugin\"><a href=\"#一、spring-boot-maven-plugin\" class=\"headerlink\" title=\"一、spring-boot-maven-plugin\"></a>一、spring-boot-maven-plugin</h2><p>使用案例如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;plugin&gt;</span><br><span class=\"line\">\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t&lt;executions&gt;</span><br><span class=\"line\">\t\t\t&lt;execution&gt;</span><br><span class=\"line\">\t\t\t\t&lt;goals&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;goal&gt;repackage&lt;/goal&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/goals&gt;</span><br><span class=\"line\">\t\t\t&lt;/execution&gt;</span><br><span class=\"line\">\t\t&lt;/executions&gt;</span><br><span class=\"line\">\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t&lt;mainClass&gt;com.aibibang.AppMain&lt;/mainClass&gt;</span><br><span class=\"line\">\t\t\t&lt;addResources&gt;true&lt;/addResources&gt;</span><br><span class=\"line\">\t\t\t&lt;excludes&gt;</span><br><span class=\"line\">\t\t\t\t&lt;exclude&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/exclude&gt;</span><br><span class=\"line\">\t\t\t&lt;/excludes&gt;</span><br><span class=\"line\">\t\t&lt;/configuration&gt;</span><br><span class=\"line\">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、自定义打包\"><a href=\"#二、自定义打包\" class=\"headerlink\" title=\"二、自定义打包\"></a>二、自定义打包</h2><p>在开发过程中经常需要将依赖包与代码包分离，配置文件与代码包分离，这样更便于部署与修改参数。依次案例基于以上场景展开，借助于maven-jar-plugin与maven-assembly-plugin。</p>\n<p>pom内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;</span><br><span class=\"line\">\t\t&lt;plugins&gt;</span><br><span class=\"line\">\t\t\t&lt;plugin&gt;  </span><br><span class=\"line\">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  </span><br><span class=\"line\">                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;  </span><br><span class=\"line\">                &lt;configuration&gt;  </span><br><span class=\"line\">                    &lt;archive&gt;    </span><br><span class=\"line\">                        &lt;manifest&gt;  </span><br><span class=\"line\">                            &lt;mainClass&gt;com.aibibang.AppMain&lt;/mainClass&gt;  </span><br><span class=\"line\">                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;  </span><br><span class=\"line\">                            &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;  </span><br><span class=\"line\">                        &lt;/manifest&gt;</span><br><span class=\"line\">                    &lt;/archive&gt;  </span><br><span class=\"line\">                    &lt;excludes&gt;  </span><br><span class=\"line\">                       &lt;exclude&gt;*.properties&lt;/exclude&gt;</span><br><span class=\"line\">                       &lt;exclude&gt;*.xml&lt;/exclude&gt;</span><br><span class=\"line\">                    &lt;/excludes&gt;  </span><br><span class=\"line\">                &lt;/configuration&gt;  </span><br><span class=\"line\">            &lt;/plugin&gt; </span><br><span class=\"line\">\t\t\t&lt;plugin&gt;</span><br><span class=\"line\">\t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">\t\t\t\t&lt;configuration&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;descriptors&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;descriptor&gt;package.xml&lt;/descriptor&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;/descriptors&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/configuration&gt;</span><br><span class=\"line\">\t\t\t\t&lt;executions&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;execution&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;goals&gt;</span><br><span class=\"line\">\t\t\t\t\t\t\t&lt;goal&gt;single&lt;/goal&gt;</span><br><span class=\"line\">\t\t\t\t\t\t&lt;/goals&gt;</span><br><span class=\"line\">\t\t\t\t\t&lt;/execution&gt;</span><br><span class=\"line\">\t\t\t\t&lt;/executions&gt;</span><br><span class=\"line\">\t\t\t&lt;/plugin&gt;</span><br><span class=\"line\">\t\t&lt;/plugins&gt;</span><br><span class=\"line\">\t&lt;/build&gt;</span><br></pre></td></tr></table></figure></p>\n<p>package.xml内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;assembly&gt;</span><br><span class=\"line\">    &lt;id&gt;bin&lt;/id&gt;</span><br><span class=\"line\">    &lt;!-- 最终打包成一个用于发布的zip文件 --&gt;</span><br><span class=\"line\">    &lt;formats&gt;</span><br><span class=\"line\">        &lt;format&gt;tar.gz&lt;/format&gt;</span><br><span class=\"line\">    &lt;/formats&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;fileSets&gt;</span><br><span class=\"line\">    \t&lt;!-- 打包jar文件 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*.jar&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">        &lt;!-- 把项目相关的启动脚本，打包进zip文件的bin目录 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.basedir&#125;/src/main/bash&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">        </span><br><span class=\"line\">        &lt;!-- 把项目的配置文件，打包进zip文件的config目录 --&gt;</span><br><span class=\"line\">        &lt;fileSet&gt;</span><br><span class=\"line\">            &lt;directory&gt;$&#123;project.build.directory&#125;/classes&lt;/directory&gt;</span><br><span class=\"line\">            &lt;outputDirectory&gt;conf&lt;/outputDirectory&gt;</span><br><span class=\"line\">            &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;*.properties&lt;/include&gt;</span><br><span class=\"line\">                &lt;include&gt;*.xml&lt;/include&gt;</span><br><span class=\"line\">            &lt;/includes&gt;</span><br><span class=\"line\">        &lt;/fileSet&gt;</span><br><span class=\"line\">    &lt;/fileSets&gt;</span><br><span class=\"line\">    &lt;!-- 把项目的依赖的jar打包到lib目录下 --&gt;</span><br><span class=\"line\">     &lt;dependencySets&gt;  </span><br><span class=\"line\">        &lt;dependencySet&gt;  </span><br><span class=\"line\">            &lt;outputDirectory&gt;lib&lt;/outputDirectory&gt;  </span><br><span class=\"line\">            &lt;scope&gt;runtime&lt;/scope&gt;  </span><br><span class=\"line\">            &lt;excludes&gt;  </span><br><span class=\"line\">                &lt;exclude&gt;$&#123;groupId&#125;:$&#123;artifactId&#125;&lt;/exclude&gt;  </span><br><span class=\"line\">            &lt;/excludes&gt;  </span><br><span class=\"line\">        &lt;/dependencySet&gt;  </span><br><span class=\"line\">    &lt;/dependencySets&gt;  </span><br><span class=\"line\">&lt;/assembly&gt;</span><br></pre></td></tr></table></figure></p>\n<p>maven-jar-plugin只是讲代码打成一个jar,而对部署包的构建是由assembly插件完成的。</p>\n<p>结合启动、停止脚本即可高效便捷的部署一个项目。</p>\n<p>start.sh:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">export JAVA_HOME=$JAVA_HOME</span><br><span class=\"line\">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class=\"line\">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class=\"line\">PIDFILE=service.pid</span><br><span class=\"line\">ROOT_DIR=&quot;$(cd $(dirname $0) &amp;&amp; pwd)&quot;</span><br><span class=\"line\">CLASSPATH=./*:$ROOT_DIR/lib/*:$ROOT_DIR/conf/</span><br><span class=\"line\">JAVA_OPTS=&quot;-Xms512m -Xmx1024m -XX:+UseParallelGC&quot;</span><br><span class=\"line\">MAIN_CLASS=com.aibibang.AppMain</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">if [ ! -d &quot;logs&quot; ]; then</span><br><span class=\"line\">   mkdir logs</span><br><span class=\"line\">fi</span><br><span class=\"line\"></span><br><span class=\"line\">if [ -f &quot;$PIDFILE&quot; ]; then</span><br><span class=\"line\">    echo &quot;Service is already start ...&quot;</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo &quot;Service  start ...&quot;</span><br><span class=\"line\">    nohup java $JAVA_OPTS -cp $CLASSPATH $MAIN_CLASS 1&gt; logs/server.out 2&gt;&amp;1  &amp;</span><br><span class=\"line\">    printf &apos;%d&apos; $! &gt; $PIDFILE</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n<p>stop.sh:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/sh</span><br><span class=\"line\">PIDFILE=service.pid</span><br><span class=\"line\"></span><br><span class=\"line\">if [ -f &quot;$PIDFILE&quot; ]; then</span><br><span class=\"line\">     kill -9 `cat $PIDFILE`</span><br><span class=\"line\">     rm -rf $PIDFILE</span><br><span class=\"line\">else</span><br><span class=\"line\">    echo &quot;Service is already stop ...&quot;</span><br><span class=\"line\">fi</span><br></pre></td></tr></table></figure></p>\n"},{"title":"metricbeat 新增kafka metrices 教程","date":"2018-10-15T09:36:56.000Z","_content":"# 项目背景\n本次教程是编写metrices,开发moduel 基本差不多，可以参考[creating-metricbeat-module](creating-metricbeat-module)。\n\n本次教程是新增kafka metrices ，增加filesize metrices,实现的功能是根据配置的kafka 数据文件目录，获取所有topic，不同patition 数据文件大小，将该数据收集到elasticsearch中，通过kibana 根据不同粒度监控kafka集群。\n# 正文\n## beats架构\n![image](https://www.elastic.co/guide/en/beats/devguide/current/images/beat_overview.png)\n## 项目生成\n```\ncd metricbeat\nmake create-metricset\n```\n根据提示输入相应的内容，然后生成field.yml文件(make update),编辑metricbeat.yml文件 ，编译然后运行即可。。\n## 配置写入字段类型及文件\ncd filesize\n\n1. 编辑fields.yml\n```\n- name: filesize\n  type: group\n  description: >\n    filesize\n  fields:\n    - name: topic\n      type: keyword\n      description: >\n        topic\n    - name: partition\n      type: long\n      description: >\n        partition\n    - name: filesize\n      type: long\n      description: >\n        topic data file size       \n\n```\n2. 编辑docs.asciidoc\n\n 略\n\n## 读取配置\n```\ntype MetricSet struct {\n    mb.BaseMetricSet\n\tdataPath string\n}\n\n// New creates a new instance of the MetricSet. New is responsible for unpacking\n// any MetricSet specific configuration options if there are any.\nfunc New(base mb.BaseMetricSet) (mb.MetricSet, error) {\n      // Unpack additional configuration options.\n\t  config := struct {\n\t\tDataPath string `config:\"dataPath\"`\n\t\t}{\n\t\t\tDataPath:\"\",\n\t\t}\n\n        err := base.Module().UnpackConfig(&config)\n\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t}\n\treturn &MetricSet{\n\t\tBaseMetricSet: base,\n\t\tdataPath: config.DataPath,\n\t}, nil\n}\n```\n\n## 指标采集\n```\nfunc (m *MetricSet) Fetch(report mb.ReporterV2) {\n    PthSep := string(os.PathSeparator)\n\tvar dataPath = m.dataPath\n    files, _ := ioutil.ReadDir(dataPath)\n    for _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n        var path = dataPath+PthSep+f.Name()\n\t\tcfiles,_ := ioutil.ReadDir(path)\n\t\tvar filesize = f.Size()\n\n\t\tfor _, cf := range cfiles {\n            filesize = filesize + cf.Size()\n\t\t}\n        \n\t\tvar name = f.Name();\n\t\tvar index = strings.LastIndex(name,\"-\")\n\t\tif index <0 {\n\t\t\tcontinue\n\t\t}\n\t\tvar topic = name[0:index]\n\t\tvar partition = name[index+1:len(name)]\n\t\tdebugf(\"topic:%v\",f.Name())\n\t\treport.Event(mb.Event{\n\t\t\tMetricSetFields: common.MapStr{\n\t\t\t\t\"topic\": topic,\n\t\t\t\t\"partition\": partition,\n                \"filesize\": filesize,\n\t\t\t},\n\t\t})\n    }\n}\n```\n## 完整代码\n```\npackage filesize\n\nimport (\n\t\"github.com/elastic/beats/libbeat/common\"\n\t\"github.com/elastic/beats/libbeat/common/cfgwarn\"\n\t\"github.com/elastic/beats/metricbeat/mb\"\n\t\"io/ioutil\"\n\t\"strings\"\n\t\"os\"\n\t\"github.com/elastic/beats/libbeat/logp\"\n)\n\n// init registers the MetricSet with the central registry as soon as the program\n// starts. The New function will be called later to instantiate an instance of\n// the MetricSet for each host defined in the module's configuration. After the\n// MetricSet has been created then Fetch will begin to be called periodically.\nfunc init() {\n\tmb.Registry.MustAddMetricSet(\"kafka\", \"filesize\", New)\n}\n\nvar debugf = logp.MakeDebug(\"kafka\")\n// MetricSet holds any configuration or state information. It must implement\n// the mb.MetricSet interface. And this is best achieved by embedding\n// mb.BaseMetricSet because it implements all of the required mb.MetricSet\n// interface methods except for Fetch.\ntype MetricSet struct {\n    mb.BaseMetricSet\n\tdataPath string\n}\n\n// New creates a new instance of the MetricSet. New is responsible for unpacking\n// any MetricSet specific configuration options if there are any.\nfunc New(base mb.BaseMetricSet) (mb.MetricSet, error) {\n      // Unpack additional configuration options.\n\t  config := struct {\n\t\tDataPath string `config:\"dataPath\"`\n\t\t}{\n\t\t\tDataPath:\"\",\n\t\t}\n\n        err := base.Module().UnpackConfig(&config)\n\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t}\n\treturn &MetricSet{\n\t\tBaseMetricSet: base,\n\t\tdataPath: config.DataPath,\n\t}, nil\n}\n\n// Fetch methods implements the data gathering and data conversion to the right\n// format. It publishes the event which is then forwarded to the output. In case\n// of an error set the Error field of mb.Event or simply call report.Error().\nfunc (m *MetricSet) Fetch(report mb.ReporterV2) {\n    PthSep := string(os.PathSeparator)\n\tvar dataPath = m.dataPath\n    files, _ := ioutil.ReadDir(dataPath)\n    for _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n        var path = dataPath+PthSep+f.Name()\n\t\tcfiles,_ := ioutil.ReadDir(path)\n\t\tvar filesize = f.Size()\n\n\t\tfor _, cf := range cfiles {\n            filesize = filesize + cf.Size()\n\t\t}\n        \n\t\tvar name = f.Name();\n\t\tvar index = strings.LastIndex(name,\"-\")\n\t\tif index <0 {\n\t\t\tcontinue\n\t\t}\n\t\tvar topic = name[0:index]\n\t\tvar partition = name[index+1:len(name)]\n\t\tdebugf(\"topic:%v\",f.Name())\n\t\treport.Event(mb.Event{\n\t\t\tMetricSetFields: common.MapStr{\n\t\t\t\t\"topic\": topic,\n\t\t\t\t\"partition\": partition,\n                \"filesize\": filesize,\n\t\t\t},\n\t\t})\n    }\n}\n\n```\n## 运行\n1. 编译\n   ```\n   make collect\n   make\n   ```\n2. 运行\n   ```\n   ./{beat} -e -d \"*\"\n   ```\n   <code>* </code>代表选择输出的debug 日志，例如<code>./metricset -e -d \"kafka\"</code> 输出kafka moduel 相关debug log\n   \n   \n**tip**: 在field.yml有变化的时候，记得先执行<code>make update</code>,该命令会重写metricbeat.yml文件。\n\n# 开发建议\n可以使用如下代码做到debug日志\n```\nvar debugf = logp.MakeDebug(\"kafka\")\ndebugf(\"topic:%v\",f.Name())\n```\n# 参考\n1.[creating-metricsets](https://www.elastic.co/guide/en/beats/devguide/current/creating-metricsets.html)\n","source":"_posts/metricbeat-新增kafka-metrices-教程.md","raw":"---\ntitle: metricbeat 新增kafka metrices 教程\ndate: 2018-10-15 17:36:56\ntags: 教程\ncategories:\n- elasticsearch\n---\n# 项目背景\n本次教程是编写metrices,开发moduel 基本差不多，可以参考[creating-metricbeat-module](creating-metricbeat-module)。\n\n本次教程是新增kafka metrices ，增加filesize metrices,实现的功能是根据配置的kafka 数据文件目录，获取所有topic，不同patition 数据文件大小，将该数据收集到elasticsearch中，通过kibana 根据不同粒度监控kafka集群。\n# 正文\n## beats架构\n![image](https://www.elastic.co/guide/en/beats/devguide/current/images/beat_overview.png)\n## 项目生成\n```\ncd metricbeat\nmake create-metricset\n```\n根据提示输入相应的内容，然后生成field.yml文件(make update),编辑metricbeat.yml文件 ，编译然后运行即可。。\n## 配置写入字段类型及文件\ncd filesize\n\n1. 编辑fields.yml\n```\n- name: filesize\n  type: group\n  description: >\n    filesize\n  fields:\n    - name: topic\n      type: keyword\n      description: >\n        topic\n    - name: partition\n      type: long\n      description: >\n        partition\n    - name: filesize\n      type: long\n      description: >\n        topic data file size       \n\n```\n2. 编辑docs.asciidoc\n\n 略\n\n## 读取配置\n```\ntype MetricSet struct {\n    mb.BaseMetricSet\n\tdataPath string\n}\n\n// New creates a new instance of the MetricSet. New is responsible for unpacking\n// any MetricSet specific configuration options if there are any.\nfunc New(base mb.BaseMetricSet) (mb.MetricSet, error) {\n      // Unpack additional configuration options.\n\t  config := struct {\n\t\tDataPath string `config:\"dataPath\"`\n\t\t}{\n\t\t\tDataPath:\"\",\n\t\t}\n\n        err := base.Module().UnpackConfig(&config)\n\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t}\n\treturn &MetricSet{\n\t\tBaseMetricSet: base,\n\t\tdataPath: config.DataPath,\n\t}, nil\n}\n```\n\n## 指标采集\n```\nfunc (m *MetricSet) Fetch(report mb.ReporterV2) {\n    PthSep := string(os.PathSeparator)\n\tvar dataPath = m.dataPath\n    files, _ := ioutil.ReadDir(dataPath)\n    for _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n        var path = dataPath+PthSep+f.Name()\n\t\tcfiles,_ := ioutil.ReadDir(path)\n\t\tvar filesize = f.Size()\n\n\t\tfor _, cf := range cfiles {\n            filesize = filesize + cf.Size()\n\t\t}\n        \n\t\tvar name = f.Name();\n\t\tvar index = strings.LastIndex(name,\"-\")\n\t\tif index <0 {\n\t\t\tcontinue\n\t\t}\n\t\tvar topic = name[0:index]\n\t\tvar partition = name[index+1:len(name)]\n\t\tdebugf(\"topic:%v\",f.Name())\n\t\treport.Event(mb.Event{\n\t\t\tMetricSetFields: common.MapStr{\n\t\t\t\t\"topic\": topic,\n\t\t\t\t\"partition\": partition,\n                \"filesize\": filesize,\n\t\t\t},\n\t\t})\n    }\n}\n```\n## 完整代码\n```\npackage filesize\n\nimport (\n\t\"github.com/elastic/beats/libbeat/common\"\n\t\"github.com/elastic/beats/libbeat/common/cfgwarn\"\n\t\"github.com/elastic/beats/metricbeat/mb\"\n\t\"io/ioutil\"\n\t\"strings\"\n\t\"os\"\n\t\"github.com/elastic/beats/libbeat/logp\"\n)\n\n// init registers the MetricSet with the central registry as soon as the program\n// starts. The New function will be called later to instantiate an instance of\n// the MetricSet for each host defined in the module's configuration. After the\n// MetricSet has been created then Fetch will begin to be called periodically.\nfunc init() {\n\tmb.Registry.MustAddMetricSet(\"kafka\", \"filesize\", New)\n}\n\nvar debugf = logp.MakeDebug(\"kafka\")\n// MetricSet holds any configuration or state information. It must implement\n// the mb.MetricSet interface. And this is best achieved by embedding\n// mb.BaseMetricSet because it implements all of the required mb.MetricSet\n// interface methods except for Fetch.\ntype MetricSet struct {\n    mb.BaseMetricSet\n\tdataPath string\n}\n\n// New creates a new instance of the MetricSet. New is responsible for unpacking\n// any MetricSet specific configuration options if there are any.\nfunc New(base mb.BaseMetricSet) (mb.MetricSet, error) {\n      // Unpack additional configuration options.\n\t  config := struct {\n\t\tDataPath string `config:\"dataPath\"`\n\t\t}{\n\t\t\tDataPath:\"\",\n\t\t}\n\n        err := base.Module().UnpackConfig(&config)\n\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t}\n\treturn &MetricSet{\n\t\tBaseMetricSet: base,\n\t\tdataPath: config.DataPath,\n\t}, nil\n}\n\n// Fetch methods implements the data gathering and data conversion to the right\n// format. It publishes the event which is then forwarded to the output. In case\n// of an error set the Error field of mb.Event or simply call report.Error().\nfunc (m *MetricSet) Fetch(report mb.ReporterV2) {\n    PthSep := string(os.PathSeparator)\n\tvar dataPath = m.dataPath\n    files, _ := ioutil.ReadDir(dataPath)\n    for _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n        var path = dataPath+PthSep+f.Name()\n\t\tcfiles,_ := ioutil.ReadDir(path)\n\t\tvar filesize = f.Size()\n\n\t\tfor _, cf := range cfiles {\n            filesize = filesize + cf.Size()\n\t\t}\n        \n\t\tvar name = f.Name();\n\t\tvar index = strings.LastIndex(name,\"-\")\n\t\tif index <0 {\n\t\t\tcontinue\n\t\t}\n\t\tvar topic = name[0:index]\n\t\tvar partition = name[index+1:len(name)]\n\t\tdebugf(\"topic:%v\",f.Name())\n\t\treport.Event(mb.Event{\n\t\t\tMetricSetFields: common.MapStr{\n\t\t\t\t\"topic\": topic,\n\t\t\t\t\"partition\": partition,\n                \"filesize\": filesize,\n\t\t\t},\n\t\t})\n    }\n}\n\n```\n## 运行\n1. 编译\n   ```\n   make collect\n   make\n   ```\n2. 运行\n   ```\n   ./{beat} -e -d \"*\"\n   ```\n   <code>* </code>代表选择输出的debug 日志，例如<code>./metricset -e -d \"kafka\"</code> 输出kafka moduel 相关debug log\n   \n   \n**tip**: 在field.yml有变化的时候，记得先执行<code>make update</code>,该命令会重写metricbeat.yml文件。\n\n# 开发建议\n可以使用如下代码做到debug日志\n```\nvar debugf = logp.MakeDebug(\"kafka\")\ndebugf(\"topic:%v\",f.Name())\n```\n# 参考\n1.[creating-metricsets](https://www.elastic.co/guide/en/beats/devguide/current/creating-metricsets.html)\n","slug":"metricbeat-新增kafka-metrices-教程","published":1,"updated":"2018-10-15T09:42:51.549Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobve600738ceeiqkygl73","content":"<h1 id=\"项目背景\"><a href=\"#项目背景\" class=\"headerlink\" title=\"项目背景\"></a>项目背景</h1><p>本次教程是编写metrices,开发moduel 基本差不多，可以参考<a href=\"creating-metricbeat-module\">creating-metricbeat-module</a>。</p>\n<p>本次教程是新增kafka metrices ，增加filesize metrices,实现的功能是根据配置的kafka 数据文件目录，获取所有topic，不同patition 数据文件大小，将该数据收集到elasticsearch中，通过kibana 根据不同粒度监控kafka集群。</p>\n<h1 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h1><h2 id=\"beats架构\"><a href=\"#beats架构\" class=\"headerlink\" title=\"beats架构\"></a>beats架构</h2><p><img src=\"https://www.elastic.co/guide/en/beats/devguide/current/images/beat_overview.png\" alt=\"image\"></p>\n<h2 id=\"项目生成\"><a href=\"#项目生成\" class=\"headerlink\" title=\"项目生成\"></a>项目生成</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd metricbeat</span><br><span class=\"line\">make create-metricset</span><br></pre></td></tr></table></figure>\n<p>根据提示输入相应的内容，然后生成field.yml文件(make update),编辑metricbeat.yml文件 ，编译然后运行即可。。</p>\n<h2 id=\"配置写入字段类型及文件\"><a href=\"#配置写入字段类型及文件\" class=\"headerlink\" title=\"配置写入字段类型及文件\"></a>配置写入字段类型及文件</h2><p>cd filesize</p>\n<ol>\n<li><p>编辑fields.yml</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- name: filesize</span><br><span class=\"line\">  type: group</span><br><span class=\"line\">  description: &gt;</span><br><span class=\"line\">    filesize</span><br><span class=\"line\">  fields:</span><br><span class=\"line\">    - name: topic</span><br><span class=\"line\">      type: keyword</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        topic</span><br><span class=\"line\">    - name: partition</span><br><span class=\"line\">      type: long</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        partition</span><br><span class=\"line\">    - name: filesize</span><br><span class=\"line\">      type: long</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        topic data file size</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>编辑docs.asciidoc</p>\n<p>略</p>\n</li>\n</ol>\n<h2 id=\"读取配置\"><a href=\"#读取配置\" class=\"headerlink\" title=\"读取配置\"></a>读取配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type MetricSet struct &#123;</span><br><span class=\"line\">    mb.BaseMetricSet</span><br><span class=\"line\">\tdataPath string</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// New creates a new instance of the MetricSet. New is responsible for unpacking</span><br><span class=\"line\">// any MetricSet specific configuration options if there are any.</span><br><span class=\"line\">func New(base mb.BaseMetricSet) (mb.MetricSet, error) &#123;</span><br><span class=\"line\">      // Unpack additional configuration options.</span><br><span class=\"line\">\t  config := struct &#123;</span><br><span class=\"line\">\t\tDataPath string `config:&quot;dataPath&quot;`</span><br><span class=\"line\">\t\t&#125;&#123;</span><br><span class=\"line\">\t\t\tDataPath:&quot;&quot;,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        err := base.Module().UnpackConfig(&amp;config)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\treturn &amp;MetricSet&#123;</span><br><span class=\"line\">\t\tBaseMetricSet: base,</span><br><span class=\"line\">\t\tdataPath: config.DataPath,</span><br><span class=\"line\">\t&#125;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"指标采集\"><a href=\"#指标采集\" class=\"headerlink\" title=\"指标采集\"></a>指标采集</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *MetricSet) Fetch(report mb.ReporterV2) &#123;</span><br><span class=\"line\">    PthSep := string(os.PathSeparator)</span><br><span class=\"line\">\tvar dataPath = m.dataPath</span><br><span class=\"line\">    files, _ := ioutil.ReadDir(dataPath)</span><br><span class=\"line\">    for _, f := range files &#123;</span><br><span class=\"line\">\t\tif !f.IsDir() &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        var path = dataPath+PthSep+f.Name()</span><br><span class=\"line\">\t\tcfiles,_ := ioutil.ReadDir(path)</span><br><span class=\"line\">\t\tvar filesize = f.Size()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor _, cf := range cfiles &#123;</span><br><span class=\"line\">            filesize = filesize + cf.Size()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tvar name = f.Name();</span><br><span class=\"line\">\t\tvar index = strings.LastIndex(name,&quot;-&quot;)</span><br><span class=\"line\">\t\tif index &lt;0 &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar topic = name[0:index]</span><br><span class=\"line\">\t\tvar partition = name[index+1:len(name)]</span><br><span class=\"line\">\t\tdebugf(&quot;topic:%v&quot;,f.Name())</span><br><span class=\"line\">\t\treport.Event(mb.Event&#123;</span><br><span class=\"line\">\t\t\tMetricSetFields: common.MapStr&#123;</span><br><span class=\"line\">\t\t\t\t&quot;topic&quot;: topic,</span><br><span class=\"line\">\t\t\t\t&quot;partition&quot;: partition,</span><br><span class=\"line\">                &quot;filesize&quot;: filesize,</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"完整代码\"><a href=\"#完整代码\" class=\"headerlink\" title=\"完整代码\"></a>完整代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package filesize</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/common&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/common/cfgwarn&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/metricbeat/mb&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;strings&quot;</span><br><span class=\"line\">\t&quot;os&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/logp&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// init registers the MetricSet with the central registry as soon as the program</span><br><span class=\"line\">// starts. The New function will be called later to instantiate an instance of</span><br><span class=\"line\">// the MetricSet for each host defined in the module&apos;s configuration. After the</span><br><span class=\"line\">// MetricSet has been created then Fetch will begin to be called periodically.</span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">\tmb.Registry.MustAddMetricSet(&quot;kafka&quot;, &quot;filesize&quot;, New)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">var debugf = logp.MakeDebug(&quot;kafka&quot;)</span><br><span class=\"line\">// MetricSet holds any configuration or state information. It must implement</span><br><span class=\"line\">// the mb.MetricSet interface. And this is best achieved by embedding</span><br><span class=\"line\">// mb.BaseMetricSet because it implements all of the required mb.MetricSet</span><br><span class=\"line\">// interface methods except for Fetch.</span><br><span class=\"line\">type MetricSet struct &#123;</span><br><span class=\"line\">    mb.BaseMetricSet</span><br><span class=\"line\">\tdataPath string</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// New creates a new instance of the MetricSet. New is responsible for unpacking</span><br><span class=\"line\">// any MetricSet specific configuration options if there are any.</span><br><span class=\"line\">func New(base mb.BaseMetricSet) (mb.MetricSet, error) &#123;</span><br><span class=\"line\">      // Unpack additional configuration options.</span><br><span class=\"line\">\t  config := struct &#123;</span><br><span class=\"line\">\t\tDataPath string `config:&quot;dataPath&quot;`</span><br><span class=\"line\">\t\t&#125;&#123;</span><br><span class=\"line\">\t\t\tDataPath:&quot;&quot;,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        err := base.Module().UnpackConfig(&amp;config)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\treturn &amp;MetricSet&#123;</span><br><span class=\"line\">\t\tBaseMetricSet: base,</span><br><span class=\"line\">\t\tdataPath: config.DataPath,</span><br><span class=\"line\">\t&#125;, nil</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Fetch methods implements the data gathering and data conversion to the right</span><br><span class=\"line\">// format. It publishes the event which is then forwarded to the output. In case</span><br><span class=\"line\">// of an error set the Error field of mb.Event or simply call report.Error().</span><br><span class=\"line\">func (m *MetricSet) Fetch(report mb.ReporterV2) &#123;</span><br><span class=\"line\">    PthSep := string(os.PathSeparator)</span><br><span class=\"line\">\tvar dataPath = m.dataPath</span><br><span class=\"line\">    files, _ := ioutil.ReadDir(dataPath)</span><br><span class=\"line\">    for _, f := range files &#123;</span><br><span class=\"line\">\t\tif !f.IsDir() &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        var path = dataPath+PthSep+f.Name()</span><br><span class=\"line\">\t\tcfiles,_ := ioutil.ReadDir(path)</span><br><span class=\"line\">\t\tvar filesize = f.Size()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor _, cf := range cfiles &#123;</span><br><span class=\"line\">            filesize = filesize + cf.Size()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tvar name = f.Name();</span><br><span class=\"line\">\t\tvar index = strings.LastIndex(name,&quot;-&quot;)</span><br><span class=\"line\">\t\tif index &lt;0 &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar topic = name[0:index]</span><br><span class=\"line\">\t\tvar partition = name[index+1:len(name)]</span><br><span class=\"line\">\t\tdebugf(&quot;topic:%v&quot;,f.Name())</span><br><span class=\"line\">\t\treport.Event(mb.Event&#123;</span><br><span class=\"line\">\t\t\tMetricSetFields: common.MapStr&#123;</span><br><span class=\"line\">\t\t\t\t&quot;topic&quot;: topic,</span><br><span class=\"line\">\t\t\t\t&quot;partition&quot;: partition,</span><br><span class=\"line\">                &quot;filesize&quot;: filesize,</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h2><ol>\n<li><p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make collect</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./&#123;beat&#125; -e -d &quot;*&quot;</span><br></pre></td></tr></table></figure>\n<p><code>* </code>代表选择输出的debug 日志，例如<code>./metricset -e -d “kafka”</code> 输出kafka moduel 相关debug log</p>\n</li>\n</ol>\n<p><strong>tip</strong>: 在field.yml有变化的时候，记得先执行<code>make update</code>,该命令会重写metricbeat.yml文件。</p>\n<h1 id=\"开发建议\"><a href=\"#开发建议\" class=\"headerlink\" title=\"开发建议\"></a>开发建议</h1><p>可以使用如下代码做到debug日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var debugf = logp.MakeDebug(&quot;kafka&quot;)</span><br><span class=\"line\">debugf(&quot;topic:%v&quot;,f.Name())</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>1.<a href=\"https://www.elastic.co/guide/en/beats/devguide/current/creating-metricsets.html\" target=\"_blank\" rel=\"noopener\">creating-metricsets</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"项目背景\"><a href=\"#项目背景\" class=\"headerlink\" title=\"项目背景\"></a>项目背景</h1><p>本次教程是编写metrices,开发moduel 基本差不多，可以参考<a href=\"creating-metricbeat-module\">creating-metricbeat-module</a>。</p>\n<p>本次教程是新增kafka metrices ，增加filesize metrices,实现的功能是根据配置的kafka 数据文件目录，获取所有topic，不同patition 数据文件大小，将该数据收集到elasticsearch中，通过kibana 根据不同粒度监控kafka集群。</p>\n<h1 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h1><h2 id=\"beats架构\"><a href=\"#beats架构\" class=\"headerlink\" title=\"beats架构\"></a>beats架构</h2><p><img src=\"https://www.elastic.co/guide/en/beats/devguide/current/images/beat_overview.png\" alt=\"image\"></p>\n<h2 id=\"项目生成\"><a href=\"#项目生成\" class=\"headerlink\" title=\"项目生成\"></a>项目生成</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd metricbeat</span><br><span class=\"line\">make create-metricset</span><br></pre></td></tr></table></figure>\n<p>根据提示输入相应的内容，然后生成field.yml文件(make update),编辑metricbeat.yml文件 ，编译然后运行即可。。</p>\n<h2 id=\"配置写入字段类型及文件\"><a href=\"#配置写入字段类型及文件\" class=\"headerlink\" title=\"配置写入字段类型及文件\"></a>配置写入字段类型及文件</h2><p>cd filesize</p>\n<ol>\n<li><p>编辑fields.yml</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- name: filesize</span><br><span class=\"line\">  type: group</span><br><span class=\"line\">  description: &gt;</span><br><span class=\"line\">    filesize</span><br><span class=\"line\">  fields:</span><br><span class=\"line\">    - name: topic</span><br><span class=\"line\">      type: keyword</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        topic</span><br><span class=\"line\">    - name: partition</span><br><span class=\"line\">      type: long</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        partition</span><br><span class=\"line\">    - name: filesize</span><br><span class=\"line\">      type: long</span><br><span class=\"line\">      description: &gt;</span><br><span class=\"line\">        topic data file size</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>编辑docs.asciidoc</p>\n<p>略</p>\n</li>\n</ol>\n<h2 id=\"读取配置\"><a href=\"#读取配置\" class=\"headerlink\" title=\"读取配置\"></a>读取配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type MetricSet struct &#123;</span><br><span class=\"line\">    mb.BaseMetricSet</span><br><span class=\"line\">\tdataPath string</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// New creates a new instance of the MetricSet. New is responsible for unpacking</span><br><span class=\"line\">// any MetricSet specific configuration options if there are any.</span><br><span class=\"line\">func New(base mb.BaseMetricSet) (mb.MetricSet, error) &#123;</span><br><span class=\"line\">      // Unpack additional configuration options.</span><br><span class=\"line\">\t  config := struct &#123;</span><br><span class=\"line\">\t\tDataPath string `config:&quot;dataPath&quot;`</span><br><span class=\"line\">\t\t&#125;&#123;</span><br><span class=\"line\">\t\t\tDataPath:&quot;&quot;,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        err := base.Module().UnpackConfig(&amp;config)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\treturn &amp;MetricSet&#123;</span><br><span class=\"line\">\t\tBaseMetricSet: base,</span><br><span class=\"line\">\t\tdataPath: config.DataPath,</span><br><span class=\"line\">\t&#125;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"指标采集\"><a href=\"#指标采集\" class=\"headerlink\" title=\"指标采集\"></a>指标采集</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *MetricSet) Fetch(report mb.ReporterV2) &#123;</span><br><span class=\"line\">    PthSep := string(os.PathSeparator)</span><br><span class=\"line\">\tvar dataPath = m.dataPath</span><br><span class=\"line\">    files, _ := ioutil.ReadDir(dataPath)</span><br><span class=\"line\">    for _, f := range files &#123;</span><br><span class=\"line\">\t\tif !f.IsDir() &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        var path = dataPath+PthSep+f.Name()</span><br><span class=\"line\">\t\tcfiles,_ := ioutil.ReadDir(path)</span><br><span class=\"line\">\t\tvar filesize = f.Size()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor _, cf := range cfiles &#123;</span><br><span class=\"line\">            filesize = filesize + cf.Size()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tvar name = f.Name();</span><br><span class=\"line\">\t\tvar index = strings.LastIndex(name,&quot;-&quot;)</span><br><span class=\"line\">\t\tif index &lt;0 &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar topic = name[0:index]</span><br><span class=\"line\">\t\tvar partition = name[index+1:len(name)]</span><br><span class=\"line\">\t\tdebugf(&quot;topic:%v&quot;,f.Name())</span><br><span class=\"line\">\t\treport.Event(mb.Event&#123;</span><br><span class=\"line\">\t\t\tMetricSetFields: common.MapStr&#123;</span><br><span class=\"line\">\t\t\t\t&quot;topic&quot;: topic,</span><br><span class=\"line\">\t\t\t\t&quot;partition&quot;: partition,</span><br><span class=\"line\">                &quot;filesize&quot;: filesize,</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"完整代码\"><a href=\"#完整代码\" class=\"headerlink\" title=\"完整代码\"></a>完整代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package filesize</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/common&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/common/cfgwarn&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/metricbeat/mb&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;strings&quot;</span><br><span class=\"line\">\t&quot;os&quot;</span><br><span class=\"line\">\t&quot;github.com/elastic/beats/libbeat/logp&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// init registers the MetricSet with the central registry as soon as the program</span><br><span class=\"line\">// starts. The New function will be called later to instantiate an instance of</span><br><span class=\"line\">// the MetricSet for each host defined in the module&apos;s configuration. After the</span><br><span class=\"line\">// MetricSet has been created then Fetch will begin to be called periodically.</span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">\tmb.Registry.MustAddMetricSet(&quot;kafka&quot;, &quot;filesize&quot;, New)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">var debugf = logp.MakeDebug(&quot;kafka&quot;)</span><br><span class=\"line\">// MetricSet holds any configuration or state information. It must implement</span><br><span class=\"line\">// the mb.MetricSet interface. And this is best achieved by embedding</span><br><span class=\"line\">// mb.BaseMetricSet because it implements all of the required mb.MetricSet</span><br><span class=\"line\">// interface methods except for Fetch.</span><br><span class=\"line\">type MetricSet struct &#123;</span><br><span class=\"line\">    mb.BaseMetricSet</span><br><span class=\"line\">\tdataPath string</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// New creates a new instance of the MetricSet. New is responsible for unpacking</span><br><span class=\"line\">// any MetricSet specific configuration options if there are any.</span><br><span class=\"line\">func New(base mb.BaseMetricSet) (mb.MetricSet, error) &#123;</span><br><span class=\"line\">      // Unpack additional configuration options.</span><br><span class=\"line\">\t  config := struct &#123;</span><br><span class=\"line\">\t\tDataPath string `config:&quot;dataPath&quot;`</span><br><span class=\"line\">\t\t&#125;&#123;</span><br><span class=\"line\">\t\t\tDataPath:&quot;&quot;,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        err := base.Module().UnpackConfig(&amp;config)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\treturn &amp;MetricSet&#123;</span><br><span class=\"line\">\t\tBaseMetricSet: base,</span><br><span class=\"line\">\t\tdataPath: config.DataPath,</span><br><span class=\"line\">\t&#125;, nil</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Fetch methods implements the data gathering and data conversion to the right</span><br><span class=\"line\">// format. It publishes the event which is then forwarded to the output. In case</span><br><span class=\"line\">// of an error set the Error field of mb.Event or simply call report.Error().</span><br><span class=\"line\">func (m *MetricSet) Fetch(report mb.ReporterV2) &#123;</span><br><span class=\"line\">    PthSep := string(os.PathSeparator)</span><br><span class=\"line\">\tvar dataPath = m.dataPath</span><br><span class=\"line\">    files, _ := ioutil.ReadDir(dataPath)</span><br><span class=\"line\">    for _, f := range files &#123;</span><br><span class=\"line\">\t\tif !f.IsDir() &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        var path = dataPath+PthSep+f.Name()</span><br><span class=\"line\">\t\tcfiles,_ := ioutil.ReadDir(path)</span><br><span class=\"line\">\t\tvar filesize = f.Size()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor _, cf := range cfiles &#123;</span><br><span class=\"line\">            filesize = filesize + cf.Size()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tvar name = f.Name();</span><br><span class=\"line\">\t\tvar index = strings.LastIndex(name,&quot;-&quot;)</span><br><span class=\"line\">\t\tif index &lt;0 &#123;</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar topic = name[0:index]</span><br><span class=\"line\">\t\tvar partition = name[index+1:len(name)]</span><br><span class=\"line\">\t\tdebugf(&quot;topic:%v&quot;,f.Name())</span><br><span class=\"line\">\t\treport.Event(mb.Event&#123;</span><br><span class=\"line\">\t\t\tMetricSetFields: common.MapStr&#123;</span><br><span class=\"line\">\t\t\t\t&quot;topic&quot;: topic,</span><br><span class=\"line\">\t\t\t\t&quot;partition&quot;: partition,</span><br><span class=\"line\">                &quot;filesize&quot;: filesize,</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h2><ol>\n<li><p>编译</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make collect</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>运行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./&#123;beat&#125; -e -d &quot;*&quot;</span><br></pre></td></tr></table></figure>\n<p><code>* </code>代表选择输出的debug 日志，例如<code>./metricset -e -d “kafka”</code> 输出kafka moduel 相关debug log</p>\n</li>\n</ol>\n<p><strong>tip</strong>: 在field.yml有变化的时候，记得先执行<code>make update</code>,该命令会重写metricbeat.yml文件。</p>\n<h1 id=\"开发建议\"><a href=\"#开发建议\" class=\"headerlink\" title=\"开发建议\"></a>开发建议</h1><p>可以使用如下代码做到debug日志<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var debugf = logp.MakeDebug(&quot;kafka&quot;)</span><br><span class=\"line\">debugf(&quot;topic:%v&quot;,f.Name())</span><br></pre></td></tr></table></figure></p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>1.<a href=\"https://www.elastic.co/guide/en/beats/devguide/current/creating-metricsets.html\" target=\"_blank\" rel=\"noopener\">creating-metricsets</a></p>\n"},{"title":"nginx 教程之 nginx 配置学习","date":"2018-01-15T11:26:39.000Z","_content":"# nginx 教程之 nginx 配置学习\n\n## 1.location配置\n\n语法规则： \n```\nlocation [=|~|~*|^~] /uri/ { … }\n\n= 表示精确匹配,这个优先级也是最高的\n\n~  表示区分大小写的正则匹配\n\n~* 表示不区分大小写的正则匹配(和上面的唯一区别就是大小写)\n\n^~ 表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。\n\n!~和!~* 分别为区分大小写不匹配及不区分大小写不匹配的正则\n\n/ 通用匹配，任何请求都会匹配到，默认匹配.\n```\n\n优先级=>^~>\n\n首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求\n\n\n一般线上的配置\n```\nlocation ~* .*\\.(js|css)?$\n{\n        expires 7d; //7天过期,后续讲解\n        access_log off; //不保存日志\n}\n \nlocation ~* .*\\.(png|jpg|gif|jpeg|bmp|ico)?$\n{        \n        expires 7d;\n        access_log off;\n}\n```\n\n## 2.nginx 逻辑运算\n\nnginx的配置中不支持if条件的逻辑与&& 逻辑或|| 运算 ，而且不支持if的嵌套语法，否则会报下面的错误：nginx: [emerg] invalid condition。\n我们可以用变量的方式来间接实现。\n要实现的语句：\n```\nif ($arg_unitid = 42012 && $uri ~/thumb/){\n echo \"www.baidu.com\";\n}\n```\n可以这么来实现，如下所示：\n```\nset $flag 0;\nif ($uri ~ ^/thumb/[0-9]+_160.jpg$){\n set $flag \"${flag}1\";\n}\nif ($arg_unitid = 42012){\n set $flag \"${flag}1\";\n}\nif ($flag = \"011\"){\n echo \"www.baidu.com\";\n}\n```\n\n## 3.ngx_http_core_module模块提供的变量\n\n参数名称 注释\n```\n$arg_PARAMETER HTTP 请求中某个参数的值，如/index.php?site=www.ttlsa.com，可以用$arg_site取得www.ttlsa.com这个值.\n$args HTTP 请求中的完整参数。例如，在请求/index.php?width=400&height=200 中，$args表示字符串width=400&height=200.\n$binary_remote_addr 二进制格式的客户端地址。例如：\\x0A\\xE0B\\x0E\n$body_bytes_sent 表示在向客户端发送的http响应中，包体部分的字节数\n$content_length 表示客户端请求头部中的Content-Length 字段\n$content_type 表示客户端请求头部中的Content-Type 字段\n$cookie_COOKIE 表示在客户端请求头部中的cookie 字段\n$document_root 表示当前请求所使用的root 配置项的值\n$uri 表示当前请求的URI，不带任何参数\n$document_uri 与$uri 含义相同\n$request_uri 表示客户端发来的原始请求URI，带完整的参数。$uri和$document_uri未必是用户的原始请求，在内部重定向后可能是重定向后的URI，而$request_uri 永远不会改变，始终是客户端的原始URI.\n$host 表示客户端请求头部中的Host字段。如果Host字段不存在，则以实际处理的server（虚拟主机）名称代替。如果Host字段中带有端口，如IP:PORT，那么$host是去掉端口的，它的值为IP。$host 是全小写的。这些特性与http_HEADER中的http_host不同，http_host只取出Host头部对应的值。 \n$hostname 表示 Nginx所在机器的名称，与 gethostbyname调用返回的值相同 \n$http_HEADER 表示当前 HTTP请求中相应头部的值。HEADER名称全小写。例如，示请求中 Host头部对应的值 用 $http_host表 \n$sent_http_HEADER 表示返回客户端的 HTTP响应中相应头部的值。HEADER名称全小写。例如，用 $sent_ http_content_type表示响应中 Content-Type头部对应的值 \n$is_args 表示请求中的 URI是否带参数，如果带参数，$is_args值为 ?，如果不带参数，则是空字符串 \n$limit_rate 表示当前连接的限速是多少，0表示无限速 \n$nginx_version 表示当前 Nginx的版本号 \n$query_string 请求 URI中的参数，与 $args相同，然而 $query_string是只读的不会改变 \n$remote_addr 表示客户端的地址 \n$remote_port 表示客户端连接使用的端口 \n$remote_user 表示使用 Auth Basic Module时定义的用户名 \n$request_filename 表示用户请求中的 URI经过 root或 alias转换后的文件路径 \n$request_body 表示 HTTP请求中的包体，该参数只在 proxy_pass或 fastcgi_pass中有意义 \n$request_body_file 表示 HTTP请求中的包体存储的临时文件名 \n$request_completion 当请求已经全部完成时，其值为 “ok”。若没有完成，就要返回客户端，则其值为空字符串；或者在断点续传等情况下使用 HTTP range访问的并不是文件的最后一块，那么其值也是空字符串。\n$request_method 表示 HTTP请求的方法名，如 GET、PUT、POST等 \n$scheme 表示 HTTP scheme，如在请求 https://nginx.com/中表示 https \n$server_addr 表示服务器地址 \n$server_name 表示服务器名称 \n$server_port 表示服务器端口 \n$server_protocol 表示服务器向客户端发送响应的协议，如 HTTP/1.1或 HTTP/1.0\n```","source":"_posts/nginx-教程之-nginx-配置学习.md","raw":"---\ntitle: nginx 教程之 nginx 配置学习\ndate: 2018-01-15 19:26:39\ntags: 开发笔记\ncategories:\n- nginx\n---\n# nginx 教程之 nginx 配置学习\n\n## 1.location配置\n\n语法规则： \n```\nlocation [=|~|~*|^~] /uri/ { … }\n\n= 表示精确匹配,这个优先级也是最高的\n\n~  表示区分大小写的正则匹配\n\n~* 表示不区分大小写的正则匹配(和上面的唯一区别就是大小写)\n\n^~ 表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。\n\n!~和!~* 分别为区分大小写不匹配及不区分大小写不匹配的正则\n\n/ 通用匹配，任何请求都会匹配到，默认匹配.\n```\n\n优先级=>^~>\n\n首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求\n\n\n一般线上的配置\n```\nlocation ~* .*\\.(js|css)?$\n{\n        expires 7d; //7天过期,后续讲解\n        access_log off; //不保存日志\n}\n \nlocation ~* .*\\.(png|jpg|gif|jpeg|bmp|ico)?$\n{        \n        expires 7d;\n        access_log off;\n}\n```\n\n## 2.nginx 逻辑运算\n\nnginx的配置中不支持if条件的逻辑与&& 逻辑或|| 运算 ，而且不支持if的嵌套语法，否则会报下面的错误：nginx: [emerg] invalid condition。\n我们可以用变量的方式来间接实现。\n要实现的语句：\n```\nif ($arg_unitid = 42012 && $uri ~/thumb/){\n echo \"www.baidu.com\";\n}\n```\n可以这么来实现，如下所示：\n```\nset $flag 0;\nif ($uri ~ ^/thumb/[0-9]+_160.jpg$){\n set $flag \"${flag}1\";\n}\nif ($arg_unitid = 42012){\n set $flag \"${flag}1\";\n}\nif ($flag = \"011\"){\n echo \"www.baidu.com\";\n}\n```\n\n## 3.ngx_http_core_module模块提供的变量\n\n参数名称 注释\n```\n$arg_PARAMETER HTTP 请求中某个参数的值，如/index.php?site=www.ttlsa.com，可以用$arg_site取得www.ttlsa.com这个值.\n$args HTTP 请求中的完整参数。例如，在请求/index.php?width=400&height=200 中，$args表示字符串width=400&height=200.\n$binary_remote_addr 二进制格式的客户端地址。例如：\\x0A\\xE0B\\x0E\n$body_bytes_sent 表示在向客户端发送的http响应中，包体部分的字节数\n$content_length 表示客户端请求头部中的Content-Length 字段\n$content_type 表示客户端请求头部中的Content-Type 字段\n$cookie_COOKIE 表示在客户端请求头部中的cookie 字段\n$document_root 表示当前请求所使用的root 配置项的值\n$uri 表示当前请求的URI，不带任何参数\n$document_uri 与$uri 含义相同\n$request_uri 表示客户端发来的原始请求URI，带完整的参数。$uri和$document_uri未必是用户的原始请求，在内部重定向后可能是重定向后的URI，而$request_uri 永远不会改变，始终是客户端的原始URI.\n$host 表示客户端请求头部中的Host字段。如果Host字段不存在，则以实际处理的server（虚拟主机）名称代替。如果Host字段中带有端口，如IP:PORT，那么$host是去掉端口的，它的值为IP。$host 是全小写的。这些特性与http_HEADER中的http_host不同，http_host只取出Host头部对应的值。 \n$hostname 表示 Nginx所在机器的名称，与 gethostbyname调用返回的值相同 \n$http_HEADER 表示当前 HTTP请求中相应头部的值。HEADER名称全小写。例如，示请求中 Host头部对应的值 用 $http_host表 \n$sent_http_HEADER 表示返回客户端的 HTTP响应中相应头部的值。HEADER名称全小写。例如，用 $sent_ http_content_type表示响应中 Content-Type头部对应的值 \n$is_args 表示请求中的 URI是否带参数，如果带参数，$is_args值为 ?，如果不带参数，则是空字符串 \n$limit_rate 表示当前连接的限速是多少，0表示无限速 \n$nginx_version 表示当前 Nginx的版本号 \n$query_string 请求 URI中的参数，与 $args相同，然而 $query_string是只读的不会改变 \n$remote_addr 表示客户端的地址 \n$remote_port 表示客户端连接使用的端口 \n$remote_user 表示使用 Auth Basic Module时定义的用户名 \n$request_filename 表示用户请求中的 URI经过 root或 alias转换后的文件路径 \n$request_body 表示 HTTP请求中的包体，该参数只在 proxy_pass或 fastcgi_pass中有意义 \n$request_body_file 表示 HTTP请求中的包体存储的临时文件名 \n$request_completion 当请求已经全部完成时，其值为 “ok”。若没有完成，就要返回客户端，则其值为空字符串；或者在断点续传等情况下使用 HTTP range访问的并不是文件的最后一块，那么其值也是空字符串。\n$request_method 表示 HTTP请求的方法名，如 GET、PUT、POST等 \n$scheme 表示 HTTP scheme，如在请求 https://nginx.com/中表示 https \n$server_addr 表示服务器地址 \n$server_name 表示服务器名称 \n$server_port 表示服务器端口 \n$server_protocol 表示服务器向客户端发送响应的协议，如 HTTP/1.1或 HTTP/1.0\n```","slug":"nginx-教程之-nginx-配置学习","published":1,"updated":"2018-01-15T11:28:20.194Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobve700768ceekpbronzj","content":"<h1 id=\"nginx-教程之-nginx-配置学习\"><a href=\"#nginx-教程之-nginx-配置学习\" class=\"headerlink\" title=\"nginx 教程之 nginx 配置学习\"></a>nginx 教程之 nginx 配置学习</h1><h2 id=\"1-location配置\"><a href=\"#1-location配置\" class=\"headerlink\" title=\"1.location配置\"></a>1.location配置</h2><p>语法规则：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location [=|~|~*|^~] /uri/ &#123; … &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">= 表示精确匹配,这个优先级也是最高的</span><br><span class=\"line\"></span><br><span class=\"line\">~  表示区分大小写的正则匹配</span><br><span class=\"line\"></span><br><span class=\"line\">~* 表示不区分大小写的正则匹配(和上面的唯一区别就是大小写)</span><br><span class=\"line\"></span><br><span class=\"line\">^~ 表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。</span><br><span class=\"line\"></span><br><span class=\"line\">!~和!~* 分别为区分大小写不匹配及不区分大小写不匹配的正则</span><br><span class=\"line\"></span><br><span class=\"line\">/ 通用匹配，任何请求都会匹配到，默认匹配.</span><br></pre></td></tr></table></figure></p>\n<p>优先级=&gt;^~&gt;</p>\n<p>首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求</p>\n<p>一般线上的配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location ~* .*\\.(js|css)?$</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        expires 7d; //7天过期,后续讲解</span><br><span class=\"line\">        access_log off; //不保存日志</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">location ~* .*\\.(png|jpg|gif|jpeg|bmp|ico)?$</span><br><span class=\"line\">&#123;        </span><br><span class=\"line\">        expires 7d;</span><br><span class=\"line\">        access_log off;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-nginx-逻辑运算\"><a href=\"#2-nginx-逻辑运算\" class=\"headerlink\" title=\"2.nginx 逻辑运算\"></a>2.nginx 逻辑运算</h2><p>nginx的配置中不支持if条件的逻辑与&amp;&amp; 逻辑或|| 运算 ，而且不支持if的嵌套语法，否则会报下面的错误：nginx: [emerg] invalid condition。<br>我们可以用变量的方式来间接实现。<br>要实现的语句：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if ($arg_unitid = 42012 &amp;&amp; $uri ~/thumb/)&#123;</span><br><span class=\"line\"> echo &quot;www.baidu.com&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>可以这么来实现，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set $flag 0;</span><br><span class=\"line\">if ($uri ~ ^/thumb/[0-9]+_160.jpg$)&#123;</span><br><span class=\"line\"> set $flag &quot;$&#123;flag&#125;1&quot;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">if ($arg_unitid = 42012)&#123;</span><br><span class=\"line\"> set $flag &quot;$&#123;flag&#125;1&quot;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">if ($flag = &quot;011&quot;)&#123;</span><br><span class=\"line\"> echo &quot;www.baidu.com&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-ngx-http-core-module模块提供的变量\"><a href=\"#3-ngx-http-core-module模块提供的变量\" class=\"headerlink\" title=\"3.ngx_http_core_module模块提供的变量\"></a>3.ngx_http_core_module模块提供的变量</h2><p>参数名称 注释<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$arg_PARAMETER HTTP 请求中某个参数的值，如/index.php?site=www.ttlsa.com，可以用$arg_site取得www.ttlsa.com这个值.</span><br><span class=\"line\">$args HTTP 请求中的完整参数。例如，在请求/index.php?width=400&amp;height=200 中，$args表示字符串width=400&amp;height=200.</span><br><span class=\"line\">$binary_remote_addr 二进制格式的客户端地址。例如：\\x0A\\xE0B\\x0E</span><br><span class=\"line\">$body_bytes_sent 表示在向客户端发送的http响应中，包体部分的字节数</span><br><span class=\"line\">$content_length 表示客户端请求头部中的Content-Length 字段</span><br><span class=\"line\">$content_type 表示客户端请求头部中的Content-Type 字段</span><br><span class=\"line\">$cookie_COOKIE 表示在客户端请求头部中的cookie 字段</span><br><span class=\"line\">$document_root 表示当前请求所使用的root 配置项的值</span><br><span class=\"line\">$uri 表示当前请求的URI，不带任何参数</span><br><span class=\"line\">$document_uri 与$uri 含义相同</span><br><span class=\"line\">$request_uri 表示客户端发来的原始请求URI，带完整的参数。$uri和$document_uri未必是用户的原始请求，在内部重定向后可能是重定向后的URI，而$request_uri 永远不会改变，始终是客户端的原始URI.</span><br><span class=\"line\">$host 表示客户端请求头部中的Host字段。如果Host字段不存在，则以实际处理的server（虚拟主机）名称代替。如果Host字段中带有端口，如IP:PORT，那么$host是去掉端口的，它的值为IP。$host 是全小写的。这些特性与http_HEADER中的http_host不同，http_host只取出Host头部对应的值。 </span><br><span class=\"line\">$hostname 表示 Nginx所在机器的名称，与 gethostbyname调用返回的值相同 </span><br><span class=\"line\">$http_HEADER 表示当前 HTTP请求中相应头部的值。HEADER名称全小写。例如，示请求中 Host头部对应的值 用 $http_host表 </span><br><span class=\"line\">$sent_http_HEADER 表示返回客户端的 HTTP响应中相应头部的值。HEADER名称全小写。例如，用 $sent_ http_content_type表示响应中 Content-Type头部对应的值 </span><br><span class=\"line\">$is_args 表示请求中的 URI是否带参数，如果带参数，$is_args值为 ?，如果不带参数，则是空字符串 </span><br><span class=\"line\">$limit_rate 表示当前连接的限速是多少，0表示无限速 </span><br><span class=\"line\">$nginx_version 表示当前 Nginx的版本号 </span><br><span class=\"line\">$query_string 请求 URI中的参数，与 $args相同，然而 $query_string是只读的不会改变 </span><br><span class=\"line\">$remote_addr 表示客户端的地址 </span><br><span class=\"line\">$remote_port 表示客户端连接使用的端口 </span><br><span class=\"line\">$remote_user 表示使用 Auth Basic Module时定义的用户名 </span><br><span class=\"line\">$request_filename 表示用户请求中的 URI经过 root或 alias转换后的文件路径 </span><br><span class=\"line\">$request_body 表示 HTTP请求中的包体，该参数只在 proxy_pass或 fastcgi_pass中有意义 </span><br><span class=\"line\">$request_body_file 表示 HTTP请求中的包体存储的临时文件名 </span><br><span class=\"line\">$request_completion 当请求已经全部完成时，其值为 “ok”。若没有完成，就要返回客户端，则其值为空字符串；或者在断点续传等情况下使用 HTTP range访问的并不是文件的最后一块，那么其值也是空字符串。</span><br><span class=\"line\">$request_method 表示 HTTP请求的方法名，如 GET、PUT、POST等 </span><br><span class=\"line\">$scheme 表示 HTTP scheme，如在请求 https://nginx.com/中表示 https </span><br><span class=\"line\">$server_addr 表示服务器地址 </span><br><span class=\"line\">$server_name 表示服务器名称 </span><br><span class=\"line\">$server_port 表示服务器端口 </span><br><span class=\"line\">$server_protocol 表示服务器向客户端发送响应的协议，如 HTTP/1.1或 HTTP/1.0</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"nginx-教程之-nginx-配置学习\"><a href=\"#nginx-教程之-nginx-配置学习\" class=\"headerlink\" title=\"nginx 教程之 nginx 配置学习\"></a>nginx 教程之 nginx 配置学习</h1><h2 id=\"1-location配置\"><a href=\"#1-location配置\" class=\"headerlink\" title=\"1.location配置\"></a>1.location配置</h2><p>语法规则：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location [=|~|~*|^~] /uri/ &#123; … &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">= 表示精确匹配,这个优先级也是最高的</span><br><span class=\"line\"></span><br><span class=\"line\">~  表示区分大小写的正则匹配</span><br><span class=\"line\"></span><br><span class=\"line\">~* 表示不区分大小写的正则匹配(和上面的唯一区别就是大小写)</span><br><span class=\"line\"></span><br><span class=\"line\">^~ 表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。</span><br><span class=\"line\"></span><br><span class=\"line\">!~和!~* 分别为区分大小写不匹配及不区分大小写不匹配的正则</span><br><span class=\"line\"></span><br><span class=\"line\">/ 通用匹配，任何请求都会匹配到，默认匹配.</span><br></pre></td></tr></table></figure></p>\n<p>优先级=&gt;^~&gt;</p>\n<p>首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求</p>\n<p>一般线上的配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">location ~* .*\\.(js|css)?$</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        expires 7d; //7天过期,后续讲解</span><br><span class=\"line\">        access_log off; //不保存日志</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">location ~* .*\\.(png|jpg|gif|jpeg|bmp|ico)?$</span><br><span class=\"line\">&#123;        </span><br><span class=\"line\">        expires 7d;</span><br><span class=\"line\">        access_log off;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-nginx-逻辑运算\"><a href=\"#2-nginx-逻辑运算\" class=\"headerlink\" title=\"2.nginx 逻辑运算\"></a>2.nginx 逻辑运算</h2><p>nginx的配置中不支持if条件的逻辑与&amp;&amp; 逻辑或|| 运算 ，而且不支持if的嵌套语法，否则会报下面的错误：nginx: [emerg] invalid condition。<br>我们可以用变量的方式来间接实现。<br>要实现的语句：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">if ($arg_unitid = 42012 &amp;&amp; $uri ~/thumb/)&#123;</span><br><span class=\"line\"> echo &quot;www.baidu.com&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>可以这么来实现，如下所示：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set $flag 0;</span><br><span class=\"line\">if ($uri ~ ^/thumb/[0-9]+_160.jpg$)&#123;</span><br><span class=\"line\"> set $flag &quot;$&#123;flag&#125;1&quot;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">if ($arg_unitid = 42012)&#123;</span><br><span class=\"line\"> set $flag &quot;$&#123;flag&#125;1&quot;;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">if ($flag = &quot;011&quot;)&#123;</span><br><span class=\"line\"> echo &quot;www.baidu.com&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-ngx-http-core-module模块提供的变量\"><a href=\"#3-ngx-http-core-module模块提供的变量\" class=\"headerlink\" title=\"3.ngx_http_core_module模块提供的变量\"></a>3.ngx_http_core_module模块提供的变量</h2><p>参数名称 注释<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$arg_PARAMETER HTTP 请求中某个参数的值，如/index.php?site=www.ttlsa.com，可以用$arg_site取得www.ttlsa.com这个值.</span><br><span class=\"line\">$args HTTP 请求中的完整参数。例如，在请求/index.php?width=400&amp;height=200 中，$args表示字符串width=400&amp;height=200.</span><br><span class=\"line\">$binary_remote_addr 二进制格式的客户端地址。例如：\\x0A\\xE0B\\x0E</span><br><span class=\"line\">$body_bytes_sent 表示在向客户端发送的http响应中，包体部分的字节数</span><br><span class=\"line\">$content_length 表示客户端请求头部中的Content-Length 字段</span><br><span class=\"line\">$content_type 表示客户端请求头部中的Content-Type 字段</span><br><span class=\"line\">$cookie_COOKIE 表示在客户端请求头部中的cookie 字段</span><br><span class=\"line\">$document_root 表示当前请求所使用的root 配置项的值</span><br><span class=\"line\">$uri 表示当前请求的URI，不带任何参数</span><br><span class=\"line\">$document_uri 与$uri 含义相同</span><br><span class=\"line\">$request_uri 表示客户端发来的原始请求URI，带完整的参数。$uri和$document_uri未必是用户的原始请求，在内部重定向后可能是重定向后的URI，而$request_uri 永远不会改变，始终是客户端的原始URI.</span><br><span class=\"line\">$host 表示客户端请求头部中的Host字段。如果Host字段不存在，则以实际处理的server（虚拟主机）名称代替。如果Host字段中带有端口，如IP:PORT，那么$host是去掉端口的，它的值为IP。$host 是全小写的。这些特性与http_HEADER中的http_host不同，http_host只取出Host头部对应的值。 </span><br><span class=\"line\">$hostname 表示 Nginx所在机器的名称，与 gethostbyname调用返回的值相同 </span><br><span class=\"line\">$http_HEADER 表示当前 HTTP请求中相应头部的值。HEADER名称全小写。例如，示请求中 Host头部对应的值 用 $http_host表 </span><br><span class=\"line\">$sent_http_HEADER 表示返回客户端的 HTTP响应中相应头部的值。HEADER名称全小写。例如，用 $sent_ http_content_type表示响应中 Content-Type头部对应的值 </span><br><span class=\"line\">$is_args 表示请求中的 URI是否带参数，如果带参数，$is_args值为 ?，如果不带参数，则是空字符串 </span><br><span class=\"line\">$limit_rate 表示当前连接的限速是多少，0表示无限速 </span><br><span class=\"line\">$nginx_version 表示当前 Nginx的版本号 </span><br><span class=\"line\">$query_string 请求 URI中的参数，与 $args相同，然而 $query_string是只读的不会改变 </span><br><span class=\"line\">$remote_addr 表示客户端的地址 </span><br><span class=\"line\">$remote_port 表示客户端连接使用的端口 </span><br><span class=\"line\">$remote_user 表示使用 Auth Basic Module时定义的用户名 </span><br><span class=\"line\">$request_filename 表示用户请求中的 URI经过 root或 alias转换后的文件路径 </span><br><span class=\"line\">$request_body 表示 HTTP请求中的包体，该参数只在 proxy_pass或 fastcgi_pass中有意义 </span><br><span class=\"line\">$request_body_file 表示 HTTP请求中的包体存储的临时文件名 </span><br><span class=\"line\">$request_completion 当请求已经全部完成时，其值为 “ok”。若没有完成，就要返回客户端，则其值为空字符串；或者在断点续传等情况下使用 HTTP range访问的并不是文件的最后一块，那么其值也是空字符串。</span><br><span class=\"line\">$request_method 表示 HTTP请求的方法名，如 GET、PUT、POST等 </span><br><span class=\"line\">$scheme 表示 HTTP scheme，如在请求 https://nginx.com/中表示 https </span><br><span class=\"line\">$server_addr 表示服务器地址 </span><br><span class=\"line\">$server_name 表示服务器名称 </span><br><span class=\"line\">$server_port 表示服务器端口 </span><br><span class=\"line\">$server_protocol 表示服务器向客户端发送响应的协议，如 HTTP/1.1或 HTTP/1.0</span><br></pre></td></tr></table></figure></p>\n"},{"title":"nginx 教程之 docker 化安装","date":"2018-01-15T11:26:21.000Z","_content":"# nginx 教程之 docker 化安装\n\n## 1.Why docker \n\n因为要依赖很多lib,安装复杂，提高了学习和使用的难度，所以推荐使用docker 部署，可以快速迁移，快速部署，更多优点详见[docker官网](https://hub.docker.com/)\n\n## 2.部署\n\n这里我们选择[官方镜像](https://hub.docker.com/_/nginx/),使用文档也可以在这个页面内找到。\n\n使用完整nginx配置文件替换默认配置信息\n```\n docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf -d nginx:1.13.7-alpine\n```\n\n当然还可以继承默认配置，这样使用\n\n```\ndocker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx:1.13.7-alpine\n```\n\n## 3.nginx image默认配置\n\n\n```\n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n## 4.相关学习资源\n\n- [网上最全面nginx教程（近100篇文章整理）](http://blog.csdn.net/jinchaoh/article/details/49450435)\n","source":"_posts/nginx-教程之-docker-化安装.md","raw":"---\ntitle: nginx 教程之 docker 化安装\ndate: 2018-01-15 19:26:21\ntags: 开发笔记\ncategories:\n- nginx\n---\n# nginx 教程之 docker 化安装\n\n## 1.Why docker \n\n因为要依赖很多lib,安装复杂，提高了学习和使用的难度，所以推荐使用docker 部署，可以快速迁移，快速部署，更多优点详见[docker官网](https://hub.docker.com/)\n\n## 2.部署\n\n这里我们选择[官方镜像](https://hub.docker.com/_/nginx/),使用文档也可以在这个页面内找到。\n\n使用完整nginx配置文件替换默认配置信息\n```\n docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf -d nginx:1.13.7-alpine\n```\n\n当然还可以继承默认配置，这样使用\n\n```\ndocker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx:1.13.7-alpine\n```\n\n## 3.nginx image默认配置\n\n\n```\n\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n## 4.相关学习资源\n\n- [网上最全面nginx教程（近100篇文章整理）](http://blog.csdn.net/jinchaoh/article/details/49450435)\n","slug":"nginx-教程之-docker-化安装","published":1,"updated":"2018-01-15T11:27:29.622Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobve900798ceeyvztslbi","content":"<h1 id=\"nginx-教程之-docker-化安装\"><a href=\"#nginx-教程之-docker-化安装\" class=\"headerlink\" title=\"nginx 教程之 docker 化安装\"></a>nginx 教程之 docker 化安装</h1><h2 id=\"1-Why-docker\"><a href=\"#1-Why-docker\" class=\"headerlink\" title=\"1.Why docker\"></a>1.Why docker</h2><p>因为要依赖很多lib,安装复杂，提高了学习和使用的难度，所以推荐使用docker 部署，可以快速迁移，快速部署，更多优点详见<a href=\"https://hub.docker.com/\" target=\"_blank\" rel=\"noopener\">docker官网</a></p>\n<h2 id=\"2-部署\"><a href=\"#2-部署\" class=\"headerlink\" title=\"2.部署\"></a>2.部署</h2><p>这里我们选择<a href=\"https://hub.docker.com/_/nginx/\" target=\"_blank\" rel=\"noopener\">官方镜像</a>,使用文档也可以在这个页面内找到。</p>\n<p>使用完整nginx配置文件替换默认配置信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf -d nginx:1.13.7-alpine</span><br></pre></td></tr></table></figure></p>\n<p>当然还可以继承默认配置，这样使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx:1.13.7-alpine</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-nginx-image默认配置\"><a href=\"#3-nginx-image默认配置\" class=\"headerlink\" title=\"3.nginx image默认配置\"></a>3.nginx image默认配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">user  nginx;</span><br><span class=\"line\">worker_processes  1;</span><br><span class=\"line\"></span><br><span class=\"line\">error_log  /var/log/nginx/error.log warn;</span><br><span class=\"line\">pid        /var/run/nginx.pid;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">events &#123;</span><br><span class=\"line\">    worker_connections  1024;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">http &#123;</span><br><span class=\"line\">    include       /etc/nginx/mime.types;</span><br><span class=\"line\">    default_type  application/octet-stream;</span><br><span class=\"line\"></span><br><span class=\"line\">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class=\"line\">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class=\"line\">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">    access_log  /var/log/nginx/access.log  main;</span><br><span class=\"line\"></span><br><span class=\"line\">    sendfile        on;</span><br><span class=\"line\">    #tcp_nopush     on;</span><br><span class=\"line\"></span><br><span class=\"line\">    keepalive_timeout  65;</span><br><span class=\"line\"></span><br><span class=\"line\">    #gzip  on;</span><br><span class=\"line\"></span><br><span class=\"line\">    include /etc/nginx/conf.d/*.conf;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-相关学习资源\"><a href=\"#4-相关学习资源\" class=\"headerlink\" title=\"4.相关学习资源\"></a>4.相关学习资源</h2><ul>\n<li><a href=\"http://blog.csdn.net/jinchaoh/article/details/49450435\" target=\"_blank\" rel=\"noopener\">网上最全面nginx教程（近100篇文章整理）</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"nginx-教程之-docker-化安装\"><a href=\"#nginx-教程之-docker-化安装\" class=\"headerlink\" title=\"nginx 教程之 docker 化安装\"></a>nginx 教程之 docker 化安装</h1><h2 id=\"1-Why-docker\"><a href=\"#1-Why-docker\" class=\"headerlink\" title=\"1.Why docker\"></a>1.Why docker</h2><p>因为要依赖很多lib,安装复杂，提高了学习和使用的难度，所以推荐使用docker 部署，可以快速迁移，快速部署，更多优点详见<a href=\"https://hub.docker.com/\" target=\"_blank\" rel=\"noopener\">docker官网</a></p>\n<h2 id=\"2-部署\"><a href=\"#2-部署\" class=\"headerlink\" title=\"2.部署\"></a>2.部署</h2><p>这里我们选择<a href=\"https://hub.docker.com/_/nginx/\" target=\"_blank\" rel=\"noopener\">官方镜像</a>,使用文档也可以在这个页面内找到。</p>\n<p>使用完整nginx配置文件替换默认配置信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf -d nginx:1.13.7-alpine</span><br></pre></td></tr></table></figure></p>\n<p>当然还可以继承默认配置，这样使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name=nginx --net=host -v /host/path/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx:1.13.7-alpine</span><br></pre></td></tr></table></figure>\n<h2 id=\"3-nginx-image默认配置\"><a href=\"#3-nginx-image默认配置\" class=\"headerlink\" title=\"3.nginx image默认配置\"></a>3.nginx image默认配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">user  nginx;</span><br><span class=\"line\">worker_processes  1;</span><br><span class=\"line\"></span><br><span class=\"line\">error_log  /var/log/nginx/error.log warn;</span><br><span class=\"line\">pid        /var/run/nginx.pid;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">events &#123;</span><br><span class=\"line\">    worker_connections  1024;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">http &#123;</span><br><span class=\"line\">    include       /etc/nginx/mime.types;</span><br><span class=\"line\">    default_type  application/octet-stream;</span><br><span class=\"line\"></span><br><span class=\"line\">    log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class=\"line\">                      &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class=\"line\">                      &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">    access_log  /var/log/nginx/access.log  main;</span><br><span class=\"line\"></span><br><span class=\"line\">    sendfile        on;</span><br><span class=\"line\">    #tcp_nopush     on;</span><br><span class=\"line\"></span><br><span class=\"line\">    keepalive_timeout  65;</span><br><span class=\"line\"></span><br><span class=\"line\">    #gzip  on;</span><br><span class=\"line\"></span><br><span class=\"line\">    include /etc/nginx/conf.d/*.conf;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-相关学习资源\"><a href=\"#4-相关学习资源\" class=\"headerlink\" title=\"4.相关学习资源\"></a>4.相关学习资源</h2><ul>\n<li><a href=\"http://blog.csdn.net/jinchaoh/article/details/49450435\" target=\"_blank\" rel=\"noopener\">网上最全面nginx教程（近100篇文章整理）</a></li>\n</ul>\n"},{"title":"node.js模块学习总结","date":"2018-01-15T11:23:42.000Z","_content":"# node.js模块学习总结\n\n## 简介\n主要是记录node.js模块学习总结，对于不是专业前端开发人员，往往不清楚到底nodejs比传统javascript多了哪些内容，针对自己自我学习，做个总结。\n\n## 正文\n\n首先推荐一个学习网站，觉得挺不错的\n```\nhttp://nqdeng.github.io/7-days-nodejs/\n```\n### 常识性\n在开始学习之前，有三个新增关键字需要理解一下：\n\n---\n1. **require**\n>require函数用于在当前模块中加载和使用别的模块，传入一个模块名，返回一个模块导出对象\n2. **exports**\n>exports对象是当前模块的导出对象，用于导出模块公有方法和属性。别的模块通过require函数使用当前模块时得到的就是当前模块的exports对象\n3. **module**\n>通过module对象可以访问到当前模块的一些相关信息，但最多的用途是替换当前模块的导出对象\n---\n\n看到以上信息有个疑问，我们经常使用的是\n\ncode1\n```\nmodule.exports = function(){\n     console.log('Hello World!');\n}\n\n```\n和\n\ncode1\n```\nexports.hello = function () {\n    console.log('Hello World!');\n};\n```\n区别在哪里？\n\n两点主要区别是在在于require在不同模块获取时，只是能获取该模块中的默认导出对象，即exports，只有通过exports.hello这种方式，即可获取该模块暴露的方法或者属性。而module.exports则是将默认的导出对象修改为一个函数。\n\n### 内置对象\n\n\n- assert - 断言\n- Buffer - 缓冲器\n- child_process - 子进程\n- cluster - 集群\n- console - 控制台\n- crypto - 加密\n- dgram - 数据报\n- dns - 域名服务器\n- Error - 异常\n- events - 事件\n- fs - 文件系统\n- global - 全局变量\n- http - HTTP\n- https - HTTPS\n- module - 模块\n- net - 网络\n- os - 操作系统\n- path - 路径\n- process - 进程\n- querystring - 查询字符串\n- readline - 逐行读取\n- repl - 交互式解释器\n- stream - 流\n- string_decoder - 字符串解码器\n- timer - 定时器\n- tls - 安全传输层\n- tty - 终端\n- url - 网址\n- util - 实用工具\n- v8 - V8引擎\n- vm - 虚拟机\n- zlib - 压缩\n\n试验的API\n- async_hooks\n- http2\n- inspector\n- napi\n- perf_hooks\n\n详细使用    [详见网址](http://nodejs.cn/api/)\n\n\n","source":"_posts/node-js模块学习总结.md","raw":"---\ntitle: node.js模块学习总结\ndate: 2018-01-15 19:23:42\ntags: 开发笔记\ncategories:\n- node.js\n---\n# node.js模块学习总结\n\n## 简介\n主要是记录node.js模块学习总结，对于不是专业前端开发人员，往往不清楚到底nodejs比传统javascript多了哪些内容，针对自己自我学习，做个总结。\n\n## 正文\n\n首先推荐一个学习网站，觉得挺不错的\n```\nhttp://nqdeng.github.io/7-days-nodejs/\n```\n### 常识性\n在开始学习之前，有三个新增关键字需要理解一下：\n\n---\n1. **require**\n>require函数用于在当前模块中加载和使用别的模块，传入一个模块名，返回一个模块导出对象\n2. **exports**\n>exports对象是当前模块的导出对象，用于导出模块公有方法和属性。别的模块通过require函数使用当前模块时得到的就是当前模块的exports对象\n3. **module**\n>通过module对象可以访问到当前模块的一些相关信息，但最多的用途是替换当前模块的导出对象\n---\n\n看到以上信息有个疑问，我们经常使用的是\n\ncode1\n```\nmodule.exports = function(){\n     console.log('Hello World!');\n}\n\n```\n和\n\ncode1\n```\nexports.hello = function () {\n    console.log('Hello World!');\n};\n```\n区别在哪里？\n\n两点主要区别是在在于require在不同模块获取时，只是能获取该模块中的默认导出对象，即exports，只有通过exports.hello这种方式，即可获取该模块暴露的方法或者属性。而module.exports则是将默认的导出对象修改为一个函数。\n\n### 内置对象\n\n\n- assert - 断言\n- Buffer - 缓冲器\n- child_process - 子进程\n- cluster - 集群\n- console - 控制台\n- crypto - 加密\n- dgram - 数据报\n- dns - 域名服务器\n- Error - 异常\n- events - 事件\n- fs - 文件系统\n- global - 全局变量\n- http - HTTP\n- https - HTTPS\n- module - 模块\n- net - 网络\n- os - 操作系统\n- path - 路径\n- process - 进程\n- querystring - 查询字符串\n- readline - 逐行读取\n- repl - 交互式解释器\n- stream - 流\n- string_decoder - 字符串解码器\n- timer - 定时器\n- tls - 安全传输层\n- tty - 终端\n- url - 网址\n- util - 实用工具\n- v8 - V8引擎\n- vm - 虚拟机\n- zlib - 压缩\n\n试验的API\n- async_hooks\n- http2\n- inspector\n- napi\n- perf_hooks\n\n详细使用    [详见网址](http://nodejs.cn/api/)\n\n\n","slug":"node-js模块学习总结","published":1,"updated":"2018-01-15T11:25:03.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobveb007d8ceeor1oornt","content":"<h1 id=\"node-js模块学习总结\"><a href=\"#node-js模块学习总结\" class=\"headerlink\" title=\"node.js模块学习总结\"></a>node.js模块学习总结</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>主要是记录node.js模块学习总结，对于不是专业前端开发人员，往往不清楚到底nodejs比传统javascript多了哪些内容，针对自己自我学习，做个总结。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><p>首先推荐一个学习网站，觉得挺不错的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://nqdeng.github.io/7-days-nodejs/</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"常识性\"><a href=\"#常识性\" class=\"headerlink\" title=\"常识性\"></a>常识性</h3><p>在开始学习之前，有三个新增关键字需要理解一下：</p>\n<hr>\n<ol>\n<li><strong>require</strong><blockquote>\n<p>require函数用于在当前模块中加载和使用别的模块，传入一个模块名，返回一个模块导出对象</p>\n</blockquote>\n</li>\n<li><strong>exports</strong><blockquote>\n<p>exports对象是当前模块的导出对象，用于导出模块公有方法和属性。别的模块通过require函数使用当前模块时得到的就是当前模块的exports对象</p>\n</blockquote>\n</li>\n<li><strong>module</strong><blockquote>\n<p>通过module对象可以访问到当前模块的一些相关信息，但最多的用途是替换当前模块的导出对象</p>\n</blockquote>\n</li>\n</ol>\n<hr>\n<p>看到以上信息有个疑问，我们经常使用的是</p>\n<p>code1<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">module.exports = function()&#123;</span><br><span class=\"line\">     console.log(&apos;Hello World!&apos;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>和</p>\n<p>code1<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exports.hello = function () &#123;</span><br><span class=\"line\">    console.log(&apos;Hello World!&apos;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>区别在哪里？</p>\n<p>两点主要区别是在在于require在不同模块获取时，只是能获取该模块中的默认导出对象，即exports，只有通过exports.hello这种方式，即可获取该模块暴露的方法或者属性。而module.exports则是将默认的导出对象修改为一个函数。</p>\n<h3 id=\"内置对象\"><a href=\"#内置对象\" class=\"headerlink\" title=\"内置对象\"></a>内置对象</h3><ul>\n<li>assert - 断言</li>\n<li>Buffer - 缓冲器</li>\n<li>child_process - 子进程</li>\n<li>cluster - 集群</li>\n<li>console - 控制台</li>\n<li>crypto - 加密</li>\n<li>dgram - 数据报</li>\n<li>dns - 域名服务器</li>\n<li>Error - 异常</li>\n<li>events - 事件</li>\n<li>fs - 文件系统</li>\n<li>global - 全局变量</li>\n<li>http - HTTP</li>\n<li>https - HTTPS</li>\n<li>module - 模块</li>\n<li>net - 网络</li>\n<li>os - 操作系统</li>\n<li>path - 路径</li>\n<li>process - 进程</li>\n<li>querystring - 查询字符串</li>\n<li>readline - 逐行读取</li>\n<li>repl - 交互式解释器</li>\n<li>stream - 流</li>\n<li>string_decoder - 字符串解码器</li>\n<li>timer - 定时器</li>\n<li>tls - 安全传输层</li>\n<li>tty - 终端</li>\n<li>url - 网址</li>\n<li>util - 实用工具</li>\n<li>v8 - V8引擎</li>\n<li>vm - 虚拟机</li>\n<li>zlib - 压缩</li>\n</ul>\n<p>试验的API</p>\n<ul>\n<li>async_hooks</li>\n<li>http2</li>\n<li>inspector</li>\n<li>napi</li>\n<li>perf_hooks</li>\n</ul>\n<p>详细使用    <a href=\"http://nodejs.cn/api/\" target=\"_blank\" rel=\"noopener\">详见网址</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"node-js模块学习总结\"><a href=\"#node-js模块学习总结\" class=\"headerlink\" title=\"node.js模块学习总结\"></a>node.js模块学习总结</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>主要是记录node.js模块学习总结，对于不是专业前端开发人员，往往不清楚到底nodejs比传统javascript多了哪些内容，针对自己自我学习，做个总结。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><p>首先推荐一个学习网站，觉得挺不错的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://nqdeng.github.io/7-days-nodejs/</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"常识性\"><a href=\"#常识性\" class=\"headerlink\" title=\"常识性\"></a>常识性</h3><p>在开始学习之前，有三个新增关键字需要理解一下：</p>\n<hr>\n<ol>\n<li><strong>require</strong><blockquote>\n<p>require函数用于在当前模块中加载和使用别的模块，传入一个模块名，返回一个模块导出对象</p>\n</blockquote>\n</li>\n<li><strong>exports</strong><blockquote>\n<p>exports对象是当前模块的导出对象，用于导出模块公有方法和属性。别的模块通过require函数使用当前模块时得到的就是当前模块的exports对象</p>\n</blockquote>\n</li>\n<li><strong>module</strong><blockquote>\n<p>通过module对象可以访问到当前模块的一些相关信息，但最多的用途是替换当前模块的导出对象</p>\n</blockquote>\n</li>\n</ol>\n<hr>\n<p>看到以上信息有个疑问，我们经常使用的是</p>\n<p>code1<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">module.exports = function()&#123;</span><br><span class=\"line\">     console.log(&apos;Hello World!&apos;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>和</p>\n<p>code1<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">exports.hello = function () &#123;</span><br><span class=\"line\">    console.log(&apos;Hello World!&apos;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>区别在哪里？</p>\n<p>两点主要区别是在在于require在不同模块获取时，只是能获取该模块中的默认导出对象，即exports，只有通过exports.hello这种方式，即可获取该模块暴露的方法或者属性。而module.exports则是将默认的导出对象修改为一个函数。</p>\n<h3 id=\"内置对象\"><a href=\"#内置对象\" class=\"headerlink\" title=\"内置对象\"></a>内置对象</h3><ul>\n<li>assert - 断言</li>\n<li>Buffer - 缓冲器</li>\n<li>child_process - 子进程</li>\n<li>cluster - 集群</li>\n<li>console - 控制台</li>\n<li>crypto - 加密</li>\n<li>dgram - 数据报</li>\n<li>dns - 域名服务器</li>\n<li>Error - 异常</li>\n<li>events - 事件</li>\n<li>fs - 文件系统</li>\n<li>global - 全局变量</li>\n<li>http - HTTP</li>\n<li>https - HTTPS</li>\n<li>module - 模块</li>\n<li>net - 网络</li>\n<li>os - 操作系统</li>\n<li>path - 路径</li>\n<li>process - 进程</li>\n<li>querystring - 查询字符串</li>\n<li>readline - 逐行读取</li>\n<li>repl - 交互式解释器</li>\n<li>stream - 流</li>\n<li>string_decoder - 字符串解码器</li>\n<li>timer - 定时器</li>\n<li>tls - 安全传输层</li>\n<li>tty - 终端</li>\n<li>url - 网址</li>\n<li>util - 实用工具</li>\n<li>v8 - V8引擎</li>\n<li>vm - 虚拟机</li>\n<li>zlib - 压缩</li>\n</ul>\n<p>试验的API</p>\n<ul>\n<li>async_hooks</li>\n<li>http2</li>\n<li>inspector</li>\n<li>napi</li>\n<li>perf_hooks</li>\n</ul>\n<p>详细使用    <a href=\"http://nodejs.cn/api/\" target=\"_blank\" rel=\"noopener\">详见网址</a></p>\n"},{"title":"node.js简介","date":"2017-08-21T09:20:57.000Z","_content":"# node.js简介\n\n## what node.js?\n\nNode.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。\nNode.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效。\nNode.js 的包管理器 npm，是全球最大的开源库生态系统。\n\n## node.js 原理\n\nNodeJS的工作原理其实就是事件循环。可以说每一条NodeJS的逻辑都是写在回调函数里面的，而回调函数都是有返回之后才异步执行的！\n\n## 应用场景\n\n既然NodeJS**处理并发**的能力强，但处理计算和逻辑的能力反而很弱，因此，如果我们把复杂的逻辑运算都搬到前端（客户端）完成，而NodeJS只需要提供异步I/O，这样就可以实现对高并发的高性能处理。情况就很多啦，比如：**RESTFUL API、实时聊天、客户端逻辑强大的单页APP**，具体的例子比如说：本地化的在线音乐应用，本地化的在线搜索应用，本地化的在线APP等。\n\n## node.js优缺点\n\n**优点**\n1. 采用事件驱动、异步编程，为网络服务而设计。其实Javascript的匿名函数和闭包特性非常适合事件驱动、异步编程。而且JavaScript也简单易学，很多前端设计人员可以很快上手做后端设计。\n2. Node.js非阻塞模式的IO处理给Node.js带来在相对低系统资源耗用下的高性能与出众的负载能力，非常适合用作依赖其它IO资源的中间层服务。\n3. Node.js轻量高效，可以认为是数据密集型分布式部署环境下的实时应用系统的完美解决方案。Node非常适合如下情况：在响应客户端之前，您预计可能有很高的流量，但所需的服务器端逻辑和处理不一定很多。\n\n**缺点**\n1. 可靠性低\n2. 单进程，单线程，只支持单核CPU，不能充分的利用多核CPU服务器。一旦这个进程崩掉，那么整个web服务就崩掉了。\n","source":"_posts/node-js简介.md","raw":"---\ntitle: node.js简介\ndate: 2017-08-21 17:20:57\ntags: 开发笔记\ncategories:\n- node.js\n---\n# node.js简介\n\n## what node.js?\n\nNode.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。\nNode.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效。\nNode.js 的包管理器 npm，是全球最大的开源库生态系统。\n\n## node.js 原理\n\nNodeJS的工作原理其实就是事件循环。可以说每一条NodeJS的逻辑都是写在回调函数里面的，而回调函数都是有返回之后才异步执行的！\n\n## 应用场景\n\n既然NodeJS**处理并发**的能力强，但处理计算和逻辑的能力反而很弱，因此，如果我们把复杂的逻辑运算都搬到前端（客户端）完成，而NodeJS只需要提供异步I/O，这样就可以实现对高并发的高性能处理。情况就很多啦，比如：**RESTFUL API、实时聊天、客户端逻辑强大的单页APP**，具体的例子比如说：本地化的在线音乐应用，本地化的在线搜索应用，本地化的在线APP等。\n\n## node.js优缺点\n\n**优点**\n1. 采用事件驱动、异步编程，为网络服务而设计。其实Javascript的匿名函数和闭包特性非常适合事件驱动、异步编程。而且JavaScript也简单易学，很多前端设计人员可以很快上手做后端设计。\n2. Node.js非阻塞模式的IO处理给Node.js带来在相对低系统资源耗用下的高性能与出众的负载能力，非常适合用作依赖其它IO资源的中间层服务。\n3. Node.js轻量高效，可以认为是数据密集型分布式部署环境下的实时应用系统的完美解决方案。Node非常适合如下情况：在响应客户端之前，您预计可能有很高的流量，但所需的服务器端逻辑和处理不一定很多。\n\n**缺点**\n1. 可靠性低\n2. 单进程，单线程，只支持单核CPU，不能充分的利用多核CPU服务器。一旦这个进程崩掉，那么整个web服务就崩掉了。\n","slug":"node-js简介","published":1,"updated":"2017-08-21T09:24:06.720Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobved007g8ceeh1jmmnrv","content":"<h1 id=\"node-js简介\"><a href=\"#node-js简介\" class=\"headerlink\" title=\"node.js简介\"></a>node.js简介</h1><h2 id=\"what-node-js\"><a href=\"#what-node-js\" class=\"headerlink\" title=\"what node.js?\"></a>what node.js?</h2><p>Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。<br>Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效。<br>Node.js 的包管理器 npm，是全球最大的开源库生态系统。</p>\n<h2 id=\"node-js-原理\"><a href=\"#node-js-原理\" class=\"headerlink\" title=\"node.js 原理\"></a>node.js 原理</h2><p>NodeJS的工作原理其实就是事件循环。可以说每一条NodeJS的逻辑都是写在回调函数里面的，而回调函数都是有返回之后才异步执行的！</p>\n<h2 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"></a>应用场景</h2><p>既然NodeJS<strong>处理并发</strong>的能力强，但处理计算和逻辑的能力反而很弱，因此，如果我们把复杂的逻辑运算都搬到前端（客户端）完成，而NodeJS只需要提供异步I/O，这样就可以实现对高并发的高性能处理。情况就很多啦，比如：<strong>RESTFUL API、实时聊天、客户端逻辑强大的单页APP</strong>，具体的例子比如说：本地化的在线音乐应用，本地化的在线搜索应用，本地化的在线APP等。</p>\n<h2 id=\"node-js优缺点\"><a href=\"#node-js优缺点\" class=\"headerlink\" title=\"node.js优缺点\"></a>node.js优缺点</h2><p><strong>优点</strong></p>\n<ol>\n<li>采用事件驱动、异步编程，为网络服务而设计。其实Javascript的匿名函数和闭包特性非常适合事件驱动、异步编程。而且JavaScript也简单易学，很多前端设计人员可以很快上手做后端设计。</li>\n<li>Node.js非阻塞模式的IO处理给Node.js带来在相对低系统资源耗用下的高性能与出众的负载能力，非常适合用作依赖其它IO资源的中间层服务。</li>\n<li>Node.js轻量高效，可以认为是数据密集型分布式部署环境下的实时应用系统的完美解决方案。Node非常适合如下情况：在响应客户端之前，您预计可能有很高的流量，但所需的服务器端逻辑和处理不一定很多。</li>\n</ol>\n<p><strong>缺点</strong></p>\n<ol>\n<li>可靠性低</li>\n<li>单进程，单线程，只支持单核CPU，不能充分的利用多核CPU服务器。一旦这个进程崩掉，那么整个web服务就崩掉了。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"node-js简介\"><a href=\"#node-js简介\" class=\"headerlink\" title=\"node.js简介\"></a>node.js简介</h1><h2 id=\"what-node-js\"><a href=\"#what-node-js\" class=\"headerlink\" title=\"what node.js?\"></a>what node.js?</h2><p>Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。<br>Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效。<br>Node.js 的包管理器 npm，是全球最大的开源库生态系统。</p>\n<h2 id=\"node-js-原理\"><a href=\"#node-js-原理\" class=\"headerlink\" title=\"node.js 原理\"></a>node.js 原理</h2><p>NodeJS的工作原理其实就是事件循环。可以说每一条NodeJS的逻辑都是写在回调函数里面的，而回调函数都是有返回之后才异步执行的！</p>\n<h2 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"></a>应用场景</h2><p>既然NodeJS<strong>处理并发</strong>的能力强，但处理计算和逻辑的能力反而很弱，因此，如果我们把复杂的逻辑运算都搬到前端（客户端）完成，而NodeJS只需要提供异步I/O，这样就可以实现对高并发的高性能处理。情况就很多啦，比如：<strong>RESTFUL API、实时聊天、客户端逻辑强大的单页APP</strong>，具体的例子比如说：本地化的在线音乐应用，本地化的在线搜索应用，本地化的在线APP等。</p>\n<h2 id=\"node-js优缺点\"><a href=\"#node-js优缺点\" class=\"headerlink\" title=\"node.js优缺点\"></a>node.js优缺点</h2><p><strong>优点</strong></p>\n<ol>\n<li>采用事件驱动、异步编程，为网络服务而设计。其实Javascript的匿名函数和闭包特性非常适合事件驱动、异步编程。而且JavaScript也简单易学，很多前端设计人员可以很快上手做后端设计。</li>\n<li>Node.js非阻塞模式的IO处理给Node.js带来在相对低系统资源耗用下的高性能与出众的负载能力，非常适合用作依赖其它IO资源的中间层服务。</li>\n<li>Node.js轻量高效，可以认为是数据密集型分布式部署环境下的实时应用系统的完美解决方案。Node非常适合如下情况：在响应客户端之前，您预计可能有很高的流量，但所需的服务器端逻辑和处理不一定很多。</li>\n</ol>\n<p><strong>缺点</strong></p>\n<ol>\n<li>可靠性低</li>\n<li>单进程，单线程，只支持单核CPU，不能充分的利用多核CPU服务器。一旦这个进程崩掉，那么整个web服务就崩掉了。</li>\n</ol>\n"},{"title":"nodejs邮件发送","date":"2017-09-17T04:25:48.000Z","_content":"# nodejs邮件发送\n\n## 介绍\nnode.js发送邮件有多种第三方模块，比较有名的是[emailjs](https://github.com/eleith/emailjs),[nodemailer](https://github.com/nodemailer/nodemailer),个人感觉emailjs更轻量级，使用更简单，nodemailer关注人更多，功能更加完善。\n## 实践\n本次选用这两种，仅实现简单发送邮件功能，更多功能，请查询官网API.\n首先新建文件夹node,然后在该文件夹下新建package.json文件，在其中增加以下配置\n```\n{\n  \"name\": \"node_email_demo\",\n  \"description\": \"application created by truman\",\n  \"version\": \"1.0.0\",\n  \"dependencies\": {\n    \"emailjs\": \"^1.0.8\",\n    \"nodemailer\": \"^4.1.0\"\n  },\n  \"repository\": \"\",\n  \"license\": \"MIT\",\n  \"engines\": {\n    \"node\": \">=6.0.0\"\n  },\n  \"readmeFilename\": \"README.md\"\n}\n```\n\n在node 目录下执行命令\n\n```\nnpm install\n```\n- emailjs\n新建myemailjs.js文件，添加如下内容：\n```\nvar email       = require(\"emailjs\");\nvar server      = email.server.connect({\n    user:    \"aibibang@sohu.com\",\n    password:\"*********\",\n    host:    \"smtp.sohu.com\",\n    ssl:     false\n});\n\n// send the message and get a callback with an error or details of the message that was sent\n server.send({\n     text:    \"i hope this works\",\n     from:    \"aibibang@sohu.com\",\n     to:      \"aibibang@sohu.com\",\n     subject: \"testing emailjs\"\n     }, function(err, message) { console.log(err || message); });\n```\n\n执行即可发送成功！\n```\nnode myemailjs.js\n```\n- nodemailer\n新建mynodemailer.js文件，添加如下内容：\n\n```\nconst nodemailer=require(\"nodemailer\");\n\nlet transporter = nodemailer.createTransport({\n    host: 'smtp.sohu.com',\n    port: 25,\n    auth: {\n            user: \"aibibang@sohu.com\", // generated ethereal user\n            pass: \"*********\"  // generated ethereal password\n    },\n    secure: false // true for 465, false for other ports\n});\nvar message = {\n    from: 'aibibang@sohu.com',\n    to: 'aibibang@sohu.com',\n    subject: 'nodemailer test',\n    text: 'i hope this works'\n};\n\ntransporter.sendMail(message, function(err){\n    if(err){\n        console.log(err);\n    }\n});\n```\n\n执行即可发送成功！\n```\nnode mynodemailer.js\n```\n## 总结\n1. 如果SMPT服务器没有进行安全校验，那么一定要去掉用户与密码，这点要和java API区别，切记，为了这个问题，花费了一天代价。\n例如如下问题：\n```\n{ Error: no form of authorization supported\n    at module.exports (/data/truman/node/node_modules/emailjs/smtp/error.js:2:13)\n    at initiate (/data/truman/node/node_modules/emailjs/smtp/smtp.js:543:44)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at attempt (/data/truman/node/node_modules/emailjs/smtp/smtp.js:415:14)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:345:13)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:201:11)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at Socket.response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:181:11)\n  code: 7,\n```\n2. 对于本地是否可以访问SMTP服务器，可以使用telnet,进行查验，如果都无法telnet通，肯定是无法发送邮件的。\n```\ntelnet smtp.sohu.com 25\n```","source":"_posts/nodejs邮件发送.md","raw":"---\ntitle: nodejs邮件发送\ndate: 2017-09-17 12:25:48\ntags: 开发笔记\ncategories:\n- node.js\n---\n# nodejs邮件发送\n\n## 介绍\nnode.js发送邮件有多种第三方模块，比较有名的是[emailjs](https://github.com/eleith/emailjs),[nodemailer](https://github.com/nodemailer/nodemailer),个人感觉emailjs更轻量级，使用更简单，nodemailer关注人更多，功能更加完善。\n## 实践\n本次选用这两种，仅实现简单发送邮件功能，更多功能，请查询官网API.\n首先新建文件夹node,然后在该文件夹下新建package.json文件，在其中增加以下配置\n```\n{\n  \"name\": \"node_email_demo\",\n  \"description\": \"application created by truman\",\n  \"version\": \"1.0.0\",\n  \"dependencies\": {\n    \"emailjs\": \"^1.0.8\",\n    \"nodemailer\": \"^4.1.0\"\n  },\n  \"repository\": \"\",\n  \"license\": \"MIT\",\n  \"engines\": {\n    \"node\": \">=6.0.0\"\n  },\n  \"readmeFilename\": \"README.md\"\n}\n```\n\n在node 目录下执行命令\n\n```\nnpm install\n```\n- emailjs\n新建myemailjs.js文件，添加如下内容：\n```\nvar email       = require(\"emailjs\");\nvar server      = email.server.connect({\n    user:    \"aibibang@sohu.com\",\n    password:\"*********\",\n    host:    \"smtp.sohu.com\",\n    ssl:     false\n});\n\n// send the message and get a callback with an error or details of the message that was sent\n server.send({\n     text:    \"i hope this works\",\n     from:    \"aibibang@sohu.com\",\n     to:      \"aibibang@sohu.com\",\n     subject: \"testing emailjs\"\n     }, function(err, message) { console.log(err || message); });\n```\n\n执行即可发送成功！\n```\nnode myemailjs.js\n```\n- nodemailer\n新建mynodemailer.js文件，添加如下内容：\n\n```\nconst nodemailer=require(\"nodemailer\");\n\nlet transporter = nodemailer.createTransport({\n    host: 'smtp.sohu.com',\n    port: 25,\n    auth: {\n            user: \"aibibang@sohu.com\", // generated ethereal user\n            pass: \"*********\"  // generated ethereal password\n    },\n    secure: false // true for 465, false for other ports\n});\nvar message = {\n    from: 'aibibang@sohu.com',\n    to: 'aibibang@sohu.com',\n    subject: 'nodemailer test',\n    text: 'i hope this works'\n};\n\ntransporter.sendMail(message, function(err){\n    if(err){\n        console.log(err);\n    }\n});\n```\n\n执行即可发送成功！\n```\nnode mynodemailer.js\n```\n## 总结\n1. 如果SMPT服务器没有进行安全校验，那么一定要去掉用户与密码，这点要和java API区别，切记，为了这个问题，花费了一天代价。\n例如如下问题：\n```\n{ Error: no form of authorization supported\n    at module.exports (/data/truman/node/node_modules/emailjs/smtp/error.js:2:13)\n    at initiate (/data/truman/node/node_modules/emailjs/smtp/smtp.js:543:44)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at attempt (/data/truman/node/node_modules/emailjs/smtp/smtp.js:415:14)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:345:13)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:201:11)\n    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)\n    at Socket.response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:181:11)\n  code: 7,\n```\n2. 对于本地是否可以访问SMTP服务器，可以使用telnet,进行查验，如果都无法telnet通，肯定是无法发送邮件的。\n```\ntelnet smtp.sohu.com 25\n```","slug":"nodejs邮件发送","published":1,"updated":"2017-09-17T04:49:32.411Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvef007j8cees0yrdwcw","content":"<h1 id=\"nodejs邮件发送\"><a href=\"#nodejs邮件发送\" class=\"headerlink\" title=\"nodejs邮件发送\"></a>nodejs邮件发送</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>node.js发送邮件有多种第三方模块，比较有名的是<a href=\"https://github.com/eleith/emailjs\" target=\"_blank\" rel=\"noopener\">emailjs</a>,<a href=\"https://github.com/nodemailer/nodemailer\" target=\"_blank\" rel=\"noopener\">nodemailer</a>,个人感觉emailjs更轻量级，使用更简单，nodemailer关注人更多，功能更加完善。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>本次选用这两种，仅实现简单发送邮件功能，更多功能，请查询官网API.<br>首先新建文件夹node,然后在该文件夹下新建package.json文件，在其中增加以下配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;name&quot;: &quot;node_email_demo&quot;,</span><br><span class=\"line\">  &quot;description&quot;: &quot;application created by truman&quot;,</span><br><span class=\"line\">  &quot;version&quot;: &quot;1.0.0&quot;,</span><br><span class=\"line\">  &quot;dependencies&quot;: &#123;</span><br><span class=\"line\">    &quot;emailjs&quot;: &quot;^1.0.8&quot;,</span><br><span class=\"line\">    &quot;nodemailer&quot;: &quot;^4.1.0&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;repository&quot;: &quot;&quot;,</span><br><span class=\"line\">  &quot;license&quot;: &quot;MIT&quot;,</span><br><span class=\"line\">  &quot;engines&quot;: &#123;</span><br><span class=\"line\">    &quot;node&quot;: &quot;&gt;=6.0.0&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;readmeFilename&quot;: &quot;README.md&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在node 目录下执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install</span><br></pre></td></tr></table></figure>\n<ul>\n<li>emailjs<br>新建myemailjs.js文件，添加如下内容：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var email       = require(&quot;emailjs&quot;);</span><br><span class=\"line\">var server      = email.server.connect(&#123;</span><br><span class=\"line\">    user:    &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">    password:&quot;*********&quot;,</span><br><span class=\"line\">    host:    &quot;smtp.sohu.com&quot;,</span><br><span class=\"line\">    ssl:     false</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">// send the message and get a callback with an error or details of the message that was sent</span><br><span class=\"line\"> server.send(&#123;</span><br><span class=\"line\">     text:    &quot;i hope this works&quot;,</span><br><span class=\"line\">     from:    &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">     to:      &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">     subject: &quot;testing emailjs&quot;</span><br><span class=\"line\">     &#125;, function(err, message) &#123; console.log(err || message); &#125;);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>执行即可发送成功！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">node myemailjs.js</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>nodemailer<br>新建mynodemailer.js文件，添加如下内容：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const nodemailer=require(&quot;nodemailer&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">let transporter = nodemailer.createTransport(&#123;</span><br><span class=\"line\">    host: &apos;smtp.sohu.com&apos;,</span><br><span class=\"line\">    port: 25,</span><br><span class=\"line\">    auth: &#123;</span><br><span class=\"line\">            user: &quot;aibibang@sohu.com&quot;, // generated ethereal user</span><br><span class=\"line\">            pass: &quot;*********&quot;  // generated ethereal password</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    secure: false // true for 465, false for other ports</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\">var message = &#123;</span><br><span class=\"line\">    from: &apos;aibibang@sohu.com&apos;,</span><br><span class=\"line\">    to: &apos;aibibang@sohu.com&apos;,</span><br><span class=\"line\">    subject: &apos;nodemailer test&apos;,</span><br><span class=\"line\">    text: &apos;i hope this works&apos;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">transporter.sendMail(message, function(err)&#123;</span><br><span class=\"line\">    if(err)&#123;</span><br><span class=\"line\">        console.log(err);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>执行即可发送成功！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">node mynodemailer.js</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ol>\n<li><p>如果SMPT服务器没有进行安全校验，那么一定要去掉用户与密码，这点要和java API区别，切记，为了这个问题，花费了一天代价。<br>例如如下问题：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; Error: no form of authorization supported</span><br><span class=\"line\">    at module.exports (/data/truman/node/node_modules/emailjs/smtp/error.js:2:13)</span><br><span class=\"line\">    at initiate (/data/truman/node/node_modules/emailjs/smtp/smtp.js:543:44)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at attempt (/data/truman/node/node_modules/emailjs/smtp/smtp.js:415:14)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:345:13)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:201:11)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at Socket.response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:181:11)</span><br><span class=\"line\">  code: 7,</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对于本地是否可以访问SMTP服务器，可以使用telnet,进行查验，如果都无法telnet通，肯定是无法发送邮件的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">telnet smtp.sohu.com 25</span><br></pre></td></tr></table></figure></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"nodejs邮件发送\"><a href=\"#nodejs邮件发送\" class=\"headerlink\" title=\"nodejs邮件发送\"></a>nodejs邮件发送</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>node.js发送邮件有多种第三方模块，比较有名的是<a href=\"https://github.com/eleith/emailjs\" target=\"_blank\" rel=\"noopener\">emailjs</a>,<a href=\"https://github.com/nodemailer/nodemailer\" target=\"_blank\" rel=\"noopener\">nodemailer</a>,个人感觉emailjs更轻量级，使用更简单，nodemailer关注人更多，功能更加完善。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>本次选用这两种，仅实现简单发送邮件功能，更多功能，请查询官网API.<br>首先新建文件夹node,然后在该文件夹下新建package.json文件，在其中增加以下配置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;name&quot;: &quot;node_email_demo&quot;,</span><br><span class=\"line\">  &quot;description&quot;: &quot;application created by truman&quot;,</span><br><span class=\"line\">  &quot;version&quot;: &quot;1.0.0&quot;,</span><br><span class=\"line\">  &quot;dependencies&quot;: &#123;</span><br><span class=\"line\">    &quot;emailjs&quot;: &quot;^1.0.8&quot;,</span><br><span class=\"line\">    &quot;nodemailer&quot;: &quot;^4.1.0&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;repository&quot;: &quot;&quot;,</span><br><span class=\"line\">  &quot;license&quot;: &quot;MIT&quot;,</span><br><span class=\"line\">  &quot;engines&quot;: &#123;</span><br><span class=\"line\">    &quot;node&quot;: &quot;&gt;=6.0.0&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;readmeFilename&quot;: &quot;README.md&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>在node 目录下执行命令</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install</span><br></pre></td></tr></table></figure>\n<ul>\n<li>emailjs<br>新建myemailjs.js文件，添加如下内容：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var email       = require(&quot;emailjs&quot;);</span><br><span class=\"line\">var server      = email.server.connect(&#123;</span><br><span class=\"line\">    user:    &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">    password:&quot;*********&quot;,</span><br><span class=\"line\">    host:    &quot;smtp.sohu.com&quot;,</span><br><span class=\"line\">    ssl:     false</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">// send the message and get a callback with an error or details of the message that was sent</span><br><span class=\"line\"> server.send(&#123;</span><br><span class=\"line\">     text:    &quot;i hope this works&quot;,</span><br><span class=\"line\">     from:    &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">     to:      &quot;aibibang@sohu.com&quot;,</span><br><span class=\"line\">     subject: &quot;testing emailjs&quot;</span><br><span class=\"line\">     &#125;, function(err, message) &#123; console.log(err || message); &#125;);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>执行即可发送成功！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">node myemailjs.js</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>nodemailer<br>新建mynodemailer.js文件，添加如下内容：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const nodemailer=require(&quot;nodemailer&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">let transporter = nodemailer.createTransport(&#123;</span><br><span class=\"line\">    host: &apos;smtp.sohu.com&apos;,</span><br><span class=\"line\">    port: 25,</span><br><span class=\"line\">    auth: &#123;</span><br><span class=\"line\">            user: &quot;aibibang@sohu.com&quot;, // generated ethereal user</span><br><span class=\"line\">            pass: &quot;*********&quot;  // generated ethereal password</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    secure: false // true for 465, false for other ports</span><br><span class=\"line\">&#125;);</span><br><span class=\"line\">var message = &#123;</span><br><span class=\"line\">    from: &apos;aibibang@sohu.com&apos;,</span><br><span class=\"line\">    to: &apos;aibibang@sohu.com&apos;,</span><br><span class=\"line\">    subject: &apos;nodemailer test&apos;,</span><br><span class=\"line\">    text: &apos;i hope this works&apos;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">transporter.sendMail(message, function(err)&#123;</span><br><span class=\"line\">    if(err)&#123;</span><br><span class=\"line\">        console.log(err);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<p>执行即可发送成功！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">node mynodemailer.js</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ol>\n<li><p>如果SMPT服务器没有进行安全校验，那么一定要去掉用户与密码，这点要和java API区别，切记，为了这个问题，花费了一天代价。<br>例如如下问题：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123; Error: no form of authorization supported</span><br><span class=\"line\">    at module.exports (/data/truman/node/node_modules/emailjs/smtp/error.js:2:13)</span><br><span class=\"line\">    at initiate (/data/truman/node/node_modules/emailjs/smtp/smtp.js:543:44)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at attempt (/data/truman/node/node_modules/emailjs/smtp/smtp.js:415:14)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:345:13)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:201:11)</span><br><span class=\"line\">    at caller (/data/truman/node/node_modules/emailjs/smtp/smtp.js:48:14)</span><br><span class=\"line\">    at Socket.response (/data/truman/node/node_modules/emailjs/smtp/smtp.js:181:11)</span><br><span class=\"line\">  code: 7,</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>对于本地是否可以访问SMTP服务器，可以使用telnet,进行查验，如果都无法telnet通，肯定是无法发送邮件的。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">telnet smtp.sohu.com 25</span><br></pre></td></tr></table></figure></li>\n</ol>\n"},{"title":"oozie搭建及examples使用教程","date":"2017-06-01T13:32:08.000Z","_content":"# oozie搭建教程\n## 简介\nApache Oozie是Hadoop工作流调度框架。它是一个运行相关的作业工作流系统。这里，用户被允许创建向非循环图工作流程，其可以在并列 Hadoop 并顺序地运行。\n\n它由两部分组成：\n\n工作流引擎：一个工作流引擎的职责是存储和运行工作流程，由 Hadoop 作业组成：MapReduce, Pig, Hive.\n协调器引擎：它运行基于预定义的时间表和数据的可用性工作流程作业。\nOozie可扩展性和可管理及时执行成千上万的工作流程(每个由几十个作业)的Hadoop集群。\n\nOozie 也非常灵活。人们可以很容易启动，停止，暂停和重新运行作业。Oozie 可以很容易地重新运行失败的工作流。可以很容易重做因宕机或故障错过或失败的作业。甚至有可能跳过一个特定故障节点。\n## 部署\n\n### 编译\n\n```\nUnix box (tested on Mac OS X and Linux)\nJava JDK 1.7+\nMaven 3.0.1+\nHadoop 0.20.2+\nPig 0.7+\n```\n\n1.下载及解压\n```\n$ wget http://apache.fayea.com/oozie/4.3.0/oozie-4.3.0.tar.gz\n$ tar xvf oozie-4.3.0.tar.gz\n```\n2.编译\n\n进入解压后的目录 oozie-4.3.0\n```\n$ mvn clean package assembly:single -DskipTests\n```\n在国内的同学可以将中央仓库设成阿里云的地址，这样下载速度能快一点，部分下载不下来的，建议手动在mvnrepository.com中央仓库下载一下\n\n### 配置\n\n编译好的文件在以下路径\n```\noozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/\n```\n**1.环境变量配置**\n```\n$ vi /etc/profile\n```\n在profile文件中添加以下内容：\n```\nexport OOZIE_HOME=/oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0\nexport PATH=$PATH:$OOZIE_HOME/bin\n```\n执行以下命令生效\n```\n$ source /etc/profile\n```\n**2.hadoop 集群集成**\n\n修改core-site.xml，添加以下信息，该教程全程采用的是root用户，因此配置中填入的是root,如果是其他用户需要改成其他用户，**切记**，不然会报没有权限的错误的。修改该文件后，还需要重启hadoop集群，以便参数生效。\n```\n <property>\n  <name>hadoop.proxyuser.root.groups</name>\n  <value>*</value>\n</property>\n\n<property>\n  <name>hadoop.proxyuser.root.hosts</name>\n  <value>*</value>\n</property>\n```\n**3.oozie配置**\n\n以下配置默认在oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/ 目录下操作\n- 配置文件修改\n\n修改conf目录下oozie-site.xml文件，默认配置可以运行，但是运行hadoop job会报错的。主要是将hadoop集群的配置信息导入oozie中\n```\n <property>\n        <name>oozie.service.HadoopAccessorService.hadoop.configurations</name>\n        <value>*=/data/bigdata/hadoop-2.6.0-cdh5.7.0/etc/hadoop</value>\n        <description>\n            Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of\n            the Hadoop service (JobTracker, HDFS). The wildcard '*' configuration is\n            used when there is no exact match for an authority. The HADOOP_CONF_DIR contains\n            the relevant Hadoop *-site.xml files. If the path is relative is looked within\n            the Oozie configuration directory; though the path can be absolute (i.e. to point\n            to Hadoop client conf/ directories in the local filesystem.\n        </description>\n</property>\n```\n- ext\n\n新建libext目录\n```\n$ cd libext\n$ wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip\n$ cd ..\n```\n- 打包\n\n```\nbin/oozie-setup.sh prepare-war\n```\n\n- 复制依赖jar\n\n```\n$ cd oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/oozie-server/webapps/oozie/WEB-INF/lib/\n$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/lib/*.jar ./lib/\n$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/*.jar ./lib/\n\n```\n删除其中mr1的包，及jsp-api.jar，jasper-compiler-5.5.23.jar\n- 上传 examples\n\n上传examples及oozie-sharelib-4.3.0\n```\n$ tar xvf oozie-examples.tar.gz\n$ tar xvf oozie-sharelib-4.3.0.tar.gz\n$ hadoop fs -put examples examples\n$ hadoop fs -put share share\n```\n\n**4.运行**\n\n初始化数据库\n```\n$ bin/ooziedb.sh create -sqlfile oozie.sql -run\n```\n以下输出即为成功：\n```\nValidate DB Connection.\nDONE\nCheck DB schema does not exist\nDONE\nCheck OOZIE_SYS table does not exist\nDONE\nCreate SQL schema\nDONE\nDONE\nCreate OOZIE_SYS table\nDONE\nOozie DB has been created for Oozie version '4.3.0'\n```\n\n守护进程运行\n```\n$ bin/oozied.sh start\n```\n\n验证是否成功\n\n```\n$ bin/oozie admin -oozie http://localhost:11000/oozie -status\n```\n输出System mode: NORMAL即表示配置成功，或者在浏览器中打开\nhttp://localhost:11000/oozie/\n\n### examples运行\n\n1.执行map-reduce\n```\n$ oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run\n```\n2.验证结果\n\n```\n$ oozie job -oozie http://localhost:11000/oozie -info 0000007-161223101553230-oozie-root-W\n\n```\n如下结果则证明成功\n\n```\nJob ID : 0000007-161223101553230-oozie-root-W\n------------------------------------------------------------------------------------------------------------------------------------\nWorkflow Name : map-reduce-wf\nApp Path      : hdfs://192.168.0.105:8020/user/root/examples/apps/map-reduce/workflow.xml\nStatus        : SUCCEEDED\nRun           : 0\nUser          : root\nGroup         : -\nCreated       : 2016-12-23 08:40 GMT\nStarted       : 2016-12-23 08:40 GMT\nLast Modified : 2016-12-23 08:41 GMT\nEnded         : 2016-12-23 08:41 GMT\nCoordAction ID: -\n\nActions\n------------------------------------------------------------------------------------------------------------------------------------\nID                                                                            Status    Ext ID                 Ext Status Err Code\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@:start:                                  OK        -                      OK         -\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@mr-node                                  OK        job_1482454814338_0025 SUCCEEDED  -\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@end                                      OK        -                      OK         -\n------------------------------------------------------------------------------------------------------------------------------------\n\n```\n","source":"_posts/oozie搭建及examples使用教程.md","raw":"---\ntitle: oozie搭建及examples使用教程\ndate: 2017-06-01 21:32:08\ntags: 大数据\ncategories:\n- oozie\n---\n# oozie搭建教程\n## 简介\nApache Oozie是Hadoop工作流调度框架。它是一个运行相关的作业工作流系统。这里，用户被允许创建向非循环图工作流程，其可以在并列 Hadoop 并顺序地运行。\n\n它由两部分组成：\n\n工作流引擎：一个工作流引擎的职责是存储和运行工作流程，由 Hadoop 作业组成：MapReduce, Pig, Hive.\n协调器引擎：它运行基于预定义的时间表和数据的可用性工作流程作业。\nOozie可扩展性和可管理及时执行成千上万的工作流程(每个由几十个作业)的Hadoop集群。\n\nOozie 也非常灵活。人们可以很容易启动，停止，暂停和重新运行作业。Oozie 可以很容易地重新运行失败的工作流。可以很容易重做因宕机或故障错过或失败的作业。甚至有可能跳过一个特定故障节点。\n## 部署\n\n### 编译\n\n```\nUnix box (tested on Mac OS X and Linux)\nJava JDK 1.7+\nMaven 3.0.1+\nHadoop 0.20.2+\nPig 0.7+\n```\n\n1.下载及解压\n```\n$ wget http://apache.fayea.com/oozie/4.3.0/oozie-4.3.0.tar.gz\n$ tar xvf oozie-4.3.0.tar.gz\n```\n2.编译\n\n进入解压后的目录 oozie-4.3.0\n```\n$ mvn clean package assembly:single -DskipTests\n```\n在国内的同学可以将中央仓库设成阿里云的地址，这样下载速度能快一点，部分下载不下来的，建议手动在mvnrepository.com中央仓库下载一下\n\n### 配置\n\n编译好的文件在以下路径\n```\noozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/\n```\n**1.环境变量配置**\n```\n$ vi /etc/profile\n```\n在profile文件中添加以下内容：\n```\nexport OOZIE_HOME=/oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0\nexport PATH=$PATH:$OOZIE_HOME/bin\n```\n执行以下命令生效\n```\n$ source /etc/profile\n```\n**2.hadoop 集群集成**\n\n修改core-site.xml，添加以下信息，该教程全程采用的是root用户，因此配置中填入的是root,如果是其他用户需要改成其他用户，**切记**，不然会报没有权限的错误的。修改该文件后，还需要重启hadoop集群，以便参数生效。\n```\n <property>\n  <name>hadoop.proxyuser.root.groups</name>\n  <value>*</value>\n</property>\n\n<property>\n  <name>hadoop.proxyuser.root.hosts</name>\n  <value>*</value>\n</property>\n```\n**3.oozie配置**\n\n以下配置默认在oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/ 目录下操作\n- 配置文件修改\n\n修改conf目录下oozie-site.xml文件，默认配置可以运行，但是运行hadoop job会报错的。主要是将hadoop集群的配置信息导入oozie中\n```\n <property>\n        <name>oozie.service.HadoopAccessorService.hadoop.configurations</name>\n        <value>*=/data/bigdata/hadoop-2.6.0-cdh5.7.0/etc/hadoop</value>\n        <description>\n            Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of\n            the Hadoop service (JobTracker, HDFS). The wildcard '*' configuration is\n            used when there is no exact match for an authority. The HADOOP_CONF_DIR contains\n            the relevant Hadoop *-site.xml files. If the path is relative is looked within\n            the Oozie configuration directory; though the path can be absolute (i.e. to point\n            to Hadoop client conf/ directories in the local filesystem.\n        </description>\n</property>\n```\n- ext\n\n新建libext目录\n```\n$ cd libext\n$ wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip\n$ cd ..\n```\n- 打包\n\n```\nbin/oozie-setup.sh prepare-war\n```\n\n- 复制依赖jar\n\n```\n$ cd oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/oozie-server/webapps/oozie/WEB-INF/lib/\n$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/lib/*.jar ./lib/\n$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/*.jar ./lib/\n\n```\n删除其中mr1的包，及jsp-api.jar，jasper-compiler-5.5.23.jar\n- 上传 examples\n\n上传examples及oozie-sharelib-4.3.0\n```\n$ tar xvf oozie-examples.tar.gz\n$ tar xvf oozie-sharelib-4.3.0.tar.gz\n$ hadoop fs -put examples examples\n$ hadoop fs -put share share\n```\n\n**4.运行**\n\n初始化数据库\n```\n$ bin/ooziedb.sh create -sqlfile oozie.sql -run\n```\n以下输出即为成功：\n```\nValidate DB Connection.\nDONE\nCheck DB schema does not exist\nDONE\nCheck OOZIE_SYS table does not exist\nDONE\nCreate SQL schema\nDONE\nDONE\nCreate OOZIE_SYS table\nDONE\nOozie DB has been created for Oozie version '4.3.0'\n```\n\n守护进程运行\n```\n$ bin/oozied.sh start\n```\n\n验证是否成功\n\n```\n$ bin/oozie admin -oozie http://localhost:11000/oozie -status\n```\n输出System mode: NORMAL即表示配置成功，或者在浏览器中打开\nhttp://localhost:11000/oozie/\n\n### examples运行\n\n1.执行map-reduce\n```\n$ oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run\n```\n2.验证结果\n\n```\n$ oozie job -oozie http://localhost:11000/oozie -info 0000007-161223101553230-oozie-root-W\n\n```\n如下结果则证明成功\n\n```\nJob ID : 0000007-161223101553230-oozie-root-W\n------------------------------------------------------------------------------------------------------------------------------------\nWorkflow Name : map-reduce-wf\nApp Path      : hdfs://192.168.0.105:8020/user/root/examples/apps/map-reduce/workflow.xml\nStatus        : SUCCEEDED\nRun           : 0\nUser          : root\nGroup         : -\nCreated       : 2016-12-23 08:40 GMT\nStarted       : 2016-12-23 08:40 GMT\nLast Modified : 2016-12-23 08:41 GMT\nEnded         : 2016-12-23 08:41 GMT\nCoordAction ID: -\n\nActions\n------------------------------------------------------------------------------------------------------------------------------------\nID                                                                            Status    Ext ID                 Ext Status Err Code\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@:start:                                  OK        -                      OK         -\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@mr-node                                  OK        job_1482454814338_0025 SUCCEEDED  -\n------------------------------------------------------------------------------------------------------------------------------------\n0000007-161223101553230-oozie-root-W@end                                      OK        -                      OK         -\n------------------------------------------------------------------------------------------------------------------------------------\n\n```\n","slug":"oozie搭建及examples使用教程","published":1,"updated":"2017-06-01T13:38:06.996Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobveh007n8ceeqkgp220t","content":"<h1 id=\"oozie搭建教程\"><a href=\"#oozie搭建教程\" class=\"headerlink\" title=\"oozie搭建教程\"></a>oozie搭建教程</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Apache Oozie是Hadoop工作流调度框架。它是一个运行相关的作业工作流系统。这里，用户被允许创建向非循环图工作流程，其可以在并列 Hadoop 并顺序地运行。</p>\n<p>它由两部分组成：</p>\n<p>工作流引擎：一个工作流引擎的职责是存储和运行工作流程，由 Hadoop 作业组成：MapReduce, Pig, Hive.<br>协调器引擎：它运行基于预定义的时间表和数据的可用性工作流程作业。<br>Oozie可扩展性和可管理及时执行成千上万的工作流程(每个由几十个作业)的Hadoop集群。</p>\n<p>Oozie 也非常灵活。人们可以很容易启动，停止，暂停和重新运行作业。Oozie 可以很容易地重新运行失败的工作流。可以很容易重做因宕机或故障错过或失败的作业。甚至有可能跳过一个特定故障节点。</p>\n<h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><h3 id=\"编译\"><a href=\"#编译\" class=\"headerlink\" title=\"编译\"></a>编译</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Unix box (tested on Mac OS X and Linux)</span><br><span class=\"line\">Java JDK 1.7+</span><br><span class=\"line\">Maven 3.0.1+</span><br><span class=\"line\">Hadoop 0.20.2+</span><br><span class=\"line\">Pig 0.7+</span><br></pre></td></tr></table></figure>\n<p>1.下载及解压<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget http://apache.fayea.com/oozie/4.3.0/oozie-4.3.0.tar.gz</span><br><span class=\"line\">$ tar xvf oozie-4.3.0.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>2.编译</p>\n<p>进入解压后的目录 oozie-4.3.0<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mvn clean package assembly:single -DskipTests</span><br></pre></td></tr></table></figure></p>\n<p>在国内的同学可以将中央仓库设成阿里云的地址，这样下载速度能快一点，部分下载不下来的，建议手动在mvnrepository.com中央仓库下载一下</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>编译好的文件在以下路径<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/</span><br></pre></td></tr></table></figure></p>\n<p><strong>1.环境变量配置</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vi /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>在profile文件中添加以下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export OOZIE_HOME=/oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0</span><br><span class=\"line\">export PATH=$PATH:$OOZIE_HOME/bin</span><br></pre></td></tr></table></figure></p>\n<p>执行以下命令生效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p><strong>2.hadoop 集群集成</strong></p>\n<p>修改core-site.xml，添加以下信息，该教程全程采用的是root用户，因此配置中填入的是root,如果是其他用户需要改成其他用户，<strong>切记</strong>，不然会报没有权限的错误的。修改该文件后，还需要重启hadoop集群，以便参数生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p><strong>3.oozie配置</strong></p>\n<p>以下配置默认在oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/ 目录下操作</p>\n<ul>\n<li>配置文件修改</li>\n</ul>\n<p>修改conf目录下oozie-site.xml文件，默认配置可以运行，但是运行hadoop job会报错的。主要是将hadoop集群的配置信息导入oozie中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;*=/data/bigdata/hadoop-2.6.0-cdh5.7.0/etc/hadoop&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;</span><br><span class=\"line\">            Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of</span><br><span class=\"line\">            the Hadoop service (JobTracker, HDFS). The wildcard &apos;*&apos; configuration is</span><br><span class=\"line\">            used when there is no exact match for an authority. The HADOOP_CONF_DIR contains</span><br><span class=\"line\">            the relevant Hadoop *-site.xml files. If the path is relative is looked within</span><br><span class=\"line\">            the Oozie configuration directory; though the path can be absolute (i.e. to point</span><br><span class=\"line\">            to Hadoop client conf/ directories in the local filesystem.</span><br><span class=\"line\">        &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>ext</li>\n</ul>\n<p>新建libext目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd libext</span><br><span class=\"line\">$ wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip</span><br><span class=\"line\">$ cd ..</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>打包</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/oozie-setup.sh prepare-war</span><br></pre></td></tr></table></figure>\n<ul>\n<li>复制依赖jar</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/oozie-server/webapps/oozie/WEB-INF/lib/</span><br><span class=\"line\">$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/lib/*.jar ./lib/</span><br><span class=\"line\">$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/*.jar ./lib/</span><br></pre></td></tr></table></figure>\n<p>删除其中mr1的包，及jsp-api.jar，jasper-compiler-5.5.23.jar</p>\n<ul>\n<li>上传 examples</li>\n</ul>\n<p>上传examples及oozie-sharelib-4.3.0<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ tar xvf oozie-examples.tar.gz</span><br><span class=\"line\">$ tar xvf oozie-sharelib-4.3.0.tar.gz</span><br><span class=\"line\">$ hadoop fs -put examples examples</span><br><span class=\"line\">$ hadoop fs -put share share</span><br></pre></td></tr></table></figure></p>\n<p><strong>4.运行</strong></p>\n<p>初始化数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/ooziedb.sh create -sqlfile oozie.sql -run</span><br></pre></td></tr></table></figure></p>\n<p>以下输出即为成功：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Validate DB Connection.</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Check DB schema does not exist</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Check OOZIE_SYS table does not exist</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Create SQL schema</span><br><span class=\"line\">DONE</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Create OOZIE_SYS table</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Oozie DB has been created for Oozie version &apos;4.3.0&apos;</span><br></pre></td></tr></table></figure></p>\n<p>守护进程运行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/oozied.sh start</span><br></pre></td></tr></table></figure></p>\n<p>验证是否成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/oozie admin -oozie http://localhost:11000/oozie -status</span><br></pre></td></tr></table></figure>\n<p>输出System mode: NORMAL即表示配置成功，或者在浏览器中打开<br><a href=\"http://localhost:11000/oozie/\" target=\"_blank\" rel=\"noopener\">http://localhost:11000/oozie/</a></p>\n<h3 id=\"examples运行\"><a href=\"#examples运行\" class=\"headerlink\" title=\"examples运行\"></a>examples运行</h3><p>1.执行map-reduce<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run</span><br></pre></td></tr></table></figure></p>\n<p>2.验证结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ oozie job -oozie http://localhost:11000/oozie -info 0000007-161223101553230-oozie-root-W</span><br></pre></td></tr></table></figure>\n<p>如下结果则证明成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Job ID : 0000007-161223101553230-oozie-root-W</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">Workflow Name : map-reduce-wf</span><br><span class=\"line\">App Path      : hdfs://192.168.0.105:8020/user/root/examples/apps/map-reduce/workflow.xml</span><br><span class=\"line\">Status        : SUCCEEDED</span><br><span class=\"line\">Run           : 0</span><br><span class=\"line\">User          : root</span><br><span class=\"line\">Group         : -</span><br><span class=\"line\">Created       : 2016-12-23 08:40 GMT</span><br><span class=\"line\">Started       : 2016-12-23 08:40 GMT</span><br><span class=\"line\">Last Modified : 2016-12-23 08:41 GMT</span><br><span class=\"line\">Ended         : 2016-12-23 08:41 GMT</span><br><span class=\"line\">CoordAction ID: -</span><br><span class=\"line\"></span><br><span class=\"line\">Actions</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">ID                                                                            Status    Ext ID                 Ext Status Err Code</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@:start:                                  OK        -                      OK         -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@mr-node                                  OK        job_1482454814338_0025 SUCCEEDED  -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@end                                      OK        -                      OK         -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"oozie搭建教程\"><a href=\"#oozie搭建教程\" class=\"headerlink\" title=\"oozie搭建教程\"></a>oozie搭建教程</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Apache Oozie是Hadoop工作流调度框架。它是一个运行相关的作业工作流系统。这里，用户被允许创建向非循环图工作流程，其可以在并列 Hadoop 并顺序地运行。</p>\n<p>它由两部分组成：</p>\n<p>工作流引擎：一个工作流引擎的职责是存储和运行工作流程，由 Hadoop 作业组成：MapReduce, Pig, Hive.<br>协调器引擎：它运行基于预定义的时间表和数据的可用性工作流程作业。<br>Oozie可扩展性和可管理及时执行成千上万的工作流程(每个由几十个作业)的Hadoop集群。</p>\n<p>Oozie 也非常灵活。人们可以很容易启动，停止，暂停和重新运行作业。Oozie 可以很容易地重新运行失败的工作流。可以很容易重做因宕机或故障错过或失败的作业。甚至有可能跳过一个特定故障节点。</p>\n<h2 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h2><h3 id=\"编译\"><a href=\"#编译\" class=\"headerlink\" title=\"编译\"></a>编译</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Unix box (tested on Mac OS X and Linux)</span><br><span class=\"line\">Java JDK 1.7+</span><br><span class=\"line\">Maven 3.0.1+</span><br><span class=\"line\">Hadoop 0.20.2+</span><br><span class=\"line\">Pig 0.7+</span><br></pre></td></tr></table></figure>\n<p>1.下载及解压<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget http://apache.fayea.com/oozie/4.3.0/oozie-4.3.0.tar.gz</span><br><span class=\"line\">$ tar xvf oozie-4.3.0.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>2.编译</p>\n<p>进入解压后的目录 oozie-4.3.0<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mvn clean package assembly:single -DskipTests</span><br></pre></td></tr></table></figure></p>\n<p>在国内的同学可以将中央仓库设成阿里云的地址，这样下载速度能快一点，部分下载不下来的，建议手动在mvnrepository.com中央仓库下载一下</p>\n<h3 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>编译好的文件在以下路径<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/</span><br></pre></td></tr></table></figure></p>\n<p><strong>1.环境变量配置</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vi /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p>在profile文件中添加以下内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export OOZIE_HOME=/oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0</span><br><span class=\"line\">export PATH=$PATH:$OOZIE_HOME/bin</span><br></pre></td></tr></table></figure></p>\n<p>执行以下命令生效<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ source /etc/profile</span><br></pre></td></tr></table></figure></p>\n<p><strong>2.hadoop 集群集成</strong></p>\n<p>修改core-site.xml，添加以下信息，该教程全程采用的是root用户，因此配置中填入的是root,如果是其他用户需要改成其他用户，<strong>切记</strong>，不然会报没有权限的错误的。修改该文件后，还需要重启hadoop集群，以便参数生效。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class=\"line\">  &lt;value&gt;*&lt;/value&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<p><strong>3.oozie配置</strong></p>\n<p>以下配置默认在oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/ 目录下操作</p>\n<ul>\n<li>配置文件修改</li>\n</ul>\n<p>修改conf目录下oozie-site.xml文件，默认配置可以运行，但是运行hadoop job会报错的。主要是将hadoop集群的配置信息导入oozie中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;*=/data/bigdata/hadoop-2.6.0-cdh5.7.0/etc/hadoop&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;</span><br><span class=\"line\">            Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of</span><br><span class=\"line\">            the Hadoop service (JobTracker, HDFS). The wildcard &apos;*&apos; configuration is</span><br><span class=\"line\">            used when there is no exact match for an authority. The HADOOP_CONF_DIR contains</span><br><span class=\"line\">            the relevant Hadoop *-site.xml files. If the path is relative is looked within</span><br><span class=\"line\">            the Oozie configuration directory; though the path can be absolute (i.e. to point</span><br><span class=\"line\">            to Hadoop client conf/ directories in the local filesystem.</span><br><span class=\"line\">        &lt;/description&gt;</span><br><span class=\"line\">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>ext</li>\n</ul>\n<p>新建libext目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd libext</span><br><span class=\"line\">$ wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip</span><br><span class=\"line\">$ cd ..</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>打包</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/oozie-setup.sh prepare-war</span><br></pre></td></tr></table></figure>\n<ul>\n<li>复制依赖jar</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd oozie-4.3.0/distro/target/oozie-4.3.0-distro/oozie-4.3.0/oozie-server/webapps/oozie/WEB-INF/lib/</span><br><span class=\"line\">$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/lib/*.jar ./lib/</span><br><span class=\"line\">$ cp -rf  /data/bigdata/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce1/*.jar ./lib/</span><br></pre></td></tr></table></figure>\n<p>删除其中mr1的包，及jsp-api.jar，jasper-compiler-5.5.23.jar</p>\n<ul>\n<li>上传 examples</li>\n</ul>\n<p>上传examples及oozie-sharelib-4.3.0<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ tar xvf oozie-examples.tar.gz</span><br><span class=\"line\">$ tar xvf oozie-sharelib-4.3.0.tar.gz</span><br><span class=\"line\">$ hadoop fs -put examples examples</span><br><span class=\"line\">$ hadoop fs -put share share</span><br></pre></td></tr></table></figure></p>\n<p><strong>4.运行</strong></p>\n<p>初始化数据库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/ooziedb.sh create -sqlfile oozie.sql -run</span><br></pre></td></tr></table></figure></p>\n<p>以下输出即为成功：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Validate DB Connection.</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Check DB schema does not exist</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Check OOZIE_SYS table does not exist</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Create SQL schema</span><br><span class=\"line\">DONE</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Create OOZIE_SYS table</span><br><span class=\"line\">DONE</span><br><span class=\"line\">Oozie DB has been created for Oozie version &apos;4.3.0&apos;</span><br></pre></td></tr></table></figure></p>\n<p>守护进程运行<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/oozied.sh start</span><br></pre></td></tr></table></figure></p>\n<p>验证是否成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/oozie admin -oozie http://localhost:11000/oozie -status</span><br></pre></td></tr></table></figure>\n<p>输出System mode: NORMAL即表示配置成功，或者在浏览器中打开<br><a href=\"http://localhost:11000/oozie/\" target=\"_blank\" rel=\"noopener\">http://localhost:11000/oozie/</a></p>\n<h3 id=\"examples运行\"><a href=\"#examples运行\" class=\"headerlink\" title=\"examples运行\"></a>examples运行</h3><p>1.执行map-reduce<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ oozie job -oozie http://localhost:11000/oozie -config examples/apps/map-reduce/job.properties -run</span><br></pre></td></tr></table></figure></p>\n<p>2.验证结果</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ oozie job -oozie http://localhost:11000/oozie -info 0000007-161223101553230-oozie-root-W</span><br></pre></td></tr></table></figure>\n<p>如下结果则证明成功</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Job ID : 0000007-161223101553230-oozie-root-W</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">Workflow Name : map-reduce-wf</span><br><span class=\"line\">App Path      : hdfs://192.168.0.105:8020/user/root/examples/apps/map-reduce/workflow.xml</span><br><span class=\"line\">Status        : SUCCEEDED</span><br><span class=\"line\">Run           : 0</span><br><span class=\"line\">User          : root</span><br><span class=\"line\">Group         : -</span><br><span class=\"line\">Created       : 2016-12-23 08:40 GMT</span><br><span class=\"line\">Started       : 2016-12-23 08:40 GMT</span><br><span class=\"line\">Last Modified : 2016-12-23 08:41 GMT</span><br><span class=\"line\">Ended         : 2016-12-23 08:41 GMT</span><br><span class=\"line\">CoordAction ID: -</span><br><span class=\"line\"></span><br><span class=\"line\">Actions</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">ID                                                                            Status    Ext ID                 Ext Status Err Code</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@:start:                                  OK        -                      OK         -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@mr-node                                  OK        job_1482454814338_0025 SUCCEEDED  -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br><span class=\"line\">0000007-161223101553230-oozie-root-W@end                                      OK        -                      OK         -</span><br><span class=\"line\">------------------------------------------------------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n"},{"title":"node.js读取json文件","date":"2017-09-17T04:27:15.000Z","_content":"# node.js读取json文件\n## 目的\n在node.js项目中如何获取json文件，本篇文章主要讲述两个方面：1.客户端 2.服务端。\n\n## 实践\n本次采用web框架为express\n\n1.客户端\n目录结构为：\n```\n/\n/public/conf/demo.json\n/index.js\n/package.json\n/index.html\n```\nindex.js内容如下：\n```\nvar express = require('express');\nvar app = express();\napp.use(express.static('public'));\napp.listen(80, function () {\n  console.log('Example app listening on port 80!');\n});\n```\n在index.html中获取如下：\n```\n$.getJSON('conf/demo.json', function (data) {\n\t\t\tconsole.log(data);\n\t})\n```\n\n2.服务端\nindex.js内容如下：\n```\nvar express = require('express');\nvar app = express();\nvar CONFPATH = \"./public/conf/demo.json\";\nvar fs = require('fs');\nvar result = JSON.parse(fs.readFileSync(CONFPATH));\n\tconsole.log(result);\napp.listen(80, function () {\n  console.log('Example app listening on port 80!');\n});\n```","source":"_posts/node-js读取json文件.md","raw":"---\ntitle: node.js读取json文件\ndate: 2017-09-17 12:27:15\ntags: 开发笔记\ncategories:\n- node.js\n---\n# node.js读取json文件\n## 目的\n在node.js项目中如何获取json文件，本篇文章主要讲述两个方面：1.客户端 2.服务端。\n\n## 实践\n本次采用web框架为express\n\n1.客户端\n目录结构为：\n```\n/\n/public/conf/demo.json\n/index.js\n/package.json\n/index.html\n```\nindex.js内容如下：\n```\nvar express = require('express');\nvar app = express();\napp.use(express.static('public'));\napp.listen(80, function () {\n  console.log('Example app listening on port 80!');\n});\n```\n在index.html中获取如下：\n```\n$.getJSON('conf/demo.json', function (data) {\n\t\t\tconsole.log(data);\n\t})\n```\n\n2.服务端\nindex.js内容如下：\n```\nvar express = require('express');\nvar app = express();\nvar CONFPATH = \"./public/conf/demo.json\";\nvar fs = require('fs');\nvar result = JSON.parse(fs.readFileSync(CONFPATH));\n\tconsole.log(result);\napp.listen(80, function () {\n  console.log('Example app listening on port 80!');\n});\n```","slug":"node-js读取json文件","published":1,"updated":"2017-09-17T04:51:01.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvej007q8cee1p50d8js","content":"<h1 id=\"node-js读取json文件\"><a href=\"#node-js读取json文件\" class=\"headerlink\" title=\"node.js读取json文件\"></a>node.js读取json文件</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>在node.js项目中如何获取json文件，本篇文章主要讲述两个方面：1.客户端 2.服务端。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>本次采用web框架为express</p>\n<p>1.客户端<br>目录结构为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/</span><br><span class=\"line\">/public/conf/demo.json</span><br><span class=\"line\">/index.js</span><br><span class=\"line\">/package.json</span><br><span class=\"line\">/index.html</span><br></pre></td></tr></table></figure></p>\n<p>index.js内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">app.use(express.static(&apos;public&apos;));</span><br><span class=\"line\">app.listen(80, function () &#123;</span><br><span class=\"line\">  console.log(&apos;Example app listening on port 80!&apos;);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>在index.html中获取如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$.getJSON(&apos;conf/demo.json&apos;, function (data) &#123;</span><br><span class=\"line\">\t\t\tconsole.log(data);</span><br><span class=\"line\">\t&#125;)</span><br></pre></td></tr></table></figure></p>\n<p>2.服务端<br>index.js内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">var CONFPATH = &quot;./public/conf/demo.json&quot;;</span><br><span class=\"line\">var fs = require(&apos;fs&apos;);</span><br><span class=\"line\">var result = JSON.parse(fs.readFileSync(CONFPATH));</span><br><span class=\"line\">\tconsole.log(result);</span><br><span class=\"line\">app.listen(80, function () &#123;</span><br><span class=\"line\">  console.log(&apos;Example app listening on port 80!&apos;);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"node-js读取json文件\"><a href=\"#node-js读取json文件\" class=\"headerlink\" title=\"node.js读取json文件\"></a>node.js读取json文件</h1><h2 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h2><p>在node.js项目中如何获取json文件，本篇文章主要讲述两个方面：1.客户端 2.服务端。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>本次采用web框架为express</p>\n<p>1.客户端<br>目录结构为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/</span><br><span class=\"line\">/public/conf/demo.json</span><br><span class=\"line\">/index.js</span><br><span class=\"line\">/package.json</span><br><span class=\"line\">/index.html</span><br></pre></td></tr></table></figure></p>\n<p>index.js内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">app.use(express.static(&apos;public&apos;));</span><br><span class=\"line\">app.listen(80, function () &#123;</span><br><span class=\"line\">  console.log(&apos;Example app listening on port 80!&apos;);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>在index.html中获取如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$.getJSON(&apos;conf/demo.json&apos;, function (data) &#123;</span><br><span class=\"line\">\t\t\tconsole.log(data);</span><br><span class=\"line\">\t&#125;)</span><br></pre></td></tr></table></figure></p>\n<p>2.服务端<br>index.js内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">var CONFPATH = &quot;./public/conf/demo.json&quot;;</span><br><span class=\"line\">var fs = require(&apos;fs&apos;);</span><br><span class=\"line\">var result = JSON.parse(fs.readFileSync(CONFPATH));</span><br><span class=\"line\">\tconsole.log(result);</span><br><span class=\"line\">app.listen(80, function () &#123;</span><br><span class=\"line\">  console.log(&apos;Example app listening on port 80!&apos;);</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n"},{"title":"spring学习（一）","date":"2016-07-10T02:38:29.000Z","_content":"# 一、注解解释\n1. @controller 控制器（注入服务）\n2. @service 服务（注入dao）\n3. @repository dao（实现dao访问）\n4. @component （把普通pojo实例化到spring容器中，相当于配置文件中的<bean id=\"\" class=\"\"/>）\n\n  @Component,@Service,@Controller,@Repository注解的类，并把这些类纳入进spring容器中管理。 \n下面写这个是引入component的扫描组件 \n```\n<context:component-scan base-package=”com.mmnc”>   \n```\n\n其中base-package为需要扫描的包（含所有子包） \n- @Service用于标注业务层组件 \n- @Controller用于标注控制层组件(如struts中的action) \n- @Repository用于标注数据访问组件，即DAO组件. \n- @Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 \n\n","source":"_posts/spring学习（一）.md","raw":"---\ntitle: spring学习（一）\ndate: 2016-07-10 10:38:29\ntags: 开发笔记\ncategories:\n- spring\n---\n# 一、注解解释\n1. @controller 控制器（注入服务）\n2. @service 服务（注入dao）\n3. @repository dao（实现dao访问）\n4. @component （把普通pojo实例化到spring容器中，相当于配置文件中的<bean id=\"\" class=\"\"/>）\n\n  @Component,@Service,@Controller,@Repository注解的类，并把这些类纳入进spring容器中管理。 \n下面写这个是引入component的扫描组件 \n```\n<context:component-scan base-package=”com.mmnc”>   \n```\n\n其中base-package为需要扫描的包（含所有子包） \n- @Service用于标注业务层组件 \n- @Controller用于标注控制层组件(如struts中的action) \n- @Repository用于标注数据访问组件，即DAO组件. \n- @Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 \n\n","slug":"spring学习（一）","published":1,"updated":"2016-07-10T02:45:01.084Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvel007u8cee4knz3i7e","content":"<h1 id=\"一、注解解释\"><a href=\"#一、注解解释\" class=\"headerlink\" title=\"一、注解解释\"></a>一、注解解释</h1><ol>\n<li>@controller 控制器（注入服务）</li>\n<li>@service 服务（注入dao）</li>\n<li>@repository dao（实现dao访问）</li>\n<li><p>@component （把普通pojo实例化到spring容器中，相当于配置文件中的<bean id=\"\" class=\"\">）</bean></p>\n<p>@Component,@Service,@Controller,@Repository注解的类，并把这些类纳入进spring容器中管理。<br>下面写这个是引入component的扫描组件 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;context:component-scan base-package=”com.mmnc”&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其中base-package为需要扫描的包（含所有子包） </p>\n<ul>\n<li>@Service用于标注业务层组件 </li>\n<li>@Controller用于标注控制层组件(如struts中的action) </li>\n<li>@Repository用于标注数据访问组件，即DAO组件. </li>\n<li>@Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"一、注解解释\"><a href=\"#一、注解解释\" class=\"headerlink\" title=\"一、注解解释\"></a>一、注解解释</h1><ol>\n<li>@controller 控制器（注入服务）</li>\n<li>@service 服务（注入dao）</li>\n<li>@repository dao（实现dao访问）</li>\n<li><p>@component （把普通pojo实例化到spring容器中，相当于配置文件中的<bean id=\"\" class=\"\">）</bean></p>\n<p>@Component,@Service,@Controller,@Repository注解的类，并把这些类纳入进spring容器中管理。<br>下面写这个是引入component的扫描组件 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;context:component-scan base-package=”com.mmnc”&gt;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>其中base-package为需要扫描的包（含所有子包） </p>\n<ul>\n<li>@Service用于标注业务层组件 </li>\n<li>@Controller用于标注控制层组件(如struts中的action) </li>\n<li>@Repository用于标注数据访问组件，即DAO组件. </li>\n<li>@Component泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 </li>\n</ul>\n"},{"title":"storm集群搭建","date":"2016-04-15T12:24:41.000Z","_content":"# storm集群搭建\n## 前言\n&#160; &#160; &#160; &#160;Storm 是Twitter的一个开源框架。Storm一个分布式的、容错的实时计算系统。Twitter Storm集群表面上类似于Hadoop集群，Hadoop上运行的是MapReduce Jobs，而Storm运行topologies；但是其本身有很大的区别，最主要的区别在于，Hadoop MapReduce Job运行最终会完结，而Storm topologies处理数据进程理论上是永久存活的，除非你将其Kill掉。\nStorm集群中包含两类节点：主控节点（Master Node）和工作节点（Work Node）。其分别对应的角色如下：\n1.  主控节点（Master Node）上运行一个被称为Nimbus的后台程序，它负责在Storm集群内分发代码，分配任务给工作机器，并且负责监控集群运行状态。Nimbus的作用类似于Hadoop中JobTracker的角色。\n2.  每个工作节点（Work Node）上运行一个被称为Supervisor的后台程序。Supervisor负责监听从Nimbus分配给它执行的任务，据此启动或停止执行任务的工作进程。每一个工作进程执行一个Topology的子集；一个运行中的Topology由分布在不同工作节点上的多个工作进程组成。\n\n## 搭建\n### 准备软件\n```\n1.JDK1.8\n2.zookeeper-3.4.5-cdh5.6.0\n3.Storm0.9.5\n```\n### JDK安装\n详见[略](http:trumandu.github.io/2016/04/15/linux环境jdk安装及配置/)\n### Zookeeper集群搭建\n1.解压\n```\ntar xvf zookeeper-3.4.5-cdh5.6.0.tar.gz\n```\n2.修改配置\n添加修改vonf/zoo.cfg\n```\ncp zoo_sample.cfg zoo.cfg\n```\n修改zoo.cfg\n```\ndataDir=/data/truman/zookeeper-3.4.5-cdh5.6.0/data\nserver.1=lab1:2888:3888\nserver.2=lab2:2888:3888\nserver.3=lab3:2888:3888\n```\n在dataDir下新增myid文件，内容为相对应的server后面的数字\n3.分发远程主机\n```\nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab30:/data/truman/ \nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab31:/data/truman/ \nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab29:/data/truman/ \n```\n4.启动和停止\n启动命令\n```\nzkServer.sh start\n```\n停止\n```\nzkServer.sh stop\n```\n### storm集群安装\n在nimbus与supervisor节点上重复以下操作\n1.修改配置\n- storm.zookeeper.servers: 因为Storm所有的信息都是存储在Zookeeper中的，所以要指定Zookeeper服务器的地址\n```\nstorm.zookeeper.servers:\n -  \"192.168.0.2\"\n -  \"192.168.0.3\"\n -  \"192.168.0.4\"\n```\n- storm.local.dir:\nNimbus和 Supervisor守护进程需要一个目录来存储一些状态信息，例如（ jars, confs, and things like that ）\n```\n storm.local.dir: \"/data/storm\"\n```\n-  nimbus.host:\nworker需要知道那一台机器是master，从而可以下载 topology jars 和confs\n```\nnimbus.host: \"192.168.0.2\"\n```\n-  supervisor.slots.ports\n对于每一个supervisor机器，我们可以通过这项来配置运行多少worker在这台机器上。每一个worker使用一个单独的port来接受消息，这个端口同样定义了那些端口是开放使用的。如果你在这里定义了5个端口，就意味着这个supervisor节点上最多可以运行5个worker。如果定义3个端口，则意味着最多可以运行3个worker。在默认情况下(即配置在defaults.yaml中)，会有有四个workers运行在 6700, 6701, 6702, and 6703端口。例如：\n```\nsupervisor.slots.ports:\n   - 6700\n   - 6701\n   - 6702\n   - 6703\n```\n要注意的是：supervisor并不会在启动时就立即启动这四个worker。而是接受到分配的任务时，才会启动，具体启动几个worker也要根据我们Topology在这个supervisor需要几个worker来确定。如果指定Topology只会由一个worker执行，那么supervisor就启动一个worker，并不会启动所有。\n- ui端口\n```\nui.port: 8998\n```\n2.启动集群\n主节点：执行以下命令\n```\nnohup $STORM_HOME/bin/storm nimbus &\nnohup $STORM_HOME/bin/storm ui &\n```\n#从节点,执行一下命令\n```\nnohup $STORM_HOME/bin/storm supervisor &\n```\n启动成功后，即可在192.168.0.2：8992 storm ui中查看服务\n## 参考\n1.http://www.tianshouzhi.com/api/tutorials/storm/17\n2.http://blog.csdn.net/xeseo/article/details/17678829\n\n\n","source":"_posts/storm集群搭建.md","raw":"---\ntitle: storm集群搭建\ndate: 2016-04-15 20:24:41\ntags: 大数据\ncategories:\n- storm\n---\n# storm集群搭建\n## 前言\n&#160; &#160; &#160; &#160;Storm 是Twitter的一个开源框架。Storm一个分布式的、容错的实时计算系统。Twitter Storm集群表面上类似于Hadoop集群，Hadoop上运行的是MapReduce Jobs，而Storm运行topologies；但是其本身有很大的区别，最主要的区别在于，Hadoop MapReduce Job运行最终会完结，而Storm topologies处理数据进程理论上是永久存活的，除非你将其Kill掉。\nStorm集群中包含两类节点：主控节点（Master Node）和工作节点（Work Node）。其分别对应的角色如下：\n1.  主控节点（Master Node）上运行一个被称为Nimbus的后台程序，它负责在Storm集群内分发代码，分配任务给工作机器，并且负责监控集群运行状态。Nimbus的作用类似于Hadoop中JobTracker的角色。\n2.  每个工作节点（Work Node）上运行一个被称为Supervisor的后台程序。Supervisor负责监听从Nimbus分配给它执行的任务，据此启动或停止执行任务的工作进程。每一个工作进程执行一个Topology的子集；一个运行中的Topology由分布在不同工作节点上的多个工作进程组成。\n\n## 搭建\n### 准备软件\n```\n1.JDK1.8\n2.zookeeper-3.4.5-cdh5.6.0\n3.Storm0.9.5\n```\n### JDK安装\n详见[略](http:trumandu.github.io/2016/04/15/linux环境jdk安装及配置/)\n### Zookeeper集群搭建\n1.解压\n```\ntar xvf zookeeper-3.4.5-cdh5.6.0.tar.gz\n```\n2.修改配置\n添加修改vonf/zoo.cfg\n```\ncp zoo_sample.cfg zoo.cfg\n```\n修改zoo.cfg\n```\ndataDir=/data/truman/zookeeper-3.4.5-cdh5.6.0/data\nserver.1=lab1:2888:3888\nserver.2=lab2:2888:3888\nserver.3=lab3:2888:3888\n```\n在dataDir下新增myid文件，内容为相对应的server后面的数字\n3.分发远程主机\n```\nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab30:/data/truman/ \nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab31:/data/truman/ \nscp -r zookeeper-3.4.5-cdh5.6.0 root@lab29:/data/truman/ \n```\n4.启动和停止\n启动命令\n```\nzkServer.sh start\n```\n停止\n```\nzkServer.sh stop\n```\n### storm集群安装\n在nimbus与supervisor节点上重复以下操作\n1.修改配置\n- storm.zookeeper.servers: 因为Storm所有的信息都是存储在Zookeeper中的，所以要指定Zookeeper服务器的地址\n```\nstorm.zookeeper.servers:\n -  \"192.168.0.2\"\n -  \"192.168.0.3\"\n -  \"192.168.0.4\"\n```\n- storm.local.dir:\nNimbus和 Supervisor守护进程需要一个目录来存储一些状态信息，例如（ jars, confs, and things like that ）\n```\n storm.local.dir: \"/data/storm\"\n```\n-  nimbus.host:\nworker需要知道那一台机器是master，从而可以下载 topology jars 和confs\n```\nnimbus.host: \"192.168.0.2\"\n```\n-  supervisor.slots.ports\n对于每一个supervisor机器，我们可以通过这项来配置运行多少worker在这台机器上。每一个worker使用一个单独的port来接受消息，这个端口同样定义了那些端口是开放使用的。如果你在这里定义了5个端口，就意味着这个supervisor节点上最多可以运行5个worker。如果定义3个端口，则意味着最多可以运行3个worker。在默认情况下(即配置在defaults.yaml中)，会有有四个workers运行在 6700, 6701, 6702, and 6703端口。例如：\n```\nsupervisor.slots.ports:\n   - 6700\n   - 6701\n   - 6702\n   - 6703\n```\n要注意的是：supervisor并不会在启动时就立即启动这四个worker。而是接受到分配的任务时，才会启动，具体启动几个worker也要根据我们Topology在这个supervisor需要几个worker来确定。如果指定Topology只会由一个worker执行，那么supervisor就启动一个worker，并不会启动所有。\n- ui端口\n```\nui.port: 8998\n```\n2.启动集群\n主节点：执行以下命令\n```\nnohup $STORM_HOME/bin/storm nimbus &\nnohup $STORM_HOME/bin/storm ui &\n```\n#从节点,执行一下命令\n```\nnohup $STORM_HOME/bin/storm supervisor &\n```\n启动成功后，即可在192.168.0.2：8992 storm ui中查看服务\n## 参考\n1.http://www.tianshouzhi.com/api/tutorials/storm/17\n2.http://blog.csdn.net/xeseo/article/details/17678829\n\n\n","slug":"storm集群搭建","published":1,"updated":"2019-08-22T12:52:19.842Z","_id":"cjzmobvem007x8cee5a5g6z9b","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"storm集群搭建\"><a href=\"#storm集群搭建\" class=\"headerlink\" title=\"storm集群搭建\"></a>storm集群搭建</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>&#160; &#160; &#160; &#160;Storm 是Twitter的一个开源框架。Storm一个分布式的、容错的实时计算系统。Twitter Storm集群表面上类似于Hadoop集群，Hadoop上运行的是MapReduce Jobs，而Storm运行topologies；但是其本身有很大的区别，最主要的区别在于，Hadoop MapReduce Job运行最终会完结，而Storm topologies处理数据进程理论上是永久存活的，除非你将其Kill掉。<br>Storm集群中包含两类节点：主控节点（Master Node）和工作节点（Work Node）。其分别对应的角色如下：</p>\n<ol>\n<li>主控节点（Master Node）上运行一个被称为Nimbus的后台程序，它负责在Storm集群内分发代码，分配任务给工作机器，并且负责监控集群运行状态。Nimbus的作用类似于Hadoop中JobTracker的角色。</li>\n<li>每个工作节点（Work Node）上运行一个被称为Supervisor的后台程序。Supervisor负责监听从Nimbus分配给它执行的任务，据此启动或停止执行任务的工作进程。每一个工作进程执行一个Topology的子集；一个运行中的Topology由分布在不同工作节点上的多个工作进程组成。</li>\n</ol>\n<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><h3 id=\"准备软件\"><a href=\"#准备软件\" class=\"headerlink\" title=\"准备软件\"></a>准备软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.JDK1.8</span><br><span class=\"line\">2.zookeeper-3.4.5-cdh5.6.0</span><br><span class=\"line\">3.Storm0.9.5</span><br></pre></td></tr></table></figure>\n<h3 id=\"JDK安装\"><a href=\"#JDK安装\" class=\"headerlink\" title=\"JDK安装\"></a>JDK安装</h3><p>详见<a href=\"http:trumandu.github.io/2016/04/15/linux环境jdk安装及配置/\" target=\"_blank\" rel=\"noopener\">略</a></p>\n<h3 id=\"Zookeeper集群搭建\"><a href=\"#Zookeeper集群搭建\" class=\"headerlink\" title=\"Zookeeper集群搭建\"></a>Zookeeper集群搭建</h3><p>1.解压<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar xvf zookeeper-3.4.5-cdh5.6.0.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>2.修改配置<br>添加修改vonf/zoo.cfg<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure></p>\n<p>修改zoo.cfg<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataDir=/data/truman/zookeeper-3.4.5-cdh5.6.0/data</span><br><span class=\"line\">server.1=lab1:2888:3888</span><br><span class=\"line\">server.2=lab2:2888:3888</span><br><span class=\"line\">server.3=lab3:2888:3888</span><br></pre></td></tr></table></figure></p>\n<p>在dataDir下新增myid文件，内容为相对应的server后面的数字<br>3.分发远程主机<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab30:/data/truman/ </span><br><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab31:/data/truman/ </span><br><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab29:/data/truman/</span><br></pre></td></tr></table></figure></p>\n<p>4.启动和停止<br>启动命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zkServer.sh start</span><br></pre></td></tr></table></figure></p>\n<p>停止<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zkServer.sh stop</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"storm集群安装\"><a href=\"#storm集群安装\" class=\"headerlink\" title=\"storm集群安装\"></a>storm集群安装</h3><p>在nimbus与supervisor节点上重复以下操作<br>1.修改配置</p>\n<ul>\n<li><p>storm.zookeeper.servers: 因为Storm所有的信息都是存储在Zookeeper中的，所以要指定Zookeeper服务器的地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">storm.zookeeper.servers:</span><br><span class=\"line\"> -  &quot;192.168.0.2&quot;</span><br><span class=\"line\"> -  &quot;192.168.0.3&quot;</span><br><span class=\"line\"> -  &quot;192.168.0.4&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>storm.local.dir:<br>Nimbus和 Supervisor守护进程需要一个目录来存储一些状态信息，例如（ jars, confs, and things like that ）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">storm.local.dir: &quot;/data/storm&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>nimbus.host:<br>worker需要知道那一台机器是master，从而可以下载 topology jars 和confs</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nimbus.host: &quot;192.168.0.2&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>supervisor.slots.ports<br>对于每一个supervisor机器，我们可以通过这项来配置运行多少worker在这台机器上。每一个worker使用一个单独的port来接受消息，这个端口同样定义了那些端口是开放使用的。如果你在这里定义了5个端口，就意味着这个supervisor节点上最多可以运行5个worker。如果定义3个端口，则意味着最多可以运行3个worker。在默认情况下(即配置在defaults.yaml中)，会有有四个workers运行在 6700, 6701, 6702, and 6703端口。例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">supervisor.slots.ports:</span><br><span class=\"line\">   - 6700</span><br><span class=\"line\">   - 6701</span><br><span class=\"line\">   - 6702</span><br><span class=\"line\">   - 6703</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>要注意的是：supervisor并不会在启动时就立即启动这四个worker。而是接受到分配的任务时，才会启动，具体启动几个worker也要根据我们Topology在这个supervisor需要几个worker来确定。如果指定Topology只会由一个worker执行，那么supervisor就启动一个worker，并不会启动所有。</p>\n<ul>\n<li>ui端口<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ui.port: 8998</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2.启动集群<br>主节点：执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup $STORM_HOME/bin/storm nimbus &amp;</span><br><span class=\"line\">nohup $STORM_HOME/bin/storm ui &amp;</span><br></pre></td></tr></table></figure></p>\n<p>#从节点,执行一下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup $STORM_HOME/bin/storm supervisor &amp;</span><br></pre></td></tr></table></figure></p>\n<p>启动成功后，即可在192.168.0.2：8992 storm ui中查看服务</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://www.tianshouzhi.com/api/tutorials/storm/17\" target=\"_blank\" rel=\"noopener\">http://www.tianshouzhi.com/api/tutorials/storm/17</a><br>2.<a href=\"http://blog.csdn.net/xeseo/article/details/17678829\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xeseo/article/details/17678829</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"storm集群搭建\"><a href=\"#storm集群搭建\" class=\"headerlink\" title=\"storm集群搭建\"></a>storm集群搭建</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>&#160; &#160; &#160; &#160;Storm 是Twitter的一个开源框架。Storm一个分布式的、容错的实时计算系统。Twitter Storm集群表面上类似于Hadoop集群，Hadoop上运行的是MapReduce Jobs，而Storm运行topologies；但是其本身有很大的区别，最主要的区别在于，Hadoop MapReduce Job运行最终会完结，而Storm topologies处理数据进程理论上是永久存活的，除非你将其Kill掉。<br>Storm集群中包含两类节点：主控节点（Master Node）和工作节点（Work Node）。其分别对应的角色如下：</p>\n<ol>\n<li>主控节点（Master Node）上运行一个被称为Nimbus的后台程序，它负责在Storm集群内分发代码，分配任务给工作机器，并且负责监控集群运行状态。Nimbus的作用类似于Hadoop中JobTracker的角色。</li>\n<li>每个工作节点（Work Node）上运行一个被称为Supervisor的后台程序。Supervisor负责监听从Nimbus分配给它执行的任务，据此启动或停止执行任务的工作进程。每一个工作进程执行一个Topology的子集；一个运行中的Topology由分布在不同工作节点上的多个工作进程组成。</li>\n</ol>\n<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><h3 id=\"准备软件\"><a href=\"#准备软件\" class=\"headerlink\" title=\"准备软件\"></a>准备软件</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1.JDK1.8</span><br><span class=\"line\">2.zookeeper-3.4.5-cdh5.6.0</span><br><span class=\"line\">3.Storm0.9.5</span><br></pre></td></tr></table></figure>\n<h3 id=\"JDK安装\"><a href=\"#JDK安装\" class=\"headerlink\" title=\"JDK安装\"></a>JDK安装</h3><p>详见<a href=\"http:trumandu.github.io/2016/04/15/linux环境jdk安装及配置/\" target=\"_blank\" rel=\"noopener\">略</a></p>\n<h3 id=\"Zookeeper集群搭建\"><a href=\"#Zookeeper集群搭建\" class=\"headerlink\" title=\"Zookeeper集群搭建\"></a>Zookeeper集群搭建</h3><p>1.解压<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar xvf zookeeper-3.4.5-cdh5.6.0.tar.gz</span><br></pre></td></tr></table></figure></p>\n<p>2.修改配置<br>添加修改vonf/zoo.cfg<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure></p>\n<p>修改zoo.cfg<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataDir=/data/truman/zookeeper-3.4.5-cdh5.6.0/data</span><br><span class=\"line\">server.1=lab1:2888:3888</span><br><span class=\"line\">server.2=lab2:2888:3888</span><br><span class=\"line\">server.3=lab3:2888:3888</span><br></pre></td></tr></table></figure></p>\n<p>在dataDir下新增myid文件，内容为相对应的server后面的数字<br>3.分发远程主机<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab30:/data/truman/ </span><br><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab31:/data/truman/ </span><br><span class=\"line\">scp -r zookeeper-3.4.5-cdh5.6.0 root@lab29:/data/truman/</span><br></pre></td></tr></table></figure></p>\n<p>4.启动和停止<br>启动命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zkServer.sh start</span><br></pre></td></tr></table></figure></p>\n<p>停止<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zkServer.sh stop</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"storm集群安装\"><a href=\"#storm集群安装\" class=\"headerlink\" title=\"storm集群安装\"></a>storm集群安装</h3><p>在nimbus与supervisor节点上重复以下操作<br>1.修改配置</p>\n<ul>\n<li><p>storm.zookeeper.servers: 因为Storm所有的信息都是存储在Zookeeper中的，所以要指定Zookeeper服务器的地址</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">storm.zookeeper.servers:</span><br><span class=\"line\"> -  &quot;192.168.0.2&quot;</span><br><span class=\"line\"> -  &quot;192.168.0.3&quot;</span><br><span class=\"line\"> -  &quot;192.168.0.4&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>storm.local.dir:<br>Nimbus和 Supervisor守护进程需要一个目录来存储一些状态信息，例如（ jars, confs, and things like that ）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">storm.local.dir: &quot;/data/storm&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>nimbus.host:<br>worker需要知道那一台机器是master，从而可以下载 topology jars 和confs</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nimbus.host: &quot;192.168.0.2&quot;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>supervisor.slots.ports<br>对于每一个supervisor机器，我们可以通过这项来配置运行多少worker在这台机器上。每一个worker使用一个单独的port来接受消息，这个端口同样定义了那些端口是开放使用的。如果你在这里定义了5个端口，就意味着这个supervisor节点上最多可以运行5个worker。如果定义3个端口，则意味着最多可以运行3个worker。在默认情况下(即配置在defaults.yaml中)，会有有四个workers运行在 6700, 6701, 6702, and 6703端口。例如：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">supervisor.slots.ports:</span><br><span class=\"line\">   - 6700</span><br><span class=\"line\">   - 6701</span><br><span class=\"line\">   - 6702</span><br><span class=\"line\">   - 6703</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>要注意的是：supervisor并不会在启动时就立即启动这四个worker。而是接受到分配的任务时，才会启动，具体启动几个worker也要根据我们Topology在这个supervisor需要几个worker来确定。如果指定Topology只会由一个worker执行，那么supervisor就启动一个worker，并不会启动所有。</p>\n<ul>\n<li>ui端口<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ui.port: 8998</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2.启动集群<br>主节点：执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup $STORM_HOME/bin/storm nimbus &amp;</span><br><span class=\"line\">nohup $STORM_HOME/bin/storm ui &amp;</span><br></pre></td></tr></table></figure></p>\n<p>#从节点,执行一下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup $STORM_HOME/bin/storm supervisor &amp;</span><br></pre></td></tr></table></figure></p>\n<p>启动成功后，即可在192.168.0.2：8992 storm ui中查看服务</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://www.tianshouzhi.com/api/tutorials/storm/17\" target=\"_blank\" rel=\"noopener\">http://www.tianshouzhi.com/api/tutorials/storm/17</a><br>2.<a href=\"http://blog.csdn.net/xeseo/article/details/17678829\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xeseo/article/details/17678829</a></p>\n"},{"title":"zookeeper运维经验","date":"2017-03-05T12:45:00.000Z","_content":"# zookeeper运维经验\n\n##  参数配置\n在默认zoo.cfg配置中，zookeeper生成的历史镜像,log不会删除，生成的频率也比较快，因此生产环境需要配置以下两个参数\n\n- autopurge.purgeInterval\n\n**解释**：清楚间隔，单位小时，默认值为0，修改次值，表示启用清楚\n- autopurge.snapRetainCount\n\n**解释**：保留数量，默认为3，最小值为3\n\n\n\n## 引用\n1. [zookeeper官网](https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_advancedConfiguration)\n","source":"_posts/zookeeper运维经验.md","raw":"---\ntitle: zookeeper运维经验\ndate: 2017-03-05 20:45:00\ntags: 大数据\ncategories:\n- zookeeper\n---\n# zookeeper运维经验\n\n##  参数配置\n在默认zoo.cfg配置中，zookeeper生成的历史镜像,log不会删除，生成的频率也比较快，因此生产环境需要配置以下两个参数\n\n- autopurge.purgeInterval\n\n**解释**：清楚间隔，单位小时，默认值为0，修改次值，表示启用清楚\n- autopurge.snapRetainCount\n\n**解释**：保留数量，默认为3，最小值为3\n\n\n\n## 引用\n1. [zookeeper官网](https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_advancedConfiguration)\n","slug":"zookeeper运维经验","published":1,"updated":"2017-03-05T12:47:20.403Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvep00818cee0hn1y2m3","content":"<h1 id=\"zookeeper运维经验\"><a href=\"#zookeeper运维经验\" class=\"headerlink\" title=\"zookeeper运维经验\"></a>zookeeper运维经验</h1><h2 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h2><p>在默认zoo.cfg配置中，zookeeper生成的历史镜像,log不会删除，生成的频率也比较快，因此生产环境需要配置以下两个参数</p>\n<ul>\n<li>autopurge.purgeInterval</li>\n</ul>\n<p><strong>解释</strong>：清楚间隔，单位小时，默认值为0，修改次值，表示启用清楚</p>\n<ul>\n<li>autopurge.snapRetainCount</li>\n</ul>\n<p><strong>解释</strong>：保留数量，默认为3，最小值为3</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_advancedConfiguration\" target=\"_blank\" rel=\"noopener\">zookeeper官网</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"zookeeper运维经验\"><a href=\"#zookeeper运维经验\" class=\"headerlink\" title=\"zookeeper运维经验\"></a>zookeeper运维经验</h1><h2 id=\"参数配置\"><a href=\"#参数配置\" class=\"headerlink\" title=\"参数配置\"></a>参数配置</h2><p>在默认zoo.cfg配置中，zookeeper生成的历史镜像,log不会删除，生成的频率也比较快，因此生产环境需要配置以下两个参数</p>\n<ul>\n<li>autopurge.purgeInterval</li>\n</ul>\n<p><strong>解释</strong>：清楚间隔，单位小时，默认值为0，修改次值，表示启用清楚</p>\n<ul>\n<li>autopurge.snapRetainCount</li>\n</ul>\n<p><strong>解释</strong>：保留数量，默认为3，最小值为3</p>\n<h2 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h2><ol>\n<li><a href=\"https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_advancedConfiguration\" target=\"_blank\" rel=\"noopener\">zookeeper官网</a></li>\n</ol>\n"},{"title":"不回忆就再也想不起来","date":"2018-10-15T10:04:45.000Z","_content":"# 不回忆，就再也想不起了\n\n![](http://blogstatic.aibibang.com/biye_1.jpg)\n\n![](http://blogstatic.aibibang.com/biye_2.jpg)\n\n毕业四年了，整整四年了，工作忙碌，生活奔波，没有就给我留下太多的时间去回忆，不回忆，就再也想不起来了。\n\n想不起来刚进校园就因甲流被隔离的那一个月，那是快乐的一个月，没有学业，只有大学的新鲜，没有看不完的书，只有发不完的呆，再也不会有带着口罩满校溜达，回忆就像一扇窗，打开它，我能看到一副风景，风景中是我丢失的碎片记忆。\n\n想不起大学的第一节课，想不到工作这么久，竟然干的工作多少还和它有点联系，烈日灼心，记忆中那节所在的教室在午后是那么的热，即使有空调，也感觉不到一丝凉意，它骨子里留给我的不是竞争，不是压力，即使我们有早操，即使我们下了下午的课，还得赶匆忙的晚自习，我骨子里是一个意志力很差的人，它教会了我坚持，不知道未来是什么样，只是依稀懂得需要坚持。\n\n世间巧合万万个，最后一节课依然是那个教室，这大概就是所谓的缘分。回忆是件可怕的事，不想倒是没什么，一想瞬间千万片段释放，不知道是不是触电。物是人非，我离它直线距离不过5公里，可这么多年都未曾再去过，再也见不到熟悉的面孔，再也不能坐在教室里安奈躁动的心。\n\n毕业的时候忙忙碌碌的，没有参加过毕业典礼，多么想再去补考，再参加一次毕业典礼。\n\n\n毕业的时候来不及说再见的人有很多","source":"_posts/不回忆就再也想不起来.md","raw":"---\ntitle: 不回忆就再也想不起来\ndate: 2018-10-15 18:04:45\ntags: 散心\ncategories:\n- 心情\n---\n# 不回忆，就再也想不起了\n\n![](http://blogstatic.aibibang.com/biye_1.jpg)\n\n![](http://blogstatic.aibibang.com/biye_2.jpg)\n\n毕业四年了，整整四年了，工作忙碌，生活奔波，没有就给我留下太多的时间去回忆，不回忆，就再也想不起来了。\n\n想不起来刚进校园就因甲流被隔离的那一个月，那是快乐的一个月，没有学业，只有大学的新鲜，没有看不完的书，只有发不完的呆，再也不会有带着口罩满校溜达，回忆就像一扇窗，打开它，我能看到一副风景，风景中是我丢失的碎片记忆。\n\n想不起大学的第一节课，想不到工作这么久，竟然干的工作多少还和它有点联系，烈日灼心，记忆中那节所在的教室在午后是那么的热，即使有空调，也感觉不到一丝凉意，它骨子里留给我的不是竞争，不是压力，即使我们有早操，即使我们下了下午的课，还得赶匆忙的晚自习，我骨子里是一个意志力很差的人，它教会了我坚持，不知道未来是什么样，只是依稀懂得需要坚持。\n\n世间巧合万万个，最后一节课依然是那个教室，这大概就是所谓的缘分。回忆是件可怕的事，不想倒是没什么，一想瞬间千万片段释放，不知道是不是触电。物是人非，我离它直线距离不过5公里，可这么多年都未曾再去过，再也见不到熟悉的面孔，再也不能坐在教室里安奈躁动的心。\n\n毕业的时候忙忙碌碌的，没有参加过毕业典礼，多么想再去补考，再参加一次毕业典礼。\n\n\n毕业的时候来不及说再见的人有很多","slug":"不回忆就再也想不起来","published":1,"updated":"2018-10-15T10:34:11.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobveq00848ceew711iw19","content":"<h1 id=\"不回忆，就再也想不起了\"><a href=\"#不回忆，就再也想不起了\" class=\"headerlink\" title=\"不回忆，就再也想不起了\"></a>不回忆，就再也想不起了</h1><p><img src=\"http://blogstatic.aibibang.com/biye_1.jpg\" alt=\"\"></p>\n<p><img src=\"http://blogstatic.aibibang.com/biye_2.jpg\" alt=\"\"></p>\n<p>毕业四年了，整整四年了，工作忙碌，生活奔波，没有就给我留下太多的时间去回忆，不回忆，就再也想不起来了。</p>\n<p>想不起来刚进校园就因甲流被隔离的那一个月，那是快乐的一个月，没有学业，只有大学的新鲜，没有看不完的书，只有发不完的呆，再也不会有带着口罩满校溜达，回忆就像一扇窗，打开它，我能看到一副风景，风景中是我丢失的碎片记忆。</p>\n<p>想不起大学的第一节课，想不到工作这么久，竟然干的工作多少还和它有点联系，烈日灼心，记忆中那节所在的教室在午后是那么的热，即使有空调，也感觉不到一丝凉意，它骨子里留给我的不是竞争，不是压力，即使我们有早操，即使我们下了下午的课，还得赶匆忙的晚自习，我骨子里是一个意志力很差的人，它教会了我坚持，不知道未来是什么样，只是依稀懂得需要坚持。</p>\n<p>世间巧合万万个，最后一节课依然是那个教室，这大概就是所谓的缘分。回忆是件可怕的事，不想倒是没什么，一想瞬间千万片段释放，不知道是不是触电。物是人非，我离它直线距离不过5公里，可这么多年都未曾再去过，再也见不到熟悉的面孔，再也不能坐在教室里安奈躁动的心。</p>\n<p>毕业的时候忙忙碌碌的，没有参加过毕业典礼，多么想再去补考，再参加一次毕业典礼。</p>\n<p>毕业的时候来不及说再见的人有很多</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"不回忆，就再也想不起了\"><a href=\"#不回忆，就再也想不起了\" class=\"headerlink\" title=\"不回忆，就再也想不起了\"></a>不回忆，就再也想不起了</h1><p><img src=\"http://blogstatic.aibibang.com/biye_1.jpg\" alt=\"\"></p>\n<p><img src=\"http://blogstatic.aibibang.com/biye_2.jpg\" alt=\"\"></p>\n<p>毕业四年了，整整四年了，工作忙碌，生活奔波，没有就给我留下太多的时间去回忆，不回忆，就再也想不起来了。</p>\n<p>想不起来刚进校园就因甲流被隔离的那一个月，那是快乐的一个月，没有学业，只有大学的新鲜，没有看不完的书，只有发不完的呆，再也不会有带着口罩满校溜达，回忆就像一扇窗，打开它，我能看到一副风景，风景中是我丢失的碎片记忆。</p>\n<p>想不起大学的第一节课，想不到工作这么久，竟然干的工作多少还和它有点联系，烈日灼心，记忆中那节所在的教室在午后是那么的热，即使有空调，也感觉不到一丝凉意，它骨子里留给我的不是竞争，不是压力，即使我们有早操，即使我们下了下午的课，还得赶匆忙的晚自习，我骨子里是一个意志力很差的人，它教会了我坚持，不知道未来是什么样，只是依稀懂得需要坚持。</p>\n<p>世间巧合万万个，最后一节课依然是那个教室，这大概就是所谓的缘分。回忆是件可怕的事，不想倒是没什么，一想瞬间千万片段释放，不知道是不是触电。物是人非，我离它直线距离不过5公里，可这么多年都未曾再去过，再也见不到熟悉的面孔，再也不能坐在教室里安奈躁动的心。</p>\n<p>毕业的时候忙忙碌碌的，没有参加过毕业典礼，多么想再去补考，再参加一次毕业典礼。</p>\n<p>毕业的时候来不及说再见的人有很多</p>\n"},{"title":"使用maven创建自定义archetype","date":"2016-08-28T11:28:16.000Z","_content":"## 前言\n在开发过程中经常需要新建工程，新建工程使用自带的archetype,往往不能满足项目开发需求，这就需要我们开发出自己的archetype。\n## 实现\n本次使用create-from-project来实现自定义archetype（方法至少两种）\n#### 1.构建模板项目\n首先使用eclipse创建一个新的maven  project，然后把配置好的一些公用的东西放到相应的目录下面 比如说会将一些常用的java代码存放到src/main/java目录下面；会将一些通用的配置文件放到src/main/resources目录下面；如果是javeEE工程，还会有一些jsp等等的文件存放到src/main/webapp目录下面\n#### 2.pom.xml编辑\n在pom.xml文件中添加以下内容\n```\n<build>\n    <pluginManagement>\n    \t<plugins>\n    \t\t<plugin>\n    \t\t\t<groupId>org.apache.maven.plugins</groupId>\n    \t\t\t<artifactId>maven-archetype-plugin</artifactId>\n    \t\t\t<version>2.4</version>\n    \t\t</plugin>\n    \t</plugins>\n    </pluginManagement>\n</build>\n```\n#### 3.编译\n在工程跟目录下运行maven命令\n```\nmvn archetype:create-from-project\n```\n然后会在target目录下面生成generated-sources目录，这个就是生成的 archetype\n#### 4.安装/发布\n 进入generated-sourced/archetype目录，运行maven命令：\n```\n    mvn install\n```\n这样就把自定义的archetype安装到本地仓库了。\n\narchetype安装的地址是在maven安装目录下面的conf/settings.xml文件中指定的(<localRepository>字节)。\n\n默认会在  ~/.m2  目录下面生成一个archetype-catalog.xml文件（和默认的settings.xml在同一个目录), 声明了该archetype的groupId、artifactId和其他属性。\n\n因为Eclipse创建maven项目过程中，选择的“Default Local”指向的地址就是 ~/.m2，所以文件archetype-catalog.xml会被eclipse自动读取，使用eclipse创建maven项目的时候可以在\"Default Local\"一项中找到刚才自定义archetype名字。\n\n> 安装到本地仓库中的archetype只可以被自己使用，如果想要共享，那么在第四步的时候使用deploy命令，不要使用install命令。\n\n#### 5.卸载\n如果想要卸载刚才安装的archetype，只需要将~/.m2目录下面的archetype-catalog.xml文件中对应的<archetype>字节段删掉，并且把本地仓库中相应groupId和artifactId下面的文件删掉就可以了。\n\n## 备注\n**问题**：eclipse中找不到自定义archetype?\n\n首先查看自定义的版本是否是0.0.1-SNAPSHOT，如果是这个的话，需要勾选include snapshot archetypes\n\n## 参考\n1.http://my.oschina.net/wangrikui/blog/498807\n2.http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013\n\n\n","source":"_posts/使用maven创建自定义archetype.md","raw":"---\ntitle: 使用maven创建自定义archetype\ndate: 2016-08-28 19:28:16\ntags: 开发笔记\ncategories:\n- java\n---\n## 前言\n在开发过程中经常需要新建工程，新建工程使用自带的archetype,往往不能满足项目开发需求，这就需要我们开发出自己的archetype。\n## 实现\n本次使用create-from-project来实现自定义archetype（方法至少两种）\n#### 1.构建模板项目\n首先使用eclipse创建一个新的maven  project，然后把配置好的一些公用的东西放到相应的目录下面 比如说会将一些常用的java代码存放到src/main/java目录下面；会将一些通用的配置文件放到src/main/resources目录下面；如果是javeEE工程，还会有一些jsp等等的文件存放到src/main/webapp目录下面\n#### 2.pom.xml编辑\n在pom.xml文件中添加以下内容\n```\n<build>\n    <pluginManagement>\n    \t<plugins>\n    \t\t<plugin>\n    \t\t\t<groupId>org.apache.maven.plugins</groupId>\n    \t\t\t<artifactId>maven-archetype-plugin</artifactId>\n    \t\t\t<version>2.4</version>\n    \t\t</plugin>\n    \t</plugins>\n    </pluginManagement>\n</build>\n```\n#### 3.编译\n在工程跟目录下运行maven命令\n```\nmvn archetype:create-from-project\n```\n然后会在target目录下面生成generated-sources目录，这个就是生成的 archetype\n#### 4.安装/发布\n 进入generated-sourced/archetype目录，运行maven命令：\n```\n    mvn install\n```\n这样就把自定义的archetype安装到本地仓库了。\n\narchetype安装的地址是在maven安装目录下面的conf/settings.xml文件中指定的(<localRepository>字节)。\n\n默认会在  ~/.m2  目录下面生成一个archetype-catalog.xml文件（和默认的settings.xml在同一个目录), 声明了该archetype的groupId、artifactId和其他属性。\n\n因为Eclipse创建maven项目过程中，选择的“Default Local”指向的地址就是 ~/.m2，所以文件archetype-catalog.xml会被eclipse自动读取，使用eclipse创建maven项目的时候可以在\"Default Local\"一项中找到刚才自定义archetype名字。\n\n> 安装到本地仓库中的archetype只可以被自己使用，如果想要共享，那么在第四步的时候使用deploy命令，不要使用install命令。\n\n#### 5.卸载\n如果想要卸载刚才安装的archetype，只需要将~/.m2目录下面的archetype-catalog.xml文件中对应的<archetype>字节段删掉，并且把本地仓库中相应groupId和artifactId下面的文件删掉就可以了。\n\n## 备注\n**问题**：eclipse中找不到自定义archetype?\n\n首先查看自定义的版本是否是0.0.1-SNAPSHOT，如果是这个的话，需要勾选include snapshot archetypes\n\n## 参考\n1.http://my.oschina.net/wangrikui/blog/498807\n2.http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013\n\n\n","slug":"使用maven创建自定义archetype","published":1,"updated":"2016-08-28T11:30:28.533Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvet00888ceeprf0f24l","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在开发过程中经常需要新建工程，新建工程使用自带的archetype,往往不能满足项目开发需求，这就需要我们开发出自己的archetype。</p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>本次使用create-from-project来实现自定义archetype（方法至少两种）</p>\n<h4 id=\"1-构建模板项目\"><a href=\"#1-构建模板项目\" class=\"headerlink\" title=\"1.构建模板项目\"></a>1.构建模板项目</h4><p>首先使用eclipse创建一个新的maven  project，然后把配置好的一些公用的东西放到相应的目录下面 比如说会将一些常用的java代码存放到src/main/java目录下面；会将一些通用的配置文件放到src/main/resources目录下面；如果是javeEE工程，还会有一些jsp等等的文件存放到src/main/webapp目录下面</p>\n<h4 id=\"2-pom-xml编辑\"><a href=\"#2-pom-xml编辑\" class=\"headerlink\" title=\"2.pom.xml编辑\"></a>2.pom.xml编辑</h4><p>在pom.xml文件中添加以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;</span><br><span class=\"line\">    &lt;pluginManagement&gt;</span><br><span class=\"line\">    \t&lt;plugins&gt;</span><br><span class=\"line\">    \t\t&lt;plugin&gt;</span><br><span class=\"line\">    \t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">    \t\t\t&lt;artifactId&gt;maven-archetype-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">    \t\t\t&lt;version&gt;2.4&lt;/version&gt;</span><br><span class=\"line\">    \t\t&lt;/plugin&gt;</span><br><span class=\"line\">    \t&lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/pluginManagement&gt;</span><br><span class=\"line\">&lt;/build&gt;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-编译\"><a href=\"#3-编译\" class=\"headerlink\" title=\"3.编译\"></a>3.编译</h4><p>在工程跟目录下运行maven命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn archetype:create-from-project</span><br></pre></td></tr></table></figure></p>\n<p>然后会在target目录下面生成generated-sources目录，这个就是生成的 archetype</p>\n<h4 id=\"4-安装-发布\"><a href=\"#4-安装-发布\" class=\"headerlink\" title=\"4.安装/发布\"></a>4.安装/发布</h4><p> 进入generated-sourced/archetype目录，运行maven命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn install</span><br></pre></td></tr></table></figure></p>\n<p>这样就把自定义的archetype安装到本地仓库了。</p>\n<p>archetype安装的地址是在maven安装目录下面的conf/settings.xml文件中指定的(<localrepository>字节)。</localrepository></p>\n<p>默认会在  ~/.m2  目录下面生成一个archetype-catalog.xml文件（和默认的settings.xml在同一个目录), 声明了该archetype的groupId、artifactId和其他属性。</p>\n<p>因为Eclipse创建maven项目过程中，选择的“Default Local”指向的地址就是 ~/.m2，所以文件archetype-catalog.xml会被eclipse自动读取，使用eclipse创建maven项目的时候可以在”Default Local”一项中找到刚才自定义archetype名字。</p>\n<blockquote>\n<p>安装到本地仓库中的archetype只可以被自己使用，如果想要共享，那么在第四步的时候使用deploy命令，不要使用install命令。</p>\n</blockquote>\n<h4 id=\"5-卸载\"><a href=\"#5-卸载\" class=\"headerlink\" title=\"5.卸载\"></a>5.卸载</h4><p>如果想要卸载刚才安装的archetype，只需要将~/.m2目录下面的archetype-catalog.xml文件中对应的<archetype>字节段删掉，并且把本地仓库中相应groupId和artifactId下面的文件删掉就可以了。</archetype></p>\n<h2 id=\"备注\"><a href=\"#备注\" class=\"headerlink\" title=\"备注\"></a>备注</h2><p><strong>问题</strong>：eclipse中找不到自定义archetype?</p>\n<p>首先查看自定义的版本是否是0.0.1-SNAPSHOT，如果是这个的话，需要勾选include snapshot archetypes</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://my.oschina.net/wangrikui/blog/498807\" target=\"_blank\" rel=\"noopener\">http://my.oschina.net/wangrikui/blog/498807</a><br>2.<a href=\"http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在开发过程中经常需要新建工程，新建工程使用自带的archetype,往往不能满足项目开发需求，这就需要我们开发出自己的archetype。</p>\n<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><p>本次使用create-from-project来实现自定义archetype（方法至少两种）</p>\n<h4 id=\"1-构建模板项目\"><a href=\"#1-构建模板项目\" class=\"headerlink\" title=\"1.构建模板项目\"></a>1.构建模板项目</h4><p>首先使用eclipse创建一个新的maven  project，然后把配置好的一些公用的东西放到相应的目录下面 比如说会将一些常用的java代码存放到src/main/java目录下面；会将一些通用的配置文件放到src/main/resources目录下面；如果是javeEE工程，还会有一些jsp等等的文件存放到src/main/webapp目录下面</p>\n<h4 id=\"2-pom-xml编辑\"><a href=\"#2-pom-xml编辑\" class=\"headerlink\" title=\"2.pom.xml编辑\"></a>2.pom.xml编辑</h4><p>在pom.xml文件中添加以下内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;build&gt;</span><br><span class=\"line\">    &lt;pluginManagement&gt;</span><br><span class=\"line\">    \t&lt;plugins&gt;</span><br><span class=\"line\">    \t\t&lt;plugin&gt;</span><br><span class=\"line\">    \t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">    \t\t\t&lt;artifactId&gt;maven-archetype-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">    \t\t\t&lt;version&gt;2.4&lt;/version&gt;</span><br><span class=\"line\">    \t\t&lt;/plugin&gt;</span><br><span class=\"line\">    \t&lt;/plugins&gt;</span><br><span class=\"line\">    &lt;/pluginManagement&gt;</span><br><span class=\"line\">&lt;/build&gt;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"3-编译\"><a href=\"#3-编译\" class=\"headerlink\" title=\"3.编译\"></a>3.编译</h4><p>在工程跟目录下运行maven命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn archetype:create-from-project</span><br></pre></td></tr></table></figure></p>\n<p>然后会在target目录下面生成generated-sources目录，这个就是生成的 archetype</p>\n<h4 id=\"4-安装-发布\"><a href=\"#4-安装-发布\" class=\"headerlink\" title=\"4.安装/发布\"></a>4.安装/发布</h4><p> 进入generated-sourced/archetype目录，运行maven命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn install</span><br></pre></td></tr></table></figure></p>\n<p>这样就把自定义的archetype安装到本地仓库了。</p>\n<p>archetype安装的地址是在maven安装目录下面的conf/settings.xml文件中指定的(<localrepository>字节)。</localrepository></p>\n<p>默认会在  ~/.m2  目录下面生成一个archetype-catalog.xml文件（和默认的settings.xml在同一个目录), 声明了该archetype的groupId、artifactId和其他属性。</p>\n<p>因为Eclipse创建maven项目过程中，选择的“Default Local”指向的地址就是 ~/.m2，所以文件archetype-catalog.xml会被eclipse自动读取，使用eclipse创建maven项目的时候可以在”Default Local”一项中找到刚才自定义archetype名字。</p>\n<blockquote>\n<p>安装到本地仓库中的archetype只可以被自己使用，如果想要共享，那么在第四步的时候使用deploy命令，不要使用install命令。</p>\n</blockquote>\n<h4 id=\"5-卸载\"><a href=\"#5-卸载\" class=\"headerlink\" title=\"5.卸载\"></a>5.卸载</h4><p>如果想要卸载刚才安装的archetype，只需要将~/.m2目录下面的archetype-catalog.xml文件中对应的<archetype>字节段删掉，并且把本地仓库中相应groupId和artifactId下面的文件删掉就可以了。</archetype></p>\n<h2 id=\"备注\"><a href=\"#备注\" class=\"headerlink\" title=\"备注\"></a>备注</h2><p><strong>问题</strong>：eclipse中找不到自定义archetype?</p>\n<p>首先查看自定义的版本是否是0.0.1-SNAPSHOT，如果是这个的话，需要勾选include snapshot archetypes</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://my.oschina.net/wangrikui/blog/498807\" target=\"_blank\" rel=\"noopener\">http://my.oschina.net/wangrikui/blog/498807</a><br>2.<a href=\"http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/sxdtzhaoxinguo/article/details/46895013</a></p>\n"},{"title":"利用winscp与putty构建自动化部署","date":"2016-05-13T12:51:48.000Z","_content":"# 利用winscp与putty构建自动化部署\n## 前言\n在运维过程中，会经常遇到维护的机器很多，更新软件版本比较繁杂，在此借鉴winscp与putty支持脚本的功能之上，使用window bat命令实现在window平台便捷部署linux上的应用。\n## 方案\n### 上传文件\n此处利用winscp。updateLoadScript.txt 具体操作代码如下：\n``` bash\noption batch on \noption confirm off \nopen scp://root:12345678@192.168.*.** \nput E:\\deploy\\tt.txt /data/projects/\nclose \nexit\n\n```\n*此处主要，要提前用winscp连接到相应主机上，猜测要从缓存中取一些东西*\n\n### 执行命令\n此处利用 putty。 command.txt 具体命令如下：\n``` bash\nexport JAVA_HOME=/opt/jdk1.8.0_45\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\ncd /data/projects/redis-client\n./start.sh stop\n./start.sh start\nps -ef|grep redis-client\n```\n\n### bat文件\n``` bash\n@echo off \necho =============================== \necho deploy RedisClient app \necho =============================== \necho please input \"y\" to continue...... \nset /p input= \nif \"%input%\"==\"y\" ( \nrem --打开控制台 \ncall D:\\WinSCP\\WinSCP.exe /script=updateLoadScript.txt  /log=upload_log.txt \ncall D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt \ncall D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt \necho deploy RedisClient successed! \n) else echo Does not execute any command \npause\n```\n## 参考\n1. winscp 使用 http://www.5iadmin.com/post/1014.html\n","source":"_posts/利用winscp与putty构建自动化部署.md","raw":"---\ntitle: 利用winscp与putty构建自动化部署\ndate: 2016-05-13 20:51:48\ntags: 部署与维护\ncategories:\n- 批处理\n---\n# 利用winscp与putty构建自动化部署\n## 前言\n在运维过程中，会经常遇到维护的机器很多，更新软件版本比较繁杂，在此借鉴winscp与putty支持脚本的功能之上，使用window bat命令实现在window平台便捷部署linux上的应用。\n## 方案\n### 上传文件\n此处利用winscp。updateLoadScript.txt 具体操作代码如下：\n``` bash\noption batch on \noption confirm off \nopen scp://root:12345678@192.168.*.** \nput E:\\deploy\\tt.txt /data/projects/\nclose \nexit\n\n```\n*此处主要，要提前用winscp连接到相应主机上，猜测要从缓存中取一些东西*\n\n### 执行命令\n此处利用 putty。 command.txt 具体命令如下：\n``` bash\nexport JAVA_HOME=/opt/jdk1.8.0_45\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\ncd /data/projects/redis-client\n./start.sh stop\n./start.sh start\nps -ef|grep redis-client\n```\n\n### bat文件\n``` bash\n@echo off \necho =============================== \necho deploy RedisClient app \necho =============================== \necho please input \"y\" to continue...... \nset /p input= \nif \"%input%\"==\"y\" ( \nrem --打开控制台 \ncall D:\\WinSCP\\WinSCP.exe /script=updateLoadScript.txt  /log=upload_log.txt \ncall D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt \ncall D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt \necho deploy RedisClient successed! \n) else echo Does not execute any command \npause\n```\n## 参考\n1. winscp 使用 http://www.5iadmin.com/post/1014.html\n","slug":"利用winscp与putty构建自动化部署","published":1,"updated":"2016-05-13T12:53:23.974Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvew008c8ceeaacmmm2s","content":"<h1 id=\"利用winscp与putty构建自动化部署\"><a href=\"#利用winscp与putty构建自动化部署\" class=\"headerlink\" title=\"利用winscp与putty构建自动化部署\"></a>利用winscp与putty构建自动化部署</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在运维过程中，会经常遇到维护的机器很多，更新软件版本比较繁杂，在此借鉴winscp与putty支持脚本的功能之上，使用window bat命令实现在window平台便捷部署linux上的应用。</p>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><h3 id=\"上传文件\"><a href=\"#上传文件\" class=\"headerlink\" title=\"上传文件\"></a>上传文件</h3><p>此处利用winscp。updateLoadScript.txt 具体操作代码如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">option batch on </span><br><span class=\"line\">option confirm off </span><br><span class=\"line\">open scp://root:12345678@192.168.*.** </span><br><span class=\"line\">put E:\\deploy\\tt.txt /data/projects/</span><br><span class=\"line\">close </span><br><span class=\"line\"><span class=\"built_in\">exit</span></span><br></pre></td></tr></table></figure></p>\n<p><em>此处主要，要提前用winscp连接到相应主机上，猜测要从缓存中取一些东西</em></p>\n<h3 id=\"执行命令\"><a href=\"#执行命令\" class=\"headerlink\" title=\"执行命令\"></a>执行命令</h3><p>此处利用 putty。 command.txt 具体命令如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/opt/jdk1.8.0_45</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$JAVA_HOME</span>/bin:<span class=\"variable\">$PATH</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> CLASSPATH=.:<span class=\"variable\">$JAVA_HOME</span>/lib/dt.jar:<span class=\"variable\">$JAVA_HOME</span>/lib/tools.jar</span><br><span class=\"line\"><span class=\"built_in\">cd</span> /data/projects/redis-client</span><br><span class=\"line\">./start.sh stop</span><br><span class=\"line\">./start.sh start</span><br><span class=\"line\">ps -ef|grep redis-client</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"bat文件\"><a href=\"#bat文件\" class=\"headerlink\" title=\"bat文件\"></a>bat文件</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@<span class=\"built_in\">echo</span> off </span><br><span class=\"line\"><span class=\"built_in\">echo</span> =============================== </span><br><span class=\"line\"><span class=\"built_in\">echo</span> deploy RedisClient app </span><br><span class=\"line\"><span class=\"built_in\">echo</span> =============================== </span><br><span class=\"line\"><span class=\"built_in\">echo</span> please input <span class=\"string\">\"y\"</span> to <span class=\"built_in\">continue</span>...... </span><br><span class=\"line\"><span class=\"built_in\">set</span> /p input= </span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"string\">\"%input%\"</span>==<span class=\"string\">\"y\"</span> ( </span><br><span class=\"line\">rem --打开控制台 </span><br><span class=\"line\">call D:\\WinSCP\\WinSCP.exe /script=updateLoadScript.txt  /<span class=\"built_in\">log</span>=upload_log.txt </span><br><span class=\"line\">call D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt </span><br><span class=\"line\">call D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt </span><br><span class=\"line\"><span class=\"built_in\">echo</span> deploy RedisClient successed! </span><br><span class=\"line\">) <span class=\"keyword\">else</span> <span class=\"built_in\">echo</span> Does not execute any <span class=\"built_in\">command</span> </span><br><span class=\"line\">pause</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li>winscp 使用 <a href=\"http://www.5iadmin.com/post/1014.html\" target=\"_blank\" rel=\"noopener\">http://www.5iadmin.com/post/1014.html</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"利用winscp与putty构建自动化部署\"><a href=\"#利用winscp与putty构建自动化部署\" class=\"headerlink\" title=\"利用winscp与putty构建自动化部署\"></a>利用winscp与putty构建自动化部署</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在运维过程中，会经常遇到维护的机器很多，更新软件版本比较繁杂，在此借鉴winscp与putty支持脚本的功能之上，使用window bat命令实现在window平台便捷部署linux上的应用。</p>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><h3 id=\"上传文件\"><a href=\"#上传文件\" class=\"headerlink\" title=\"上传文件\"></a>上传文件</h3><p>此处利用winscp。updateLoadScript.txt 具体操作代码如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">option batch on </span><br><span class=\"line\">option confirm off </span><br><span class=\"line\">open scp://root:12345678@192.168.*.** </span><br><span class=\"line\">put E:\\deploy\\tt.txt /data/projects/</span><br><span class=\"line\">close </span><br><span class=\"line\"><span class=\"built_in\">exit</span></span><br></pre></td></tr></table></figure></p>\n<p><em>此处主要，要提前用winscp连接到相应主机上，猜测要从缓存中取一些东西</em></p>\n<h3 id=\"执行命令\"><a href=\"#执行命令\" class=\"headerlink\" title=\"执行命令\"></a>执行命令</h3><p>此处利用 putty。 command.txt 具体命令如下：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/opt/jdk1.8.0_45</span><br><span class=\"line\"><span class=\"built_in\">export</span> PATH=<span class=\"variable\">$JAVA_HOME</span>/bin:<span class=\"variable\">$PATH</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> CLASSPATH=.:<span class=\"variable\">$JAVA_HOME</span>/lib/dt.jar:<span class=\"variable\">$JAVA_HOME</span>/lib/tools.jar</span><br><span class=\"line\"><span class=\"built_in\">cd</span> /data/projects/redis-client</span><br><span class=\"line\">./start.sh stop</span><br><span class=\"line\">./start.sh start</span><br><span class=\"line\">ps -ef|grep redis-client</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"bat文件\"><a href=\"#bat文件\" class=\"headerlink\" title=\"bat文件\"></a>bat文件</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@<span class=\"built_in\">echo</span> off </span><br><span class=\"line\"><span class=\"built_in\">echo</span> =============================== </span><br><span class=\"line\"><span class=\"built_in\">echo</span> deploy RedisClient app </span><br><span class=\"line\"><span class=\"built_in\">echo</span> =============================== </span><br><span class=\"line\"><span class=\"built_in\">echo</span> please input <span class=\"string\">\"y\"</span> to <span class=\"built_in\">continue</span>...... </span><br><span class=\"line\"><span class=\"built_in\">set</span> /p input= </span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"string\">\"%input%\"</span>==<span class=\"string\">\"y\"</span> ( </span><br><span class=\"line\">rem --打开控制台 </span><br><span class=\"line\">call D:\\WinSCP\\WinSCP.exe /script=updateLoadScript.txt  /<span class=\"built_in\">log</span>=upload_log.txt </span><br><span class=\"line\">call D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt </span><br><span class=\"line\">call D:\\WinSCP\\PuTTY\\putty.exe -ssh -pw 12345678 root@192.168.*.** -m command.txt </span><br><span class=\"line\"><span class=\"built_in\">echo</span> deploy RedisClient successed! </span><br><span class=\"line\">) <span class=\"keyword\">else</span> <span class=\"built_in\">echo</span> Does not execute any <span class=\"built_in\">command</span> </span><br><span class=\"line\">pause</span><br></pre></td></tr></table></figure>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li>winscp 使用 <a href=\"http://www.5iadmin.com/post/1014.html\" target=\"_blank\" rel=\"noopener\">http://www.5iadmin.com/post/1014.html</a></li>\n</ol>\n"},{"title":"如何优雅的下线kafka leader","date":"2019-03-04T13:35:38.000Z","_content":"","source":"_posts/如何优雅的下线kafkaleader.md","raw":"---\ntitle: 如何优雅的下线kafka leader\ndate: 2019-03-04 21:35:38\ntags: 运维\ncategories:\n- kafka\n---\n","slug":"如何优雅的下线kafkaleader","published":1,"updated":"2019-03-04T13:39:03.354Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvey008g8ceemul5cl1o","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"在express站点中使用ejs模板引擎","date":"2017-09-17T04:27:52.000Z","_content":"# 在express站点中使用ejs模板引擎\n选择ejs 是因为使用类似jsp技术，使用方式很像，页面可读性要比jade要好，个人习惯使用ejs。\n\n## 配置模板\n```\nvar express = require('express');\n\nvar app = express();\napp.set('views', './views')\napp.engine('.html', require('ejs').__express);\napp.set('view engine', 'html');\n\n```\n## 后台代码\n\n```\n app.use('/demo', function (req, res, next) {\n    res.render('demo', {'template':\"ejs});\n  });\n```\n\n\n## 模板代码\ndemo.html\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<body>\n      hello <%=template %>\n</body>\n</html>\n\n```","source":"_posts/在express站点中使用ejs模板引擎.md","raw":"---\ntitle: 在express站点中使用ejs模板引擎\ndate: 2017-09-17 12:27:52\ntags: 开发笔记\ncategories:\n- node.js\n---\n# 在express站点中使用ejs模板引擎\n选择ejs 是因为使用类似jsp技术，使用方式很像，页面可读性要比jade要好，个人习惯使用ejs。\n\n## 配置模板\n```\nvar express = require('express');\n\nvar app = express();\napp.set('views', './views')\napp.engine('.html', require('ejs').__express);\napp.set('view engine', 'html');\n\n```\n## 后台代码\n\n```\n app.use('/demo', function (req, res, next) {\n    res.render('demo', {'template':\"ejs});\n  });\n```\n\n\n## 模板代码\ndemo.html\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<body>\n      hello <%=template %>\n</body>\n</html>\n\n```","slug":"在express站点中使用ejs模板引擎","published":1,"updated":"2017-09-17T04:28:14.012Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvf0008j8cee7haumd5e","content":"<h1 id=\"在express站点中使用ejs模板引擎\"><a href=\"#在express站点中使用ejs模板引擎\" class=\"headerlink\" title=\"在express站点中使用ejs模板引擎\"></a>在express站点中使用ejs模板引擎</h1><p>选择ejs 是因为使用类似jsp技术，使用方式很像，页面可读性要比jade要好，个人习惯使用ejs。</p>\n<h2 id=\"配置模板\"><a href=\"#配置模板\" class=\"headerlink\" title=\"配置模板\"></a>配置模板</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\"></span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">app.set(&apos;views&apos;, &apos;./views&apos;)</span><br><span class=\"line\">app.engine(&apos;.html&apos;, require(&apos;ejs&apos;).__express);</span><br><span class=\"line\">app.set(&apos;view engine&apos;, &apos;html&apos;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"后台代码\"><a href=\"#后台代码\" class=\"headerlink\" title=\"后台代码\"></a>后台代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">app.use(&apos;/demo&apos;, function (req, res, next) &#123;</span><br><span class=\"line\">   res.render(&apos;demo&apos;, &#123;&apos;template&apos;:&quot;ejs&#125;);</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"模板代码\"><a href=\"#模板代码\" class=\"headerlink\" title=\"模板代码\"></a>模板代码</h2><p>demo.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=&quot;en&quot;&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">      hello &lt;%=template %&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"在express站点中使用ejs模板引擎\"><a href=\"#在express站点中使用ejs模板引擎\" class=\"headerlink\" title=\"在express站点中使用ejs模板引擎\"></a>在express站点中使用ejs模板引擎</h1><p>选择ejs 是因为使用类似jsp技术，使用方式很像，页面可读性要比jade要好，个人习惯使用ejs。</p>\n<h2 id=\"配置模板\"><a href=\"#配置模板\" class=\"headerlink\" title=\"配置模板\"></a>配置模板</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var express = require(&apos;express&apos;);</span><br><span class=\"line\"></span><br><span class=\"line\">var app = express();</span><br><span class=\"line\">app.set(&apos;views&apos;, &apos;./views&apos;)</span><br><span class=\"line\">app.engine(&apos;.html&apos;, require(&apos;ejs&apos;).__express);</span><br><span class=\"line\">app.set(&apos;view engine&apos;, &apos;html&apos;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"后台代码\"><a href=\"#后台代码\" class=\"headerlink\" title=\"后台代码\"></a>后台代码</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">app.use(&apos;/demo&apos;, function (req, res, next) &#123;</span><br><span class=\"line\">   res.render(&apos;demo&apos;, &#123;&apos;template&apos;:&quot;ejs&#125;);</span><br><span class=\"line\"> &#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"模板代码\"><a href=\"#模板代码\" class=\"headerlink\" title=\"模板代码\"></a>模板代码</h2><p>demo.html<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!DOCTYPE html&gt;</span><br><span class=\"line\">&lt;html lang=&quot;en&quot;&gt;</span><br><span class=\"line\">&lt;body&gt;</span><br><span class=\"line\">      hello &lt;%=template %&gt;</span><br><span class=\"line\">&lt;/body&gt;</span><br><span class=\"line\">&lt;/html&gt;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"利用外部表读取orc文件","date":"2016-08-21T12:45:56.000Z","_content":"# 利用外部表读取orc文件\n\n## 前言\n因为orc文件压缩，并且可以快速加载到hive中，因为这种应用于hadoop平台的上文件获得许多开发者的注意。\n\nORC : Optimized Row Columnar (ORC) file\n据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。它的设计目标是来克服Hive其他格式的缺陷。运用ORC File可以提高Hive的读、写以及处理数据的性能。 \n## 方案\n可以利用构建hive中外部表来读取orc文件。\n### 1.建表\n其中location即为orc文件所在的目录，该目录下新增的文件，都可以被hive查出\n```\nCREATE EXTERNAL TABLE test_orc_v2(\nstatus string,\ntype string\n)\nstored as orc location \"/user/test\";\n```\n### 2.orc文件生成\n```\nimport java.io.IOException;\nimport java.util.List;\n\nimport org.apache.crunch.types.orc.OrcUtils;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.ql.io.orc.OrcFile;\nimport org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions;\nimport org.apache.hadoop.hive.ql.io.orc.OrcStruct;\nimport org.apache.hadoop.hive.ql.io.orc.Reader;\nimport org.apache.hadoop.hive.ql.io.orc.RecordReader;\nimport org.apache.hadoop.hive.ql.io.orc.Writer;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\nimport org.apache.hadoop.io.Text;\npublic class HdfsFileTest {\n\n\tpublic static void main(String[] args) throws IOException {\n\t\tSystem.setProperty(\"hadoop.home.dir\", \"D:/hadoop\");\n\t\tHdfsFileTest test = new HdfsFileTest();\n\t\ttest.createOrcFile();\n\t}\n\n\tpublic void createOrcFile() throws IOException {\n\t\tString typeStr = \"struct<status:string,type:string>\";\n\t\tTypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeStr);\n\t\tObjectInspector inspector = OrcStruct.createObjectInspector(typeInfo);\n\n\n\t\tConfiguration conf = new Configuration();\n\t\tPath tempPath = new Path(\"/user/test/test1.orc\");\n\n\t\tWriter writer = OrcFile.createWriter(tempPath, OrcFile.writerOptions(conf).inspector(inspector).stripeSize(100000).bufferSize(10000));\n        \n\t\tOrcStruct struct = OrcUtils.createOrcStruct(typeInfo,new Text(\"OK1\"),new Text(\"http2\"));\n\t\t\n\t\twriter.addRow(struct);\n\t\twriter.close();\n\t\t\n\t\tReaderOptions options = OrcFile.readerOptions(conf); \n\t    Reader reader = OrcFile.createReader(tempPath, options); \n\t    RecordReader rows = reader.rows(); \n\t \n\t    List  next =   (List) ObjectInspectorUtils.copyToStandardJavaObject(rows.next(null), \n\t        reader.getObjectInspector()); \n\t    System.out.println(next.toString());\n\t    rows.close(); \n\t}\n\n}\n```\n其中OrcUtils使用如下依赖，在pom中注意添加\n```\n<dependency>\n\t<groupId>org.apache.crunch</groupId>\n\t<artifactId>crunch-hive</artifactId>\n\t<version>0.14.0</version>\n</dependency>\n```\n","source":"_posts/利用外部表读取orc文件.md","raw":"---\ntitle: 利用外部表读取orc文件\ndate: 2016-08-21 20:45:56\ntags: 大数据\ncategories:\n- hive\n---\n# 利用外部表读取orc文件\n\n## 前言\n因为orc文件压缩，并且可以快速加载到hive中，因为这种应用于hadoop平台的上文件获得许多开发者的注意。\n\nORC : Optimized Row Columnar (ORC) file\n据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。它的设计目标是来克服Hive其他格式的缺陷。运用ORC File可以提高Hive的读、写以及处理数据的性能。 \n## 方案\n可以利用构建hive中外部表来读取orc文件。\n### 1.建表\n其中location即为orc文件所在的目录，该目录下新增的文件，都可以被hive查出\n```\nCREATE EXTERNAL TABLE test_orc_v2(\nstatus string,\ntype string\n)\nstored as orc location \"/user/test\";\n```\n### 2.orc文件生成\n```\nimport java.io.IOException;\nimport java.util.List;\n\nimport org.apache.crunch.types.orc.OrcUtils;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.ql.io.orc.OrcFile;\nimport org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions;\nimport org.apache.hadoop.hive.ql.io.orc.OrcStruct;\nimport org.apache.hadoop.hive.ql.io.orc.Reader;\nimport org.apache.hadoop.hive.ql.io.orc.RecordReader;\nimport org.apache.hadoop.hive.ql.io.orc.Writer;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\nimport org.apache.hadoop.io.Text;\npublic class HdfsFileTest {\n\n\tpublic static void main(String[] args) throws IOException {\n\t\tSystem.setProperty(\"hadoop.home.dir\", \"D:/hadoop\");\n\t\tHdfsFileTest test = new HdfsFileTest();\n\t\ttest.createOrcFile();\n\t}\n\n\tpublic void createOrcFile() throws IOException {\n\t\tString typeStr = \"struct<status:string,type:string>\";\n\t\tTypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeStr);\n\t\tObjectInspector inspector = OrcStruct.createObjectInspector(typeInfo);\n\n\n\t\tConfiguration conf = new Configuration();\n\t\tPath tempPath = new Path(\"/user/test/test1.orc\");\n\n\t\tWriter writer = OrcFile.createWriter(tempPath, OrcFile.writerOptions(conf).inspector(inspector).stripeSize(100000).bufferSize(10000));\n        \n\t\tOrcStruct struct = OrcUtils.createOrcStruct(typeInfo,new Text(\"OK1\"),new Text(\"http2\"));\n\t\t\n\t\twriter.addRow(struct);\n\t\twriter.close();\n\t\t\n\t\tReaderOptions options = OrcFile.readerOptions(conf); \n\t    Reader reader = OrcFile.createReader(tempPath, options); \n\t    RecordReader rows = reader.rows(); \n\t \n\t    List  next =   (List) ObjectInspectorUtils.copyToStandardJavaObject(rows.next(null), \n\t        reader.getObjectInspector()); \n\t    System.out.println(next.toString());\n\t    rows.close(); \n\t}\n\n}\n```\n其中OrcUtils使用如下依赖，在pom中注意添加\n```\n<dependency>\n\t<groupId>org.apache.crunch</groupId>\n\t<artifactId>crunch-hive</artifactId>\n\t<version>0.14.0</version>\n</dependency>\n```\n","slug":"利用外部表读取orc文件","published":1,"updated":"2016-08-21T12:46:37.976Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvf3008o8cee2i3wawyw","content":"<h1 id=\"利用外部表读取orc文件\"><a href=\"#利用外部表读取orc文件\" class=\"headerlink\" title=\"利用外部表读取orc文件\"></a>利用外部表读取orc文件</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>因为orc文件压缩，并且可以快速加载到hive中，因为这种应用于hadoop平台的上文件获得许多开发者的注意。</p>\n<p>ORC : Optimized Row Columnar (ORC) file<br>据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。它的设计目标是来克服Hive其他格式的缺陷。运用ORC File可以提高Hive的读、写以及处理数据的性能。 </p>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><p>可以利用构建hive中外部表来读取orc文件。</p>\n<h3 id=\"1-建表\"><a href=\"#1-建表\" class=\"headerlink\" title=\"1.建表\"></a>1.建表</h3><p>其中location即为orc文件所在的目录，该目录下新增的文件，都可以被hive查出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE test_orc_v2(</span><br><span class=\"line\">status string,</span><br><span class=\"line\">type string</span><br><span class=\"line\">)</span><br><span class=\"line\">stored as orc location &quot;/user/test&quot;;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-orc文件生成\"><a href=\"#2-orc文件生成\" class=\"headerlink\" title=\"2.orc文件生成\"></a>2.orc文件生成</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.io.IOException;</span><br><span class=\"line\">import java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.crunch.types.orc.OrcUtils;</span><br><span class=\"line\">import org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\">import org.apache.hadoop.fs.Path;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcFile;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcStruct;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.Reader;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.RecordReader;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.Writer;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\">public class HdfsFileTest &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static void main(String[] args) throws IOException &#123;</span><br><span class=\"line\">\t\tSystem.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:/hadoop&quot;);</span><br><span class=\"line\">\t\tHdfsFileTest test = new HdfsFileTest();</span><br><span class=\"line\">\t\ttest.createOrcFile();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void createOrcFile() throws IOException &#123;</span><br><span class=\"line\">\t\tString typeStr = &quot;struct&lt;status:string,type:string&gt;&quot;;</span><br><span class=\"line\">\t\tTypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeStr);</span><br><span class=\"line\">\t\tObjectInspector inspector = OrcStruct.createObjectInspector(typeInfo);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tConfiguration conf = new Configuration();</span><br><span class=\"line\">\t\tPath tempPath = new Path(&quot;/user/test/test1.orc&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tWriter writer = OrcFile.createWriter(tempPath, OrcFile.writerOptions(conf).inspector(inspector).stripeSize(100000).bufferSize(10000));</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tOrcStruct struct = OrcUtils.createOrcStruct(typeInfo,new Text(&quot;OK1&quot;),new Text(&quot;http2&quot;));</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\twriter.addRow(struct);</span><br><span class=\"line\">\t\twriter.close();</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tReaderOptions options = OrcFile.readerOptions(conf); </span><br><span class=\"line\">\t    Reader reader = OrcFile.createReader(tempPath, options); </span><br><span class=\"line\">\t    RecordReader rows = reader.rows(); </span><br><span class=\"line\">\t </span><br><span class=\"line\">\t    List  next =   (List) ObjectInspectorUtils.copyToStandardJavaObject(rows.next(null), </span><br><span class=\"line\">\t        reader.getObjectInspector()); </span><br><span class=\"line\">\t    System.out.println(next.toString());</span><br><span class=\"line\">\t    rows.close(); </span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中OrcUtils使用如下依赖，在pom中注意添加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.crunch&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;crunch-hive&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.14.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"利用外部表读取orc文件\"><a href=\"#利用外部表读取orc文件\" class=\"headerlink\" title=\"利用外部表读取orc文件\"></a>利用外部表读取orc文件</h1><h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>因为orc文件压缩，并且可以快速加载到hive中，因为这种应用于hadoop平台的上文件获得许多开发者的注意。</p>\n<p>ORC : Optimized Row Columnar (ORC) file<br>据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。它的设计目标是来克服Hive其他格式的缺陷。运用ORC File可以提高Hive的读、写以及处理数据的性能。 </p>\n<h2 id=\"方案\"><a href=\"#方案\" class=\"headerlink\" title=\"方案\"></a>方案</h2><p>可以利用构建hive中外部表来读取orc文件。</p>\n<h3 id=\"1-建表\"><a href=\"#1-建表\" class=\"headerlink\" title=\"1.建表\"></a>1.建表</h3><p>其中location即为orc文件所在的目录，该目录下新增的文件，都可以被hive查出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE EXTERNAL TABLE test_orc_v2(</span><br><span class=\"line\">status string,</span><br><span class=\"line\">type string</span><br><span class=\"line\">)</span><br><span class=\"line\">stored as orc location &quot;/user/test&quot;;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-orc文件生成\"><a href=\"#2-orc文件生成\" class=\"headerlink\" title=\"2.orc文件生成\"></a>2.orc文件生成</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.io.IOException;</span><br><span class=\"line\">import java.util.List;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.crunch.types.orc.OrcUtils;</span><br><span class=\"line\">import org.apache.hadoop.conf.Configuration;</span><br><span class=\"line\">import org.apache.hadoop.fs.Path;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcFile;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcFile.ReaderOptions;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.OrcStruct;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.Reader;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.RecordReader;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.io.orc.Writer;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\">public class HdfsFileTest &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static void main(String[] args) throws IOException &#123;</span><br><span class=\"line\">\t\tSystem.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:/hadoop&quot;);</span><br><span class=\"line\">\t\tHdfsFileTest test = new HdfsFileTest();</span><br><span class=\"line\">\t\ttest.createOrcFile();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic void createOrcFile() throws IOException &#123;</span><br><span class=\"line\">\t\tString typeStr = &quot;struct&lt;status:string,type:string&gt;&quot;;</span><br><span class=\"line\">\t\tTypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeStr);</span><br><span class=\"line\">\t\tObjectInspector inspector = OrcStruct.createObjectInspector(typeInfo);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tConfiguration conf = new Configuration();</span><br><span class=\"line\">\t\tPath tempPath = new Path(&quot;/user/test/test1.orc&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tWriter writer = OrcFile.createWriter(tempPath, OrcFile.writerOptions(conf).inspector(inspector).stripeSize(100000).bufferSize(10000));</span><br><span class=\"line\">        </span><br><span class=\"line\">\t\tOrcStruct struct = OrcUtils.createOrcStruct(typeInfo,new Text(&quot;OK1&quot;),new Text(&quot;http2&quot;));</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\twriter.addRow(struct);</span><br><span class=\"line\">\t\twriter.close();</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tReaderOptions options = OrcFile.readerOptions(conf); </span><br><span class=\"line\">\t    Reader reader = OrcFile.createReader(tempPath, options); </span><br><span class=\"line\">\t    RecordReader rows = reader.rows(); </span><br><span class=\"line\">\t </span><br><span class=\"line\">\t    List  next =   (List) ObjectInspectorUtils.copyToStandardJavaObject(rows.next(null), </span><br><span class=\"line\">\t        reader.getObjectInspector()); </span><br><span class=\"line\">\t    System.out.println(next.toString());</span><br><span class=\"line\">\t    rows.close(); </span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中OrcUtils使用如下依赖，在pom中注意添加<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.crunch&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;crunch-hive&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.14.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"如何精准化爬取github repository给star用户信息","date":"2019-03-13T12:41:26.000Z","_content":"## 前言\n最近开源了一个redis内存分析一站式平台RCT,想让更多的人指定我们这个项目，正好看到同类几个开源项目，就想着爬取关注的用户，给他们发邮件，推广一下我们的项目。\n## 实践\n因为本地linux机器安装的python版本为2.7.0，因此使用python2语法\n### 安装依赖\n```\n$ pip install PyGithub\n```\n### 代码实现\n```\nfrom github import Github\nimport os\nimport codecs\ndef create(newfile):\n    if not os.path.exists(newfile):\n         f = open(newfile,'w')\n         print newfile\n         f.close()\n    return\nfileName='rct.txt'\nrespository='xaecbd/RCT'\ng=Github(\"token\",timeout=300)\n\nrepo=g.get_repo(respository)\nstargazers=repo.get_stargazers_with_dates()\ncreate(fileName)\nwith codecs.open(fileName,'a', 'utf-8') as f:\n    for people in stargazers:\n        print people.user.email\n        if people.user.email is not None:\n            if people.user.name is None:\n                name = people.user.email\n            else:\n                name = people.user.name\n            data=(name)+\":\"+(people.user.email)+\"\\r\\n\"\n            f.write(data)\n```\n### 运行\n```\npython crawer.py\n```\n### 总结\n在运行过程中发现关于用户数据是输出的时候才再去github查询的，这也就意味着，及时stargazers已经获取，还会继续去调用github api。可能会存在网络问题。\n## 参考\n1. [怎样通过GitHub API获取某个项目被Star的详细信息](https://blog.csdn.net/qysh123/article/details/79782963)\n2. [PyGithub](https://github.com/PyGithub/PyGithub)","source":"_posts/如何精准化爬取github-repository给star用户信息.md","raw":"---\ntitle: 如何精准化爬取github repository给star用户信息\ndate: 2019-03-13 20:41:26\ntags: 爬虫\ncategories:\n- python\n---\n## 前言\n最近开源了一个redis内存分析一站式平台RCT,想让更多的人指定我们这个项目，正好看到同类几个开源项目，就想着爬取关注的用户，给他们发邮件，推广一下我们的项目。\n## 实践\n因为本地linux机器安装的python版本为2.7.0，因此使用python2语法\n### 安装依赖\n```\n$ pip install PyGithub\n```\n### 代码实现\n```\nfrom github import Github\nimport os\nimport codecs\ndef create(newfile):\n    if not os.path.exists(newfile):\n         f = open(newfile,'w')\n         print newfile\n         f.close()\n    return\nfileName='rct.txt'\nrespository='xaecbd/RCT'\ng=Github(\"token\",timeout=300)\n\nrepo=g.get_repo(respository)\nstargazers=repo.get_stargazers_with_dates()\ncreate(fileName)\nwith codecs.open(fileName,'a', 'utf-8') as f:\n    for people in stargazers:\n        print people.user.email\n        if people.user.email is not None:\n            if people.user.name is None:\n                name = people.user.email\n            else:\n                name = people.user.name\n            data=(name)+\":\"+(people.user.email)+\"\\r\\n\"\n            f.write(data)\n```\n### 运行\n```\npython crawer.py\n```\n### 总结\n在运行过程中发现关于用户数据是输出的时候才再去github查询的，这也就意味着，及时stargazers已经获取，还会继续去调用github api。可能会存在网络问题。\n## 参考\n1. [怎样通过GitHub API获取某个项目被Star的详细信息](https://blog.csdn.net/qysh123/article/details/79782963)\n2. [PyGithub](https://github.com/PyGithub/PyGithub)","slug":"如何精准化爬取github-repository给star用户信息","published":1,"updated":"2019-03-13T12:43:03.040Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvf5008r8cee812j5558","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近开源了一个redis内存分析一站式平台RCT,想让更多的人指定我们这个项目，正好看到同类几个开源项目，就想着爬取关注的用户，给他们发邮件，推广一下我们的项目。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>因为本地linux机器安装的python版本为2.7.0，因此使用python2语法</p>\n<h3 id=\"安装依赖\"><a href=\"#安装依赖\" class=\"headerlink\" title=\"安装依赖\"></a>安装依赖</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install PyGithub</span><br></pre></td></tr></table></figure>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from github import Github</span><br><span class=\"line\">import os</span><br><span class=\"line\">import codecs</span><br><span class=\"line\">def create(newfile):</span><br><span class=\"line\">    if not os.path.exists(newfile):</span><br><span class=\"line\">         f = open(newfile,&apos;w&apos;)</span><br><span class=\"line\">         print newfile</span><br><span class=\"line\">         f.close()</span><br><span class=\"line\">    return</span><br><span class=\"line\">fileName=&apos;rct.txt&apos;</span><br><span class=\"line\">respository=&apos;xaecbd/RCT&apos;</span><br><span class=\"line\">g=Github(&quot;token&quot;,timeout=300)</span><br><span class=\"line\"></span><br><span class=\"line\">repo=g.get_repo(respository)</span><br><span class=\"line\">stargazers=repo.get_stargazers_with_dates()</span><br><span class=\"line\">create(fileName)</span><br><span class=\"line\">with codecs.open(fileName,&apos;a&apos;, &apos;utf-8&apos;) as f:</span><br><span class=\"line\">    for people in stargazers:</span><br><span class=\"line\">        print people.user.email</span><br><span class=\"line\">        if people.user.email is not None:</span><br><span class=\"line\">            if people.user.name is None:</span><br><span class=\"line\">                name = people.user.email</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                name = people.user.name</span><br><span class=\"line\">            data=(name)+&quot;:&quot;+(people.user.email)+&quot;\\r\\n&quot;</span><br><span class=\"line\">            f.write(data)</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python crawer.py</span><br></pre></td></tr></table></figure>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在运行过程中发现关于用户数据是输出的时候才再去github查询的，这也就意味着，及时stargazers已经获取，还会继续去调用github api。可能会存在网络问题。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/qysh123/article/details/79782963\" target=\"_blank\" rel=\"noopener\">怎样通过GitHub API获取某个项目被Star的详细信息</a></li>\n<li><a href=\"https://github.com/PyGithub/PyGithub\" target=\"_blank\" rel=\"noopener\">PyGithub</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>最近开源了一个redis内存分析一站式平台RCT,想让更多的人指定我们这个项目，正好看到同类几个开源项目，就想着爬取关注的用户，给他们发邮件，推广一下我们的项目。</p>\n<h2 id=\"实践\"><a href=\"#实践\" class=\"headerlink\" title=\"实践\"></a>实践</h2><p>因为本地linux机器安装的python版本为2.7.0，因此使用python2语法</p>\n<h3 id=\"安装依赖\"><a href=\"#安装依赖\" class=\"headerlink\" title=\"安装依赖\"></a>安装依赖</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ pip install PyGithub</span><br></pre></td></tr></table></figure>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from github import Github</span><br><span class=\"line\">import os</span><br><span class=\"line\">import codecs</span><br><span class=\"line\">def create(newfile):</span><br><span class=\"line\">    if not os.path.exists(newfile):</span><br><span class=\"line\">         f = open(newfile,&apos;w&apos;)</span><br><span class=\"line\">         print newfile</span><br><span class=\"line\">         f.close()</span><br><span class=\"line\">    return</span><br><span class=\"line\">fileName=&apos;rct.txt&apos;</span><br><span class=\"line\">respository=&apos;xaecbd/RCT&apos;</span><br><span class=\"line\">g=Github(&quot;token&quot;,timeout=300)</span><br><span class=\"line\"></span><br><span class=\"line\">repo=g.get_repo(respository)</span><br><span class=\"line\">stargazers=repo.get_stargazers_with_dates()</span><br><span class=\"line\">create(fileName)</span><br><span class=\"line\">with codecs.open(fileName,&apos;a&apos;, &apos;utf-8&apos;) as f:</span><br><span class=\"line\">    for people in stargazers:</span><br><span class=\"line\">        print people.user.email</span><br><span class=\"line\">        if people.user.email is not None:</span><br><span class=\"line\">            if people.user.name is None:</span><br><span class=\"line\">                name = people.user.email</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                name = people.user.name</span><br><span class=\"line\">            data=(name)+&quot;:&quot;+(people.user.email)+&quot;\\r\\n&quot;</span><br><span class=\"line\">            f.write(data)</span><br></pre></td></tr></table></figure>\n<h3 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python crawer.py</span><br></pre></td></tr></table></figure>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在运行过程中发现关于用户数据是输出的时候才再去github查询的，这也就意味着，及时stargazers已经获取，还会继续去调用github api。可能会存在网络问题。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/qysh123/article/details/79782963\" target=\"_blank\" rel=\"noopener\">怎样通过GitHub API获取某个项目被Star的详细信息</a></li>\n<li><a href=\"https://github.com/PyGithub/PyGithub\" target=\"_blank\" rel=\"noopener\">PyGithub</a></li>\n</ol>\n"},{"title":"开源项目学习记录","date":"2016-07-10T02:41:46.000Z","_content":"# 开源项目学习记录\n1. Twemproxy\n-简介：应用于redis代理服务，做客户端与服务端实例代理，数据自动分片，在redis cluster不成熟前被业界大量使用。\n-地址：https://github.com/twitter/twemproxy\n2. divlote collector\n-简介：用来收集web用户点击行为数据，并将数据发送到kafka,hadoop中，高效简单便捷。\n-地址：http://divolte.io/\n3. cachecloud\n-简介：  CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。\n-地址：https://github.com/sohutv/cachecloud\n4. ElasticSearch+Kibanna+Packetbeat\n-简介：ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshar。\n-地址：https://www.elastic.co/downloads/beats","source":"_posts/开源项目学习记录.md","raw":"---\ntitle: 开源项目学习记录\ndate: 2016-07-10 10:41:46\ntags:\ncategories:\n- 记录\n---\n# 开源项目学习记录\n1. Twemproxy\n-简介：应用于redis代理服务，做客户端与服务端实例代理，数据自动分片，在redis cluster不成熟前被业界大量使用。\n-地址：https://github.com/twitter/twemproxy\n2. divlote collector\n-简介：用来收集web用户点击行为数据，并将数据发送到kafka,hadoop中，高效简单便捷。\n-地址：http://divolte.io/\n3. cachecloud\n-简介：  CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。\n-地址：https://github.com/sohutv/cachecloud\n4. ElasticSearch+Kibanna+Packetbeat\n-简介：ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshar。\n-地址：https://www.elastic.co/downloads/beats","slug":"开源项目学习记录","published":1,"updated":"2016-07-18T13:35:50.040Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvf7008v8ceemh1v768u","content":"<h1 id=\"开源项目学习记录\"><a href=\"#开源项目学习记录\" class=\"headerlink\" title=\"开源项目学习记录\"></a>开源项目学习记录</h1><ol>\n<li>Twemproxy<br>-简介：应用于redis代理服务，做客户端与服务端实例代理，数据自动分片，在redis cluster不成熟前被业界大量使用。<br>-地址：<a href=\"https://github.com/twitter/twemproxy\" target=\"_blank\" rel=\"noopener\">https://github.com/twitter/twemproxy</a></li>\n<li>divlote collector<br>-简介：用来收集web用户点击行为数据，并将数据发送到kafka,hadoop中，高效简单便捷。<br>-地址：<a href=\"http://divolte.io/\" target=\"_blank\" rel=\"noopener\">http://divolte.io/</a></li>\n<li>cachecloud<br>-简介：  CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。<br>-地址：<a href=\"https://github.com/sohutv/cachecloud\" target=\"_blank\" rel=\"noopener\">https://github.com/sohutv/cachecloud</a></li>\n<li>ElasticSearch+Kibanna+Packetbeat<br>-简介：ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshar。<br>-地址：<a href=\"https://www.elastic.co/downloads/beats\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/downloads/beats</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"开源项目学习记录\"><a href=\"#开源项目学习记录\" class=\"headerlink\" title=\"开源项目学习记录\"></a>开源项目学习记录</h1><ol>\n<li>Twemproxy<br>-简介：应用于redis代理服务，做客户端与服务端实例代理，数据自动分片，在redis cluster不成熟前被业界大量使用。<br>-地址：<a href=\"https://github.com/twitter/twemproxy\" target=\"_blank\" rel=\"noopener\">https://github.com/twitter/twemproxy</a></li>\n<li>divlote collector<br>-简介：用来收集web用户点击行为数据，并将数据发送到kafka,hadoop中，高效简单便捷。<br>-地址：<a href=\"http://divolte.io/\" target=\"_blank\" rel=\"noopener\">http://divolte.io/</a></li>\n<li>cachecloud<br>-简介：  CacheCloud提供一个Redis云管理平台：实现多种类型(Redis Standalone、Redis Sentinel、Redis Cluster)自动部署、解决Redis实例碎片化现象、提供完善统计、监控、运维功能、减少运维成本和误操作，提高机器的利用率，提供灵活的伸缩性，提供方便的接入客户端。<br>-地址：<a href=\"https://github.com/sohutv/cachecloud\" target=\"_blank\" rel=\"noopener\">https://github.com/sohutv/cachecloud</a></li>\n<li>ElasticSearch+Kibanna+Packetbeat<br>-简介：ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎。Kibanna是针对ElasticSearch一个界面UI。Packetbeat是监控网络数据包一个工具，分布式wireshar。<br>-地址：<a href=\"https://www.elastic.co/downloads/beats\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/downloads/beats</a></li>\n</ol>\n"},{"title":"浅析零拷贝技术","toc":false,"date":"2019-06-14T13:49:43.000Z","_content":"\n## 前言\n\n>零拷贝（英语：Zero-copy）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。\n\n零拷贝操作减少了**在用户空间与内核空间之间切换模式的次数**。\n\n举例来说，如果要读取一个文件并通过网络发送它，传统方式下如下图，**传统的I/O操作进行了4次用户空间与内核空间的上下文切换，以及4次数据拷贝。其中4次数据拷贝中包括了2次DMA拷贝和2次CPU拷贝**。通过零拷贝技术完成相同的操作，减少了在用户空间与内核空间交互，并且不需要CPU复制数据。\n\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-1.png)\n\n\n## linux中零拷贝技术\n\n### Linux系统的“用户空间”和“内核空间”\n\n从Linux系统上看，除了引导系统的BIN区，整个内存空间主要被分成两个部分：内核空间(Kernel space)、用户空间(User space)。\n\n“用户空间”和“内核空间”的空间、操作权限以及作用都是不一样的。\n\n内核空间是Linux自身使用的内存空间，主要提供给程序调度、内存分配、连接硬件资源等程序逻辑使用；用户空间则是提供给各个进程的主要空间。\n\n用户空间不具有访问内核空间资源的权限，因此如果应用程序需要使用到内核空间的资源，则需要通过系统调用来完成：从用户空间切换到内核空间，然后在完成相关操作后再从内核空间切换回用户空间。\n### Linux 中零拷贝技术的实现方向\n\n- 直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，但是硬件上的数据不会拷贝一份到内核空间，而是直接拷贝至了用户空间，因此直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。\n\n- 在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝。本文主要讨论的就是该方式下的零拷贝机制。\n\n- copy-on-write(写时复制技术)：在某些情况下，Linux操作系统的内核空间缓冲区可能被多个应用程序所共享，操作系统有可能会将用户空间缓冲区地址映射到内核空间缓存区中。当应用程序需要对共享的数据进行修改的时候，才需要真正地拷贝数据到应用程序的用户空间缓冲区中，并且对自己用户空间的缓冲区的数据进行修改不会影响到其他共享数据的应用程序。所以，如果应用程序不需要对数据进行任何修改的话，就不会存在数据从系统内核空间缓冲区拷贝到用户空间缓冲区的操作。\n\n### mmap\n\n```\nbuf = mmap(diskfd, len);\nwrite(sockfd, buf, len);\n```\n应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。\n\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-2.png)\n\n使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。\n\n当然也有办法解决，本文忽略解决办法。\n\n### sendfile\n\n```\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);\n```\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D-03.jpg)\n\nsendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝。\n### splice\n\nsendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据：\n```\n#define _GNU_SOURCE         /* See feature_test_macros(7) */\n#include <fcntl.h>\nssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);\n```\n## java中如何使用零拷贝\n\n## DMA简介\n\nio读写的方式：中断，DMA\n\nDMA 直接内存存取,是允许外设组件将I/O数据直接传送到主存储器中并且传输不需要CPU的参与，以此将CPU解放出来去完成其他的事情。，相较于中断方式，减少cpu中断次数，不用cpu拷贝数据。\n\n主要流程如下：\n\n1. 用户进程发起数据读取请求\n2. 系统调度为该进程分配cpu\n3. cpu向DMA发送io请求\n4. 用户进程等待io完成，让出cpu\n5. 系统调度cpu执行其他任务\n6. 数据写入至io控制器的缓冲寄存器\n7. DMA不断获取缓冲寄存器中的数据（需要cpu时钟）\n8. 传输至内存（需要cpu时钟）\n9. 所需的全部数据获取完毕后向cpu发出中断信号\n\n[更多参考](https://blog.csdn.net/xiaofei0859/article/details/74321216)\n\n## 参考\n\n1. [浅析Linux中的零拷贝技术](https://www.jianshu.com/p/fad3339e3448)\n2. [零复制](https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6)\n3. [零拷贝原理-数据的收发-软中断和DMA](https://blog.csdn.net/xiaofei0859/article/details/74321216)\n4. [浅谈 Linux下的零拷贝机制](https://cloud.tencent.com/developer/article/1152609)","source":"_posts/浅析零拷贝技术.md","raw":"---\ntitle: 浅析零拷贝技术\ntags:\n  - 基础理论知识\ncategories:\n  - linux\ntoc: false\ndate: 2019-06-14 21:49:43\n---\n\n## 前言\n\n>零拷贝（英语：Zero-copy）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。\n\n零拷贝操作减少了**在用户空间与内核空间之间切换模式的次数**。\n\n举例来说，如果要读取一个文件并通过网络发送它，传统方式下如下图，**传统的I/O操作进行了4次用户空间与内核空间的上下文切换，以及4次数据拷贝。其中4次数据拷贝中包括了2次DMA拷贝和2次CPU拷贝**。通过零拷贝技术完成相同的操作，减少了在用户空间与内核空间交互，并且不需要CPU复制数据。\n\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-1.png)\n\n\n## linux中零拷贝技术\n\n### Linux系统的“用户空间”和“内核空间”\n\n从Linux系统上看，除了引导系统的BIN区，整个内存空间主要被分成两个部分：内核空间(Kernel space)、用户空间(User space)。\n\n“用户空间”和“内核空间”的空间、操作权限以及作用都是不一样的。\n\n内核空间是Linux自身使用的内存空间，主要提供给程序调度、内存分配、连接硬件资源等程序逻辑使用；用户空间则是提供给各个进程的主要空间。\n\n用户空间不具有访问内核空间资源的权限，因此如果应用程序需要使用到内核空间的资源，则需要通过系统调用来完成：从用户空间切换到内核空间，然后在完成相关操作后再从内核空间切换回用户空间。\n### Linux 中零拷贝技术的实现方向\n\n- 直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，但是硬件上的数据不会拷贝一份到内核空间，而是直接拷贝至了用户空间，因此直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。\n\n- 在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝。本文主要讨论的就是该方式下的零拷贝机制。\n\n- copy-on-write(写时复制技术)：在某些情况下，Linux操作系统的内核空间缓冲区可能被多个应用程序所共享，操作系统有可能会将用户空间缓冲区地址映射到内核空间缓存区中。当应用程序需要对共享的数据进行修改的时候，才需要真正地拷贝数据到应用程序的用户空间缓冲区中，并且对自己用户空间的缓冲区的数据进行修改不会影响到其他共享数据的应用程序。所以，如果应用程序不需要对数据进行任何修改的话，就不会存在数据从系统内核空间缓冲区拷贝到用户空间缓冲区的操作。\n\n### mmap\n\n```\nbuf = mmap(diskfd, len);\nwrite(sockfd, buf, len);\n```\n应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。\n\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-2.png)\n\n使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。\n\n当然也有办法解决，本文忽略解决办法。\n\n### sendfile\n\n```\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);\n```\n![](http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D-03.jpg)\n\nsendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝。\n### splice\n\nsendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据：\n```\n#define _GNU_SOURCE         /* See feature_test_macros(7) */\n#include <fcntl.h>\nssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);\n```\n## java中如何使用零拷贝\n\n## DMA简介\n\nio读写的方式：中断，DMA\n\nDMA 直接内存存取,是允许外设组件将I/O数据直接传送到主存储器中并且传输不需要CPU的参与，以此将CPU解放出来去完成其他的事情。，相较于中断方式，减少cpu中断次数，不用cpu拷贝数据。\n\n主要流程如下：\n\n1. 用户进程发起数据读取请求\n2. 系统调度为该进程分配cpu\n3. cpu向DMA发送io请求\n4. 用户进程等待io完成，让出cpu\n5. 系统调度cpu执行其他任务\n6. 数据写入至io控制器的缓冲寄存器\n7. DMA不断获取缓冲寄存器中的数据（需要cpu时钟）\n8. 传输至内存（需要cpu时钟）\n9. 所需的全部数据获取完毕后向cpu发出中断信号\n\n[更多参考](https://blog.csdn.net/xiaofei0859/article/details/74321216)\n\n## 参考\n\n1. [浅析Linux中的零拷贝技术](https://www.jianshu.com/p/fad3339e3448)\n2. [零复制](https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6)\n3. [零拷贝原理-数据的收发-软中断和DMA](https://blog.csdn.net/xiaofei0859/article/details/74321216)\n4. [浅谈 Linux下的零拷贝机制](https://cloud.tencent.com/developer/article/1152609)","slug":"浅析零拷贝技术","published":1,"updated":"2019-06-14T13:50:09.701Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvf9008z8ceemcbcxppt","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>零拷贝（英语：Zero-copy）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。</p>\n</blockquote>\n<p>零拷贝操作减少了<strong>在用户空间与内核空间之间切换模式的次数</strong>。</p>\n<p>举例来说，如果要读取一个文件并通过网络发送它，传统方式下如下图，<strong>传统的I/O操作进行了4次用户空间与内核空间的上下文切换，以及4次数据拷贝。其中4次数据拷贝中包括了2次DMA拷贝和2次CPU拷贝</strong>。通过零拷贝技术完成相同的操作，减少了在用户空间与内核空间交互，并且不需要CPU复制数据。</p>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-1.png\" alt=\"\"></p>\n<h2 id=\"linux中零拷贝技术\"><a href=\"#linux中零拷贝技术\" class=\"headerlink\" title=\"linux中零拷贝技术\"></a>linux中零拷贝技术</h2><h3 id=\"Linux系统的“用户空间”和“内核空间”\"><a href=\"#Linux系统的“用户空间”和“内核空间”\" class=\"headerlink\" title=\"Linux系统的“用户空间”和“内核空间”\"></a>Linux系统的“用户空间”和“内核空间”</h3><p>从Linux系统上看，除了引导系统的BIN区，整个内存空间主要被分成两个部分：内核空间(Kernel space)、用户空间(User space)。</p>\n<p>“用户空间”和“内核空间”的空间、操作权限以及作用都是不一样的。</p>\n<p>内核空间是Linux自身使用的内存空间，主要提供给程序调度、内存分配、连接硬件资源等程序逻辑使用；用户空间则是提供给各个进程的主要空间。</p>\n<p>用户空间不具有访问内核空间资源的权限，因此如果应用程序需要使用到内核空间的资源，则需要通过系统调用来完成：从用户空间切换到内核空间，然后在完成相关操作后再从内核空间切换回用户空间。</p>\n<h3 id=\"Linux-中零拷贝技术的实现方向\"><a href=\"#Linux-中零拷贝技术的实现方向\" class=\"headerlink\" title=\"Linux 中零拷贝技术的实现方向\"></a>Linux 中零拷贝技术的实现方向</h3><ul>\n<li><p>直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，但是硬件上的数据不会拷贝一份到内核空间，而是直接拷贝至了用户空间，因此直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。</p>\n</li>\n<li><p>在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝。本文主要讨论的就是该方式下的零拷贝机制。</p>\n</li>\n<li><p>copy-on-write(写时复制技术)：在某些情况下，Linux操作系统的内核空间缓冲区可能被多个应用程序所共享，操作系统有可能会将用户空间缓冲区地址映射到内核空间缓存区中。当应用程序需要对共享的数据进行修改的时候，才需要真正地拷贝数据到应用程序的用户空间缓冲区中，并且对自己用户空间的缓冲区的数据进行修改不会影响到其他共享数据的应用程序。所以，如果应用程序不需要对数据进行任何修改的话，就不会存在数据从系统内核空间缓冲区拷贝到用户空间缓冲区的操作。</p>\n</li>\n</ul>\n<h3 id=\"mmap\"><a href=\"#mmap\" class=\"headerlink\" title=\"mmap\"></a>mmap</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">buf = mmap(diskfd, len);</span><br><span class=\"line\">write(sockfd, buf, len);</span><br></pre></td></tr></table></figure>\n<p>应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。</p>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-2.png\" alt=\"\"></p>\n<p>使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。</p>\n<p>当然也有办法解决，本文忽略解决办法。</p>\n<h3 id=\"sendfile\"><a href=\"#sendfile\" class=\"headerlink\" title=\"sendfile\"></a>sendfile</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D-03.jpg\" alt=\"\"></p>\n<p>sendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝。</p>\n<h3 id=\"splice\"><a href=\"#splice\" class=\"headerlink\" title=\"splice\"></a>splice</h3><p>sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _GNU_SOURCE         /* See feature_test_macros(7) */</span><br><span class=\"line\">#include &lt;fcntl.h&gt;</span><br><span class=\"line\">ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"java中如何使用零拷贝\"><a href=\"#java中如何使用零拷贝\" class=\"headerlink\" title=\"java中如何使用零拷贝\"></a>java中如何使用零拷贝</h2><h2 id=\"DMA简介\"><a href=\"#DMA简介\" class=\"headerlink\" title=\"DMA简介\"></a>DMA简介</h2><p>io读写的方式：中断，DMA</p>\n<p>DMA 直接内存存取,是允许外设组件将I/O数据直接传送到主存储器中并且传输不需要CPU的参与，以此将CPU解放出来去完成其他的事情。，相较于中断方式，减少cpu中断次数，不用cpu拷贝数据。</p>\n<p>主要流程如下：</p>\n<ol>\n<li>用户进程发起数据读取请求</li>\n<li>系统调度为该进程分配cpu</li>\n<li>cpu向DMA发送io请求</li>\n<li>用户进程等待io完成，让出cpu</li>\n<li>系统调度cpu执行其他任务</li>\n<li>数据写入至io控制器的缓冲寄存器</li>\n<li>DMA不断获取缓冲寄存器中的数据（需要cpu时钟）</li>\n<li>传输至内存（需要cpu时钟）</li>\n<li>所需的全部数据获取完毕后向cpu发出中断信号</li>\n</ol>\n<p><a href=\"https://blog.csdn.net/xiaofei0859/article/details/74321216\" target=\"_blank\" rel=\"noopener\">更多参考</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.jianshu.com/p/fad3339e3448\" target=\"_blank\" rel=\"noopener\">浅析Linux中的零拷贝技术</a></li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6\" target=\"_blank\" rel=\"noopener\">零复制</a></li>\n<li><a href=\"https://blog.csdn.net/xiaofei0859/article/details/74321216\" target=\"_blank\" rel=\"noopener\">零拷贝原理-数据的收发-软中断和DMA</a></li>\n<li><a href=\"https://cloud.tencent.com/developer/article/1152609\" target=\"_blank\" rel=\"noopener\">浅谈 Linux下的零拷贝机制</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><blockquote>\n<p>零拷贝（英语：Zero-copy）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。</p>\n</blockquote>\n<p>零拷贝操作减少了<strong>在用户空间与内核空间之间切换模式的次数</strong>。</p>\n<p>举例来说，如果要读取一个文件并通过网络发送它，传统方式下如下图，<strong>传统的I/O操作进行了4次用户空间与内核空间的上下文切换，以及4次数据拷贝。其中4次数据拷贝中包括了2次DMA拷贝和2次CPU拷贝</strong>。通过零拷贝技术完成相同的操作，减少了在用户空间与内核空间交互，并且不需要CPU复制数据。</p>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-1.png\" alt=\"\"></p>\n<h2 id=\"linux中零拷贝技术\"><a href=\"#linux中零拷贝技术\" class=\"headerlink\" title=\"linux中零拷贝技术\"></a>linux中零拷贝技术</h2><h3 id=\"Linux系统的“用户空间”和“内核空间”\"><a href=\"#Linux系统的“用户空间”和“内核空间”\" class=\"headerlink\" title=\"Linux系统的“用户空间”和“内核空间”\"></a>Linux系统的“用户空间”和“内核空间”</h3><p>从Linux系统上看，除了引导系统的BIN区，整个内存空间主要被分成两个部分：内核空间(Kernel space)、用户空间(User space)。</p>\n<p>“用户空间”和“内核空间”的空间、操作权限以及作用都是不一样的。</p>\n<p>内核空间是Linux自身使用的内存空间，主要提供给程序调度、内存分配、连接硬件资源等程序逻辑使用；用户空间则是提供给各个进程的主要空间。</p>\n<p>用户空间不具有访问内核空间资源的权限，因此如果应用程序需要使用到内核空间的资源，则需要通过系统调用来完成：从用户空间切换到内核空间，然后在完成相关操作后再从内核空间切换回用户空间。</p>\n<h3 id=\"Linux-中零拷贝技术的实现方向\"><a href=\"#Linux-中零拷贝技术的实现方向\" class=\"headerlink\" title=\"Linux 中零拷贝技术的实现方向\"></a>Linux 中零拷贝技术的实现方向</h3><ul>\n<li><p>直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，但是硬件上的数据不会拷贝一份到内核空间，而是直接拷贝至了用户空间，因此直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。</p>\n</li>\n<li><p>在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝。本文主要讨论的就是该方式下的零拷贝机制。</p>\n</li>\n<li><p>copy-on-write(写时复制技术)：在某些情况下，Linux操作系统的内核空间缓冲区可能被多个应用程序所共享，操作系统有可能会将用户空间缓冲区地址映射到内核空间缓存区中。当应用程序需要对共享的数据进行修改的时候，才需要真正地拷贝数据到应用程序的用户空间缓冲区中，并且对自己用户空间的缓冲区的数据进行修改不会影响到其他共享数据的应用程序。所以，如果应用程序不需要对数据进行任何修改的话，就不会存在数据从系统内核空间缓冲区拷贝到用户空间缓冲区的操作。</p>\n</li>\n</ul>\n<h3 id=\"mmap\"><a href=\"#mmap\" class=\"headerlink\" title=\"mmap\"></a>mmap</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">buf = mmap(diskfd, len);</span><br><span class=\"line\">write(sockfd, buf, len);</span><br></pre></td></tr></table></figure>\n<p>应用程序调用mmap()，磁盘上的数据会通过DMA被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用write(),操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。</p>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF-2.png\" alt=\"\"></p>\n<p>使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用mmap是有代价的。当你使用mmap时，你可能会遇到一些隐藏的陷阱。例如，当你的程序map了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被SIGBUS信号终止。SIGBUS信号默认会杀死你的进程并产生一个coredump,如果你的服务器这样被中止了，那会产生一笔损失。</p>\n<p>当然也有办法解决，本文忽略解决办法。</p>\n<h3 id=\"sendfile\"><a href=\"#sendfile\" class=\"headerlink\" title=\"sendfile\"></a>sendfile</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://blogstatic.aibibang.com/%E6%B5%85%E6%9E%90%E9%9B%B6%E6%8B%B7%E8%B4%9D-03.jpg\" alt=\"\"></p>\n<p>sendfile实现的零拷贝I/O只使用了2次用户空间与内核空间的上下文切换，以及3次数据的拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝。</p>\n<h3 id=\"splice\"><a href=\"#splice\" class=\"headerlink\" title=\"splice\"></a>splice</h3><p>sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在2.6.17版本引入splice系统调用，用于在两个文件描述符中移动数据：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#define _GNU_SOURCE         /* See feature_test_macros(7) */</span><br><span class=\"line\">#include &lt;fcntl.h&gt;</span><br><span class=\"line\">ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"java中如何使用零拷贝\"><a href=\"#java中如何使用零拷贝\" class=\"headerlink\" title=\"java中如何使用零拷贝\"></a>java中如何使用零拷贝</h2><h2 id=\"DMA简介\"><a href=\"#DMA简介\" class=\"headerlink\" title=\"DMA简介\"></a>DMA简介</h2><p>io读写的方式：中断，DMA</p>\n<p>DMA 直接内存存取,是允许外设组件将I/O数据直接传送到主存储器中并且传输不需要CPU的参与，以此将CPU解放出来去完成其他的事情。，相较于中断方式，减少cpu中断次数，不用cpu拷贝数据。</p>\n<p>主要流程如下：</p>\n<ol>\n<li>用户进程发起数据读取请求</li>\n<li>系统调度为该进程分配cpu</li>\n<li>cpu向DMA发送io请求</li>\n<li>用户进程等待io完成，让出cpu</li>\n<li>系统调度cpu执行其他任务</li>\n<li>数据写入至io控制器的缓冲寄存器</li>\n<li>DMA不断获取缓冲寄存器中的数据（需要cpu时钟）</li>\n<li>传输至内存（需要cpu时钟）</li>\n<li>所需的全部数据获取完毕后向cpu发出中断信号</li>\n</ol>\n<p><a href=\"https://blog.csdn.net/xiaofei0859/article/details/74321216\" target=\"_blank\" rel=\"noopener\">更多参考</a></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.jianshu.com/p/fad3339e3448\" target=\"_blank\" rel=\"noopener\">浅析Linux中的零拷贝技术</a></li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E9%9B%B6%E5%A4%8D%E5%88%B6\" target=\"_blank\" rel=\"noopener\">零复制</a></li>\n<li><a href=\"https://blog.csdn.net/xiaofei0859/article/details/74321216\" target=\"_blank\" rel=\"noopener\">零拷贝原理-数据的收发-软中断和DMA</a></li>\n<li><a href=\"https://cloud.tencent.com/developer/article/1152609\" target=\"_blank\" rel=\"noopener\">浅谈 Linux下的零拷贝机制</a></li>\n</ol>\n"},{"title":"算法篇之排序","date":"2018-10-29T10:58:52.000Z","_content":"## 初衷\n为什么我要写算法篇，记得刚入大学的第一节课，老师就教我们**软件=算法+数据结构**，而我恰恰这两点学的最差了，学习这块两个初衷：1.阿里机试折腰，让我清楚的意识到自己的问题 2.职业瓶颈，已经工作四年了，知识面有了，但是缺乏深度。暂且先选择将基础算法打扎实点。\n\n**声明以下所有的定义不具有权威性，皆来自我对该算法的理解。**\n## 1.初级排序\n### 1.1冒泡法\n这大概是最简答的排序，可我一直是搞错的。 \n**定义**：将数组相邻的数进行对比，直到选出最大值或者最小值，每次冒出一个数，后面的逻辑不再处理它\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length-1; i++) {// 控制循环次数\n\t\t\tfor (int j = 0; j < wait2Sort.length - 1 - i; j++) {\n\t\t\t\tif (wait2Sort[j] > wait2Sort[j + 1]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j + 1];\n\t\t\t\t\twait2Sort[j + 1] = temp;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\nN个数字要排序完成，总共进行N-1趟排序，每第 i 趟的排序次数为 (N-i) 次，所以可以用双重循环语句，外层控制循环多少趟，内层控制每一趟的循环次数。\n\n按说算法是这样的确实没错，但是网络上还有另外一种写法，就是第一层循环N次，按说是结果不会错，但是算法多此一举。我想追究的是会不会多算一次。例如如下写法：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length; i++) {// 控制循环次数\n\t\t\tfor (int j = 0; j < wait2Sort.length - 1 - i; j++) {\n\t\t\t\tif (wait2Sort[j] > wait2Sort[j + 1]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j + 1];\n\t\t\t\t\twait2Sort[j + 1] = temp;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n至于N次是否进行运算，我们就需要探讨当i=N-1时，是否进入第二个循环，当i=N-1时，第二个循环的条件就变成了j<N-1-(N-1)即j<0，这个永远不会成立，因此最后一次的循环不会进入二层循环。相对于正确算法只多了一次判断，性能可以忽略不计。\n### 1.2选择排序法\n#### 1.2.1初阶选择排序法\n**定义**：选取确定的数据依次与其他数据进行对比\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length - 1; i++) {\n\t\t\tfor (int j = i + 1; j < wait2Sort.length; j++) {\n\t\t\t\tif (wait2Sort[i] > wait2Sort[j]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[i];\n\t\t\t\t\twait2Sort[i] = temp;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n```\n#### 1.2.2进阶选择排序法\n**定义**：选取确定的数据依次与其他数据进行对比，但是和1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length-1; i++) {\n\t\t\tint min = i;\n\t\t\tfor (int j = i + 1; j < wait2Sort.length; j++) {\n\t\t\t\tif (wait2Sort[min] > wait2Sort[j]) {\n\t\t\t\t\tmin = j;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (min != i) {\n\t\t\t\tint temp = wait2Sort[min];\n\t\t\t\twait2Sort[min] = wait2Sort[i];\n\t\t\t\twait2Sort[i] = temp;\n\t\t\t}\n\t\t}\n```\n该算法较1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。少了多次交换，相较于前两种性能更好！\n### 1.3插入排序\n**定义**：像整理牌一样，将每一张牌插入到有序数组中适当的位置。左侧永远是有序的，当运行到数据最右端，即排序完毕，这种算法对于数组中部分数据是有序的话，性能会有很大的提升！\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 1; i < wait2Sort.length; i++) {\n\t\t\tfor (int j = i; j > 0 && (wait2Sort[j - 1]) > wait2Sort[j]; j--) {\n\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\twait2Sort[j] = wait2Sort[j - 1];\n\t\t\t\twait2Sort[j - 1] = temp;\n\t\t\t}\n\t\t}\n```\n![](https://images2015.cnblogs.com/blog/1024555/201611/1024555-20161126000335346-416319390.png)\n### 1.4希尔排序\n**定义**：希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止\n**实现**:\n```\n\t\t// 从小到大排序\n\t\tint h = 1;\n\t\twhile (h < wait2Sort.length / 3)\n\t\t\th = 3 * h + 1;\n\t\twhile (h >= 1) {\n\t\t\tfor (int i = h; i < wait2Sort.length; i++) {\n\t\t\t\tfor (int j = i; j - h >= 0 && wait2Sort[j - h] > wait2Sort[j]; j = j - h) {\n\t\t\t\t\tint temp = wait2Sort[j - h];\n\t\t\t\t\twait2Sort[j - h] = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = temp;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\th = h / 3;\n\t\t}\n```\n  上面已经演示了以4为初始间隔对包含10个数据项的数组进行排序的情况。对于更大的数组开始的间隔也应该更大。然后间隔不断减小，直到间隔变成1。\n  \n   举例来说，含有1000个数据项的数组可能先以364为增量，然后以121为增量，以40为增量，以13为增量，以4为增量，最后以 1为增量进行希尔排序。用来形成间隔的数列被称为间隔序列。这里所表示的间隔序列由**Knuth**提出，此序列是很常用的。在排序算法中，首先在一个短小的循环中使用序列的生成公式来计算出最初的间隔。h值最初被赋为1，然后应用公式<code>h=3*h+1</code>生成序列1,4,13,40,121,364，等等。当间隔大于数组大小的时候，这个过程停止。\n   \n```\n\t\t// 从小到大排序\n\t\tfor (int h = wait2Sort.length / 2; h > 0; h = h / 2) {\n\t\t\tfor (int i = h; i < wait2Sort.length; i++) {\n\t\t\t\tfor (int j = i; j>=h && wait2Sort[j - h] > wait2Sort[j]; j = j - h) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j - h];\n\t\t\t\t\twait2Sort[j - h] = temp;\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n```\n这个也是正确的，间距为N/2\n## 2.归并排序\n**定义**：归并排序,也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法\n\n如　设有数列{6，202，100，301，38，8，1}\n初始状态：6,202,100,301,38,8,1\n第一次归并后：{6,202},{100,301},{8,38},{1}，比较次数：3；\n第二次归并后：{6,100,202,301}，{1,8,38}，比较次数：4；\n第三次归并后：{1,6,8,38,100,202,301},比较次数：4；\n总的比较次数为：3+4+4=11；\n逆序数为14；\n\n**实现**:\n## 3.快速排序\n## 4.堆排序\n\n## 总结\n## 参考\n1. [图解排序算法(二)之希尔排序](https://www.cnblogs.com/chengxiao/p/6104371.html)\n","source":"_posts/算法篇之排序.md","raw":"---\ntitle: 算法篇之排序\ndate: 2018-10-29 18:58:52\ntags: 算法\ncategories:\n- 算法\n---\n## 初衷\n为什么我要写算法篇，记得刚入大学的第一节课，老师就教我们**软件=算法+数据结构**，而我恰恰这两点学的最差了，学习这块两个初衷：1.阿里机试折腰，让我清楚的意识到自己的问题 2.职业瓶颈，已经工作四年了，知识面有了，但是缺乏深度。暂且先选择将基础算法打扎实点。\n\n**声明以下所有的定义不具有权威性，皆来自我对该算法的理解。**\n## 1.初级排序\n### 1.1冒泡法\n这大概是最简答的排序，可我一直是搞错的。 \n**定义**：将数组相邻的数进行对比，直到选出最大值或者最小值，每次冒出一个数，后面的逻辑不再处理它\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length-1; i++) {// 控制循环次数\n\t\t\tfor (int j = 0; j < wait2Sort.length - 1 - i; j++) {\n\t\t\t\tif (wait2Sort[j] > wait2Sort[j + 1]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j + 1];\n\t\t\t\t\twait2Sort[j + 1] = temp;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\nN个数字要排序完成，总共进行N-1趟排序，每第 i 趟的排序次数为 (N-i) 次，所以可以用双重循环语句，外层控制循环多少趟，内层控制每一趟的循环次数。\n\n按说算法是这样的确实没错，但是网络上还有另外一种写法，就是第一层循环N次，按说是结果不会错，但是算法多此一举。我想追究的是会不会多算一次。例如如下写法：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length; i++) {// 控制循环次数\n\t\t\tfor (int j = 0; j < wait2Sort.length - 1 - i; j++) {\n\t\t\t\tif (wait2Sort[j] > wait2Sort[j + 1]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j + 1];\n\t\t\t\t\twait2Sort[j + 1] = temp;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n```\n至于N次是否进行运算，我们就需要探讨当i=N-1时，是否进入第二个循环，当i=N-1时，第二个循环的条件就变成了j<N-1-(N-1)即j<0，这个永远不会成立，因此最后一次的循环不会进入二层循环。相对于正确算法只多了一次判断，性能可以忽略不计。\n### 1.2选择排序法\n#### 1.2.1初阶选择排序法\n**定义**：选取确定的数据依次与其他数据进行对比\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length - 1; i++) {\n\t\t\tfor (int j = i + 1; j < wait2Sort.length; j++) {\n\t\t\t\tif (wait2Sort[i] > wait2Sort[j]) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[i];\n\t\t\t\t\twait2Sort[i] = temp;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n```\n#### 1.2.2进阶选择排序法\n**定义**：选取确定的数据依次与其他数据进行对比，但是和1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 0; i < wait2Sort.length-1; i++) {\n\t\t\tint min = i;\n\t\t\tfor (int j = i + 1; j < wait2Sort.length; j++) {\n\t\t\t\tif (wait2Sort[min] > wait2Sort[j]) {\n\t\t\t\t\tmin = j;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (min != i) {\n\t\t\t\tint temp = wait2Sort[min];\n\t\t\t\twait2Sort[min] = wait2Sort[i];\n\t\t\t\twait2Sort[i] = temp;\n\t\t\t}\n\t\t}\n```\n该算法较1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。少了多次交换，相较于前两种性能更好！\n### 1.3插入排序\n**定义**：像整理牌一样，将每一张牌插入到有序数组中适当的位置。左侧永远是有序的，当运行到数据最右端，即排序完毕，这种算法对于数组中部分数据是有序的话，性能会有很大的提升！\n**实现**：\n```\n\t\t// 从小到大排序\n\t\tfor (int i = 1; i < wait2Sort.length; i++) {\n\t\t\tfor (int j = i; j > 0 && (wait2Sort[j - 1]) > wait2Sort[j]; j--) {\n\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\twait2Sort[j] = wait2Sort[j - 1];\n\t\t\t\twait2Sort[j - 1] = temp;\n\t\t\t}\n\t\t}\n```\n![](https://images2015.cnblogs.com/blog/1024555/201611/1024555-20161126000335346-416319390.png)\n### 1.4希尔排序\n**定义**：希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止\n**实现**:\n```\n\t\t// 从小到大排序\n\t\tint h = 1;\n\t\twhile (h < wait2Sort.length / 3)\n\t\t\th = 3 * h + 1;\n\t\twhile (h >= 1) {\n\t\t\tfor (int i = h; i < wait2Sort.length; i++) {\n\t\t\t\tfor (int j = i; j - h >= 0 && wait2Sort[j - h] > wait2Sort[j]; j = j - h) {\n\t\t\t\t\tint temp = wait2Sort[j - h];\n\t\t\t\t\twait2Sort[j - h] = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = temp;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\th = h / 3;\n\t\t}\n```\n  上面已经演示了以4为初始间隔对包含10个数据项的数组进行排序的情况。对于更大的数组开始的间隔也应该更大。然后间隔不断减小，直到间隔变成1。\n  \n   举例来说，含有1000个数据项的数组可能先以364为增量，然后以121为增量，以40为增量，以13为增量，以4为增量，最后以 1为增量进行希尔排序。用来形成间隔的数列被称为间隔序列。这里所表示的间隔序列由**Knuth**提出，此序列是很常用的。在排序算法中，首先在一个短小的循环中使用序列的生成公式来计算出最初的间隔。h值最初被赋为1，然后应用公式<code>h=3*h+1</code>生成序列1,4,13,40,121,364，等等。当间隔大于数组大小的时候，这个过程停止。\n   \n```\n\t\t// 从小到大排序\n\t\tfor (int h = wait2Sort.length / 2; h > 0; h = h / 2) {\n\t\t\tfor (int i = h; i < wait2Sort.length; i++) {\n\t\t\t\tfor (int j = i; j>=h && wait2Sort[j - h] > wait2Sort[j]; j = j - h) {\n\t\t\t\t\tint temp = wait2Sort[j];\n\t\t\t\t\twait2Sort[j] = wait2Sort[j - h];\n\t\t\t\t\twait2Sort[j - h] = temp;\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n```\n这个也是正确的，间距为N/2\n## 2.归并排序\n**定义**：归并排序,也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法\n\n如　设有数列{6，202，100，301，38，8，1}\n初始状态：6,202,100,301,38,8,1\n第一次归并后：{6,202},{100,301},{8,38},{1}，比较次数：3；\n第二次归并后：{6,100,202,301}，{1,8,38}，比较次数：4；\n第三次归并后：{1,6,8,38,100,202,301},比较次数：4；\n总的比较次数为：3+4+4=11；\n逆序数为14；\n\n**实现**:\n## 3.快速排序\n## 4.堆排序\n\n## 总结\n## 参考\n1. [图解排序算法(二)之希尔排序](https://www.cnblogs.com/chengxiao/p/6104371.html)\n","slug":"算法篇之排序","published":1,"updated":"2018-11-14T13:52:23.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvfc00938cee6te99ur8","content":"<h2 id=\"初衷\"><a href=\"#初衷\" class=\"headerlink\" title=\"初衷\"></a>初衷</h2><p>为什么我要写算法篇，记得刚入大学的第一节课，老师就教我们<strong>软件=算法+数据结构</strong>，而我恰恰这两点学的最差了，学习这块两个初衷：1.阿里机试折腰，让我清楚的意识到自己的问题 2.职业瓶颈，已经工作四年了，知识面有了，但是缺乏深度。暂且先选择将基础算法打扎实点。</p>\n<p><strong>声明以下所有的定义不具有权威性，皆来自我对该算法的理解。</strong></p>\n<h2 id=\"1-初级排序\"><a href=\"#1-初级排序\" class=\"headerlink\" title=\"1.初级排序\"></a>1.初级排序</h2><h3 id=\"1-1冒泡法\"><a href=\"#1-1冒泡法\" class=\"headerlink\" title=\"1.1冒泡法\"></a>1.1冒泡法</h3><p>这大概是最简答的排序，可我一直是搞错的。<br><strong>定义</strong>：将数组相邻的数进行对比，直到选出最大值或者最小值，每次冒出一个数，后面的逻辑不再处理它<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length-1; i++) &#123;// 控制循环次数</span><br><span class=\"line\">\tfor (int j = 0; j &lt; wait2Sort.length - 1 - i; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[j] &gt; wait2Sort[j + 1]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j + 1];</span><br><span class=\"line\">\t\t\twait2Sort[j + 1] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>N个数字要排序完成，总共进行N-1趟排序，每第 i 趟的排序次数为 (N-i) 次，所以可以用双重循环语句，外层控制循环多少趟，内层控制每一趟的循环次数。</p>\n<p>按说算法是这样的确实没错，但是网络上还有另外一种写法，就是第一层循环N次，按说是结果不会错，但是算法多此一举。我想追究的是会不会多算一次。例如如下写法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length; i++) &#123;// 控制循环次数</span><br><span class=\"line\">\tfor (int j = 0; j &lt; wait2Sort.length - 1 - i; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[j] &gt; wait2Sort[j + 1]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j + 1];</span><br><span class=\"line\">\t\t\twait2Sort[j + 1] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>至于N次是否进行运算，我们就需要探讨当i=N-1时，是否进入第二个循环，当i=N-1时，第二个循环的条件就变成了j&lt;N-1-(N-1)即j&lt;0，这个永远不会成立，因此最后一次的循环不会进入二层循环。相对于正确算法只多了一次判断，性能可以忽略不计。</p>\n<h3 id=\"1-2选择排序法\"><a href=\"#1-2选择排序法\" class=\"headerlink\" title=\"1.2选择排序法\"></a>1.2选择排序法</h3><h4 id=\"1-2-1初阶选择排序法\"><a href=\"#1-2-1初阶选择排序法\" class=\"headerlink\" title=\"1.2.1初阶选择排序法\"></a>1.2.1初阶选择排序法</h4><p><strong>定义</strong>：选取确定的数据依次与其他数据进行对比<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length - 1; i++) &#123;</span><br><span class=\"line\">\tfor (int j = i + 1; j &lt; wait2Sort.length; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[i] &gt; wait2Sort[j]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[i];</span><br><span class=\"line\">\t\t\twait2Sort[i] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-2进阶选择排序法\"><a href=\"#1-2-2进阶选择排序法\" class=\"headerlink\" title=\"1.2.2进阶选择排序法\"></a>1.2.2进阶选择排序法</h4><p><strong>定义</strong>：选取确定的数据依次与其他数据进行对比，但是和1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length-1; i++) &#123;</span><br><span class=\"line\">\tint min = i;</span><br><span class=\"line\">\tfor (int j = i + 1; j &lt; wait2Sort.length; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[min] &gt; wait2Sort[j]) &#123;</span><br><span class=\"line\">\t\t\tmin = j;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif (min != i) &#123;</span><br><span class=\"line\">\t\tint temp = wait2Sort[min];</span><br><span class=\"line\">\t\twait2Sort[min] = wait2Sort[i];</span><br><span class=\"line\">\t\twait2Sort[i] = temp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>该算法较1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。少了多次交换，相较于前两种性能更好！</p>\n<h3 id=\"1-3插入排序\"><a href=\"#1-3插入排序\" class=\"headerlink\" title=\"1.3插入排序\"></a>1.3插入排序</h3><p><strong>定义</strong>：像整理牌一样，将每一张牌插入到有序数组中适当的位置。左侧永远是有序的，当运行到数据最右端，即排序完毕，这种算法对于数组中部分数据是有序的话，性能会有很大的提升！<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 1; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\tfor (int j = i; j &gt; 0 &amp;&amp; (wait2Sort[j - 1]) &gt; wait2Sort[j]; j--) &#123;</span><br><span class=\"line\">\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\twait2Sort[j] = wait2Sort[j - 1];</span><br><span class=\"line\">\t\twait2Sort[j - 1] = temp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://images2015.cnblogs.com/blog/1024555/201611/1024555-20161126000335346-416319390.png\" alt=\"\"></p>\n<h3 id=\"1-4希尔排序\"><a href=\"#1-4希尔排序\" class=\"headerlink\" title=\"1.4希尔排序\"></a>1.4希尔排序</h3><p><strong>定义</strong>：希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止<br><strong>实现</strong>:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">int h = 1;</span><br><span class=\"line\">while (h &lt; wait2Sort.length / 3)</span><br><span class=\"line\">\th = 3 * h + 1;</span><br><span class=\"line\">while (h &gt;= 1) &#123;</span><br><span class=\"line\">\tfor (int i = h; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\t\tfor (int j = i; j - h &gt;= 0 &amp;&amp; wait2Sort[j - h] &gt; wait2Sort[j]; j = j - h) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j - h];</span><br><span class=\"line\">\t\t\twait2Sort[j - h] = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\th = h / 3;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>  上面已经演示了以4为初始间隔对包含10个数据项的数组进行排序的情况。对于更大的数组开始的间隔也应该更大。然后间隔不断减小，直到间隔变成1。</p>\n<p>   举例来说，含有1000个数据项的数组可能先以364为增量，然后以121为增量，以40为增量，以13为增量，以4为增量，最后以 1为增量进行希尔排序。用来形成间隔的数列被称为间隔序列。这里所表示的间隔序列由<strong>Knuth</strong>提出，此序列是很常用的。在排序算法中，首先在一个短小的循环中使用序列的生成公式来计算出最初的间隔。h值最初被赋为1，然后应用公式<code>h=3*h+1</code>生成序列1,4,13,40,121,364，等等。当间隔大于数组大小的时候，这个过程停止。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int h = wait2Sort.length / 2; h &gt; 0; h = h / 2) &#123;</span><br><span class=\"line\">\tfor (int i = h; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\t\tfor (int j = i; j&gt;=h &amp;&amp; wait2Sort[j - h] &gt; wait2Sort[j]; j = j - h) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j - h];</span><br><span class=\"line\">\t\t\twait2Sort[j - h] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个也是正确的，间距为N/2</p>\n<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2.归并排序\"></a>2.归并排序</h2><p><strong>定义</strong>：归并排序,也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法</p>\n<p>如　设有数列{6，202，100，301，38，8，1}<br>初始状态：6,202,100,301,38,8,1<br>第一次归并后：{6,202},{100,301},{8,38},{1}，比较次数：3；<br>第二次归并后：{6,100,202,301}，{1,8,38}，比较次数：4；<br>第三次归并后：{1,6,8,38,100,202,301},比较次数：4；<br>总的比较次数为：3+4+4=11；<br>逆序数为14；</p>\n<p><strong>实现</strong>:</p>\n<h2 id=\"3-快速排序\"><a href=\"#3-快速排序\" class=\"headerlink\" title=\"3.快速排序\"></a>3.快速排序</h2><h2 id=\"4-堆排序\"><a href=\"#4-堆排序\" class=\"headerlink\" title=\"4.堆排序\"></a>4.堆排序</h2><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.cnblogs.com/chengxiao/p/6104371.html\" target=\"_blank\" rel=\"noopener\">图解排序算法(二)之希尔排序</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"初衷\"><a href=\"#初衷\" class=\"headerlink\" title=\"初衷\"></a>初衷</h2><p>为什么我要写算法篇，记得刚入大学的第一节课，老师就教我们<strong>软件=算法+数据结构</strong>，而我恰恰这两点学的最差了，学习这块两个初衷：1.阿里机试折腰，让我清楚的意识到自己的问题 2.职业瓶颈，已经工作四年了，知识面有了，但是缺乏深度。暂且先选择将基础算法打扎实点。</p>\n<p><strong>声明以下所有的定义不具有权威性，皆来自我对该算法的理解。</strong></p>\n<h2 id=\"1-初级排序\"><a href=\"#1-初级排序\" class=\"headerlink\" title=\"1.初级排序\"></a>1.初级排序</h2><h3 id=\"1-1冒泡法\"><a href=\"#1-1冒泡法\" class=\"headerlink\" title=\"1.1冒泡法\"></a>1.1冒泡法</h3><p>这大概是最简答的排序，可我一直是搞错的。<br><strong>定义</strong>：将数组相邻的数进行对比，直到选出最大值或者最小值，每次冒出一个数，后面的逻辑不再处理它<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length-1; i++) &#123;// 控制循环次数</span><br><span class=\"line\">\tfor (int j = 0; j &lt; wait2Sort.length - 1 - i; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[j] &gt; wait2Sort[j + 1]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j + 1];</span><br><span class=\"line\">\t\t\twait2Sort[j + 1] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>N个数字要排序完成，总共进行N-1趟排序，每第 i 趟的排序次数为 (N-i) 次，所以可以用双重循环语句，外层控制循环多少趟，内层控制每一趟的循环次数。</p>\n<p>按说算法是这样的确实没错，但是网络上还有另外一种写法，就是第一层循环N次，按说是结果不会错，但是算法多此一举。我想追究的是会不会多算一次。例如如下写法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length; i++) &#123;// 控制循环次数</span><br><span class=\"line\">\tfor (int j = 0; j &lt; wait2Sort.length - 1 - i; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[j] &gt; wait2Sort[j + 1]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j + 1];</span><br><span class=\"line\">\t\t\twait2Sort[j + 1] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>至于N次是否进行运算，我们就需要探讨当i=N-1时，是否进入第二个循环，当i=N-1时，第二个循环的条件就变成了j&lt;N-1-(N-1)即j&lt;0，这个永远不会成立，因此最后一次的循环不会进入二层循环。相对于正确算法只多了一次判断，性能可以忽略不计。</p>\n<h3 id=\"1-2选择排序法\"><a href=\"#1-2选择排序法\" class=\"headerlink\" title=\"1.2选择排序法\"></a>1.2选择排序法</h3><h4 id=\"1-2-1初阶选择排序法\"><a href=\"#1-2-1初阶选择排序法\" class=\"headerlink\" title=\"1.2.1初阶选择排序法\"></a>1.2.1初阶选择排序法</h4><p><strong>定义</strong>：选取确定的数据依次与其他数据进行对比<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length - 1; i++) &#123;</span><br><span class=\"line\">\tfor (int j = i + 1; j &lt; wait2Sort.length; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[i] &gt; wait2Sort[j]) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[i];</span><br><span class=\"line\">\t\t\twait2Sort[i] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1-2-2进阶选择排序法\"><a href=\"#1-2-2进阶选择排序法\" class=\"headerlink\" title=\"1.2.2进阶选择排序法\"></a>1.2.2进阶选择排序法</h4><p><strong>定义</strong>：选取确定的数据依次与其他数据进行对比，但是和1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 0; i &lt; wait2Sort.length-1; i++) &#123;</span><br><span class=\"line\">\tint min = i;</span><br><span class=\"line\">\tfor (int j = i + 1; j &lt; wait2Sort.length; j++) &#123;</span><br><span class=\"line\">\t\tif (wait2Sort[min] &gt; wait2Sort[j]) &#123;</span><br><span class=\"line\">\t\t\tmin = j;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif (min != i) &#123;</span><br><span class=\"line\">\t\tint temp = wait2Sort[min];</span><br><span class=\"line\">\t\twait2Sort[min] = wait2Sort[i];</span><br><span class=\"line\">\t\twait2Sort[i] = temp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>该算法较1.2.1区别在于内部循环只确定最大数的索引，数据交换是在外层循环做的。少了多次交换，相较于前两种性能更好！</p>\n<h3 id=\"1-3插入排序\"><a href=\"#1-3插入排序\" class=\"headerlink\" title=\"1.3插入排序\"></a>1.3插入排序</h3><p><strong>定义</strong>：像整理牌一样，将每一张牌插入到有序数组中适当的位置。左侧永远是有序的，当运行到数据最右端，即排序完毕，这种算法对于数组中部分数据是有序的话，性能会有很大的提升！<br><strong>实现</strong>：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int i = 1; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\tfor (int j = i; j &gt; 0 &amp;&amp; (wait2Sort[j - 1]) &gt; wait2Sort[j]; j--) &#123;</span><br><span class=\"line\">\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\twait2Sort[j] = wait2Sort[j - 1];</span><br><span class=\"line\">\t\twait2Sort[j - 1] = temp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><img src=\"https://images2015.cnblogs.com/blog/1024555/201611/1024555-20161126000335346-416319390.png\" alt=\"\"></p>\n<h3 id=\"1-4希尔排序\"><a href=\"#1-4希尔排序\" class=\"headerlink\" title=\"1.4希尔排序\"></a>1.4希尔排序</h3><p><strong>定义</strong>：希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止<br><strong>实现</strong>:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">int h = 1;</span><br><span class=\"line\">while (h &lt; wait2Sort.length / 3)</span><br><span class=\"line\">\th = 3 * h + 1;</span><br><span class=\"line\">while (h &gt;= 1) &#123;</span><br><span class=\"line\">\tfor (int i = h; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\t\tfor (int j = i; j - h &gt;= 0 &amp;&amp; wait2Sort[j - h] &gt; wait2Sort[j]; j = j - h) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j - h];</span><br><span class=\"line\">\t\t\twait2Sort[j - h] = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\th = h / 3;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>  上面已经演示了以4为初始间隔对包含10个数据项的数组进行排序的情况。对于更大的数组开始的间隔也应该更大。然后间隔不断减小，直到间隔变成1。</p>\n<p>   举例来说，含有1000个数据项的数组可能先以364为增量，然后以121为增量，以40为增量，以13为增量，以4为增量，最后以 1为增量进行希尔排序。用来形成间隔的数列被称为间隔序列。这里所表示的间隔序列由<strong>Knuth</strong>提出，此序列是很常用的。在排序算法中，首先在一个短小的循环中使用序列的生成公式来计算出最初的间隔。h值最初被赋为1，然后应用公式<code>h=3*h+1</code>生成序列1,4,13,40,121,364，等等。当间隔大于数组大小的时候，这个过程停止。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 从小到大排序</span><br><span class=\"line\">for (int h = wait2Sort.length / 2; h &gt; 0; h = h / 2) &#123;</span><br><span class=\"line\">\tfor (int i = h; i &lt; wait2Sort.length; i++) &#123;</span><br><span class=\"line\">\t\tfor (int j = i; j&gt;=h &amp;&amp; wait2Sort[j - h] &gt; wait2Sort[j]; j = j - h) &#123;</span><br><span class=\"line\">\t\t\tint temp = wait2Sort[j];</span><br><span class=\"line\">\t\t\twait2Sort[j] = wait2Sort[j - h];</span><br><span class=\"line\">\t\t\twait2Sort[j - h] = temp;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这个也是正确的，间距为N/2</p>\n<h2 id=\"2-归并排序\"><a href=\"#2-归并排序\" class=\"headerlink\" title=\"2.归并排序\"></a>2.归并排序</h2><p><strong>定义</strong>：归并排序,也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法</p>\n<p>如　设有数列{6，202，100，301，38，8，1}<br>初始状态：6,202,100,301,38,8,1<br>第一次归并后：{6,202},{100,301},{8,38},{1}，比较次数：3；<br>第二次归并后：{6,100,202,301}，{1,8,38}，比较次数：4；<br>第三次归并后：{1,6,8,38,100,202,301},比较次数：4；<br>总的比较次数为：3+4+4=11；<br>逆序数为14；</p>\n<p><strong>实现</strong>:</p>\n<h2 id=\"3-快速排序\"><a href=\"#3-快速排序\" class=\"headerlink\" title=\"3.快速排序\"></a>3.快速排序</h2><h2 id=\"4-堆排序\"><a href=\"#4-堆排序\" class=\"headerlink\" title=\"4.堆排序\"></a>4.堆排序</h2><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.cnblogs.com/chengxiao/p/6104371.html\" target=\"_blank\" rel=\"noopener\">图解排序算法(二)之希尔排序</a></li>\n</ol>\n"},{"title":"百度地图调用问题解决方案","date":"2018-07-22T10:32:11.000Z","_content":"\n# 百度地图调用问题解决方案\n\n## 背景\n\n做地图可视化，我们经常使用百度地图，如果是纯js,在html 中调用，一般没什么问题，可以很好的融合，但是放在node.js(vue/angularjs/react)中经常会出现\n```\nBmap is not defined\n```\n\n## 问题原因\n\n产生上面的问题是因为百度地图api 会异步加载，在该js 还未加载完毕，即调用，即会产生该问题。这个问题发现许多人遇到，但是网上的回答特别少，有几个有帮助的也是提供思路，基本都是异步加载再调用。也就是等资源加载完毕再调用。\n\n## 解决方案\n\n### 新建map.js\n\n```\nexport function MP() {\n  return new Promise(function (resolve, reject) {\n    // 如果已加载直接返回\n    if(typeof BMap !== 'undefined') {\n      resolve(BMap);\n      return true;\n    }\n    // 百度地图异步加载回调处理\n    window.onBMapCallback = function () {\n      console.log('百度地图脚本初始化成功...');\n      resolve(BMap);\n    };\n    const script = document.createElement('script');\n    script.type = 'text/javascript';\n    //script.src = 'http://api.map.baidu.com/api?v=2.0&ak=ZUONbpqGBsYGXNIYHicvbAbM';\n    script.src = 'http://api.map.baidu.com/api?v=2.0&ak=ZUONbpqGBsYGXNIYHicvbAbM&s=1&callback=onBMapCallback';\n    script.onerror = reject;\n    document.head.appendChild(script);\n  });\n}\n\n```\n\n### 使用\n\n```\nimport { MP } from './map.js';\n\nMP().then(BMap => {\n     //调用\n    }).catch(error =>{\n      console.log('error');\n    });\n```\n\n\n## 相关项目\n\n- [Bmap](https://github.com/TrumanDu/bmap) kibana 插件，开发该项目遇到这个问题","source":"_posts/百度地图调用问题解决方案.md","raw":"---\ntitle: 百度地图调用问题解决方案\ndate: 2018-07-22 18:32:11\ntags: 开发笔记\ncategories:\n- 前端技术\n---\n\n# 百度地图调用问题解决方案\n\n## 背景\n\n做地图可视化，我们经常使用百度地图，如果是纯js,在html 中调用，一般没什么问题，可以很好的融合，但是放在node.js(vue/angularjs/react)中经常会出现\n```\nBmap is not defined\n```\n\n## 问题原因\n\n产生上面的问题是因为百度地图api 会异步加载，在该js 还未加载完毕，即调用，即会产生该问题。这个问题发现许多人遇到，但是网上的回答特别少，有几个有帮助的也是提供思路，基本都是异步加载再调用。也就是等资源加载完毕再调用。\n\n## 解决方案\n\n### 新建map.js\n\n```\nexport function MP() {\n  return new Promise(function (resolve, reject) {\n    // 如果已加载直接返回\n    if(typeof BMap !== 'undefined') {\n      resolve(BMap);\n      return true;\n    }\n    // 百度地图异步加载回调处理\n    window.onBMapCallback = function () {\n      console.log('百度地图脚本初始化成功...');\n      resolve(BMap);\n    };\n    const script = document.createElement('script');\n    script.type = 'text/javascript';\n    //script.src = 'http://api.map.baidu.com/api?v=2.0&ak=ZUONbpqGBsYGXNIYHicvbAbM';\n    script.src = 'http://api.map.baidu.com/api?v=2.0&ak=ZUONbpqGBsYGXNIYHicvbAbM&s=1&callback=onBMapCallback';\n    script.onerror = reject;\n    document.head.appendChild(script);\n  });\n}\n\n```\n\n### 使用\n\n```\nimport { MP } from './map.js';\n\nMP().then(BMap => {\n     //调用\n    }).catch(error =>{\n      console.log('error');\n    });\n```\n\n\n## 相关项目\n\n- [Bmap](https://github.com/TrumanDu/bmap) kibana 插件，开发该项目遇到这个问题","slug":"百度地图调用问题解决方案","published":1,"updated":"2018-07-22T11:51:34.153Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvfd00968ceewr1a89ek","content":"<h1 id=\"百度地图调用问题解决方案\"><a href=\"#百度地图调用问题解决方案\" class=\"headerlink\" title=\"百度地图调用问题解决方案\"></a>百度地图调用问题解决方案</h1><h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>做地图可视化，我们经常使用百度地图，如果是纯js,在html 中调用，一般没什么问题，可以很好的融合，但是放在node.js(vue/angularjs/react)中经常会出现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Bmap is not defined</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>产生上面的问题是因为百度地图api 会异步加载，在该js 还未加载完毕，即调用，即会产生该问题。这个问题发现许多人遇到，但是网上的回答特别少，有几个有帮助的也是提供思路，基本都是异步加载再调用。也就是等资源加载完毕再调用。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><h3 id=\"新建map-js\"><a href=\"#新建map-js\" class=\"headerlink\" title=\"新建map.js\"></a>新建map.js</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export function MP() &#123;</span><br><span class=\"line\">  return new Promise(function (resolve, reject) &#123;</span><br><span class=\"line\">    // 如果已加载直接返回</span><br><span class=\"line\">    if(typeof BMap !== &apos;undefined&apos;) &#123;</span><br><span class=\"line\">      resolve(BMap);</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 百度地图异步加载回调处理</span><br><span class=\"line\">    window.onBMapCallback = function () &#123;</span><br><span class=\"line\">      console.log(&apos;百度地图脚本初始化成功...&apos;);</span><br><span class=\"line\">      resolve(BMap);</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    const script = document.createElement(&apos;script&apos;);</span><br><span class=\"line\">    script.type = &apos;text/javascript&apos;;</span><br><span class=\"line\">    //script.src = &apos;http://api.map.baidu.com/api?v=2.0&amp;ak=ZUONbpqGBsYGXNIYHicvbAbM&apos;;</span><br><span class=\"line\">    script.src = &apos;http://api.map.baidu.com/api?v=2.0&amp;ak=ZUONbpqGBsYGXNIYHicvbAbM&amp;s=1&amp;callback=onBMapCallback&apos;;</span><br><span class=\"line\">    script.onerror = reject;</span><br><span class=\"line\">    document.head.appendChild(script);</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123; MP &#125; from &apos;./map.js&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">MP().then(BMap =&gt; &#123;</span><br><span class=\"line\">     //调用</span><br><span class=\"line\">    &#125;).catch(error =&gt;&#123;</span><br><span class=\"line\">      console.log(&apos;error&apos;);</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"相关项目\"><a href=\"#相关项目\" class=\"headerlink\" title=\"相关项目\"></a>相关项目</h2><ul>\n<li><a href=\"https://github.com/TrumanDu/bmap\" target=\"_blank\" rel=\"noopener\">Bmap</a> kibana 插件，开发该项目遇到这个问题</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"百度地图调用问题解决方案\"><a href=\"#百度地图调用问题解决方案\" class=\"headerlink\" title=\"百度地图调用问题解决方案\"></a>百度地图调用问题解决方案</h1><h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>做地图可视化，我们经常使用百度地图，如果是纯js,在html 中调用，一般没什么问题，可以很好的融合，但是放在node.js(vue/angularjs/react)中经常会出现<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Bmap is not defined</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"问题原因\"><a href=\"#问题原因\" class=\"headerlink\" title=\"问题原因\"></a>问题原因</h2><p>产生上面的问题是因为百度地图api 会异步加载，在该js 还未加载完毕，即调用，即会产生该问题。这个问题发现许多人遇到，但是网上的回答特别少，有几个有帮助的也是提供思路，基本都是异步加载再调用。也就是等资源加载完毕再调用。</p>\n<h2 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h2><h3 id=\"新建map-js\"><a href=\"#新建map-js\" class=\"headerlink\" title=\"新建map.js\"></a>新建map.js</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export function MP() &#123;</span><br><span class=\"line\">  return new Promise(function (resolve, reject) &#123;</span><br><span class=\"line\">    // 如果已加载直接返回</span><br><span class=\"line\">    if(typeof BMap !== &apos;undefined&apos;) &#123;</span><br><span class=\"line\">      resolve(BMap);</span><br><span class=\"line\">      return true;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 百度地图异步加载回调处理</span><br><span class=\"line\">    window.onBMapCallback = function () &#123;</span><br><span class=\"line\">      console.log(&apos;百度地图脚本初始化成功...&apos;);</span><br><span class=\"line\">      resolve(BMap);</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    const script = document.createElement(&apos;script&apos;);</span><br><span class=\"line\">    script.type = &apos;text/javascript&apos;;</span><br><span class=\"line\">    //script.src = &apos;http://api.map.baidu.com/api?v=2.0&amp;ak=ZUONbpqGBsYGXNIYHicvbAbM&apos;;</span><br><span class=\"line\">    script.src = &apos;http://api.map.baidu.com/api?v=2.0&amp;ak=ZUONbpqGBsYGXNIYHicvbAbM&amp;s=1&amp;callback=onBMapCallback&apos;;</span><br><span class=\"line\">    script.onerror = reject;</span><br><span class=\"line\">    document.head.appendChild(script);</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import &#123; MP &#125; from &apos;./map.js&apos;;</span><br><span class=\"line\"></span><br><span class=\"line\">MP().then(BMap =&gt; &#123;</span><br><span class=\"line\">     //调用</span><br><span class=\"line\">    &#125;).catch(error =&gt;&#123;</span><br><span class=\"line\">      console.log(&apos;error&apos;);</span><br><span class=\"line\">    &#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"相关项目\"><a href=\"#相关项目\" class=\"headerlink\" title=\"相关项目\"></a>相关项目</h2><ul>\n<li><a href=\"https://github.com/TrumanDu/bmap\" target=\"_blank\" rel=\"noopener\">Bmap</a> kibana 插件，开发该项目遇到这个问题</li>\n</ul>\n"},{"title":"KSQL Tutorials and Examples","toc":false,"date":"2019-03-15T13:47:07.000Z","_content":"\n## 1. KSQL是什么？\nKSQL是Apache Kafka的流式SQL引擎，让你可以SQL语方式句执行流处理任务。KSQL降低了数据流处理这个领域的准入门槛，为使用Kafka处理数据提供了一种简单的、完全交互的SQL界面。你不再需要用Java或Python之类的编程语言编写代码了！KSQL具有这些特点：开源（采用Apache 2.0许可证）、分布式、可扩展、可靠、实时。它支持众多功能强大的数据流处理操作，包括**聚合、连接、加窗（windowing）和sessionization**（捕获单一访问者的网站会话时间范围内所有的点击流事件）等等。\n## 2. KSQL能解决什么问题？\n- **流式ETL**\n\nApache Kafka是为数据管道的流行选择。 KSQL使得在管道中转换数据变得简单，准备好消息以便在另一个系统中干净地着陆。 \n- **实时监控和分析**\n\n通过快速构建实时仪表板，生成指标以及创建自定义警报和消息，跟踪，了解和管理基础架构，应用程序和数据源。 \n- **数据探索和发现**\n\n在Kafka中导航并浏览您的数据。\n- **异常检测** \n\n通过毫秒级延迟识别模式并发现实时数据中的异常，使您能够正确地表现出异常事件并分别处理欺诈活动。\n- **个性化**\n\n为用户创建数据驱动的实时体验和洞察力。\n- **传感器数据和物联网**\n\n理解并提供传感器数据的方式和位置。\n- **客户360视图**\n\n通过各种渠道全面了解客户的每一次互动，实时不断地整合新信息。\n## 3.KSQL架构及组件\n![](https://github.com/TrumanDu/pic_repository/blob/master/ksql-architecture-and-components1.png?raw=true)\n\n- **KSQL服务器**\n\nKSQL服务器运行执行KSQL查询的引擎。这包括处理，读取和写入目标Kafka集群的数据。 KSQL服务器构成KSQL集群，可以在容器，虚拟机和裸机中运行。您可以在实时操作期间向/从同一KSQL群集添加和删除服务器，以根据需要弹性扩展KSQL的处理能力。您可以部署不同的KSQL集群以实现工作负载隔离。 \n- **KSQL CLI**\n\n你可以使用KSQL命令行界面（CLI）以交互方式编写KSQL查询。 KSQL CLI充当KSQL服务器的客户端。对于生产方案，您还可以将KSQL服务器配置为以非交互式“无头”配置运行，从而阻止KSQL CLI访问。\n\nKSQL服务器，客户端，查询和应用程序在Kafka brokers之外，在单独的JVM实例中运行，或者在完全独立的集群中运行。\n## 4. 入门教程\n### 4.1 数据类型及术语\n#### 4.1.1 数据类型\n- BOOLEAN\n- INTEGER\n- BIGINT\n- DOUBLE\n- VARCHAR (or STRING)\n- ARRAY<ArrayType> (JSON and AVRO only. Index starts from 0)\n- MAP<VARCHAR, ValueType> (JSON and AVRO only)\n- STRUCT<FieldName FieldType, ...> (JSON and AVRO only) The STRUCT type requires you to specify a list of fields. \n\n#### 4.1.2 术语\n- Stream\n\n流是结构化数据的无界序列。例如，我们可以进行一系列金融交易，例如“Alice向Bob发送了100美元，然后Charlie向Bob发送了50美元”。流中的事实是不可变的，这意味着可以将新事实插入到流中，但是现有事实永远不会被更新或删除。可以从Kafka主题创建流，也可以从现有流派生流。流的基础数据在topic中持久存储（持久化）\n\n- Table\n\n表是流或其他表的视图，表示不断变化的事实的集合。例如，我们可以有一个表格，其中包含最新的财务信息，例如“Bob的当前账户余额为150美元”。它相当于传统的数据库表，但通过流式语义（如窗口）进行了丰富。表中的事实是可变的，这意味着可以将新事实插入表中，并且可以更新或删除现有事实。可以从Kafka主题创建表，也可以从现有流和表派生表。在这两种情况下，表的基础数据都在topic中持久存储（持久化）。\n\n- STRUCT\n\n在KSQL 5.0及更高版本中，您可以使用CREATE STREAM和CREATE TABLE语句中的STRUCT类型以Avro和JSON格式读取嵌套数据。\n\nCREATE STREAM/TABLE (from a topic)\n\nCREATE STREAM/TABLE AS SELECT (from existing streams/tables)\n\nSELECT (non-persistent query)\n\n例如\n```\nCREATE STREAM orders (\n  orderId BIGINT,\n  address STRUCT<street VARCHAR, zip INTEGER>) WITH (...);\n  \nSELECT address->city, address->zip FROM orders;\n```\n- KSQL Time Units\n1. DAY, DAYS\n2. HOUR, HOURS\n3. MINUTE, MINUTES\n4. SECOND, SECONDS\n5. MILLISECOND, MILLISECONDS\n\n### 4.2 如何使用？\n目前confluent KSQL支持两种方式，一种是ksql-cli，一种是rest\n#### ksql-cli\n这里我仅演示使用docker方式\n```\nsudo docker run -it --rm  confluentinc/cp-ksql-cli:5.1.2  http://192.168.0.11:8088\n[sudo] password for bigdata:\n\n                  ===========================================\n                  =        _  __ _____  ____  _             =\n                  =       | |/ // ____|/ __ \\| |            =\n                  =       | ' /| (___ | |  | | |            =\n                  =       |  <  \\___ \\| |  | | |            =\n                  =       | . \\ ____) | |__| | |____        =\n                  =       |_|\\_\\_____/ \\___\\_\\______|       =\n                  =                                         =\n                  =  Streaming SQL Engine for Apache Kafka® =\n                  ===========================================\n\nCopyright 2017-2018 Confluent Inc.\n\nCLI v5.1.2, Server v5.1.2 located at http://192.168.0.11:8088\n\nHaving trouble? Type 'help' (case-insensitive) for a rundown of how things work!\n\nksql> show streams;\n\n Stream Name      | Kafka Topic              | Format\n------------------------------------------------------\n EC_BOT_DETECTION | EC_bot_detection.replica | JSON\n DEMO             | trumantest               | JSON\n------------------------------------------------------\n\n```\n#### rest\n```\ncurl --request POST \\\n  --url http://192.168.0.11:8088/ksql \\\n  --header 'Content-Type: application/vnd.ksql.v1+json; charset=utf-8' \\\n  --data '{\\r\\n  \"ksql\": \"LIST STREAMS;\",\\r\\n  \"streamsProperties\": {}\\r\\n}'\n```\nrest主要有两个接口1.ksql2.query，对于一般查询语句可以使用query接口。其他的命令操作一般使用ksql接口。\n### 4.3 快速指南\n#### 4.3.1 常用支持命令\n命令|描述\n---|---\nCREATE STREAM|\nCREATE TABLE|\nCREATE STREAM AS SELECT|\nCREATE TABLE AS SELECT|\nINSERT INTO|\nDESCRIBE|\nDESCRIBE FUNCTION|\nEXPLAIN|\nDROP STREAM [IF EXISTS] [DELETE TOPIC];|\nDROP TABLE [IF EXISTS] [DELETE TOPIC];|\nPRINT|\nSELECT|在select下支持更多sql语法，这里不一一列出\nSHOW FUNCTIONS|\nSHOW TOPICS|\nSHOW STREAMS|\nSHOW TABLES|\nSHOW QUERIES|列出持久化查询\n#### 4.3.2 KSQL Language Elements\n- DDL(数据定义语言)\n\nDDL包括以下:\n1. CREATE STREAM\n2. CREATE TABLE\n3. DROP STREAM\n4. DROP TABLE\n5. CREATE STREAM AS SELECT (CSAS)\n6. CREATE TABLE AS SELECT (CTAS)\n- DML(数据处理语言)\n\nDML包括以下：\n1. SELECT\n2. INSERT INTO\n3. CREATE STREAM AS SELECT (CSAS)\n4. CREATE TABLE AS SELECT (CTAS)\n\n#### 4.3.3 Time and Windows in KSQL\nKSQL支持时间窗口操作，以下是目前支持的几种类型：\n\nWindow type|行为|描述\n---|---|---\nHopping Window|\t基于时间|\t固定持续时间，重叠的窗口\nTumbling Window|基于时间|\t固定持续时间，非重叠，无间隙窗口\nSession Window|\t基于session|动态大小，不重叠，数据驱动的窗口\n\n### 4.4 使用案例\n#### 案例场景\n这里我演示一个反爬虫检测的案例， 根据EC_bot_detection的数据抓取一分钟内超过限制的IP.\n##### 创建stream\n根据消费kafka中EC_bot_detection.replica数据，定义不同字段的数据类型，按照该数据定义语句如下\n```\nCREATE STREAM ec_bot_detection (action VARCHAR, nvtc VARCHAR,nid VARCHAR,ip VARCHAR,msg VARCHAR,level VARCHAR,verb VARCHAR,url VARCHAR,purl VARCHAR,ua VARCHAR,valid INTEGER,nNvtc VARCHAR,nNid VARCHAR,cip VARCHAR,sip VARCHAR,xpost VARCHAR,time VARCHAR,id VARCHAR,xprotocol VARCHAR,xhost VARCHAR,xport VARCHAR,xpath VARCHAR,xquery VARCHAR,xpathalias VARCHAR,xkey VARCHAR,importance INTEGER,xpprotocol VARCHAR,xphost VARCHAR,xpport VARCHAR,xppath VARCHAR,xpquery VARCHAR,org VARCHAR,cyc VARCHAR,isp VARCHAR,cycName VARCHAR,region VARCHAR,city VARCHAR,geoLocation VARCHAR,zipcode VARCHAR) WITH (VALUE_FORMAT = 'JSON',KAFKA_TOPIC = 'EC_bot_detection.replica');\n```\n##### 自定义sql\n查询语句\nbot_detection_1min\n```\nSELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like '%.tw' and level='-1' AND importance=1 AND (msg<>'W' OR msg is null) and action like'crawler%' AND ip NOT LIKE '127.0.0.1' AND ip NOT LIKE '172.16.%' AND ip <> ''  AND xpathalias !='LandingpageOverviewcontent4mobile'  AND xpathalias !='CommonCommonrecaptchavalidate'  GROUP BY ip,org,cyc HAVING count(1)>300;\n```\n##### 输出结果\n为了将查询结果存储到kafka中，我们可以使用 \n\nCREATE TABLE TableName as SELECT ...\n```\nCREATE TABLE bot_detection_1min as SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like '%.tw' and level='-1' AND importance=1 AND (msg<>'W' OR msg is null) and action like'crawler%' AND ip NOT LIKE '127.0.0.1' AND ip NOT LIKE '172.16.%' AND ip <> ''  AND xpathalias !='LandingpageOverviewcontent4mobile'  AND xpathalias !='CommonCommonrecaptchavalidate'  GROUP BY ip,org,cyc HAVING count(1)>300;\n\n```\n##### 总结\n1. having不支持别名，可以直接使用聚合函数。例如 HAVING count(1)>300\n2. count(distinct xkey)不支持，可以使用自定义函数（array_length）实现数组求大小。例如 array_length(COLLECT_SET(xkey))\n3. not in不支持，可以使用！=替换\n\n### 4.5 高阶应用之自定义函数\n目前官方提供的函数不足以满足业务场景的需求情况下，我们可以扩展自定义函数udf/udaf\n#### 4.5.1 udf函数\n用户自定义函数，可以实现针对column操作的函数，例如以下案例，实现字母转大写\n```\n@UdfDescription(name = \"upper\", description = \"字符串转大写\")\npublic class Upper {\n\t@Udf\n\tpublic String upper(final String column) {\n\t\treturn column.toUpperCase();\n\t}\n\n}\n```\n#### 4.5.2 udaf函数\n用户自定义聚合函数，以下是聚合求百分比函数demo\n```\n@UdafDescription(name = \"percent\", description = \"聚合求百分比\")\npublic class Percent {\n\t@UdafFactory(description = \"statistics percent\")\n\tpublic static Udaf<Integer, Map<String, Double>> createStddev() {\n\t\treturn new Udaf<Integer, Map<String, Double>>() {\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> initialize() {\n\t\t\t\tMap<String, Double> stats = new HashMap<>();\n\t\t\t\tstats.put(\"sum\", 0.0);\n\t\t\t\tstats.put(\"bussiness\", 0.0);\n\t\t\t\tstats.put(\"percent\", 0.0);\n\t\t\t\treturn stats;\n\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> aggregate(final Integer val, final Map<String, Double> aggregateValue) {\n\t\t\t\tDouble sum = aggregateValue.getOrDefault(\"sum\", 0.0) + 1;\n\t\t\t\tDouble bussiness = aggregateValue.getOrDefault(\"bussiness\", 0.0);\n\t\t\t\tif (val % 4 == 0) {\n\t\t\t\t\tbussiness = bussiness + 1;\n\t\t\t\t}\n\n\t\t\t\tDouble percent = bussiness / sum;\n\n\t\t\t\tMap<String, Double> agg = new HashMap<>();\n\t\t\t\tagg.put(\"sum\", sum);\n\t\t\t\tagg.put(\"bussiness\", bussiness);\n\t\t\t\tagg.put(\"percent\", percent);\n\t\t\t\treturn agg;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> merge(final Map<String, Double> aggOne, final Map<String, Double> aggTwo) {\n\n\t\t\t\tDouble sum = aggOne.getOrDefault(\"sum\", 0.0) + aggTwo.getOrDefault(\"sum\", 0.0);\n\t\t\t\tDouble bussiness = aggOne.getOrDefault(\"bussiness\", 0.0) + aggTwo.getOrDefault(\"bussiness\", 0.0);\n\t\t\t\tDouble percent = bussiness / sum;\n\n\t\t\t\tMap<String, Double> agg = new HashMap<>();\n\t\t\t\tagg.put(\"sum\", sum);\n\t\t\t\tagg.put(\"bussiness\", bussiness);\n\t\t\t\tagg.put(\"percent\", percent);\n\t\t\t\treturn agg;\n\t\t\t}\n\t\t};\n\n\t}\n}\n```\n## 5. 已知问题\n通过两周ksql的使用，现发现有如下问题不能满足\n- distinct\n```\nselect distinct ip from #table;\nselect count(distinct ip) from #table\n```\n- sub query\n```\nselect ip from (select * from table);\n```\n- case when\n```\nsum(case when nvtc='' then 1 else 0 end)\n```\n- in/not in\n```\nxpathalias not in ('Landingpage','Wishlist','CommonMessagepage','Common')\n```\n\n## 6.参考\n1. [KSQL](https://docs.confluent.io/current/ksql/docs/index.html)\n2. [KSQL REST API Reference](https://docs.confluent.io/current/ksql/docs/developer-guide/api.html)\n3. [KSQL Syntax Reference](https://docs.confluent.io/current/ksql/docs/developer-guide/syntax-reference.html#create-stream-as-select)\n4. [Time and Windows in KSQL](https://docs.confluent.io/current/ksql/docs/concepts/time-and-windows-in-ksql-queries.html)\n5. [KSQL Custom Function Reference (UDF and UDAF)](https://docs.confluent.io/current/ksql/docs/developer-guide/udf.html)","source":"_posts/KSQL-Tutorials-and-Examples.md","raw":"---\ntitle: KSQL Tutorials and Examples\ntags:\n  - 实时分析\n  - 专栏\n  - kafka\ncategories:\n  - ksql\ntoc: false\ndate: 2019-03-15 21:47:07\n---\n\n## 1. KSQL是什么？\nKSQL是Apache Kafka的流式SQL引擎，让你可以SQL语方式句执行流处理任务。KSQL降低了数据流处理这个领域的准入门槛，为使用Kafka处理数据提供了一种简单的、完全交互的SQL界面。你不再需要用Java或Python之类的编程语言编写代码了！KSQL具有这些特点：开源（采用Apache 2.0许可证）、分布式、可扩展、可靠、实时。它支持众多功能强大的数据流处理操作，包括**聚合、连接、加窗（windowing）和sessionization**（捕获单一访问者的网站会话时间范围内所有的点击流事件）等等。\n## 2. KSQL能解决什么问题？\n- **流式ETL**\n\nApache Kafka是为数据管道的流行选择。 KSQL使得在管道中转换数据变得简单，准备好消息以便在另一个系统中干净地着陆。 \n- **实时监控和分析**\n\n通过快速构建实时仪表板，生成指标以及创建自定义警报和消息，跟踪，了解和管理基础架构，应用程序和数据源。 \n- **数据探索和发现**\n\n在Kafka中导航并浏览您的数据。\n- **异常检测** \n\n通过毫秒级延迟识别模式并发现实时数据中的异常，使您能够正确地表现出异常事件并分别处理欺诈活动。\n- **个性化**\n\n为用户创建数据驱动的实时体验和洞察力。\n- **传感器数据和物联网**\n\n理解并提供传感器数据的方式和位置。\n- **客户360视图**\n\n通过各种渠道全面了解客户的每一次互动，实时不断地整合新信息。\n## 3.KSQL架构及组件\n![](https://github.com/TrumanDu/pic_repository/blob/master/ksql-architecture-and-components1.png?raw=true)\n\n- **KSQL服务器**\n\nKSQL服务器运行执行KSQL查询的引擎。这包括处理，读取和写入目标Kafka集群的数据。 KSQL服务器构成KSQL集群，可以在容器，虚拟机和裸机中运行。您可以在实时操作期间向/从同一KSQL群集添加和删除服务器，以根据需要弹性扩展KSQL的处理能力。您可以部署不同的KSQL集群以实现工作负载隔离。 \n- **KSQL CLI**\n\n你可以使用KSQL命令行界面（CLI）以交互方式编写KSQL查询。 KSQL CLI充当KSQL服务器的客户端。对于生产方案，您还可以将KSQL服务器配置为以非交互式“无头”配置运行，从而阻止KSQL CLI访问。\n\nKSQL服务器，客户端，查询和应用程序在Kafka brokers之外，在单独的JVM实例中运行，或者在完全独立的集群中运行。\n## 4. 入门教程\n### 4.1 数据类型及术语\n#### 4.1.1 数据类型\n- BOOLEAN\n- INTEGER\n- BIGINT\n- DOUBLE\n- VARCHAR (or STRING)\n- ARRAY<ArrayType> (JSON and AVRO only. Index starts from 0)\n- MAP<VARCHAR, ValueType> (JSON and AVRO only)\n- STRUCT<FieldName FieldType, ...> (JSON and AVRO only) The STRUCT type requires you to specify a list of fields. \n\n#### 4.1.2 术语\n- Stream\n\n流是结构化数据的无界序列。例如，我们可以进行一系列金融交易，例如“Alice向Bob发送了100美元，然后Charlie向Bob发送了50美元”。流中的事实是不可变的，这意味着可以将新事实插入到流中，但是现有事实永远不会被更新或删除。可以从Kafka主题创建流，也可以从现有流派生流。流的基础数据在topic中持久存储（持久化）\n\n- Table\n\n表是流或其他表的视图，表示不断变化的事实的集合。例如，我们可以有一个表格，其中包含最新的财务信息，例如“Bob的当前账户余额为150美元”。它相当于传统的数据库表，但通过流式语义（如窗口）进行了丰富。表中的事实是可变的，这意味着可以将新事实插入表中，并且可以更新或删除现有事实。可以从Kafka主题创建表，也可以从现有流和表派生表。在这两种情况下，表的基础数据都在topic中持久存储（持久化）。\n\n- STRUCT\n\n在KSQL 5.0及更高版本中，您可以使用CREATE STREAM和CREATE TABLE语句中的STRUCT类型以Avro和JSON格式读取嵌套数据。\n\nCREATE STREAM/TABLE (from a topic)\n\nCREATE STREAM/TABLE AS SELECT (from existing streams/tables)\n\nSELECT (non-persistent query)\n\n例如\n```\nCREATE STREAM orders (\n  orderId BIGINT,\n  address STRUCT<street VARCHAR, zip INTEGER>) WITH (...);\n  \nSELECT address->city, address->zip FROM orders;\n```\n- KSQL Time Units\n1. DAY, DAYS\n2. HOUR, HOURS\n3. MINUTE, MINUTES\n4. SECOND, SECONDS\n5. MILLISECOND, MILLISECONDS\n\n### 4.2 如何使用？\n目前confluent KSQL支持两种方式，一种是ksql-cli，一种是rest\n#### ksql-cli\n这里我仅演示使用docker方式\n```\nsudo docker run -it --rm  confluentinc/cp-ksql-cli:5.1.2  http://192.168.0.11:8088\n[sudo] password for bigdata:\n\n                  ===========================================\n                  =        _  __ _____  ____  _             =\n                  =       | |/ // ____|/ __ \\| |            =\n                  =       | ' /| (___ | |  | | |            =\n                  =       |  <  \\___ \\| |  | | |            =\n                  =       | . \\ ____) | |__| | |____        =\n                  =       |_|\\_\\_____/ \\___\\_\\______|       =\n                  =                                         =\n                  =  Streaming SQL Engine for Apache Kafka® =\n                  ===========================================\n\nCopyright 2017-2018 Confluent Inc.\n\nCLI v5.1.2, Server v5.1.2 located at http://192.168.0.11:8088\n\nHaving trouble? Type 'help' (case-insensitive) for a rundown of how things work!\n\nksql> show streams;\n\n Stream Name      | Kafka Topic              | Format\n------------------------------------------------------\n EC_BOT_DETECTION | EC_bot_detection.replica | JSON\n DEMO             | trumantest               | JSON\n------------------------------------------------------\n\n```\n#### rest\n```\ncurl --request POST \\\n  --url http://192.168.0.11:8088/ksql \\\n  --header 'Content-Type: application/vnd.ksql.v1+json; charset=utf-8' \\\n  --data '{\\r\\n  \"ksql\": \"LIST STREAMS;\",\\r\\n  \"streamsProperties\": {}\\r\\n}'\n```\nrest主要有两个接口1.ksql2.query，对于一般查询语句可以使用query接口。其他的命令操作一般使用ksql接口。\n### 4.3 快速指南\n#### 4.3.1 常用支持命令\n命令|描述\n---|---\nCREATE STREAM|\nCREATE TABLE|\nCREATE STREAM AS SELECT|\nCREATE TABLE AS SELECT|\nINSERT INTO|\nDESCRIBE|\nDESCRIBE FUNCTION|\nEXPLAIN|\nDROP STREAM [IF EXISTS] [DELETE TOPIC];|\nDROP TABLE [IF EXISTS] [DELETE TOPIC];|\nPRINT|\nSELECT|在select下支持更多sql语法，这里不一一列出\nSHOW FUNCTIONS|\nSHOW TOPICS|\nSHOW STREAMS|\nSHOW TABLES|\nSHOW QUERIES|列出持久化查询\n#### 4.3.2 KSQL Language Elements\n- DDL(数据定义语言)\n\nDDL包括以下:\n1. CREATE STREAM\n2. CREATE TABLE\n3. DROP STREAM\n4. DROP TABLE\n5. CREATE STREAM AS SELECT (CSAS)\n6. CREATE TABLE AS SELECT (CTAS)\n- DML(数据处理语言)\n\nDML包括以下：\n1. SELECT\n2. INSERT INTO\n3. CREATE STREAM AS SELECT (CSAS)\n4. CREATE TABLE AS SELECT (CTAS)\n\n#### 4.3.3 Time and Windows in KSQL\nKSQL支持时间窗口操作，以下是目前支持的几种类型：\n\nWindow type|行为|描述\n---|---|---\nHopping Window|\t基于时间|\t固定持续时间，重叠的窗口\nTumbling Window|基于时间|\t固定持续时间，非重叠，无间隙窗口\nSession Window|\t基于session|动态大小，不重叠，数据驱动的窗口\n\n### 4.4 使用案例\n#### 案例场景\n这里我演示一个反爬虫检测的案例， 根据EC_bot_detection的数据抓取一分钟内超过限制的IP.\n##### 创建stream\n根据消费kafka中EC_bot_detection.replica数据，定义不同字段的数据类型，按照该数据定义语句如下\n```\nCREATE STREAM ec_bot_detection (action VARCHAR, nvtc VARCHAR,nid VARCHAR,ip VARCHAR,msg VARCHAR,level VARCHAR,verb VARCHAR,url VARCHAR,purl VARCHAR,ua VARCHAR,valid INTEGER,nNvtc VARCHAR,nNid VARCHAR,cip VARCHAR,sip VARCHAR,xpost VARCHAR,time VARCHAR,id VARCHAR,xprotocol VARCHAR,xhost VARCHAR,xport VARCHAR,xpath VARCHAR,xquery VARCHAR,xpathalias VARCHAR,xkey VARCHAR,importance INTEGER,xpprotocol VARCHAR,xphost VARCHAR,xpport VARCHAR,xppath VARCHAR,xpquery VARCHAR,org VARCHAR,cyc VARCHAR,isp VARCHAR,cycName VARCHAR,region VARCHAR,city VARCHAR,geoLocation VARCHAR,zipcode VARCHAR) WITH (VALUE_FORMAT = 'JSON',KAFKA_TOPIC = 'EC_bot_detection.replica');\n```\n##### 自定义sql\n查询语句\nbot_detection_1min\n```\nSELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like '%.tw' and level='-1' AND importance=1 AND (msg<>'W' OR msg is null) and action like'crawler%' AND ip NOT LIKE '127.0.0.1' AND ip NOT LIKE '172.16.%' AND ip <> ''  AND xpathalias !='LandingpageOverviewcontent4mobile'  AND xpathalias !='CommonCommonrecaptchavalidate'  GROUP BY ip,org,cyc HAVING count(1)>300;\n```\n##### 输出结果\n为了将查询结果存储到kafka中，我们可以使用 \n\nCREATE TABLE TableName as SELECT ...\n```\nCREATE TABLE bot_detection_1min as SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like '%.tw' and level='-1' AND importance=1 AND (msg<>'W' OR msg is null) and action like'crawler%' AND ip NOT LIKE '127.0.0.1' AND ip NOT LIKE '172.16.%' AND ip <> ''  AND xpathalias !='LandingpageOverviewcontent4mobile'  AND xpathalias !='CommonCommonrecaptchavalidate'  GROUP BY ip,org,cyc HAVING count(1)>300;\n\n```\n##### 总结\n1. having不支持别名，可以直接使用聚合函数。例如 HAVING count(1)>300\n2. count(distinct xkey)不支持，可以使用自定义函数（array_length）实现数组求大小。例如 array_length(COLLECT_SET(xkey))\n3. not in不支持，可以使用！=替换\n\n### 4.5 高阶应用之自定义函数\n目前官方提供的函数不足以满足业务场景的需求情况下，我们可以扩展自定义函数udf/udaf\n#### 4.5.1 udf函数\n用户自定义函数，可以实现针对column操作的函数，例如以下案例，实现字母转大写\n```\n@UdfDescription(name = \"upper\", description = \"字符串转大写\")\npublic class Upper {\n\t@Udf\n\tpublic String upper(final String column) {\n\t\treturn column.toUpperCase();\n\t}\n\n}\n```\n#### 4.5.2 udaf函数\n用户自定义聚合函数，以下是聚合求百分比函数demo\n```\n@UdafDescription(name = \"percent\", description = \"聚合求百分比\")\npublic class Percent {\n\t@UdafFactory(description = \"statistics percent\")\n\tpublic static Udaf<Integer, Map<String, Double>> createStddev() {\n\t\treturn new Udaf<Integer, Map<String, Double>>() {\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> initialize() {\n\t\t\t\tMap<String, Double> stats = new HashMap<>();\n\t\t\t\tstats.put(\"sum\", 0.0);\n\t\t\t\tstats.put(\"bussiness\", 0.0);\n\t\t\t\tstats.put(\"percent\", 0.0);\n\t\t\t\treturn stats;\n\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> aggregate(final Integer val, final Map<String, Double> aggregateValue) {\n\t\t\t\tDouble sum = aggregateValue.getOrDefault(\"sum\", 0.0) + 1;\n\t\t\t\tDouble bussiness = aggregateValue.getOrDefault(\"bussiness\", 0.0);\n\t\t\t\tif (val % 4 == 0) {\n\t\t\t\t\tbussiness = bussiness + 1;\n\t\t\t\t}\n\n\t\t\t\tDouble percent = bussiness / sum;\n\n\t\t\t\tMap<String, Double> agg = new HashMap<>();\n\t\t\t\tagg.put(\"sum\", sum);\n\t\t\t\tagg.put(\"bussiness\", bussiness);\n\t\t\t\tagg.put(\"percent\", percent);\n\t\t\t\treturn agg;\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic Map<String, Double> merge(final Map<String, Double> aggOne, final Map<String, Double> aggTwo) {\n\n\t\t\t\tDouble sum = aggOne.getOrDefault(\"sum\", 0.0) + aggTwo.getOrDefault(\"sum\", 0.0);\n\t\t\t\tDouble bussiness = aggOne.getOrDefault(\"bussiness\", 0.0) + aggTwo.getOrDefault(\"bussiness\", 0.0);\n\t\t\t\tDouble percent = bussiness / sum;\n\n\t\t\t\tMap<String, Double> agg = new HashMap<>();\n\t\t\t\tagg.put(\"sum\", sum);\n\t\t\t\tagg.put(\"bussiness\", bussiness);\n\t\t\t\tagg.put(\"percent\", percent);\n\t\t\t\treturn agg;\n\t\t\t}\n\t\t};\n\n\t}\n}\n```\n## 5. 已知问题\n通过两周ksql的使用，现发现有如下问题不能满足\n- distinct\n```\nselect distinct ip from #table;\nselect count(distinct ip) from #table\n```\n- sub query\n```\nselect ip from (select * from table);\n```\n- case when\n```\nsum(case when nvtc='' then 1 else 0 end)\n```\n- in/not in\n```\nxpathalias not in ('Landingpage','Wishlist','CommonMessagepage','Common')\n```\n\n## 6.参考\n1. [KSQL](https://docs.confluent.io/current/ksql/docs/index.html)\n2. [KSQL REST API Reference](https://docs.confluent.io/current/ksql/docs/developer-guide/api.html)\n3. [KSQL Syntax Reference](https://docs.confluent.io/current/ksql/docs/developer-guide/syntax-reference.html#create-stream-as-select)\n4. [Time and Windows in KSQL](https://docs.confluent.io/current/ksql/docs/concepts/time-and-windows-in-ksql-queries.html)\n5. [KSQL Custom Function Reference (UDF and UDAF)](https://docs.confluent.io/current/ksql/docs/developer-guide/udf.html)","slug":"KSQL-Tutorials-and-Examples","published":1,"updated":"2019-06-21T15:47:34.477Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvg800a78ceef1yojrjj","content":"<h2 id=\"1-KSQL是什么？\"><a href=\"#1-KSQL是什么？\" class=\"headerlink\" title=\"1. KSQL是什么？\"></a>1. KSQL是什么？</h2><p>KSQL是Apache Kafka的流式SQL引擎，让你可以SQL语方式句执行流处理任务。KSQL降低了数据流处理这个领域的准入门槛，为使用Kafka处理数据提供了一种简单的、完全交互的SQL界面。你不再需要用Java或Python之类的编程语言编写代码了！KSQL具有这些特点：开源（采用Apache 2.0许可证）、分布式、可扩展、可靠、实时。它支持众多功能强大的数据流处理操作，包括<strong>聚合、连接、加窗（windowing）和sessionization</strong>（捕获单一访问者的网站会话时间范围内所有的点击流事件）等等。</p>\n<h2 id=\"2-KSQL能解决什么问题？\"><a href=\"#2-KSQL能解决什么问题？\" class=\"headerlink\" title=\"2. KSQL能解决什么问题？\"></a>2. KSQL能解决什么问题？</h2><ul>\n<li><strong>流式ETL</strong></li>\n</ul>\n<p>Apache Kafka是为数据管道的流行选择。 KSQL使得在管道中转换数据变得简单，准备好消息以便在另一个系统中干净地着陆。 </p>\n<ul>\n<li><strong>实时监控和分析</strong></li>\n</ul>\n<p>通过快速构建实时仪表板，生成指标以及创建自定义警报和消息，跟踪，了解和管理基础架构，应用程序和数据源。 </p>\n<ul>\n<li><strong>数据探索和发现</strong></li>\n</ul>\n<p>在Kafka中导航并浏览您的数据。</p>\n<ul>\n<li><strong>异常检测</strong> </li>\n</ul>\n<p>通过毫秒级延迟识别模式并发现实时数据中的异常，使您能够正确地表现出异常事件并分别处理欺诈活动。</p>\n<ul>\n<li><strong>个性化</strong></li>\n</ul>\n<p>为用户创建数据驱动的实时体验和洞察力。</p>\n<ul>\n<li><strong>传感器数据和物联网</strong></li>\n</ul>\n<p>理解并提供传感器数据的方式和位置。</p>\n<ul>\n<li><strong>客户360视图</strong></li>\n</ul>\n<p>通过各种渠道全面了解客户的每一次互动，实时不断地整合新信息。</p>\n<h2 id=\"3-KSQL架构及组件\"><a href=\"#3-KSQL架构及组件\" class=\"headerlink\" title=\"3.KSQL架构及组件\"></a>3.KSQL架构及组件</h2><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/ksql-architecture-and-components1.png?raw=true\" alt=\"\"></p>\n<ul>\n<li><strong>KSQL服务器</strong></li>\n</ul>\n<p>KSQL服务器运行执行KSQL查询的引擎。这包括处理，读取和写入目标Kafka集群的数据。 KSQL服务器构成KSQL集群，可以在容器，虚拟机和裸机中运行。您可以在实时操作期间向/从同一KSQL群集添加和删除服务器，以根据需要弹性扩展KSQL的处理能力。您可以部署不同的KSQL集群以实现工作负载隔离。 </p>\n<ul>\n<li><strong>KSQL CLI</strong></li>\n</ul>\n<p>你可以使用KSQL命令行界面（CLI）以交互方式编写KSQL查询。 KSQL CLI充当KSQL服务器的客户端。对于生产方案，您还可以将KSQL服务器配置为以非交互式“无头”配置运行，从而阻止KSQL CLI访问。</p>\n<p>KSQL服务器，客户端，查询和应用程序在Kafka brokers之外，在单独的JVM实例中运行，或者在完全独立的集群中运行。</p>\n<h2 id=\"4-入门教程\"><a href=\"#4-入门教程\" class=\"headerlink\" title=\"4. 入门教程\"></a>4. 入门教程</h2><h3 id=\"4-1-数据类型及术语\"><a href=\"#4-1-数据类型及术语\" class=\"headerlink\" title=\"4.1 数据类型及术语\"></a>4.1 数据类型及术语</h3><h4 id=\"4-1-1-数据类型\"><a href=\"#4-1-1-数据类型\" class=\"headerlink\" title=\"4.1.1 数据类型\"></a>4.1.1 数据类型</h4><ul>\n<li>BOOLEAN</li>\n<li>INTEGER</li>\n<li>BIGINT</li>\n<li>DOUBLE</li>\n<li>VARCHAR (or STRING)</li>\n<li>ARRAY<arraytype> (JSON and AVRO only. Index starts from 0)</arraytype></li>\n<li>MAP&lt;VARCHAR, ValueType&gt; (JSON and AVRO only)</li>\n<li>STRUCT<fieldname fieldtype,=\"\" ...=\"\"> (JSON and AVRO only) The STRUCT type requires you to specify a list of fields. </fieldname></li>\n</ul>\n<h4 id=\"4-1-2-术语\"><a href=\"#4-1-2-术语\" class=\"headerlink\" title=\"4.1.2 术语\"></a>4.1.2 术语</h4><ul>\n<li>Stream</li>\n</ul>\n<p>流是结构化数据的无界序列。例如，我们可以进行一系列金融交易，例如“Alice向Bob发送了100美元，然后Charlie向Bob发送了50美元”。流中的事实是不可变的，这意味着可以将新事实插入到流中，但是现有事实永远不会被更新或删除。可以从Kafka主题创建流，也可以从现有流派生流。流的基础数据在topic中持久存储（持久化）</p>\n<ul>\n<li>Table</li>\n</ul>\n<p>表是流或其他表的视图，表示不断变化的事实的集合。例如，我们可以有一个表格，其中包含最新的财务信息，例如“Bob的当前账户余额为150美元”。它相当于传统的数据库表，但通过流式语义（如窗口）进行了丰富。表中的事实是可变的，这意味着可以将新事实插入表中，并且可以更新或删除现有事实。可以从Kafka主题创建表，也可以从现有流和表派生表。在这两种情况下，表的基础数据都在topic中持久存储（持久化）。</p>\n<ul>\n<li>STRUCT</li>\n</ul>\n<p>在KSQL 5.0及更高版本中，您可以使用CREATE STREAM和CREATE TABLE语句中的STRUCT类型以Avro和JSON格式读取嵌套数据。</p>\n<p>CREATE STREAM/TABLE (from a topic)</p>\n<p>CREATE STREAM/TABLE AS SELECT (from existing streams/tables)</p>\n<p>SELECT (non-persistent query)</p>\n<p>例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM orders (</span><br><span class=\"line\">  orderId BIGINT,</span><br><span class=\"line\">  address STRUCT&lt;street VARCHAR, zip INTEGER&gt;) WITH (...);</span><br><span class=\"line\">  </span><br><span class=\"line\">SELECT address-&gt;city, address-&gt;zip FROM orders;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>KSQL Time Units</li>\n</ul>\n<ol>\n<li>DAY, DAYS</li>\n<li>HOUR, HOURS</li>\n<li>MINUTE, MINUTES</li>\n<li>SECOND, SECONDS</li>\n<li>MILLISECOND, MILLISECONDS</li>\n</ol>\n<h3 id=\"4-2-如何使用？\"><a href=\"#4-2-如何使用？\" class=\"headerlink\" title=\"4.2 如何使用？\"></a>4.2 如何使用？</h3><p>目前confluent KSQL支持两种方式，一种是ksql-cli，一种是rest</p>\n<h4 id=\"ksql-cli\"><a href=\"#ksql-cli\" class=\"headerlink\" title=\"ksql-cli\"></a>ksql-cli</h4><p>这里我仅演示使用docker方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -it --rm  confluentinc/cp-ksql-cli:5.1.2  http://192.168.0.11:8088</span><br><span class=\"line\">[sudo] password for bigdata:</span><br><span class=\"line\"></span><br><span class=\"line\">                  ===========================================</span><br><span class=\"line\">                  =        _  __ _____  ____  _             =</span><br><span class=\"line\">                  =       | |/ // ____|/ __ \\| |            =</span><br><span class=\"line\">                  =       | &apos; /| (___ | |  | | |            =</span><br><span class=\"line\">                  =       |  &lt;  \\___ \\| |  | | |            =</span><br><span class=\"line\">                  =       | . \\ ____) | |__| | |____        =</span><br><span class=\"line\">                  =       |_|\\_\\_____/ \\___\\_\\______|       =</span><br><span class=\"line\">                  =                                         =</span><br><span class=\"line\">                  =  Streaming SQL Engine for Apache Kafka® =</span><br><span class=\"line\">                  ===========================================</span><br><span class=\"line\"></span><br><span class=\"line\">Copyright 2017-2018 Confluent Inc.</span><br><span class=\"line\"></span><br><span class=\"line\">CLI v5.1.2, Server v5.1.2 located at http://192.168.0.11:8088</span><br><span class=\"line\"></span><br><span class=\"line\">Having trouble? Type &apos;help&apos; (case-insensitive) for a rundown of how things work!</span><br><span class=\"line\"></span><br><span class=\"line\">ksql&gt; show streams;</span><br><span class=\"line\"></span><br><span class=\"line\"> Stream Name      | Kafka Topic              | Format</span><br><span class=\"line\">------------------------------------------------------</span><br><span class=\"line\"> EC_BOT_DETECTION | EC_bot_detection.replica | JSON</span><br><span class=\"line\"> DEMO             | trumantest               | JSON</span><br><span class=\"line\">------------------------------------------------------</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"rest\"><a href=\"#rest\" class=\"headerlink\" title=\"rest\"></a>rest</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl --request POST \\</span><br><span class=\"line\">  --url http://192.168.0.11:8088/ksql \\</span><br><span class=\"line\">  --header &apos;Content-Type: application/vnd.ksql.v1+json; charset=utf-8&apos; \\</span><br><span class=\"line\">  --data &apos;&#123;\\r\\n  &quot;ksql&quot;: &quot;LIST STREAMS;&quot;,\\r\\n  &quot;streamsProperties&quot;: &#123;&#125;\\r\\n&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>rest主要有两个接口1.ksql2.query，对于一般查询语句可以使用query接口。其他的命令操作一般使用ksql接口。</p>\n<h3 id=\"4-3-快速指南\"><a href=\"#4-3-快速指南\" class=\"headerlink\" title=\"4.3 快速指南\"></a>4.3 快速指南</h3><h4 id=\"4-3-1-常用支持命令\"><a href=\"#4-3-1-常用支持命令\" class=\"headerlink\" title=\"4.3.1 常用支持命令\"></a>4.3.1 常用支持命令</h4><table>\n<thead>\n<tr>\n<th>命令</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CREATE STREAM</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE TABLE</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE STREAM AS SELECT</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE TABLE AS SELECT</td>\n<td></td>\n</tr>\n<tr>\n<td>INSERT INTO</td>\n<td></td>\n</tr>\n<tr>\n<td>DESCRIBE</td>\n<td></td>\n</tr>\n<tr>\n<td>DESCRIBE FUNCTION</td>\n<td></td>\n</tr>\n<tr>\n<td>EXPLAIN</td>\n<td></td>\n</tr>\n<tr>\n<td>DROP STREAM [IF EXISTS] [DELETE TOPIC];</td>\n<td></td>\n</tr>\n<tr>\n<td>DROP TABLE [IF EXISTS] [DELETE TOPIC];</td>\n<td></td>\n</tr>\n<tr>\n<td>PRINT</td>\n<td></td>\n</tr>\n<tr>\n<td>SELECT</td>\n<td>在select下支持更多sql语法，这里不一一列出</td>\n</tr>\n<tr>\n<td>SHOW FUNCTIONS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW TOPICS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW STREAMS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW TABLES</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW QUERIES</td>\n<td>列出持久化查询</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"4-3-2-KSQL-Language-Elements\"><a href=\"#4-3-2-KSQL-Language-Elements\" class=\"headerlink\" title=\"4.3.2 KSQL Language Elements\"></a>4.3.2 KSQL Language Elements</h4><ul>\n<li>DDL(数据定义语言)</li>\n</ul>\n<p>DDL包括以下:</p>\n<ol>\n<li>CREATE STREAM</li>\n<li>CREATE TABLE</li>\n<li>DROP STREAM</li>\n<li>DROP TABLE</li>\n<li>CREATE STREAM AS SELECT (CSAS)</li>\n<li>CREATE TABLE AS SELECT (CTAS)</li>\n</ol>\n<ul>\n<li>DML(数据处理语言)</li>\n</ul>\n<p>DML包括以下：</p>\n<ol>\n<li>SELECT</li>\n<li>INSERT INTO</li>\n<li>CREATE STREAM AS SELECT (CSAS)</li>\n<li>CREATE TABLE AS SELECT (CTAS)</li>\n</ol>\n<h4 id=\"4-3-3-Time-and-Windows-in-KSQL\"><a href=\"#4-3-3-Time-and-Windows-in-KSQL\" class=\"headerlink\" title=\"4.3.3 Time and Windows in KSQL\"></a>4.3.3 Time and Windows in KSQL</h4><p>KSQL支持时间窗口操作，以下是目前支持的几种类型：</p>\n<table>\n<thead>\n<tr>\n<th>Window type</th>\n<th>行为</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hopping Window</td>\n<td>基于时间</td>\n<td>固定持续时间，重叠的窗口</td>\n</tr>\n<tr>\n<td>Tumbling Window</td>\n<td>基于时间</td>\n<td>固定持续时间，非重叠，无间隙窗口</td>\n</tr>\n<tr>\n<td>Session Window</td>\n<td>基于session</td>\n<td>动态大小，不重叠，数据驱动的窗口</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"4-4-使用案例\"><a href=\"#4-4-使用案例\" class=\"headerlink\" title=\"4.4 使用案例\"></a>4.4 使用案例</h3><h4 id=\"案例场景\"><a href=\"#案例场景\" class=\"headerlink\" title=\"案例场景\"></a>案例场景</h4><p>这里我演示一个反爬虫检测的案例， 根据EC_bot_detection的数据抓取一分钟内超过限制的IP.</p>\n<h5 id=\"创建stream\"><a href=\"#创建stream\" class=\"headerlink\" title=\"创建stream\"></a>创建stream</h5><p>根据消费kafka中EC_bot_detection.replica数据，定义不同字段的数据类型，按照该数据定义语句如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM ec_bot_detection (action VARCHAR, nvtc VARCHAR,nid VARCHAR,ip VARCHAR,msg VARCHAR,level VARCHAR,verb VARCHAR,url VARCHAR,purl VARCHAR,ua VARCHAR,valid INTEGER,nNvtc VARCHAR,nNid VARCHAR,cip VARCHAR,sip VARCHAR,xpost VARCHAR,time VARCHAR,id VARCHAR,xprotocol VARCHAR,xhost VARCHAR,xport VARCHAR,xpath VARCHAR,xquery VARCHAR,xpathalias VARCHAR,xkey VARCHAR,importance INTEGER,xpprotocol VARCHAR,xphost VARCHAR,xpport VARCHAR,xppath VARCHAR,xpquery VARCHAR,org VARCHAR,cyc VARCHAR,isp VARCHAR,cycName VARCHAR,region VARCHAR,city VARCHAR,geoLocation VARCHAR,zipcode VARCHAR) WITH (VALUE_FORMAT = &apos;JSON&apos;,KAFKA_TOPIC = &apos;EC_bot_detection.replica&apos;);</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"自定义sql\"><a href=\"#自定义sql\" class=\"headerlink\" title=\"自定义sql\"></a>自定义sql</h5><p>查询语句<br>bot_detection_1min<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like &apos;%.tw&apos; and level=&apos;-1&apos; AND importance=1 AND (msg&lt;&gt;&apos;W&apos; OR msg is null) and action like&apos;crawler%&apos; AND ip NOT LIKE &apos;127.0.0.1&apos; AND ip NOT LIKE &apos;172.16.%&apos; AND ip &lt;&gt; &apos;&apos;  AND xpathalias !=&apos;LandingpageOverviewcontent4mobile&apos;  AND xpathalias !=&apos;CommonCommonrecaptchavalidate&apos;  GROUP BY ip,org,cyc HAVING count(1)&gt;300;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"输出结果\"><a href=\"#输出结果\" class=\"headerlink\" title=\"输出结果\"></a>输出结果</h5><p>为了将查询结果存储到kafka中，我们可以使用 </p>\n<p>CREATE TABLE TableName as SELECT …<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE bot_detection_1min as SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like &apos;%.tw&apos; and level=&apos;-1&apos; AND importance=1 AND (msg&lt;&gt;&apos;W&apos; OR msg is null) and action like&apos;crawler%&apos; AND ip NOT LIKE &apos;127.0.0.1&apos; AND ip NOT LIKE &apos;172.16.%&apos; AND ip &lt;&gt; &apos;&apos;  AND xpathalias !=&apos;LandingpageOverviewcontent4mobile&apos;  AND xpathalias !=&apos;CommonCommonrecaptchavalidate&apos;  GROUP BY ip,org,cyc HAVING count(1)&gt;300;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h5><ol>\n<li>having不支持别名，可以直接使用聚合函数。例如 HAVING count(1)&gt;300</li>\n<li>count(distinct xkey)不支持，可以使用自定义函数（array_length）实现数组求大小。例如 array_length(COLLECT_SET(xkey))</li>\n<li>not in不支持，可以使用！=替换</li>\n</ol>\n<h3 id=\"4-5-高阶应用之自定义函数\"><a href=\"#4-5-高阶应用之自定义函数\" class=\"headerlink\" title=\"4.5 高阶应用之自定义函数\"></a>4.5 高阶应用之自定义函数</h3><p>目前官方提供的函数不足以满足业务场景的需求情况下，我们可以扩展自定义函数udf/udaf</p>\n<h4 id=\"4-5-1-udf函数\"><a href=\"#4-5-1-udf函数\" class=\"headerlink\" title=\"4.5.1 udf函数\"></a>4.5.1 udf函数</h4><p>用户自定义函数，可以实现针对column操作的函数，例如以下案例，实现字母转大写<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@UdfDescription(name = &quot;upper&quot;, description = &quot;字符串转大写&quot;)</span><br><span class=\"line\">public class Upper &#123;</span><br><span class=\"line\">\t@Udf</span><br><span class=\"line\">\tpublic String upper(final String column) &#123;</span><br><span class=\"line\">\t\treturn column.toUpperCase();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-5-2-udaf函数\"><a href=\"#4-5-2-udaf函数\" class=\"headerlink\" title=\"4.5.2 udaf函数\"></a>4.5.2 udaf函数</h4><p>用户自定义聚合函数，以下是聚合求百分比函数demo<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@UdafDescription(name = &quot;percent&quot;, description = &quot;聚合求百分比&quot;)</span><br><span class=\"line\">public class Percent &#123;</span><br><span class=\"line\">\t@UdafFactory(description = &quot;statistics percent&quot;)</span><br><span class=\"line\">\tpublic static Udaf&lt;Integer, Map&lt;String, Double&gt;&gt; createStddev() &#123;</span><br><span class=\"line\">\t\treturn new Udaf&lt;Integer, Map&lt;String, Double&gt;&gt;() &#123;</span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; initialize() &#123;</span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; stats = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;sum&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;percent&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\treturn stats;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; aggregate(final Integer val, final Map&lt;String, Double&gt; aggregateValue) &#123;</span><br><span class=\"line\">\t\t\t\tDouble sum = aggregateValue.getOrDefault(&quot;sum&quot;, 0.0) + 1;</span><br><span class=\"line\">\t\t\t\tDouble bussiness = aggregateValue.getOrDefault(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tif (val % 4 == 0) &#123;</span><br><span class=\"line\">\t\t\t\t\tbussiness = bussiness + 1;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tDouble percent = bussiness / sum;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; agg = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;sum&quot;, sum);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;bussiness&quot;, bussiness);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;percent&quot;, percent);</span><br><span class=\"line\">\t\t\t\treturn agg;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; merge(final Map&lt;String, Double&gt; aggOne, final Map&lt;String, Double&gt; aggTwo) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tDouble sum = aggOne.getOrDefault(&quot;sum&quot;, 0.0) + aggTwo.getOrDefault(&quot;sum&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tDouble bussiness = aggOne.getOrDefault(&quot;bussiness&quot;, 0.0) + aggTwo.getOrDefault(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tDouble percent = bussiness / sum;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; agg = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;sum&quot;, sum);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;bussiness&quot;, bussiness);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;percent&quot;, percent);</span><br><span class=\"line\">\t\t\t\treturn agg;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"5-已知问题\"><a href=\"#5-已知问题\" class=\"headerlink\" title=\"5. 已知问题\"></a>5. 已知问题</h2><p>通过两周ksql的使用，现发现有如下问题不能满足</p>\n<ul>\n<li><p>distinct</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select distinct ip from #table;</span><br><span class=\"line\">select count(distinct ip) from #table</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sub query</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select ip from (select * from table);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>case when</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum(case when nvtc=&apos;&apos; then 1 else 0 end)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>in/not in</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xpathalias not in (&apos;Landingpage&apos;,&apos;Wishlist&apos;,&apos;CommonMessagepage&apos;,&apos;Common&apos;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6.参考\"></a>6.参考</h2><ol>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/index.html\" target=\"_blank\" rel=\"noopener\">KSQL</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/api.html\" target=\"_blank\" rel=\"noopener\">KSQL REST API Reference</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/syntax-reference.html#create-stream-as-select\" target=\"_blank\" rel=\"noopener\">KSQL Syntax Reference</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/concepts/time-and-windows-in-ksql-queries.html\" target=\"_blank\" rel=\"noopener\">Time and Windows in KSQL</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/udf.html\" target=\"_blank\" rel=\"noopener\">KSQL Custom Function Reference (UDF and UDAF)</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-KSQL是什么？\"><a href=\"#1-KSQL是什么？\" class=\"headerlink\" title=\"1. KSQL是什么？\"></a>1. KSQL是什么？</h2><p>KSQL是Apache Kafka的流式SQL引擎，让你可以SQL语方式句执行流处理任务。KSQL降低了数据流处理这个领域的准入门槛，为使用Kafka处理数据提供了一种简单的、完全交互的SQL界面。你不再需要用Java或Python之类的编程语言编写代码了！KSQL具有这些特点：开源（采用Apache 2.0许可证）、分布式、可扩展、可靠、实时。它支持众多功能强大的数据流处理操作，包括<strong>聚合、连接、加窗（windowing）和sessionization</strong>（捕获单一访问者的网站会话时间范围内所有的点击流事件）等等。</p>\n<h2 id=\"2-KSQL能解决什么问题？\"><a href=\"#2-KSQL能解决什么问题？\" class=\"headerlink\" title=\"2. KSQL能解决什么问题？\"></a>2. KSQL能解决什么问题？</h2><ul>\n<li><strong>流式ETL</strong></li>\n</ul>\n<p>Apache Kafka是为数据管道的流行选择。 KSQL使得在管道中转换数据变得简单，准备好消息以便在另一个系统中干净地着陆。 </p>\n<ul>\n<li><strong>实时监控和分析</strong></li>\n</ul>\n<p>通过快速构建实时仪表板，生成指标以及创建自定义警报和消息，跟踪，了解和管理基础架构，应用程序和数据源。 </p>\n<ul>\n<li><strong>数据探索和发现</strong></li>\n</ul>\n<p>在Kafka中导航并浏览您的数据。</p>\n<ul>\n<li><strong>异常检测</strong> </li>\n</ul>\n<p>通过毫秒级延迟识别模式并发现实时数据中的异常，使您能够正确地表现出异常事件并分别处理欺诈活动。</p>\n<ul>\n<li><strong>个性化</strong></li>\n</ul>\n<p>为用户创建数据驱动的实时体验和洞察力。</p>\n<ul>\n<li><strong>传感器数据和物联网</strong></li>\n</ul>\n<p>理解并提供传感器数据的方式和位置。</p>\n<ul>\n<li><strong>客户360视图</strong></li>\n</ul>\n<p>通过各种渠道全面了解客户的每一次互动，实时不断地整合新信息。</p>\n<h2 id=\"3-KSQL架构及组件\"><a href=\"#3-KSQL架构及组件\" class=\"headerlink\" title=\"3.KSQL架构及组件\"></a>3.KSQL架构及组件</h2><p><img src=\"https://github.com/TrumanDu/pic_repository/blob/master/ksql-architecture-and-components1.png?raw=true\" alt=\"\"></p>\n<ul>\n<li><strong>KSQL服务器</strong></li>\n</ul>\n<p>KSQL服务器运行执行KSQL查询的引擎。这包括处理，读取和写入目标Kafka集群的数据。 KSQL服务器构成KSQL集群，可以在容器，虚拟机和裸机中运行。您可以在实时操作期间向/从同一KSQL群集添加和删除服务器，以根据需要弹性扩展KSQL的处理能力。您可以部署不同的KSQL集群以实现工作负载隔离。 </p>\n<ul>\n<li><strong>KSQL CLI</strong></li>\n</ul>\n<p>你可以使用KSQL命令行界面（CLI）以交互方式编写KSQL查询。 KSQL CLI充当KSQL服务器的客户端。对于生产方案，您还可以将KSQL服务器配置为以非交互式“无头”配置运行，从而阻止KSQL CLI访问。</p>\n<p>KSQL服务器，客户端，查询和应用程序在Kafka brokers之外，在单独的JVM实例中运行，或者在完全独立的集群中运行。</p>\n<h2 id=\"4-入门教程\"><a href=\"#4-入门教程\" class=\"headerlink\" title=\"4. 入门教程\"></a>4. 入门教程</h2><h3 id=\"4-1-数据类型及术语\"><a href=\"#4-1-数据类型及术语\" class=\"headerlink\" title=\"4.1 数据类型及术语\"></a>4.1 数据类型及术语</h3><h4 id=\"4-1-1-数据类型\"><a href=\"#4-1-1-数据类型\" class=\"headerlink\" title=\"4.1.1 数据类型\"></a>4.1.1 数据类型</h4><ul>\n<li>BOOLEAN</li>\n<li>INTEGER</li>\n<li>BIGINT</li>\n<li>DOUBLE</li>\n<li>VARCHAR (or STRING)</li>\n<li>ARRAY<arraytype> (JSON and AVRO only. Index starts from 0)</arraytype></li>\n<li>MAP&lt;VARCHAR, ValueType&gt; (JSON and AVRO only)</li>\n<li>STRUCT<fieldname fieldtype,=\"\" ...=\"\"> (JSON and AVRO only) The STRUCT type requires you to specify a list of fields. </fieldname></li>\n</ul>\n<h4 id=\"4-1-2-术语\"><a href=\"#4-1-2-术语\" class=\"headerlink\" title=\"4.1.2 术语\"></a>4.1.2 术语</h4><ul>\n<li>Stream</li>\n</ul>\n<p>流是结构化数据的无界序列。例如，我们可以进行一系列金融交易，例如“Alice向Bob发送了100美元，然后Charlie向Bob发送了50美元”。流中的事实是不可变的，这意味着可以将新事实插入到流中，但是现有事实永远不会被更新或删除。可以从Kafka主题创建流，也可以从现有流派生流。流的基础数据在topic中持久存储（持久化）</p>\n<ul>\n<li>Table</li>\n</ul>\n<p>表是流或其他表的视图，表示不断变化的事实的集合。例如，我们可以有一个表格，其中包含最新的财务信息，例如“Bob的当前账户余额为150美元”。它相当于传统的数据库表，但通过流式语义（如窗口）进行了丰富。表中的事实是可变的，这意味着可以将新事实插入表中，并且可以更新或删除现有事实。可以从Kafka主题创建表，也可以从现有流和表派生表。在这两种情况下，表的基础数据都在topic中持久存储（持久化）。</p>\n<ul>\n<li>STRUCT</li>\n</ul>\n<p>在KSQL 5.0及更高版本中，您可以使用CREATE STREAM和CREATE TABLE语句中的STRUCT类型以Avro和JSON格式读取嵌套数据。</p>\n<p>CREATE STREAM/TABLE (from a topic)</p>\n<p>CREATE STREAM/TABLE AS SELECT (from existing streams/tables)</p>\n<p>SELECT (non-persistent query)</p>\n<p>例如<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM orders (</span><br><span class=\"line\">  orderId BIGINT,</span><br><span class=\"line\">  address STRUCT&lt;street VARCHAR, zip INTEGER&gt;) WITH (...);</span><br><span class=\"line\">  </span><br><span class=\"line\">SELECT address-&gt;city, address-&gt;zip FROM orders;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li>KSQL Time Units</li>\n</ul>\n<ol>\n<li>DAY, DAYS</li>\n<li>HOUR, HOURS</li>\n<li>MINUTE, MINUTES</li>\n<li>SECOND, SECONDS</li>\n<li>MILLISECOND, MILLISECONDS</li>\n</ol>\n<h3 id=\"4-2-如何使用？\"><a href=\"#4-2-如何使用？\" class=\"headerlink\" title=\"4.2 如何使用？\"></a>4.2 如何使用？</h3><p>目前confluent KSQL支持两种方式，一种是ksql-cli，一种是rest</p>\n<h4 id=\"ksql-cli\"><a href=\"#ksql-cli\" class=\"headerlink\" title=\"ksql-cli\"></a>ksql-cli</h4><p>这里我仅演示使用docker方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -it --rm  confluentinc/cp-ksql-cli:5.1.2  http://192.168.0.11:8088</span><br><span class=\"line\">[sudo] password for bigdata:</span><br><span class=\"line\"></span><br><span class=\"line\">                  ===========================================</span><br><span class=\"line\">                  =        _  __ _____  ____  _             =</span><br><span class=\"line\">                  =       | |/ // ____|/ __ \\| |            =</span><br><span class=\"line\">                  =       | &apos; /| (___ | |  | | |            =</span><br><span class=\"line\">                  =       |  &lt;  \\___ \\| |  | | |            =</span><br><span class=\"line\">                  =       | . \\ ____) | |__| | |____        =</span><br><span class=\"line\">                  =       |_|\\_\\_____/ \\___\\_\\______|       =</span><br><span class=\"line\">                  =                                         =</span><br><span class=\"line\">                  =  Streaming SQL Engine for Apache Kafka® =</span><br><span class=\"line\">                  ===========================================</span><br><span class=\"line\"></span><br><span class=\"line\">Copyright 2017-2018 Confluent Inc.</span><br><span class=\"line\"></span><br><span class=\"line\">CLI v5.1.2, Server v5.1.2 located at http://192.168.0.11:8088</span><br><span class=\"line\"></span><br><span class=\"line\">Having trouble? Type &apos;help&apos; (case-insensitive) for a rundown of how things work!</span><br><span class=\"line\"></span><br><span class=\"line\">ksql&gt; show streams;</span><br><span class=\"line\"></span><br><span class=\"line\"> Stream Name      | Kafka Topic              | Format</span><br><span class=\"line\">------------------------------------------------------</span><br><span class=\"line\"> EC_BOT_DETECTION | EC_bot_detection.replica | JSON</span><br><span class=\"line\"> DEMO             | trumantest               | JSON</span><br><span class=\"line\">------------------------------------------------------</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"rest\"><a href=\"#rest\" class=\"headerlink\" title=\"rest\"></a>rest</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl --request POST \\</span><br><span class=\"line\">  --url http://192.168.0.11:8088/ksql \\</span><br><span class=\"line\">  --header &apos;Content-Type: application/vnd.ksql.v1+json; charset=utf-8&apos; \\</span><br><span class=\"line\">  --data &apos;&#123;\\r\\n  &quot;ksql&quot;: &quot;LIST STREAMS;&quot;,\\r\\n  &quot;streamsProperties&quot;: &#123;&#125;\\r\\n&#125;&apos;</span><br></pre></td></tr></table></figure>\n<p>rest主要有两个接口1.ksql2.query，对于一般查询语句可以使用query接口。其他的命令操作一般使用ksql接口。</p>\n<h3 id=\"4-3-快速指南\"><a href=\"#4-3-快速指南\" class=\"headerlink\" title=\"4.3 快速指南\"></a>4.3 快速指南</h3><h4 id=\"4-3-1-常用支持命令\"><a href=\"#4-3-1-常用支持命令\" class=\"headerlink\" title=\"4.3.1 常用支持命令\"></a>4.3.1 常用支持命令</h4><table>\n<thead>\n<tr>\n<th>命令</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CREATE STREAM</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE TABLE</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE STREAM AS SELECT</td>\n<td></td>\n</tr>\n<tr>\n<td>CREATE TABLE AS SELECT</td>\n<td></td>\n</tr>\n<tr>\n<td>INSERT INTO</td>\n<td></td>\n</tr>\n<tr>\n<td>DESCRIBE</td>\n<td></td>\n</tr>\n<tr>\n<td>DESCRIBE FUNCTION</td>\n<td></td>\n</tr>\n<tr>\n<td>EXPLAIN</td>\n<td></td>\n</tr>\n<tr>\n<td>DROP STREAM [IF EXISTS] [DELETE TOPIC];</td>\n<td></td>\n</tr>\n<tr>\n<td>DROP TABLE [IF EXISTS] [DELETE TOPIC];</td>\n<td></td>\n</tr>\n<tr>\n<td>PRINT</td>\n<td></td>\n</tr>\n<tr>\n<td>SELECT</td>\n<td>在select下支持更多sql语法，这里不一一列出</td>\n</tr>\n<tr>\n<td>SHOW FUNCTIONS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW TOPICS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW STREAMS</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW TABLES</td>\n<td></td>\n</tr>\n<tr>\n<td>SHOW QUERIES</td>\n<td>列出持久化查询</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"4-3-2-KSQL-Language-Elements\"><a href=\"#4-3-2-KSQL-Language-Elements\" class=\"headerlink\" title=\"4.3.2 KSQL Language Elements\"></a>4.3.2 KSQL Language Elements</h4><ul>\n<li>DDL(数据定义语言)</li>\n</ul>\n<p>DDL包括以下:</p>\n<ol>\n<li>CREATE STREAM</li>\n<li>CREATE TABLE</li>\n<li>DROP STREAM</li>\n<li>DROP TABLE</li>\n<li>CREATE STREAM AS SELECT (CSAS)</li>\n<li>CREATE TABLE AS SELECT (CTAS)</li>\n</ol>\n<ul>\n<li>DML(数据处理语言)</li>\n</ul>\n<p>DML包括以下：</p>\n<ol>\n<li>SELECT</li>\n<li>INSERT INTO</li>\n<li>CREATE STREAM AS SELECT (CSAS)</li>\n<li>CREATE TABLE AS SELECT (CTAS)</li>\n</ol>\n<h4 id=\"4-3-3-Time-and-Windows-in-KSQL\"><a href=\"#4-3-3-Time-and-Windows-in-KSQL\" class=\"headerlink\" title=\"4.3.3 Time and Windows in KSQL\"></a>4.3.3 Time and Windows in KSQL</h4><p>KSQL支持时间窗口操作，以下是目前支持的几种类型：</p>\n<table>\n<thead>\n<tr>\n<th>Window type</th>\n<th>行为</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hopping Window</td>\n<td>基于时间</td>\n<td>固定持续时间，重叠的窗口</td>\n</tr>\n<tr>\n<td>Tumbling Window</td>\n<td>基于时间</td>\n<td>固定持续时间，非重叠，无间隙窗口</td>\n</tr>\n<tr>\n<td>Session Window</td>\n<td>基于session</td>\n<td>动态大小，不重叠，数据驱动的窗口</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"4-4-使用案例\"><a href=\"#4-4-使用案例\" class=\"headerlink\" title=\"4.4 使用案例\"></a>4.4 使用案例</h3><h4 id=\"案例场景\"><a href=\"#案例场景\" class=\"headerlink\" title=\"案例场景\"></a>案例场景</h4><p>这里我演示一个反爬虫检测的案例， 根据EC_bot_detection的数据抓取一分钟内超过限制的IP.</p>\n<h5 id=\"创建stream\"><a href=\"#创建stream\" class=\"headerlink\" title=\"创建stream\"></a>创建stream</h5><p>根据消费kafka中EC_bot_detection.replica数据，定义不同字段的数据类型，按照该数据定义语句如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM ec_bot_detection (action VARCHAR, nvtc VARCHAR,nid VARCHAR,ip VARCHAR,msg VARCHAR,level VARCHAR,verb VARCHAR,url VARCHAR,purl VARCHAR,ua VARCHAR,valid INTEGER,nNvtc VARCHAR,nNid VARCHAR,cip VARCHAR,sip VARCHAR,xpost VARCHAR,time VARCHAR,id VARCHAR,xprotocol VARCHAR,xhost VARCHAR,xport VARCHAR,xpath VARCHAR,xquery VARCHAR,xpathalias VARCHAR,xkey VARCHAR,importance INTEGER,xpprotocol VARCHAR,xphost VARCHAR,xpport VARCHAR,xppath VARCHAR,xpquery VARCHAR,org VARCHAR,cyc VARCHAR,isp VARCHAR,cycName VARCHAR,region VARCHAR,city VARCHAR,geoLocation VARCHAR,zipcode VARCHAR) WITH (VALUE_FORMAT = &apos;JSON&apos;,KAFKA_TOPIC = &apos;EC_bot_detection.replica&apos;);</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"自定义sql\"><a href=\"#自定义sql\" class=\"headerlink\" title=\"自定义sql\"></a>自定义sql</h5><p>查询语句<br>bot_detection_1min<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like &apos;%.tw&apos; and level=&apos;-1&apos; AND importance=1 AND (msg&lt;&gt;&apos;W&apos; OR msg is null) and action like&apos;crawler%&apos; AND ip NOT LIKE &apos;127.0.0.1&apos; AND ip NOT LIKE &apos;172.16.%&apos; AND ip &lt;&gt; &apos;&apos;  AND xpathalias !=&apos;LandingpageOverviewcontent4mobile&apos;  AND xpathalias !=&apos;CommonCommonrecaptchavalidate&apos;  GROUP BY ip,org,cyc HAVING count(1)&gt;300;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"输出结果\"><a href=\"#输出结果\" class=\"headerlink\" title=\"输出结果\"></a>输出结果</h5><p>为了将查询结果存储到kafka中，我们可以使用 </p>\n<p>CREATE TABLE TableName as SELECT …<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE bot_detection_1min as SELECT ip,org,cyc,count(1) as ccount FROM EC_BOT_DETECTION WINDOW TUMBLING (SIZE 60 SECONDS) WHERE xhost not like &apos;%.tw&apos; and level=&apos;-1&apos; AND importance=1 AND (msg&lt;&gt;&apos;W&apos; OR msg is null) and action like&apos;crawler%&apos; AND ip NOT LIKE &apos;127.0.0.1&apos; AND ip NOT LIKE &apos;172.16.%&apos; AND ip &lt;&gt; &apos;&apos;  AND xpathalias !=&apos;LandingpageOverviewcontent4mobile&apos;  AND xpathalias !=&apos;CommonCommonrecaptchavalidate&apos;  GROUP BY ip,org,cyc HAVING count(1)&gt;300;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h5><ol>\n<li>having不支持别名，可以直接使用聚合函数。例如 HAVING count(1)&gt;300</li>\n<li>count(distinct xkey)不支持，可以使用自定义函数（array_length）实现数组求大小。例如 array_length(COLLECT_SET(xkey))</li>\n<li>not in不支持，可以使用！=替换</li>\n</ol>\n<h3 id=\"4-5-高阶应用之自定义函数\"><a href=\"#4-5-高阶应用之自定义函数\" class=\"headerlink\" title=\"4.5 高阶应用之自定义函数\"></a>4.5 高阶应用之自定义函数</h3><p>目前官方提供的函数不足以满足业务场景的需求情况下，我们可以扩展自定义函数udf/udaf</p>\n<h4 id=\"4-5-1-udf函数\"><a href=\"#4-5-1-udf函数\" class=\"headerlink\" title=\"4.5.1 udf函数\"></a>4.5.1 udf函数</h4><p>用户自定义函数，可以实现针对column操作的函数，例如以下案例，实现字母转大写<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@UdfDescription(name = &quot;upper&quot;, description = &quot;字符串转大写&quot;)</span><br><span class=\"line\">public class Upper &#123;</span><br><span class=\"line\">\t@Udf</span><br><span class=\"line\">\tpublic String upper(final String column) &#123;</span><br><span class=\"line\">\t\treturn column.toUpperCase();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"4-5-2-udaf函数\"><a href=\"#4-5-2-udaf函数\" class=\"headerlink\" title=\"4.5.2 udaf函数\"></a>4.5.2 udaf函数</h4><p>用户自定义聚合函数，以下是聚合求百分比函数demo<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@UdafDescription(name = &quot;percent&quot;, description = &quot;聚合求百分比&quot;)</span><br><span class=\"line\">public class Percent &#123;</span><br><span class=\"line\">\t@UdafFactory(description = &quot;statistics percent&quot;)</span><br><span class=\"line\">\tpublic static Udaf&lt;Integer, Map&lt;String, Double&gt;&gt; createStddev() &#123;</span><br><span class=\"line\">\t\treturn new Udaf&lt;Integer, Map&lt;String, Double&gt;&gt;() &#123;</span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; initialize() &#123;</span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; stats = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;sum&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tstats.put(&quot;percent&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\treturn stats;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; aggregate(final Integer val, final Map&lt;String, Double&gt; aggregateValue) &#123;</span><br><span class=\"line\">\t\t\t\tDouble sum = aggregateValue.getOrDefault(&quot;sum&quot;, 0.0) + 1;</span><br><span class=\"line\">\t\t\t\tDouble bussiness = aggregateValue.getOrDefault(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tif (val % 4 == 0) &#123;</span><br><span class=\"line\">\t\t\t\t\tbussiness = bussiness + 1;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tDouble percent = bussiness / sum;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; agg = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;sum&quot;, sum);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;bussiness&quot;, bussiness);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;percent&quot;, percent);</span><br><span class=\"line\">\t\t\t\treturn agg;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic Map&lt;String, Double&gt; merge(final Map&lt;String, Double&gt; aggOne, final Map&lt;String, Double&gt; aggTwo) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tDouble sum = aggOne.getOrDefault(&quot;sum&quot;, 0.0) + aggTwo.getOrDefault(&quot;sum&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tDouble bussiness = aggOne.getOrDefault(&quot;bussiness&quot;, 0.0) + aggTwo.getOrDefault(&quot;bussiness&quot;, 0.0);</span><br><span class=\"line\">\t\t\t\tDouble percent = bussiness / sum;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t\tMap&lt;String, Double&gt; agg = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;sum&quot;, sum);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;bussiness&quot;, bussiness);</span><br><span class=\"line\">\t\t\t\tagg.put(&quot;percent&quot;, percent);</span><br><span class=\"line\">\t\t\t\treturn agg;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"5-已知问题\"><a href=\"#5-已知问题\" class=\"headerlink\" title=\"5. 已知问题\"></a>5. 已知问题</h2><p>通过两周ksql的使用，现发现有如下问题不能满足</p>\n<ul>\n<li><p>distinct</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select distinct ip from #table;</span><br><span class=\"line\">select count(distinct ip) from #table</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>sub query</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select ip from (select * from table);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>case when</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum(case when nvtc=&apos;&apos; then 1 else 0 end)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>in/not in</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xpathalias not in (&apos;Landingpage&apos;,&apos;Wishlist&apos;,&apos;CommonMessagepage&apos;,&apos;Common&apos;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6.参考\"></a>6.参考</h2><ol>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/index.html\" target=\"_blank\" rel=\"noopener\">KSQL</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/api.html\" target=\"_blank\" rel=\"noopener\">KSQL REST API Reference</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/syntax-reference.html#create-stream-as-select\" target=\"_blank\" rel=\"noopener\">KSQL Syntax Reference</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/concepts/time-and-windows-in-ksql-queries.html\" target=\"_blank\" rel=\"noopener\">Time and Windows in KSQL</a></li>\n<li><a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/udf.html\" target=\"_blank\" rel=\"noopener\">KSQL Custom Function Reference (UDF and UDAF)</a></li>\n</ol>\n"},{"title":"Kafka面试题与答案全套整理","toc":false,"date":"2019-04-13T11:36:34.000Z","_content":"\n[toc]\n## 声明\n本文问题参考[朱小厮的博客](https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A)。写这个只是为了检验自己最近的学习成果，因为是自己的理解，**如果问题，欢迎指出**。废话少说，上干货。\n## 正文\n### 1. Kafka的用途有哪些？使用场景如何？\n>总结下来就几个字:异步处理、日常系统解耦、削峰、提速、广播\n\n>如果再说具体一点例如:消息,网站活动追踪,监测指标,日志聚合,流处理,事件采集,提交日志等\n### 2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\n>ISR:In-Sync Replicas 副本同步队列\n\n>AR:Assigned Replicas 所有副本\n\n>ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。\n\n### 3. Kafka中的HW、LEO、LSO、LW等分别代表什么？\n>HW:High Watermark 高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置上一条信息。\n\n>LEO:LogEndOffset 当前日志文件中下一条待写信息的offset\n\n>HW/LEO这两个都是指最后一条的下一条的位置而不是指最后一条的位置。\n\n>LSO:Last Stable Offset 对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同\n\n>LW:Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值\n\n### 4. Kafka中是怎么体现消息顺序性的？\n>kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。\n整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.\n\n### 5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\n>拦截器->序列化器->分区器\n### 6. Kafka生产者客户端的整体结构是什么样子的？\n\n### 7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？\n>2个，主线程和Sender线程。主线程负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。Sender线程负责将RecordAccumulator中消息发送到kafka中.\n\n### 9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\n\n### 10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\n>不正确，通过自定义分区分配策略，可以将一个consumer指定消费所有partition。\n### 11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\n>offset+1\n### 12. 有哪些情形会造成重复消费？\n>消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)\n### 13. 那些情景下会造成消息漏消费？\n>消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束)\n### 14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\n>1.在每个线程中新建一个KafkaConsumer\n\n>2.单线程创建KafkaConsumer，多个处理线程处理消息（难点在于是否要考虑消息顺序性，offset的提交方式）\n\n### 15. 简述消费者与消费组之间的关系\n>消费者从属与消费组，消费偏移以消费组为单位。每个消费组可以独立消费主题的所有数据，同一消费组内消费者共同消费主题数据，每个分区只能被同一消费组内一个消费者消费。\n\n### 16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\n>创建:在zk上/brokers/topics/下节点 kafkabroker会监听节点变化创建主题\n删除:调用脚本删除topic会在zk上将topic设置待删除标志，kafka后台有定时的线程会扫描所有需要删除的topic进行删除\n### 17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n>可以\n### 18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\n>不可以\n### 19. 创建topic时如何选择合适的分区数？\n>根据集群的机器数量和需要的吞吐量来决定适合的分区数\n### 20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\n>__consumer_offsets 以下划线开头，保存消费组的偏移\n### 21. 优先副本是什么？它有什么特殊的作用？\n>优先副本 会是默认的leader副本 发生leader变化时重选举会优先选择优先副本作为leader\n### 22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\n>创建主题时\n如果不手动指定分配方式 有两种分配方式\n\n>消费组内分配\n### 23. 简述Kafka的日志目录结构\n>每个partition一个文件夹，包含四类文件.index .log  .timeindex  leader-epoch-checkpoint\n.index .log  .timeindex 三个文件成对出现 前缀为上一个segment的最后一个消息的偏移 log文件中保存了所有的消息 index文件中保存了稀疏的相对偏移的索引 timeindex保存的则是时间索引\nleader-epoch-checkpoint中保存了每一任leader开始写入消息时的offset 会定时更新\nfollower被选为leader时会根据这个确定哪些消息可用\n\n### 24. Kafka中有那些索引文件？\n>如上\n### 25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\n>1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件\n\n>2.offset-x为在文件中的相对偏移\n\n>3.通过index文件中记录的索引找到最近的消息的位置\n\n>4.从最近位置开始逐条寻找\n### 26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\n>原理同上 但是时间的因为消息体中不带有时间戳 所以不精确\n### 27. 聊一聊你对Kafka的Log Retention的理解\n>kafka留存策略包括 删除和压缩两种\n删除: 根据时间和大小两个方式进行删除 大小是整个partition日志文件的大小\n超过的会从老到新依次删除 时间指日志文件中的最大时间戳而非文件的最后修改时间\n压缩: 相同key的value只保存一个 压缩过的是clean 未压缩的dirty  压缩之后的偏移量不连续 未压缩时连续\n### 28. 聊一聊你对Kafka的Log Compaction的理解\n\n### 29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\n\n### 30. 聊一聊Kafka的延时操作的原理\n\n### 31. 聊一聊Kafka控制器的作用\n\n### 32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\n\n### 33. Kafka中的幂等是怎么实现的\n>pid+序号实现，单个producer内幂等\n### 33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话...只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ....”）\n\n### 34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\n\n### 35. 失效副本是指什么？有那些应对措施？\n\n### 36. 多副本下，各个副本中的HW和LEO的演变过程\n\n### 37. 为什么Kafka不支持读写分离？\n\n### 38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）\n\n### 39. Kafka中怎么实现死信队列和重试队列？\n\n### 40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\n\n### 41. Kafka中怎么做消息审计？\n\n### 42. Kafka中怎么做消息轨迹？\n\n### 43. Kafka中有那些配置参数比较有意思？聊一聊你的看法\n\n### 44. Kafka中有那些命名比较有意思？聊一聊你的看法\n\n### 45. Kafka有哪些指标需要着重关注？\n>生产者关注MessagesInPerSec、BytesOutPerSec、BytesInPerSec 消费者关注消费延迟Lag\n### 46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)\n>参考 [如何监控kafka消费Lag情况](http://trumandu.github.io/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/)\n### 47. Kafka的那些设计让它有如此高的性能？\n>零拷贝，页缓存，顺序写\n### 48. Kafka有什么优缺点？\n\n### 49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\n\n### 50. 为什么选择Kafka?\n>吞吐量高，大数据消息系统唯一选择。\n\n### 51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？\n\n### 52. 怎么样才能确保Kafka极大程度上的可靠性？\n\n### 53. 聊一聊你对Kafka生态的理解\n>confluent全家桶(connect/kafka stream/ksql/center/rest proxy等)，开源监控管理工具kafka-manager,kmanager等\n## 参考\n1. [Kafka面试题全套整理 | 划重点要考！](https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A)\n2. [Kafka科普系列 | 什么是LSO？](https://blog.csdn.net/u013256816/article/details/88985769)\n3. [Kafka面试题](https://www.jianshu.com/p/e4879aed5d43)","source":"_posts/Kafka面试题与答案全套整理.md","raw":"---\ntitle: Kafka面试题与答案全套整理\ntags:\n  - 技术分享\n  - 专栏\ncategories:\n  - kafka\ntoc: false\ndate: 2019-04-13 19:36:34\n---\n\n[toc]\n## 声明\n本文问题参考[朱小厮的博客](https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A)。写这个只是为了检验自己最近的学习成果，因为是自己的理解，**如果问题，欢迎指出**。废话少说，上干货。\n## 正文\n### 1. Kafka的用途有哪些？使用场景如何？\n>总结下来就几个字:异步处理、日常系统解耦、削峰、提速、广播\n\n>如果再说具体一点例如:消息,网站活动追踪,监测指标,日志聚合,流处理,事件采集,提交日志等\n### 2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\n>ISR:In-Sync Replicas 副本同步队列\n\n>AR:Assigned Replicas 所有副本\n\n>ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。\n\n### 3. Kafka中的HW、LEO、LSO、LW等分别代表什么？\n>HW:High Watermark 高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置上一条信息。\n\n>LEO:LogEndOffset 当前日志文件中下一条待写信息的offset\n\n>HW/LEO这两个都是指最后一条的下一条的位置而不是指最后一条的位置。\n\n>LSO:Last Stable Offset 对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同\n\n>LW:Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值\n\n### 4. Kafka中是怎么体现消息顺序性的？\n>kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。\n整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.\n\n### 5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\n>拦截器->序列化器->分区器\n### 6. Kafka生产者客户端的整体结构是什么样子的？\n\n### 7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？\n>2个，主线程和Sender线程。主线程负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。Sender线程负责将RecordAccumulator中消息发送到kafka中.\n\n### 9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\n\n### 10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\n>不正确，通过自定义分区分配策略，可以将一个consumer指定消费所有partition。\n### 11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\n>offset+1\n### 12. 有哪些情形会造成重复消费？\n>消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)\n### 13. 那些情景下会造成消息漏消费？\n>消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束)\n### 14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\n>1.在每个线程中新建一个KafkaConsumer\n\n>2.单线程创建KafkaConsumer，多个处理线程处理消息（难点在于是否要考虑消息顺序性，offset的提交方式）\n\n### 15. 简述消费者与消费组之间的关系\n>消费者从属与消费组，消费偏移以消费组为单位。每个消费组可以独立消费主题的所有数据，同一消费组内消费者共同消费主题数据，每个分区只能被同一消费组内一个消费者消费。\n\n### 16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\n>创建:在zk上/brokers/topics/下节点 kafkabroker会监听节点变化创建主题\n删除:调用脚本删除topic会在zk上将topic设置待删除标志，kafka后台有定时的线程会扫描所有需要删除的topic进行删除\n### 17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n>可以\n### 18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\n>不可以\n### 19. 创建topic时如何选择合适的分区数？\n>根据集群的机器数量和需要的吞吐量来决定适合的分区数\n### 20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\n>__consumer_offsets 以下划线开头，保存消费组的偏移\n### 21. 优先副本是什么？它有什么特殊的作用？\n>优先副本 会是默认的leader副本 发生leader变化时重选举会优先选择优先副本作为leader\n### 22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\n>创建主题时\n如果不手动指定分配方式 有两种分配方式\n\n>消费组内分配\n### 23. 简述Kafka的日志目录结构\n>每个partition一个文件夹，包含四类文件.index .log  .timeindex  leader-epoch-checkpoint\n.index .log  .timeindex 三个文件成对出现 前缀为上一个segment的最后一个消息的偏移 log文件中保存了所有的消息 index文件中保存了稀疏的相对偏移的索引 timeindex保存的则是时间索引\nleader-epoch-checkpoint中保存了每一任leader开始写入消息时的offset 会定时更新\nfollower被选为leader时会根据这个确定哪些消息可用\n\n### 24. Kafka中有那些索引文件？\n>如上\n### 25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\n>1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件\n\n>2.offset-x为在文件中的相对偏移\n\n>3.通过index文件中记录的索引找到最近的消息的位置\n\n>4.从最近位置开始逐条寻找\n### 26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\n>原理同上 但是时间的因为消息体中不带有时间戳 所以不精确\n### 27. 聊一聊你对Kafka的Log Retention的理解\n>kafka留存策略包括 删除和压缩两种\n删除: 根据时间和大小两个方式进行删除 大小是整个partition日志文件的大小\n超过的会从老到新依次删除 时间指日志文件中的最大时间戳而非文件的最后修改时间\n压缩: 相同key的value只保存一个 压缩过的是clean 未压缩的dirty  压缩之后的偏移量不连续 未压缩时连续\n### 28. 聊一聊你对Kafka的Log Compaction的理解\n\n### 29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\n\n### 30. 聊一聊Kafka的延时操作的原理\n\n### 31. 聊一聊Kafka控制器的作用\n\n### 32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\n\n### 33. Kafka中的幂等是怎么实现的\n>pid+序号实现，单个producer内幂等\n### 33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话...只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ....”）\n\n### 34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\n\n### 35. 失效副本是指什么？有那些应对措施？\n\n### 36. 多副本下，各个副本中的HW和LEO的演变过程\n\n### 37. 为什么Kafka不支持读写分离？\n\n### 38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）\n\n### 39. Kafka中怎么实现死信队列和重试队列？\n\n### 40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\n\n### 41. Kafka中怎么做消息审计？\n\n### 42. Kafka中怎么做消息轨迹？\n\n### 43. Kafka中有那些配置参数比较有意思？聊一聊你的看法\n\n### 44. Kafka中有那些命名比较有意思？聊一聊你的看法\n\n### 45. Kafka有哪些指标需要着重关注？\n>生产者关注MessagesInPerSec、BytesOutPerSec、BytesInPerSec 消费者关注消费延迟Lag\n### 46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)\n>参考 [如何监控kafka消费Lag情况](http://trumandu.github.io/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/)\n### 47. Kafka的那些设计让它有如此高的性能？\n>零拷贝，页缓存，顺序写\n### 48. Kafka有什么优缺点？\n\n### 49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\n\n### 50. 为什么选择Kafka?\n>吞吐量高，大数据消息系统唯一选择。\n\n### 51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？\n\n### 52. 怎么样才能确保Kafka极大程度上的可靠性？\n\n### 53. 聊一聊你对Kafka生态的理解\n>confluent全家桶(connect/kafka stream/ksql/center/rest proxy等)，开源监控管理工具kafka-manager,kmanager等\n## 参考\n1. [Kafka面试题全套整理 | 划重点要考！](https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A)\n2. [Kafka科普系列 | 什么是LSO？](https://blog.csdn.net/u013256816/article/details/88985769)\n3. [Kafka面试题](https://www.jianshu.com/p/e4879aed5d43)","slug":"Kafka面试题与答案全套整理","published":1,"updated":"2019-06-21T15:47:10.384Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvga00a88ceepfv0rfj1","content":"<p>[toc]</p>\n<h2 id=\"声明\"><a href=\"#声明\" class=\"headerlink\" title=\"声明\"></a>声明</h2><p>本文问题参考<a href=\"https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A\" target=\"_blank\" rel=\"noopener\">朱小厮的博客</a>。写这个只是为了检验自己最近的学习成果，因为是自己的理解，<strong>如果问题，欢迎指出</strong>。废话少说，上干货。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-Kafka的用途有哪些？使用场景如何？\"><a href=\"#1-Kafka的用途有哪些？使用场景如何？\" class=\"headerlink\" title=\"1. Kafka的用途有哪些？使用场景如何？\"></a>1. Kafka的用途有哪些？使用场景如何？</h3><blockquote>\n<p>总结下来就几个字:异步处理、日常系统解耦、削峰、提速、广播</p>\n</blockquote>\n<blockquote>\n<p>如果再说具体一点例如:消息,网站活动追踪,监测指标,日志聚合,流处理,事件采集,提交日志等</p>\n</blockquote>\n<h3 id=\"2-Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\"><a href=\"#2-Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\" class=\"headerlink\" title=\"2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\"></a>2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么</h3><blockquote>\n<p>ISR:In-Sync Replicas 副本同步队列</p>\n</blockquote>\n<blockquote>\n<p>AR:Assigned Replicas 所有副本</p>\n</blockquote>\n<blockquote>\n<p>ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。</p>\n</blockquote>\n<h3 id=\"3-Kafka中的HW、LEO、LSO、LW等分别代表什么？\"><a href=\"#3-Kafka中的HW、LEO、LSO、LW等分别代表什么？\" class=\"headerlink\" title=\"3. Kafka中的HW、LEO、LSO、LW等分别代表什么？\"></a>3. Kafka中的HW、LEO、LSO、LW等分别代表什么？</h3><blockquote>\n<p>HW:High Watermark 高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置上一条信息。</p>\n</blockquote>\n<blockquote>\n<p>LEO:LogEndOffset 当前日志文件中下一条待写信息的offset</p>\n</blockquote>\n<blockquote>\n<p>HW/LEO这两个都是指最后一条的下一条的位置而不是指最后一条的位置。</p>\n</blockquote>\n<blockquote>\n<p>LSO:Last Stable Offset 对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同</p>\n</blockquote>\n<blockquote>\n<p>LW:Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值</p>\n</blockquote>\n<h3 id=\"4-Kafka中是怎么体现消息顺序性的？\"><a href=\"#4-Kafka中是怎么体现消息顺序性的？\" class=\"headerlink\" title=\"4. Kafka中是怎么体现消息顺序性的？\"></a>4. Kafka中是怎么体现消息顺序性的？</h3><blockquote>\n<p>kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。<br>整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.</p>\n</blockquote>\n<h3 id=\"5-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\"><a href=\"#5-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\" class=\"headerlink\" title=\"5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\"></a>5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h3><blockquote>\n<p>拦截器-&gt;序列化器-&gt;分区器</p>\n</blockquote>\n<h3 id=\"6-Kafka生产者客户端的整体结构是什么样子的？\"><a href=\"#6-Kafka生产者客户端的整体结构是什么样子的？\" class=\"headerlink\" title=\"6. Kafka生产者客户端的整体结构是什么样子的？\"></a>6. Kafka生产者客户端的整体结构是什么样子的？</h3><h3 id=\"7-Kafka生产者客户端中使用了几个线程来处理？分别是什么？\"><a href=\"#7-Kafka生产者客户端中使用了几个线程来处理？分别是什么？\" class=\"headerlink\" title=\"7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？\"></a>7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？</h3><blockquote>\n<p>2个，主线程和Sender线程。主线程负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。Sender线程负责将RecordAccumulator中消息发送到kafka中.</p>\n</blockquote>\n<h3 id=\"9-Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\"><a href=\"#9-Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\" class=\"headerlink\" title=\"9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\"></a>9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？</h3><h3 id=\"10-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\"><a href=\"#10-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\" class=\"headerlink\" title=\"10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\"></a>10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？</h3><blockquote>\n<p>不正确，通过自定义分区分配策略，可以将一个consumer指定消费所有partition。</p>\n</blockquote>\n<h3 id=\"11-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1\"><a href=\"#11-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1\" class=\"headerlink\" title=\"11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\"></a>11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</h3><blockquote>\n<p>offset+1</p>\n</blockquote>\n<h3 id=\"12-有哪些情形会造成重复消费？\"><a href=\"#12-有哪些情形会造成重复消费？\" class=\"headerlink\" title=\"12. 有哪些情形会造成重复消费？\"></a>12. 有哪些情形会造成重复消费？</h3><blockquote>\n<p>消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)</p>\n</blockquote>\n<h3 id=\"13-那些情景下会造成消息漏消费？\"><a href=\"#13-那些情景下会造成消息漏消费？\" class=\"headerlink\" title=\"13. 那些情景下会造成消息漏消费？\"></a>13. 那些情景下会造成消息漏消费？</h3><blockquote>\n<p>消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束)</p>\n</blockquote>\n<h3 id=\"14-KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\"><a href=\"#14-KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\" class=\"headerlink\" title=\"14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\"></a>14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？</h3><blockquote>\n<p>1.在每个线程中新建一个KafkaConsumer</p>\n</blockquote>\n<blockquote>\n<p>2.单线程创建KafkaConsumer，多个处理线程处理消息（难点在于是否要考虑消息顺序性，offset的提交方式）</p>\n</blockquote>\n<h3 id=\"15-简述消费者与消费组之间的关系\"><a href=\"#15-简述消费者与消费组之间的关系\" class=\"headerlink\" title=\"15. 简述消费者与消费组之间的关系\"></a>15. 简述消费者与消费组之间的关系</h3><blockquote>\n<p>消费者从属与消费组，消费偏移以消费组为单位。每个消费组可以独立消费主题的所有数据，同一消费组内消费者共同消费主题数据，每个分区只能被同一消费组内一个消费者消费。</p>\n</blockquote>\n<h3 id=\"16-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\"><a href=\"#16-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\" class=\"headerlink\" title=\"16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\"></a>16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h3><blockquote>\n<p>创建:在zk上/brokers/topics/下节点 kafkabroker会监听节点变化创建主题<br>删除:调用脚本删除topic会在zk上将topic设置待删除标志，kafka后台有定时的线程会扫描所有需要删除的topic进行删除</p>\n</blockquote>\n<h3 id=\"17-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\"><a href=\"#17-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\" class=\"headerlink\" title=\"17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\"></a>17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h3><blockquote>\n<p>可以</p>\n</blockquote>\n<h3 id=\"18-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\"><a href=\"#18-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\" class=\"headerlink\" title=\"18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\"></a>18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h3><blockquote>\n<p>不可以</p>\n</blockquote>\n<h3 id=\"19-创建topic时如何选择合适的分区数？\"><a href=\"#19-创建topic时如何选择合适的分区数？\" class=\"headerlink\" title=\"19. 创建topic时如何选择合适的分区数？\"></a>19. 创建topic时如何选择合适的分区数？</h3><blockquote>\n<p>根据集群的机器数量和需要的吞吐量来决定适合的分区数</p>\n</blockquote>\n<h3 id=\"20-Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\"><a href=\"#20-Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\" class=\"headerlink\" title=\"20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\"></a>20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？</h3><blockquote>\n<p>__consumer_offsets 以下划线开头，保存消费组的偏移</p>\n</blockquote>\n<h3 id=\"21-优先副本是什么？它有什么特殊的作用？\"><a href=\"#21-优先副本是什么？它有什么特殊的作用？\" class=\"headerlink\" title=\"21. 优先副本是什么？它有什么特殊的作用？\"></a>21. 优先副本是什么？它有什么特殊的作用？</h3><blockquote>\n<p>优先副本 会是默认的leader副本 发生leader变化时重选举会优先选择优先副本作为leader</p>\n</blockquote>\n<h3 id=\"22-Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\"><a href=\"#22-Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\" class=\"headerlink\" title=\"22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\"></a>22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理</h3><blockquote>\n<p>创建主题时<br>如果不手动指定分配方式 有两种分配方式</p>\n</blockquote>\n<blockquote>\n<p>消费组内分配</p>\n</blockquote>\n<h3 id=\"23-简述Kafka的日志目录结构\"><a href=\"#23-简述Kafka的日志目录结构\" class=\"headerlink\" title=\"23. 简述Kafka的日志目录结构\"></a>23. 简述Kafka的日志目录结构</h3><blockquote>\n<p>每个partition一个文件夹，包含四类文件.index .log  .timeindex  leader-epoch-checkpoint<br>.index .log  .timeindex 三个文件成对出现 前缀为上一个segment的最后一个消息的偏移 log文件中保存了所有的消息 index文件中保存了稀疏的相对偏移的索引 timeindex保存的则是时间索引<br>leader-epoch-checkpoint中保存了每一任leader开始写入消息时的offset 会定时更新<br>follower被选为leader时会根据这个确定哪些消息可用</p>\n</blockquote>\n<h3 id=\"24-Kafka中有那些索引文件？\"><a href=\"#24-Kafka中有那些索引文件？\" class=\"headerlink\" title=\"24. Kafka中有那些索引文件？\"></a>24. Kafka中有那些索引文件？</h3><blockquote>\n<p>如上</p>\n</blockquote>\n<h3 id=\"25-如果我指定了一个offset，Kafka怎么查找到对应的消息？\"><a href=\"#25-如果我指定了一个offset，Kafka怎么查找到对应的消息？\" class=\"headerlink\" title=\"25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\"></a>25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？</h3><blockquote>\n<p>1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件</p>\n</blockquote>\n<blockquote>\n<p>2.offset-x为在文件中的相对偏移</p>\n</blockquote>\n<blockquote>\n<p>3.通过index文件中记录的索引找到最近的消息的位置</p>\n</blockquote>\n<blockquote>\n<p>4.从最近位置开始逐条寻找</p>\n</blockquote>\n<h3 id=\"26-如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\"><a href=\"#26-如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\" class=\"headerlink\" title=\"26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\"></a>26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</h3><blockquote>\n<p>原理同上 但是时间的因为消息体中不带有时间戳 所以不精确</p>\n</blockquote>\n<h3 id=\"27-聊一聊你对Kafka的Log-Retention的理解\"><a href=\"#27-聊一聊你对Kafka的Log-Retention的理解\" class=\"headerlink\" title=\"27. 聊一聊你对Kafka的Log Retention的理解\"></a>27. 聊一聊你对Kafka的Log Retention的理解</h3><blockquote>\n<p>kafka留存策略包括 删除和压缩两种<br>删除: 根据时间和大小两个方式进行删除 大小是整个partition日志文件的大小<br>超过的会从老到新依次删除 时间指日志文件中的最大时间戳而非文件的最后修改时间<br>压缩: 相同key的value只保存一个 压缩过的是clean 未压缩的dirty  压缩之后的偏移量不连续 未压缩时连续</p>\n</blockquote>\n<h3 id=\"28-聊一聊你对Kafka的Log-Compaction的理解\"><a href=\"#28-聊一聊你对Kafka的Log-Compaction的理解\" class=\"headerlink\" title=\"28. 聊一聊你对Kafka的Log Compaction的理解\"></a>28. 聊一聊你对Kafka的Log Compaction的理解</h3><h3 id=\"29-聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\"><a href=\"#29-聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\" class=\"headerlink\" title=\"29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\"></a>29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）</h3><h3 id=\"30-聊一聊Kafka的延时操作的原理\"><a href=\"#30-聊一聊Kafka的延时操作的原理\" class=\"headerlink\" title=\"30. 聊一聊Kafka的延时操作的原理\"></a>30. 聊一聊Kafka的延时操作的原理</h3><h3 id=\"31-聊一聊Kafka控制器的作用\"><a href=\"#31-聊一聊Kafka控制器的作用\" class=\"headerlink\" title=\"31. 聊一聊Kafka控制器的作用\"></a>31. 聊一聊Kafka控制器的作用</h3><h3 id=\"32-消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\"><a href=\"#32-消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\" class=\"headerlink\" title=\"32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\"></a>32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）</h3><h3 id=\"33-Kafka中的幂等是怎么实现的\"><a href=\"#33-Kafka中的幂等是怎么实现的\" class=\"headerlink\" title=\"33. Kafka中的幂等是怎么实现的\"></a>33. Kafka中的幂等是怎么实现的</h3><blockquote>\n<p>pid+序号实现，单个producer内幂等</p>\n</blockquote>\n<h3 id=\"33-Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）\"><a href=\"#33-Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）\" class=\"headerlink\" title=\"33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）\"></a>33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）</h3><h3 id=\"34-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\"><a href=\"#34-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\" class=\"headerlink\" title=\"34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\"></a>34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</h3><h3 id=\"35-失效副本是指什么？有那些应对措施？\"><a href=\"#35-失效副本是指什么？有那些应对措施？\" class=\"headerlink\" title=\"35. 失效副本是指什么？有那些应对措施？\"></a>35. 失效副本是指什么？有那些应对措施？</h3><h3 id=\"36-多副本下，各个副本中的HW和LEO的演变过程\"><a href=\"#36-多副本下，各个副本中的HW和LEO的演变过程\" class=\"headerlink\" title=\"36. 多副本下，各个副本中的HW和LEO的演变过程\"></a>36. 多副本下，各个副本中的HW和LEO的演变过程</h3><h3 id=\"37-为什么Kafka不支持读写分离？\"><a href=\"#37-为什么Kafka不支持读写分离？\" class=\"headerlink\" title=\"37. 为什么Kafka不支持读写分离？\"></a>37. 为什么Kafka不支持读写分离？</h3><h3 id=\"38-Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）\"><a href=\"#38-Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）\" class=\"headerlink\" title=\"38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）\"></a>38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）</h3><h3 id=\"39-Kafka中怎么实现死信队列和重试队列？\"><a href=\"#39-Kafka中怎么实现死信队列和重试队列？\" class=\"headerlink\" title=\"39. Kafka中怎么实现死信队列和重试队列？\"></a>39. Kafka中怎么实现死信队列和重试队列？</h3><h3 id=\"40-Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\"><a href=\"#40-Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\" class=\"headerlink\" title=\"40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\"></a>40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）</h3><h3 id=\"41-Kafka中怎么做消息审计？\"><a href=\"#41-Kafka中怎么做消息审计？\" class=\"headerlink\" title=\"41. Kafka中怎么做消息审计？\"></a>41. Kafka中怎么做消息审计？</h3><h3 id=\"42-Kafka中怎么做消息轨迹？\"><a href=\"#42-Kafka中怎么做消息轨迹？\" class=\"headerlink\" title=\"42. Kafka中怎么做消息轨迹？\"></a>42. Kafka中怎么做消息轨迹？</h3><h3 id=\"43-Kafka中有那些配置参数比较有意思？聊一聊你的看法\"><a href=\"#43-Kafka中有那些配置参数比较有意思？聊一聊你的看法\" class=\"headerlink\" title=\"43. Kafka中有那些配置参数比较有意思？聊一聊你的看法\"></a>43. Kafka中有那些配置参数比较有意思？聊一聊你的看法</h3><h3 id=\"44-Kafka中有那些命名比较有意思？聊一聊你的看法\"><a href=\"#44-Kafka中有那些命名比较有意思？聊一聊你的看法\" class=\"headerlink\" title=\"44. Kafka中有那些命名比较有意思？聊一聊你的看法\"></a>44. Kafka中有那些命名比较有意思？聊一聊你的看法</h3><h3 id=\"45-Kafka有哪些指标需要着重关注？\"><a href=\"#45-Kafka有哪些指标需要着重关注？\" class=\"headerlink\" title=\"45. Kafka有哪些指标需要着重关注？\"></a>45. Kafka有哪些指标需要着重关注？</h3><blockquote>\n<p>生产者关注MessagesInPerSec、BytesOutPerSec、BytesInPerSec 消费者关注消费延迟Lag</p>\n</blockquote>\n<h3 id=\"46-怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同\"><a href=\"#46-怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同\" class=\"headerlink\" title=\"46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)\"></a>46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)</h3><blockquote>\n<p>参考 <a href=\"http://trumandu.github.io/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/\">如何监控kafka消费Lag情况</a></p>\n</blockquote>\n<h3 id=\"47-Kafka的那些设计让它有如此高的性能？\"><a href=\"#47-Kafka的那些设计让它有如此高的性能？\" class=\"headerlink\" title=\"47. Kafka的那些设计让它有如此高的性能？\"></a>47. Kafka的那些设计让它有如此高的性能？</h3><blockquote>\n<p>零拷贝，页缓存，顺序写</p>\n</blockquote>\n<h3 id=\"48-Kafka有什么优缺点？\"><a href=\"#48-Kafka有什么优缺点？\" class=\"headerlink\" title=\"48. Kafka有什么优缺点？\"></a>48. Kafka有什么优缺点？</h3><h3 id=\"49-还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\"><a href=\"#49-还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\" class=\"headerlink\" title=\"49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\"></a>49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？</h3><h3 id=\"50-为什么选择Kafka\"><a href=\"#50-为什么选择Kafka\" class=\"headerlink\" title=\"50. 为什么选择Kafka?\"></a>50. 为什么选择Kafka?</h3><blockquote>\n<p>吞吐量高，大数据消息系统唯一选择。</p>\n</blockquote>\n<h3 id=\"51-在使用Kafka的过程中遇到过什么困难？怎么解决的？\"><a href=\"#51-在使用Kafka的过程中遇到过什么困难？怎么解决的？\" class=\"headerlink\" title=\"51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？\"></a>51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？</h3><h3 id=\"52-怎么样才能确保Kafka极大程度上的可靠性？\"><a href=\"#52-怎么样才能确保Kafka极大程度上的可靠性？\" class=\"headerlink\" title=\"52. 怎么样才能确保Kafka极大程度上的可靠性？\"></a>52. 怎么样才能确保Kafka极大程度上的可靠性？</h3><h3 id=\"53-聊一聊你对Kafka生态的理解\"><a href=\"#53-聊一聊你对Kafka生态的理解\" class=\"headerlink\" title=\"53. 聊一聊你对Kafka生态的理解\"></a>53. 聊一聊你对Kafka生态的理解</h3><blockquote>\n<p>confluent全家桶(connect/kafka stream/ksql/center/rest proxy等)，开源监控管理工具kafka-manager,kmanager等</p>\n</blockquote>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A\" target=\"_blank\" rel=\"noopener\">Kafka面试题全套整理 | 划重点要考！</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/88985769\" target=\"_blank\" rel=\"noopener\">Kafka科普系列 | 什么是LSO？</a></li>\n<li><a href=\"https://www.jianshu.com/p/e4879aed5d43\" target=\"_blank\" rel=\"noopener\">Kafka面试题</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>[toc]</p>\n<h2 id=\"声明\"><a href=\"#声明\" class=\"headerlink\" title=\"声明\"></a>声明</h2><p>本文问题参考<a href=\"https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A\" target=\"_blank\" rel=\"noopener\">朱小厮的博客</a>。写这个只是为了检验自己最近的学习成果，因为是自己的理解，<strong>如果问题，欢迎指出</strong>。废话少说，上干货。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-Kafka的用途有哪些？使用场景如何？\"><a href=\"#1-Kafka的用途有哪些？使用场景如何？\" class=\"headerlink\" title=\"1. Kafka的用途有哪些？使用场景如何？\"></a>1. Kafka的用途有哪些？使用场景如何？</h3><blockquote>\n<p>总结下来就几个字:异步处理、日常系统解耦、削峰、提速、广播</p>\n</blockquote>\n<blockquote>\n<p>如果再说具体一点例如:消息,网站活动追踪,监测指标,日志聚合,流处理,事件采集,提交日志等</p>\n</blockquote>\n<h3 id=\"2-Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\"><a href=\"#2-Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\" class=\"headerlink\" title=\"2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么\"></a>2. Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么</h3><blockquote>\n<p>ISR:In-Sync Replicas 副本同步队列</p>\n</blockquote>\n<blockquote>\n<p>AR:Assigned Replicas 所有副本</p>\n</blockquote>\n<blockquote>\n<p>ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。</p>\n</blockquote>\n<h3 id=\"3-Kafka中的HW、LEO、LSO、LW等分别代表什么？\"><a href=\"#3-Kafka中的HW、LEO、LSO、LW等分别代表什么？\" class=\"headerlink\" title=\"3. Kafka中的HW、LEO、LSO、LW等分别代表什么？\"></a>3. Kafka中的HW、LEO、LSO、LW等分别代表什么？</h3><blockquote>\n<p>HW:High Watermark 高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置上一条信息。</p>\n</blockquote>\n<blockquote>\n<p>LEO:LogEndOffset 当前日志文件中下一条待写信息的offset</p>\n</blockquote>\n<blockquote>\n<p>HW/LEO这两个都是指最后一条的下一条的位置而不是指最后一条的位置。</p>\n</blockquote>\n<blockquote>\n<p>LSO:Last Stable Offset 对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同</p>\n</blockquote>\n<blockquote>\n<p>LW:Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值</p>\n</blockquote>\n<h3 id=\"4-Kafka中是怎么体现消息顺序性的？\"><a href=\"#4-Kafka中是怎么体现消息顺序性的？\" class=\"headerlink\" title=\"4. Kafka中是怎么体现消息顺序性的？\"></a>4. Kafka中是怎么体现消息顺序性的？</h3><blockquote>\n<p>kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。<br>整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.</p>\n</blockquote>\n<h3 id=\"5-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\"><a href=\"#5-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\" class=\"headerlink\" title=\"5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？\"></a>5. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h3><blockquote>\n<p>拦截器-&gt;序列化器-&gt;分区器</p>\n</blockquote>\n<h3 id=\"6-Kafka生产者客户端的整体结构是什么样子的？\"><a href=\"#6-Kafka生产者客户端的整体结构是什么样子的？\" class=\"headerlink\" title=\"6. Kafka生产者客户端的整体结构是什么样子的？\"></a>6. Kafka生产者客户端的整体结构是什么样子的？</h3><h3 id=\"7-Kafka生产者客户端中使用了几个线程来处理？分别是什么？\"><a href=\"#7-Kafka生产者客户端中使用了几个线程来处理？分别是什么？\" class=\"headerlink\" title=\"7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？\"></a>7. Kafka生产者客户端中使用了几个线程来处理？分别是什么？</h3><blockquote>\n<p>2个，主线程和Sender线程。主线程负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。Sender线程负责将RecordAccumulator中消息发送到kafka中.</p>\n</blockquote>\n<h3 id=\"9-Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\"><a href=\"#9-Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\" class=\"headerlink\" title=\"9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？\"></a>9. Kafka的旧版Scala的消费者客户端的设计有什么缺陷？</h3><h3 id=\"10-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\"><a href=\"#10-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\" class=\"headerlink\" title=\"10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\"></a>10. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？</h3><blockquote>\n<p>不正确，通过自定义分区分配策略，可以将一个consumer指定消费所有partition。</p>\n</blockquote>\n<h3 id=\"11-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1\"><a href=\"#11-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1\" class=\"headerlink\" title=\"11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\"></a>11. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</h3><blockquote>\n<p>offset+1</p>\n</blockquote>\n<h3 id=\"12-有哪些情形会造成重复消费？\"><a href=\"#12-有哪些情形会造成重复消费？\" class=\"headerlink\" title=\"12. 有哪些情形会造成重复消费？\"></a>12. 有哪些情形会造成重复消费？</h3><blockquote>\n<p>消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)</p>\n</blockquote>\n<h3 id=\"13-那些情景下会造成消息漏消费？\"><a href=\"#13-那些情景下会造成消息漏消费？\" class=\"headerlink\" title=\"13. 那些情景下会造成消息漏消费？\"></a>13. 那些情景下会造成消息漏消费？</h3><blockquote>\n<p>消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束)</p>\n</blockquote>\n<h3 id=\"14-KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\"><a href=\"#14-KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\" class=\"headerlink\" title=\"14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\"></a>14. KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？</h3><blockquote>\n<p>1.在每个线程中新建一个KafkaConsumer</p>\n</blockquote>\n<blockquote>\n<p>2.单线程创建KafkaConsumer，多个处理线程处理消息（难点在于是否要考虑消息顺序性，offset的提交方式）</p>\n</blockquote>\n<h3 id=\"15-简述消费者与消费组之间的关系\"><a href=\"#15-简述消费者与消费组之间的关系\" class=\"headerlink\" title=\"15. 简述消费者与消费组之间的关系\"></a>15. 简述消费者与消费组之间的关系</h3><blockquote>\n<p>消费者从属与消费组，消费偏移以消费组为单位。每个消费组可以独立消费主题的所有数据，同一消费组内消费者共同消费主题数据，每个分区只能被同一消费组内一个消费者消费。</p>\n</blockquote>\n<h3 id=\"16-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\"><a href=\"#16-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\" class=\"headerlink\" title=\"16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\"></a>16. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h3><blockquote>\n<p>创建:在zk上/brokers/topics/下节点 kafkabroker会监听节点变化创建主题<br>删除:调用脚本删除topic会在zk上将topic设置待删除标志，kafka后台有定时的线程会扫描所有需要删除的topic进行删除</p>\n</blockquote>\n<h3 id=\"17-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\"><a href=\"#17-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\" class=\"headerlink\" title=\"17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\"></a>17. topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h3><blockquote>\n<p>可以</p>\n</blockquote>\n<h3 id=\"18-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\"><a href=\"#18-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\" class=\"headerlink\" title=\"18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？\"></a>18. topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h3><blockquote>\n<p>不可以</p>\n</blockquote>\n<h3 id=\"19-创建topic时如何选择合适的分区数？\"><a href=\"#19-创建topic时如何选择合适的分区数？\" class=\"headerlink\" title=\"19. 创建topic时如何选择合适的分区数？\"></a>19. 创建topic时如何选择合适的分区数？</h3><blockquote>\n<p>根据集群的机器数量和需要的吞吐量来决定适合的分区数</p>\n</blockquote>\n<h3 id=\"20-Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\"><a href=\"#20-Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\" class=\"headerlink\" title=\"20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\"></a>20. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？</h3><blockquote>\n<p>__consumer_offsets 以下划线开头，保存消费组的偏移</p>\n</blockquote>\n<h3 id=\"21-优先副本是什么？它有什么特殊的作用？\"><a href=\"#21-优先副本是什么？它有什么特殊的作用？\" class=\"headerlink\" title=\"21. 优先副本是什么？它有什么特殊的作用？\"></a>21. 优先副本是什么？它有什么特殊的作用？</h3><blockquote>\n<p>优先副本 会是默认的leader副本 发生leader变化时重选举会优先选择优先副本作为leader</p>\n</blockquote>\n<h3 id=\"22-Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\"><a href=\"#22-Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\" class=\"headerlink\" title=\"22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理\"></a>22. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理</h3><blockquote>\n<p>创建主题时<br>如果不手动指定分配方式 有两种分配方式</p>\n</blockquote>\n<blockquote>\n<p>消费组内分配</p>\n</blockquote>\n<h3 id=\"23-简述Kafka的日志目录结构\"><a href=\"#23-简述Kafka的日志目录结构\" class=\"headerlink\" title=\"23. 简述Kafka的日志目录结构\"></a>23. 简述Kafka的日志目录结构</h3><blockquote>\n<p>每个partition一个文件夹，包含四类文件.index .log  .timeindex  leader-epoch-checkpoint<br>.index .log  .timeindex 三个文件成对出现 前缀为上一个segment的最后一个消息的偏移 log文件中保存了所有的消息 index文件中保存了稀疏的相对偏移的索引 timeindex保存的则是时间索引<br>leader-epoch-checkpoint中保存了每一任leader开始写入消息时的offset 会定时更新<br>follower被选为leader时会根据这个确定哪些消息可用</p>\n</blockquote>\n<h3 id=\"24-Kafka中有那些索引文件？\"><a href=\"#24-Kafka中有那些索引文件？\" class=\"headerlink\" title=\"24. Kafka中有那些索引文件？\"></a>24. Kafka中有那些索引文件？</h3><blockquote>\n<p>如上</p>\n</blockquote>\n<h3 id=\"25-如果我指定了一个offset，Kafka怎么查找到对应的消息？\"><a href=\"#25-如果我指定了一个offset，Kafka怎么查找到对应的消息？\" class=\"headerlink\" title=\"25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？\"></a>25. 如果我指定了一个offset，Kafka怎么查找到对应的消息？</h3><blockquote>\n<p>1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件</p>\n</blockquote>\n<blockquote>\n<p>2.offset-x为在文件中的相对偏移</p>\n</blockquote>\n<blockquote>\n<p>3.通过index文件中记录的索引找到最近的消息的位置</p>\n</blockquote>\n<blockquote>\n<p>4.从最近位置开始逐条寻找</p>\n</blockquote>\n<h3 id=\"26-如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\"><a href=\"#26-如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\" class=\"headerlink\" title=\"26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？\"></a>26. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</h3><blockquote>\n<p>原理同上 但是时间的因为消息体中不带有时间戳 所以不精确</p>\n</blockquote>\n<h3 id=\"27-聊一聊你对Kafka的Log-Retention的理解\"><a href=\"#27-聊一聊你对Kafka的Log-Retention的理解\" class=\"headerlink\" title=\"27. 聊一聊你对Kafka的Log Retention的理解\"></a>27. 聊一聊你对Kafka的Log Retention的理解</h3><blockquote>\n<p>kafka留存策略包括 删除和压缩两种<br>删除: 根据时间和大小两个方式进行删除 大小是整个partition日志文件的大小<br>超过的会从老到新依次删除 时间指日志文件中的最大时间戳而非文件的最后修改时间<br>压缩: 相同key的value只保存一个 压缩过的是clean 未压缩的dirty  压缩之后的偏移量不连续 未压缩时连续</p>\n</blockquote>\n<h3 id=\"28-聊一聊你对Kafka的Log-Compaction的理解\"><a href=\"#28-聊一聊你对Kafka的Log-Compaction的理解\" class=\"headerlink\" title=\"28. 聊一聊你对Kafka的Log Compaction的理解\"></a>28. 聊一聊你对Kafka的Log Compaction的理解</h3><h3 id=\"29-聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\"><a href=\"#29-聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\" class=\"headerlink\" title=\"29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）\"></a>29. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）</h3><h3 id=\"30-聊一聊Kafka的延时操作的原理\"><a href=\"#30-聊一聊Kafka的延时操作的原理\" class=\"headerlink\" title=\"30. 聊一聊Kafka的延时操作的原理\"></a>30. 聊一聊Kafka的延时操作的原理</h3><h3 id=\"31-聊一聊Kafka控制器的作用\"><a href=\"#31-聊一聊Kafka控制器的作用\" class=\"headerlink\" title=\"31. 聊一聊Kafka控制器的作用\"></a>31. 聊一聊Kafka控制器的作用</h3><h3 id=\"32-消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\"><a href=\"#32-消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\" class=\"headerlink\" title=\"32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）\"></a>32. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）</h3><h3 id=\"33-Kafka中的幂等是怎么实现的\"><a href=\"#33-Kafka中的幂等是怎么实现的\" class=\"headerlink\" title=\"33. Kafka中的幂等是怎么实现的\"></a>33. Kafka中的幂等是怎么实现的</h3><blockquote>\n<p>pid+序号实现，单个producer内幂等</p>\n</blockquote>\n<h3 id=\"33-Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）\"><a href=\"#33-Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）\" class=\"headerlink\" title=\"33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）\"></a>33. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）</h3><h3 id=\"34-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\"><a href=\"#34-Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\" class=\"headerlink\" title=\"34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？\"></a>34. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</h3><h3 id=\"35-失效副本是指什么？有那些应对措施？\"><a href=\"#35-失效副本是指什么？有那些应对措施？\" class=\"headerlink\" title=\"35. 失效副本是指什么？有那些应对措施？\"></a>35. 失效副本是指什么？有那些应对措施？</h3><h3 id=\"36-多副本下，各个副本中的HW和LEO的演变过程\"><a href=\"#36-多副本下，各个副本中的HW和LEO的演变过程\" class=\"headerlink\" title=\"36. 多副本下，各个副本中的HW和LEO的演变过程\"></a>36. 多副本下，各个副本中的HW和LEO的演变过程</h3><h3 id=\"37-为什么Kafka不支持读写分离？\"><a href=\"#37-为什么Kafka不支持读写分离？\" class=\"headerlink\" title=\"37. 为什么Kafka不支持读写分离？\"></a>37. 为什么Kafka不支持读写分离？</h3><h3 id=\"38-Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）\"><a href=\"#38-Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）\" class=\"headerlink\" title=\"38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）\"></a>38. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）</h3><h3 id=\"39-Kafka中怎么实现死信队列和重试队列？\"><a href=\"#39-Kafka中怎么实现死信队列和重试队列？\" class=\"headerlink\" title=\"39. Kafka中怎么实现死信队列和重试队列？\"></a>39. Kafka中怎么实现死信队列和重试队列？</h3><h3 id=\"40-Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\"><a href=\"#40-Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\" class=\"headerlink\" title=\"40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\"></a>40. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）</h3><h3 id=\"41-Kafka中怎么做消息审计？\"><a href=\"#41-Kafka中怎么做消息审计？\" class=\"headerlink\" title=\"41. Kafka中怎么做消息审计？\"></a>41. Kafka中怎么做消息审计？</h3><h3 id=\"42-Kafka中怎么做消息轨迹？\"><a href=\"#42-Kafka中怎么做消息轨迹？\" class=\"headerlink\" title=\"42. Kafka中怎么做消息轨迹？\"></a>42. Kafka中怎么做消息轨迹？</h3><h3 id=\"43-Kafka中有那些配置参数比较有意思？聊一聊你的看法\"><a href=\"#43-Kafka中有那些配置参数比较有意思？聊一聊你的看法\" class=\"headerlink\" title=\"43. Kafka中有那些配置参数比较有意思？聊一聊你的看法\"></a>43. Kafka中有那些配置参数比较有意思？聊一聊你的看法</h3><h3 id=\"44-Kafka中有那些命名比较有意思？聊一聊你的看法\"><a href=\"#44-Kafka中有那些命名比较有意思？聊一聊你的看法\" class=\"headerlink\" title=\"44. Kafka中有那些命名比较有意思？聊一聊你的看法\"></a>44. Kafka中有那些命名比较有意思？聊一聊你的看法</h3><h3 id=\"45-Kafka有哪些指标需要着重关注？\"><a href=\"#45-Kafka有哪些指标需要着重关注？\" class=\"headerlink\" title=\"45. Kafka有哪些指标需要着重关注？\"></a>45. Kafka有哪些指标需要着重关注？</h3><blockquote>\n<p>生产者关注MessagesInPerSec、BytesOutPerSec、BytesInPerSec 消费者关注消费延迟Lag</p>\n</blockquote>\n<h3 id=\"46-怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同\"><a href=\"#46-怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同\" class=\"headerlink\" title=\"46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)\"></a>46. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)</h3><blockquote>\n<p>参考 <a href=\"http://trumandu.github.io/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/\">如何监控kafka消费Lag情况</a></p>\n</blockquote>\n<h3 id=\"47-Kafka的那些设计让它有如此高的性能？\"><a href=\"#47-Kafka的那些设计让它有如此高的性能？\" class=\"headerlink\" title=\"47. Kafka的那些设计让它有如此高的性能？\"></a>47. Kafka的那些设计让它有如此高的性能？</h3><blockquote>\n<p>零拷贝，页缓存，顺序写</p>\n</blockquote>\n<h3 id=\"48-Kafka有什么优缺点？\"><a href=\"#48-Kafka有什么优缺点？\" class=\"headerlink\" title=\"48. Kafka有什么优缺点？\"></a>48. Kafka有什么优缺点？</h3><h3 id=\"49-还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\"><a href=\"#49-还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\" class=\"headerlink\" title=\"49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\"></a>49. 还用过什么同质类的其它产品，与Kafka相比有什么优缺点？</h3><h3 id=\"50-为什么选择Kafka\"><a href=\"#50-为什么选择Kafka\" class=\"headerlink\" title=\"50. 为什么选择Kafka?\"></a>50. 为什么选择Kafka?</h3><blockquote>\n<p>吞吐量高，大数据消息系统唯一选择。</p>\n</blockquote>\n<h3 id=\"51-在使用Kafka的过程中遇到过什么困难？怎么解决的？\"><a href=\"#51-在使用Kafka的过程中遇到过什么困难？怎么解决的？\" class=\"headerlink\" title=\"51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？\"></a>51. 在使用Kafka的过程中遇到过什么困难？怎么解决的？</h3><h3 id=\"52-怎么样才能确保Kafka极大程度上的可靠性？\"><a href=\"#52-怎么样才能确保Kafka极大程度上的可靠性？\" class=\"headerlink\" title=\"52. 怎么样才能确保Kafka极大程度上的可靠性？\"></a>52. 怎么样才能确保Kafka极大程度上的可靠性？</h3><h3 id=\"53-聊一聊你对Kafka生态的理解\"><a href=\"#53-聊一聊你对Kafka生态的理解\" class=\"headerlink\" title=\"53. 聊一聊你对Kafka生态的理解\"></a>53. 聊一聊你对Kafka生态的理解</h3><blockquote>\n<p>confluent全家桶(connect/kafka stream/ksql/center/rest proxy等)，开源监控管理工具kafka-manager,kmanager等</p>\n</blockquote>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://mp.weixin.qq.com/s/I-YsRKcjcBv4eOop-jjO0A\" target=\"_blank\" rel=\"noopener\">Kafka面试题全套整理 | 划重点要考！</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/88985769\" target=\"_blank\" rel=\"noopener\">Kafka科普系列 | 什么是LSO？</a></li>\n<li><a href=\"https://www.jianshu.com/p/e4879aed5d43\" target=\"_blank\" rel=\"noopener\">Kafka面试题</a></li>\n</ol>\n"},{"title":"Mapredure迁移数据到Hbase","date":"2016-10-24T13:11:23.000Z","_content":"## 目标\n1. 将txt数据通过mapredure写到hbase中\n2. 将sqlserver数据写入hive表中，从hive表中写入hbase\n3. 将sqlserver数据写入hbase\n## 实现\n### 1.第一个目标\n\n一、上传原数据文件\n\n将data.txt文件上传到到hdfs上，内容如下：\n```\nkey1\tcol1\tvalue1  \nkey2\tcol2\tvalue2  \nkey3\tcol3\tvalue3  \nkey4\tcol4\tvalue4  \n```\n数据以制表符（\\t）分割。\n\n二、将数据写成HFile\n\n通过mapredure将data.txt按hbase表格式写成hfile\n\npom.xml文件中依赖如下\n```\n<!-- hadoop -->\n<dependency>\n\t<groupId>org.apache.hadoop</groupId>\n\t<artifactId>hadoop-client</artifactId>\n\t<version>2.6.0-cdh5.4.0</version>\n</dependency>\n<!-- hbase -->\n<dependency>\n\t<groupId>org.apache.hbase</groupId>\n\t<artifactId>hbase-client</artifactId>\n\t<version>1.0.0-cdh5.4.0</version>\n</dependency>\n<dependency>\n\t<groupId>org.apache.hbase</groupId>\n\t<artifactId>hbase-server</artifactId>\n\t<version>1.0.0-cdh5.4.0</version>\n</dependency>\n```\n\n编写BulkLoadMapper\n```\npublic class BulkLoadMapper extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put> {\n\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadMapper.class);\n    private String dataSeperator;\n    private String columnFamily1;\n    private String columnFamily2;\n\n    public void setup(Context context) {\n        Configuration configuration = context.getConfiguration();//获取作业参数\n        dataSeperator = configuration.get(\"data.seperator\");\n        columnFamily1 = configuration.get(\"COLUMN_FAMILY_1\");\n        columnFamily2 = configuration.get(\"COLUMN_FAMILY_2\");\n    }\n\n    public void map(LongWritable key, Text value, Context context){\n        try {\n            String[] values = value.toString().split(dataSeperator);\n            ImmutableBytesWritable rowKey = new ImmutableBytesWritable(Bytes.toBytes(values[0]));\n            Put put = new Put(Bytes.toBytes(values[0]));\n            put.addColumn(Bytes.toBytes(columnFamily1), Bytes.toBytes(\"words\"), Bytes.toBytes(values[1]));\n            put.addColumn(Bytes.toBytes(columnFamily2), Bytes.toBytes(\"sum\"), Bytes.toBytes(values[2]));\n            \n            context.write(rowKey, put);\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n\n    }\n\n}\n```\n编写BulkLoadDriver\n```\npublic class BulkLoadDriver extends Configured implements Tool {\n\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadDriver.class);\n\tprivate static final String DATA_SEPERATOR = \"\\t\";\n    private static final String TABLE_NAME = \"truman\";//表名\n    private static final String COLUMN_FAMILY_1=\"personal\";//列组1\n    private static final String COLUMN_FAMILY_2=\"professional\";//列组2\n\n    public static void main(String[] args) {\n    \tSystem.setProperty(\"hadoop.home.dir\", \"D:/hadoop\");\n    \tSystem.setProperty(\"HADOOP_USER_NAME\", \"root\");\n    \tlogger.info(\"---------------------------------------------\");\n        try {\n            int response = ToolRunner.run(HBaseConfiguration.create(), new BulkLoadDriver(), args);\n            if(response == 0) {\n                System.out.println(\"Job is successfully completed...\");\n            } else {\n                System.out.println(\"Job failed...\");\n            }\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        String inputPath = \"/user/truman/data.txt\";\n        String outputPath = \"/user/truman/hfile\";\n        /**\n         * 设置作业参数\n         */\n        Configuration configuration = getConf();\n       \n        configuration.set(\"mapreduce.framework.name\", \"yarn\");\n        configuration.set(\"yarn.resourcemanager.address\", \"192.168.1.2:8032\");\n        configuration.set(\"yarn.resourcemanager.scheduler.address\", \"192.168.1.2:8030\");\n        configuration.set(\"fs.defaultFS\", \"hdfs://192.168.1.2:8020\");\n        configuration.set(\"mapred.jar\", \"D://workspace//SqlDataToHbase//target//SqlDataToHbase-0.0.1-SNAPSHOT-jar-with-dependencies.jar\");\n        \n        configuration.set(\"data.seperator\", DATA_SEPERATOR);\n        configuration.set(\"hbase.table.name\", TABLE_NAME);\n        configuration.set(\"COLUMN_FAMILY_1\", COLUMN_FAMILY_1);\n        configuration.set(\"COLUMN_FAMILY_2\", COLUMN_FAMILY_2);\n        \n       /* configuration.set(\"hbase.zookeeper.quorum\", \"192.168.1.2,192.168.1.3,192.168.1.4\");\n        configuration.set(\"hbase.zookeeper.property.clientPort\", \"2181\");*/\n        \n        Job job = Job.getInstance(configuration, \"Bulk Loading HBase Table::\" + TABLE_NAME);\n        job.setJarByClass(BulkLoadDriver.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setMapOutputKeyClass(ImmutableBytesWritable.class);//指定输出键类\n        job.setMapOutputValueClass(Put.class);//指定输出值类\n        job.setMapperClass(BulkLoadMapper.class);//指定Map函数\n        FileInputFormat.addInputPaths(job, inputPath);//输入路径\n        FileSystem fs = FileSystem.get(configuration);\n        Path output = new Path(outputPath);\n        if (fs.exists(output)) {\n            fs.delete(output, true);//如果输出路径存在，就将其删除\n        }\n        FileOutputFormat.setOutputPath(job, output);//输出路径\n        \n\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        TableName tableName = TableName.valueOf(TABLE_NAME);\n        HFileOutputFormat2.configureIncrementalLoad(job, connection.getTable(tableName), connection.getRegionLocator(tableName));\n        job.waitForCompletion(true);\n        if (job.isSuccessful()){\n            HFileLoader.doBulkLoad(outputPath, TABLE_NAME,configuration);//导入数据\n            return 0;\n        } else {\n            return 1;\n        }\n    }\n\n}\n```\n整个项目需要将hbase-site.xml、yarn-site.xml、mapred-site.xml放入resources下。本地运行出错的话，再加入org.apache.hadoop.io.nativeio.NativeIO到当前工程中\n\n三、数据加载\n\n1. 命令方式\n\n首先修改hadoop-env.sh配置，加入以下：\n```\nexport HBASE_HOME=/data/bigdata/hbase-1.0.0-cdh5.4.0\nexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0.jar:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0-tests.jar:$HBASE_HOME/conf:$HBASE_HOME/lib/zookeeper-3.4.5-cdh5.4.0.jar:$HBASE_HOME/lib/guava-12.0.1.jar:$HBASE_HOME/lib/hbase-client-1.0.0-cdh5.4.0.jar:$HADOOP_CLASSPATH:$HBASE_HOME/lib/*\n```\n\n将数据按HFile写入到hdfs中，然后进入$HBASE_HOME/bin中执行以下命令\n```\n/data/bigdata/hadoop-2.6.0-cdh5.4.0/bin/hadoop jar ../lib/hbase-server-1.0.0-cdh5.4.0.jar completebulkload /user/truman/hfile  truman\n```\n\n2. java方式\n```\npublic class HFileLoader {\n    public static void doBulkLoad(String pathToHFile, String tableName,Configuration configuration){\n        try {\n            \n            HBaseConfiguration.addHbaseResources(configuration);\n            LoadIncrementalHFiles loadFfiles = new LoadIncrementalHFiles(configuration);\n            HTable hTable = new HTable(configuration, tableName);//指定表名\n            loadFfiles.doBulkLoad(new Path(pathToHFile), hTable);//导入数据\n            System.out.println(\"Bulk Load Completed..\");\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n\n    }\n\n}\n```\n四、结果查询\n```\n[root@LAB3 bin]# ./hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nType \"exit<RETURN>\" to leave the HBase Shell\nVersion 1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015\n\nhbase(main):001:0> scan 'truman'\nROW                          COLUMN+CELL\n key1                        column=personal:words, timestamp=1476179060738, value=col1\n key1                        column=professional:sum, timestamp=1476179060738, value=value1\n key2                        column=personal:words, timestamp=1476179060738, value=col2\n key2                        column=professional:sum, timestamp=1476179060738, value=value2\n key3                        column=personal:words, timestamp=1476179060738, value=col3\n key3                        column=professional:sum, timestamp=1476179060738, value=value3\n key4                        column=personal:words, timestamp=1476179060738, value=col4\n key4                        column=professional:sum, timestamp=1476179060738, value=value4\n4 row(s) in 0.4300 seconds\n\n```","source":"_posts/Mapredure迁移数据到Hbase.md","raw":"---\ntitle: Mapredure迁移数据到Hbase\ndate: 2016-10-24 21:11:23\ntags: 大数据\ncategories:\n- hadoop\n---\n## 目标\n1. 将txt数据通过mapredure写到hbase中\n2. 将sqlserver数据写入hive表中，从hive表中写入hbase\n3. 将sqlserver数据写入hbase\n## 实现\n### 1.第一个目标\n\n一、上传原数据文件\n\n将data.txt文件上传到到hdfs上，内容如下：\n```\nkey1\tcol1\tvalue1  \nkey2\tcol2\tvalue2  \nkey3\tcol3\tvalue3  \nkey4\tcol4\tvalue4  \n```\n数据以制表符（\\t）分割。\n\n二、将数据写成HFile\n\n通过mapredure将data.txt按hbase表格式写成hfile\n\npom.xml文件中依赖如下\n```\n<!-- hadoop -->\n<dependency>\n\t<groupId>org.apache.hadoop</groupId>\n\t<artifactId>hadoop-client</artifactId>\n\t<version>2.6.0-cdh5.4.0</version>\n</dependency>\n<!-- hbase -->\n<dependency>\n\t<groupId>org.apache.hbase</groupId>\n\t<artifactId>hbase-client</artifactId>\n\t<version>1.0.0-cdh5.4.0</version>\n</dependency>\n<dependency>\n\t<groupId>org.apache.hbase</groupId>\n\t<artifactId>hbase-server</artifactId>\n\t<version>1.0.0-cdh5.4.0</version>\n</dependency>\n```\n\n编写BulkLoadMapper\n```\npublic class BulkLoadMapper extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put> {\n\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadMapper.class);\n    private String dataSeperator;\n    private String columnFamily1;\n    private String columnFamily2;\n\n    public void setup(Context context) {\n        Configuration configuration = context.getConfiguration();//获取作业参数\n        dataSeperator = configuration.get(\"data.seperator\");\n        columnFamily1 = configuration.get(\"COLUMN_FAMILY_1\");\n        columnFamily2 = configuration.get(\"COLUMN_FAMILY_2\");\n    }\n\n    public void map(LongWritable key, Text value, Context context){\n        try {\n            String[] values = value.toString().split(dataSeperator);\n            ImmutableBytesWritable rowKey = new ImmutableBytesWritable(Bytes.toBytes(values[0]));\n            Put put = new Put(Bytes.toBytes(values[0]));\n            put.addColumn(Bytes.toBytes(columnFamily1), Bytes.toBytes(\"words\"), Bytes.toBytes(values[1]));\n            put.addColumn(Bytes.toBytes(columnFamily2), Bytes.toBytes(\"sum\"), Bytes.toBytes(values[2]));\n            \n            context.write(rowKey, put);\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n\n    }\n\n}\n```\n编写BulkLoadDriver\n```\npublic class BulkLoadDriver extends Configured implements Tool {\n\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadDriver.class);\n\tprivate static final String DATA_SEPERATOR = \"\\t\";\n    private static final String TABLE_NAME = \"truman\";//表名\n    private static final String COLUMN_FAMILY_1=\"personal\";//列组1\n    private static final String COLUMN_FAMILY_2=\"professional\";//列组2\n\n    public static void main(String[] args) {\n    \tSystem.setProperty(\"hadoop.home.dir\", \"D:/hadoop\");\n    \tSystem.setProperty(\"HADOOP_USER_NAME\", \"root\");\n    \tlogger.info(\"---------------------------------------------\");\n        try {\n            int response = ToolRunner.run(HBaseConfiguration.create(), new BulkLoadDriver(), args);\n            if(response == 0) {\n                System.out.println(\"Job is successfully completed...\");\n            } else {\n                System.out.println(\"Job failed...\");\n            }\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        String inputPath = \"/user/truman/data.txt\";\n        String outputPath = \"/user/truman/hfile\";\n        /**\n         * 设置作业参数\n         */\n        Configuration configuration = getConf();\n       \n        configuration.set(\"mapreduce.framework.name\", \"yarn\");\n        configuration.set(\"yarn.resourcemanager.address\", \"192.168.1.2:8032\");\n        configuration.set(\"yarn.resourcemanager.scheduler.address\", \"192.168.1.2:8030\");\n        configuration.set(\"fs.defaultFS\", \"hdfs://192.168.1.2:8020\");\n        configuration.set(\"mapred.jar\", \"D://workspace//SqlDataToHbase//target//SqlDataToHbase-0.0.1-SNAPSHOT-jar-with-dependencies.jar\");\n        \n        configuration.set(\"data.seperator\", DATA_SEPERATOR);\n        configuration.set(\"hbase.table.name\", TABLE_NAME);\n        configuration.set(\"COLUMN_FAMILY_1\", COLUMN_FAMILY_1);\n        configuration.set(\"COLUMN_FAMILY_2\", COLUMN_FAMILY_2);\n        \n       /* configuration.set(\"hbase.zookeeper.quorum\", \"192.168.1.2,192.168.1.3,192.168.1.4\");\n        configuration.set(\"hbase.zookeeper.property.clientPort\", \"2181\");*/\n        \n        Job job = Job.getInstance(configuration, \"Bulk Loading HBase Table::\" + TABLE_NAME);\n        job.setJarByClass(BulkLoadDriver.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setMapOutputKeyClass(ImmutableBytesWritable.class);//指定输出键类\n        job.setMapOutputValueClass(Put.class);//指定输出值类\n        job.setMapperClass(BulkLoadMapper.class);//指定Map函数\n        FileInputFormat.addInputPaths(job, inputPath);//输入路径\n        FileSystem fs = FileSystem.get(configuration);\n        Path output = new Path(outputPath);\n        if (fs.exists(output)) {\n            fs.delete(output, true);//如果输出路径存在，就将其删除\n        }\n        FileOutputFormat.setOutputPath(job, output);//输出路径\n        \n\n        Connection connection = ConnectionFactory.createConnection(configuration);\n        TableName tableName = TableName.valueOf(TABLE_NAME);\n        HFileOutputFormat2.configureIncrementalLoad(job, connection.getTable(tableName), connection.getRegionLocator(tableName));\n        job.waitForCompletion(true);\n        if (job.isSuccessful()){\n            HFileLoader.doBulkLoad(outputPath, TABLE_NAME,configuration);//导入数据\n            return 0;\n        } else {\n            return 1;\n        }\n    }\n\n}\n```\n整个项目需要将hbase-site.xml、yarn-site.xml、mapred-site.xml放入resources下。本地运行出错的话，再加入org.apache.hadoop.io.nativeio.NativeIO到当前工程中\n\n三、数据加载\n\n1. 命令方式\n\n首先修改hadoop-env.sh配置，加入以下：\n```\nexport HBASE_HOME=/data/bigdata/hbase-1.0.0-cdh5.4.0\nexport HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0.jar:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0-tests.jar:$HBASE_HOME/conf:$HBASE_HOME/lib/zookeeper-3.4.5-cdh5.4.0.jar:$HBASE_HOME/lib/guava-12.0.1.jar:$HBASE_HOME/lib/hbase-client-1.0.0-cdh5.4.0.jar:$HADOOP_CLASSPATH:$HBASE_HOME/lib/*\n```\n\n将数据按HFile写入到hdfs中，然后进入$HBASE_HOME/bin中执行以下命令\n```\n/data/bigdata/hadoop-2.6.0-cdh5.4.0/bin/hadoop jar ../lib/hbase-server-1.0.0-cdh5.4.0.jar completebulkload /user/truman/hfile  truman\n```\n\n2. java方式\n```\npublic class HFileLoader {\n    public static void doBulkLoad(String pathToHFile, String tableName,Configuration configuration){\n        try {\n            \n            HBaseConfiguration.addHbaseResources(configuration);\n            LoadIncrementalHFiles loadFfiles = new LoadIncrementalHFiles(configuration);\n            HTable hTable = new HTable(configuration, tableName);//指定表名\n            loadFfiles.doBulkLoad(new Path(pathToHFile), hTable);//导入数据\n            System.out.println(\"Bulk Load Completed..\");\n        } catch(Exception exception) {\n            exception.printStackTrace();\n        }\n\n    }\n\n}\n```\n四、结果查询\n```\n[root@LAB3 bin]# ./hbase shell\nHBase Shell; enter 'help<RETURN>' for list of supported commands.\nType \"exit<RETURN>\" to leave the HBase Shell\nVersion 1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015\n\nhbase(main):001:0> scan 'truman'\nROW                          COLUMN+CELL\n key1                        column=personal:words, timestamp=1476179060738, value=col1\n key1                        column=professional:sum, timestamp=1476179060738, value=value1\n key2                        column=personal:words, timestamp=1476179060738, value=col2\n key2                        column=professional:sum, timestamp=1476179060738, value=value2\n key3                        column=personal:words, timestamp=1476179060738, value=col3\n key3                        column=professional:sum, timestamp=1476179060738, value=value3\n key4                        column=personal:words, timestamp=1476179060738, value=col4\n key4                        column=professional:sum, timestamp=1476179060738, value=value4\n4 row(s) in 0.4300 seconds\n\n```","slug":"Mapredure迁移数据到Hbase","published":1,"updated":"2019-08-22T12:38:39.869Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgb00ab8ceeyfjbgdtw","content":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ol>\n<li>将txt数据通过mapredure写到hbase中</li>\n<li>将sqlserver数据写入hive表中，从hive表中写入hbase</li>\n<li>将sqlserver数据写入hbase<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><h3 id=\"1-第一个目标\"><a href=\"#1-第一个目标\" class=\"headerlink\" title=\"1.第一个目标\"></a>1.第一个目标</h3></li>\n</ol>\n<p>一、上传原数据文件</p>\n<p>将data.txt文件上传到到hdfs上，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">key1\tcol1\tvalue1  </span><br><span class=\"line\">key2\tcol2\tvalue2  </span><br><span class=\"line\">key3\tcol3\tvalue3  </span><br><span class=\"line\">key4\tcol4\tvalue4</span><br></pre></td></tr></table></figure></p>\n<p>数据以制表符（\\t）分割。</p>\n<p>二、将数据写成HFile</p>\n<p>通过mapredure将data.txt按hbase表格式写成hfile</p>\n<p>pom.xml文件中依赖如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- hadoop --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;2.6.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- hbase --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hbase-server&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>编写BulkLoadMapper<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class BulkLoadMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123;</span><br><span class=\"line\">\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadMapper.class);</span><br><span class=\"line\">    private String dataSeperator;</span><br><span class=\"line\">    private String columnFamily1;</span><br><span class=\"line\">    private String columnFamily2;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void setup(Context context) &#123;</span><br><span class=\"line\">        Configuration configuration = context.getConfiguration();//获取作业参数</span><br><span class=\"line\">        dataSeperator = configuration.get(&quot;data.seperator&quot;);</span><br><span class=\"line\">        columnFamily1 = configuration.get(&quot;COLUMN_FAMILY_1&quot;);</span><br><span class=\"line\">        columnFamily2 = configuration.get(&quot;COLUMN_FAMILY_2&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void map(LongWritable key, Text value, Context context)&#123;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            String[] values = value.toString().split(dataSeperator);</span><br><span class=\"line\">            ImmutableBytesWritable rowKey = new ImmutableBytesWritable(Bytes.toBytes(values[0]));</span><br><span class=\"line\">            Put put = new Put(Bytes.toBytes(values[0]));</span><br><span class=\"line\">            put.addColumn(Bytes.toBytes(columnFamily1), Bytes.toBytes(&quot;words&quot;), Bytes.toBytes(values[1]));</span><br><span class=\"line\">            put.addColumn(Bytes.toBytes(columnFamily2), Bytes.toBytes(&quot;sum&quot;), Bytes.toBytes(values[2]));</span><br><span class=\"line\">            </span><br><span class=\"line\">            context.write(rowKey, put);</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>编写BulkLoadDriver<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class BulkLoadDriver extends Configured implements Tool &#123;</span><br><span class=\"line\">\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadDriver.class);</span><br><span class=\"line\">\tprivate static final String DATA_SEPERATOR = &quot;\\t&quot;;</span><br><span class=\"line\">    private static final String TABLE_NAME = &quot;truman&quot;;//表名</span><br><span class=\"line\">    private static final String COLUMN_FAMILY_1=&quot;personal&quot;;//列组1</span><br><span class=\"line\">    private static final String COLUMN_FAMILY_2=&quot;professional&quot;;//列组2</span><br><span class=\"line\"></span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">    \tSystem.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:/hadoop&quot;);</span><br><span class=\"line\">    \tSystem.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);</span><br><span class=\"line\">    \tlogger.info(&quot;---------------------------------------------&quot;);</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            int response = ToolRunner.run(HBaseConfiguration.create(), new BulkLoadDriver(), args);</span><br><span class=\"line\">            if(response == 0) &#123;</span><br><span class=\"line\">                System.out.println(&quot;Job is successfully completed...&quot;);</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                System.out.println(&quot;Job failed...&quot;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public int run(String[] args) throws Exception &#123;</span><br><span class=\"line\">        String inputPath = &quot;/user/truman/data.txt&quot;;</span><br><span class=\"line\">        String outputPath = &quot;/user/truman/hfile&quot;;</span><br><span class=\"line\">        /**</span><br><span class=\"line\">         * 设置作业参数</span><br><span class=\"line\">         */</span><br><span class=\"line\">        Configuration configuration = getConf();</span><br><span class=\"line\">       </span><br><span class=\"line\">        configuration.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);</span><br><span class=\"line\">        configuration.set(&quot;yarn.resourcemanager.address&quot;, &quot;192.168.1.2:8032&quot;);</span><br><span class=\"line\">        configuration.set(&quot;yarn.resourcemanager.scheduler.address&quot;, &quot;192.168.1.2:8030&quot;);</span><br><span class=\"line\">        configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://192.168.1.2:8020&quot;);</span><br><span class=\"line\">        configuration.set(&quot;mapred.jar&quot;, &quot;D://workspace//SqlDataToHbase//target//SqlDataToHbase-0.0.1-SNAPSHOT-jar-with-dependencies.jar&quot;);</span><br><span class=\"line\">        </span><br><span class=\"line\">        configuration.set(&quot;data.seperator&quot;, DATA_SEPERATOR);</span><br><span class=\"line\">        configuration.set(&quot;hbase.table.name&quot;, TABLE_NAME);</span><br><span class=\"line\">        configuration.set(&quot;COLUMN_FAMILY_1&quot;, COLUMN_FAMILY_1);</span><br><span class=\"line\">        configuration.set(&quot;COLUMN_FAMILY_2&quot;, COLUMN_FAMILY_2);</span><br><span class=\"line\">        </span><br><span class=\"line\">       /* configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.1.2,192.168.1.3,192.168.1.4&quot;);</span><br><span class=\"line\">        configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);*/</span><br><span class=\"line\">        </span><br><span class=\"line\">        Job job = Job.getInstance(configuration, &quot;Bulk Loading HBase Table::&quot; + TABLE_NAME);</span><br><span class=\"line\">        job.setJarByClass(BulkLoadDriver.class);</span><br><span class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class=\"line\">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);//指定输出键类</span><br><span class=\"line\">        job.setMapOutputValueClass(Put.class);//指定输出值类</span><br><span class=\"line\">        job.setMapperClass(BulkLoadMapper.class);//指定Map函数</span><br><span class=\"line\">        FileInputFormat.addInputPaths(job, inputPath);//输入路径</span><br><span class=\"line\">        FileSystem fs = FileSystem.get(configuration);</span><br><span class=\"line\">        Path output = new Path(outputPath);</span><br><span class=\"line\">        if (fs.exists(output)) &#123;</span><br><span class=\"line\">            fs.delete(output, true);//如果输出路径存在，就将其删除</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, output);//输出路径</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\">        Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class=\"line\">        TableName tableName = TableName.valueOf(TABLE_NAME);</span><br><span class=\"line\">        HFileOutputFormat2.configureIncrementalLoad(job, connection.getTable(tableName), connection.getRegionLocator(tableName));</span><br><span class=\"line\">        job.waitForCompletion(true);</span><br><span class=\"line\">        if (job.isSuccessful())&#123;</span><br><span class=\"line\">            HFileLoader.doBulkLoad(outputPath, TABLE_NAME,configuration);//导入数据</span><br><span class=\"line\">            return 0;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            return 1;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>整个项目需要将hbase-site.xml、yarn-site.xml、mapred-site.xml放入resources下。本地运行出错的话，再加入org.apache.hadoop.io.nativeio.NativeIO到当前工程中</p>\n<p>三、数据加载</p>\n<ol>\n<li>命令方式</li>\n</ol>\n<p>首先修改hadoop-env.sh配置，加入以下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HBASE_HOME=/data/bigdata/hbase-1.0.0-cdh5.4.0</span><br><span class=\"line\">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0.jar:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0-tests.jar:$HBASE_HOME/conf:$HBASE_HOME/lib/zookeeper-3.4.5-cdh5.4.0.jar:$HBASE_HOME/lib/guava-12.0.1.jar:$HBASE_HOME/lib/hbase-client-1.0.0-cdh5.4.0.jar:$HADOOP_CLASSPATH:$HBASE_HOME/lib/*</span><br></pre></td></tr></table></figure></p>\n<p>将数据按HFile写入到hdfs中，然后进入$HBASE_HOME/bin中执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/data/bigdata/hadoop-2.6.0-cdh5.4.0/bin/hadoop jar ../lib/hbase-server-1.0.0-cdh5.4.0.jar completebulkload /user/truman/hfile  truman</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>java方式<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class HFileLoader &#123;</span><br><span class=\"line\">    public static void doBulkLoad(String pathToHFile, String tableName,Configuration configuration)&#123;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            </span><br><span class=\"line\">            HBaseConfiguration.addHbaseResources(configuration);</span><br><span class=\"line\">            LoadIncrementalHFiles loadFfiles = new LoadIncrementalHFiles(configuration);</span><br><span class=\"line\">            HTable hTable = new HTable(configuration, tableName);//指定表名</span><br><span class=\"line\">            loadFfiles.doBulkLoad(new Path(pathToHFile), hTable);//导入数据</span><br><span class=\"line\">            System.out.println(&quot;Bulk Load Completed..&quot;);</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>四、结果查询<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@LAB3 bin]# ./hbase shell</span><br><span class=\"line\">HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.</span><br><span class=\"line\">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class=\"line\">Version 1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):001:0&gt; scan &apos;truman&apos;</span><br><span class=\"line\">ROW                          COLUMN+CELL</span><br><span class=\"line\"> key1                        column=personal:words, timestamp=1476179060738, value=col1</span><br><span class=\"line\"> key1                        column=professional:sum, timestamp=1476179060738, value=value1</span><br><span class=\"line\"> key2                        column=personal:words, timestamp=1476179060738, value=col2</span><br><span class=\"line\"> key2                        column=professional:sum, timestamp=1476179060738, value=value2</span><br><span class=\"line\"> key3                        column=personal:words, timestamp=1476179060738, value=col3</span><br><span class=\"line\"> key3                        column=professional:sum, timestamp=1476179060738, value=value3</span><br><span class=\"line\"> key4                        column=personal:words, timestamp=1476179060738, value=col4</span><br><span class=\"line\"> key4                        column=professional:sum, timestamp=1476179060738, value=value4</span><br><span class=\"line\">4 row(s) in 0.4300 seconds</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><ol>\n<li>将txt数据通过mapredure写到hbase中</li>\n<li>将sqlserver数据写入hive表中，从hive表中写入hbase</li>\n<li>将sqlserver数据写入hbase<h2 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h2><h3 id=\"1-第一个目标\"><a href=\"#1-第一个目标\" class=\"headerlink\" title=\"1.第一个目标\"></a>1.第一个目标</h3></li>\n</ol>\n<p>一、上传原数据文件</p>\n<p>将data.txt文件上传到到hdfs上，内容如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">key1\tcol1\tvalue1  </span><br><span class=\"line\">key2\tcol2\tvalue2  </span><br><span class=\"line\">key3\tcol3\tvalue3  </span><br><span class=\"line\">key4\tcol4\tvalue4</span><br></pre></td></tr></table></figure></p>\n<p>数据以制表符（\\t）分割。</p>\n<p>二、将数据写成HFile</p>\n<p>通过mapredure将data.txt按hbase表格式写成hfile</p>\n<p>pom.xml文件中依赖如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;!-- hadoop --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;2.6.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;!-- hbase --&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;hbase-server&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;1.0.0-cdh5.4.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p>\n<p>编写BulkLoadMapper<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class BulkLoadMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123;</span><br><span class=\"line\">\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadMapper.class);</span><br><span class=\"line\">    private String dataSeperator;</span><br><span class=\"line\">    private String columnFamily1;</span><br><span class=\"line\">    private String columnFamily2;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void setup(Context context) &#123;</span><br><span class=\"line\">        Configuration configuration = context.getConfiguration();//获取作业参数</span><br><span class=\"line\">        dataSeperator = configuration.get(&quot;data.seperator&quot;);</span><br><span class=\"line\">        columnFamily1 = configuration.get(&quot;COLUMN_FAMILY_1&quot;);</span><br><span class=\"line\">        columnFamily2 = configuration.get(&quot;COLUMN_FAMILY_2&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public void map(LongWritable key, Text value, Context context)&#123;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            String[] values = value.toString().split(dataSeperator);</span><br><span class=\"line\">            ImmutableBytesWritable rowKey = new ImmutableBytesWritable(Bytes.toBytes(values[0]));</span><br><span class=\"line\">            Put put = new Put(Bytes.toBytes(values[0]));</span><br><span class=\"line\">            put.addColumn(Bytes.toBytes(columnFamily1), Bytes.toBytes(&quot;words&quot;), Bytes.toBytes(values[1]));</span><br><span class=\"line\">            put.addColumn(Bytes.toBytes(columnFamily2), Bytes.toBytes(&quot;sum&quot;), Bytes.toBytes(values[2]));</span><br><span class=\"line\">            </span><br><span class=\"line\">            context.write(rowKey, put);</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>编写BulkLoadDriver<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class BulkLoadDriver extends Configured implements Tool &#123;</span><br><span class=\"line\">\tprivate static final Logger logger = LoggerFactory.getLogger(BulkLoadDriver.class);</span><br><span class=\"line\">\tprivate static final String DATA_SEPERATOR = &quot;\\t&quot;;</span><br><span class=\"line\">    private static final String TABLE_NAME = &quot;truman&quot;;//表名</span><br><span class=\"line\">    private static final String COLUMN_FAMILY_1=&quot;personal&quot;;//列组1</span><br><span class=\"line\">    private static final String COLUMN_FAMILY_2=&quot;professional&quot;;//列组2</span><br><span class=\"line\"></span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">    \tSystem.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:/hadoop&quot;);</span><br><span class=\"line\">    \tSystem.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);</span><br><span class=\"line\">    \tlogger.info(&quot;---------------------------------------------&quot;);</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            int response = ToolRunner.run(HBaseConfiguration.create(), new BulkLoadDriver(), args);</span><br><span class=\"line\">            if(response == 0) &#123;</span><br><span class=\"line\">                System.out.println(&quot;Job is successfully completed...&quot;);</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                System.out.println(&quot;Job failed...&quot;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public int run(String[] args) throws Exception &#123;</span><br><span class=\"line\">        String inputPath = &quot;/user/truman/data.txt&quot;;</span><br><span class=\"line\">        String outputPath = &quot;/user/truman/hfile&quot;;</span><br><span class=\"line\">        /**</span><br><span class=\"line\">         * 设置作业参数</span><br><span class=\"line\">         */</span><br><span class=\"line\">        Configuration configuration = getConf();</span><br><span class=\"line\">       </span><br><span class=\"line\">        configuration.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);</span><br><span class=\"line\">        configuration.set(&quot;yarn.resourcemanager.address&quot;, &quot;192.168.1.2:8032&quot;);</span><br><span class=\"line\">        configuration.set(&quot;yarn.resourcemanager.scheduler.address&quot;, &quot;192.168.1.2:8030&quot;);</span><br><span class=\"line\">        configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://192.168.1.2:8020&quot;);</span><br><span class=\"line\">        configuration.set(&quot;mapred.jar&quot;, &quot;D://workspace//SqlDataToHbase//target//SqlDataToHbase-0.0.1-SNAPSHOT-jar-with-dependencies.jar&quot;);</span><br><span class=\"line\">        </span><br><span class=\"line\">        configuration.set(&quot;data.seperator&quot;, DATA_SEPERATOR);</span><br><span class=\"line\">        configuration.set(&quot;hbase.table.name&quot;, TABLE_NAME);</span><br><span class=\"line\">        configuration.set(&quot;COLUMN_FAMILY_1&quot;, COLUMN_FAMILY_1);</span><br><span class=\"line\">        configuration.set(&quot;COLUMN_FAMILY_2&quot;, COLUMN_FAMILY_2);</span><br><span class=\"line\">        </span><br><span class=\"line\">       /* configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.1.2,192.168.1.3,192.168.1.4&quot;);</span><br><span class=\"line\">        configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);*/</span><br><span class=\"line\">        </span><br><span class=\"line\">        Job job = Job.getInstance(configuration, &quot;Bulk Loading HBase Table::&quot; + TABLE_NAME);</span><br><span class=\"line\">        job.setJarByClass(BulkLoadDriver.class);</span><br><span class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class=\"line\">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);//指定输出键类</span><br><span class=\"line\">        job.setMapOutputValueClass(Put.class);//指定输出值类</span><br><span class=\"line\">        job.setMapperClass(BulkLoadMapper.class);//指定Map函数</span><br><span class=\"line\">        FileInputFormat.addInputPaths(job, inputPath);//输入路径</span><br><span class=\"line\">        FileSystem fs = FileSystem.get(configuration);</span><br><span class=\"line\">        Path output = new Path(outputPath);</span><br><span class=\"line\">        if (fs.exists(output)) &#123;</span><br><span class=\"line\">            fs.delete(output, true);//如果输出路径存在，就将其删除</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        FileOutputFormat.setOutputPath(job, output);//输出路径</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\">        Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class=\"line\">        TableName tableName = TableName.valueOf(TABLE_NAME);</span><br><span class=\"line\">        HFileOutputFormat2.configureIncrementalLoad(job, connection.getTable(tableName), connection.getRegionLocator(tableName));</span><br><span class=\"line\">        job.waitForCompletion(true);</span><br><span class=\"line\">        if (job.isSuccessful())&#123;</span><br><span class=\"line\">            HFileLoader.doBulkLoad(outputPath, TABLE_NAME,configuration);//导入数据</span><br><span class=\"line\">            return 0;</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            return 1;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>整个项目需要将hbase-site.xml、yarn-site.xml、mapred-site.xml放入resources下。本地运行出错的话，再加入org.apache.hadoop.io.nativeio.NativeIO到当前工程中</p>\n<p>三、数据加载</p>\n<ol>\n<li>命令方式</li>\n</ol>\n<p>首先修改hadoop-env.sh配置，加入以下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export HBASE_HOME=/data/bigdata/hbase-1.0.0-cdh5.4.0</span><br><span class=\"line\">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0.jar:$HBASE_HOME/lib/hbase-server-1.0.0-cdh5.4.0-tests.jar:$HBASE_HOME/conf:$HBASE_HOME/lib/zookeeper-3.4.5-cdh5.4.0.jar:$HBASE_HOME/lib/guava-12.0.1.jar:$HBASE_HOME/lib/hbase-client-1.0.0-cdh5.4.0.jar:$HADOOP_CLASSPATH:$HBASE_HOME/lib/*</span><br></pre></td></tr></table></figure></p>\n<p>将数据按HFile写入到hdfs中，然后进入$HBASE_HOME/bin中执行以下命令<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/data/bigdata/hadoop-2.6.0-cdh5.4.0/bin/hadoop jar ../lib/hbase-server-1.0.0-cdh5.4.0.jar completebulkload /user/truman/hfile  truman</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>java方式<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class HFileLoader &#123;</span><br><span class=\"line\">    public static void doBulkLoad(String pathToHFile, String tableName,Configuration configuration)&#123;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">            </span><br><span class=\"line\">            HBaseConfiguration.addHbaseResources(configuration);</span><br><span class=\"line\">            LoadIncrementalHFiles loadFfiles = new LoadIncrementalHFiles(configuration);</span><br><span class=\"line\">            HTable hTable = new HTable(configuration, tableName);//指定表名</span><br><span class=\"line\">            loadFfiles.doBulkLoad(new Path(pathToHFile), hTable);//导入数据</span><br><span class=\"line\">            System.out.println(&quot;Bulk Load Completed..&quot;);</span><br><span class=\"line\">        &#125; catch(Exception exception) &#123;</span><br><span class=\"line\">            exception.printStackTrace();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>四、结果查询<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@LAB3 bin]# ./hbase shell</span><br><span class=\"line\">HBase Shell; enter &apos;help&lt;RETURN&gt;&apos; for list of supported commands.</span><br><span class=\"line\">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class=\"line\">Version 1.0.0-cdh5.4.0, rUnknown, Tue Apr 21 12:19:34 PDT 2015</span><br><span class=\"line\"></span><br><span class=\"line\">hbase(main):001:0&gt; scan &apos;truman&apos;</span><br><span class=\"line\">ROW                          COLUMN+CELL</span><br><span class=\"line\"> key1                        column=personal:words, timestamp=1476179060738, value=col1</span><br><span class=\"line\"> key1                        column=professional:sum, timestamp=1476179060738, value=value1</span><br><span class=\"line\"> key2                        column=personal:words, timestamp=1476179060738, value=col2</span><br><span class=\"line\"> key2                        column=professional:sum, timestamp=1476179060738, value=value2</span><br><span class=\"line\"> key3                        column=personal:words, timestamp=1476179060738, value=col3</span><br><span class=\"line\"> key3                        column=professional:sum, timestamp=1476179060738, value=value3</span><br><span class=\"line\"> key4                        column=personal:words, timestamp=1476179060738, value=col4</span><br><span class=\"line\"> key4                        column=professional:sum, timestamp=1476179060738, value=value4</span><br><span class=\"line\">4 row(s) in 0.4300 seconds</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Redis4.0 新特性尝鲜","date":"2017-07-19T14:17:00.000Z","_content":"# Redis4.0 新特性尝鲜\n\n## 1.目录结构\n\n1. [目录](#1-目录结构)\n2. [前言](#2-前言)\n3. [环境搭建](#3-环境搭建)\n   1. [下载](#31-下载)\n   2. [安装](#32-编译安装)\n   3. [集群搭建](#33-新建集群)\n4. [特性尝鲜](#4-特性尝鲜)\n   1. [注意事项](#41-注意事项)\n   2. [升级内容](#42-升级内容)\n   3. [模块系统](#43-模块系统)\n   4. [PSYNC 2.0](#44-PSYNC)\n   5. [缓存驱逐策略优化](#45-缓存驱逐策略优化)\n   6. [非阻塞 DEL 、 FLUSHDB 和 FLUSHALL](#46-)\n   7. [交换数据库](#47-交换数据库)\n   8. [混合RDBAOF持久化格式](#48-)\n   9. [内存命令](#49-内存命令)\n   10. [兼容 NAT 和 Docker](#410-兼容NAT和Docker)\n5. [引用](#5-引用)\n\n## 2. 前言\n\n**2017-07-14 redis 4.0  Stable  version release**,新增了许多新功能，此次专门抽出时间，探索一些功能，把握Redis未来的发展方向，同时积累经验，为未来升级Redis打下坚实基础。学习新的东西，如果不将它记录下来，过上两周，基本上就忘记做过什么了，对知识的掌握不利，以后尽量所有的学习都能产生文字记录，便于自己总结学习各种技术，也能给新人带去一点便利。\n\n## 3. 环境搭建\n\n安装方式，较之前没有多大变化，还是写一下，便于新手学习。\n\n### 3.1. 下载\n```\n$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz\n$ tar xzf redis-4.0.0.tar.gz\n```\n\n### 3.2. 编译安装\n```\n$ cd redis-4.0.0\n$ make\n```\n如果需要将redis-cli,redis-server等相关命令安装到/bin目录下，全局使用的话，可以使用如下命令：\n```\n$ make install\n```\n或者利用软连接实现：\n```\n$ make\n$ ln -s  redis-4.0.0/src/redis-server  /bin/redis-server\n$ ln -s  redis-4.0.0/src/redis-cli  /bin/redis-cli\n```\n在编译中可能会遇到如下问题：\n```\n zmalloc.h:50:31: 错误：jemalloc/jemalloc.h：没有那个文件或目录\n```\n出现这个问题是libc 并不是默认的 分配器， 默认的是 jemalloc, 因为 jemalloc 被证明 有更少的 fragmentation problems 比libc，关于更详细信息可以查看redis中REAME,md，其中有详细介绍，此处不再细说。\n\n解决办法：\n```\nmake MALLOC=libc\n```\n\n### 3.3. 新建集群\n\n按照官方指导，搭建3m+3s集群，端口号7000-7005\n\n以下为创建脚本：\n\n```\n#!/bin/bash\n nodes=(7000 7001 7002 7003 7004 7005)\n HOME_DIR=`pwd`\nfunction create(){\n echo \"create\"\n if [ -d redis_cluster ];then\n rm -rf  redis_cluster\n fi\n\n mkdir  redis_cluster\n cd redis_cluster\n\n mkdir `echo ${nodes[*]}`\n\n for var in ${nodes[*]}\n do\n  cd $var\n  touch $var-redis.conf\n  echo \"port\" $var >> $var-redis.conf\n  echo \"cluster-enabled yes\" >> $var-redis.conf\n  echo \"cluster-config-file nodes.conf\" >> $var-redis.conf\n  echo \"cluster-node-timeout 5000\" >> $var-redis.conf\n  echo \"appendonly yes\" >> $var-redis.conf\n  echo \"daemonize yes\" >> $var-redis.conf\n  echo \"pidfile redis.pid\" >> $var-redis.conf\n  cd ../\n done\n run;\n src/redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005\n\n}\n\nfunction run(){\necho run\n for var in ${nodes[*]}\n do\n   cd $HOME_DIR/redis_cluster/$var/\n   redis-server $var-redis.conf\n   cd $HOME_DIR/\n   echo Start redis ports ${var} finish.\n done\n}\nfunction stop(){\n cd $HOME_DIR/redis_cluster/\n DIRS=`ls -l | grep \"^d\" | awk '{print $NF}'`\n  for d in $DIRS\n  do\n    PID=`cat $d/redis.pid`\n    if [ -n \"$PID\" ];then\n       kill -9 $PID\n       rm -f $d/redis.pid\n    fi\n  done\n  cd  $HOME_DIR/\n  echo Stop redis ports ${DIRS} finish.\n}\n\n\ncase $1 in\n     create)\n      echo \" create redis cluster 锛?000,7001,7002,7003,7004.7005,7006,7007\"\n      create\n      ;;\n     run)\n      run\n      ;;\n     stop)\n      stop\n     ;;\n     *)\n      echo \"Usage:[create|run|stop]\"\n      ;;\nesac\n```\n\n执行\n\n```\nsh createCluster.sh create\n```\n一路回车即可，注意构建集群需要使用redis-trib.rb，需要额外执行\n\n```\ngem install redis\n```\n结果如下：\n\n```\n[bigdata@trumanlab1 redis-4.0.0]$ redis-cli -c -h 127.0.0.1  -p 7000\n127.0.0.1:7000> cluster nodes\n63e698210378f4fda23c4070bea3b46f93952811 127.0.0.1:7000@17000 myself,master - 0 1500391716000 1 connected 0-5460\n4091630217b64ece9a6fa55c26687a10c1f9a8b5 127.0.0.1:7001@17001 master - 0 1500391717501 2 connected 5461-10922\n0b23853c3aa4f5b942a6239bc7bca71df36e121c 127.0.0.1:7005@17005 slave 12d05fb7c20bf001759f80cf3f65ad9df4b01af5 0 1500391716498 6 connected\n12d05fb7c20bf001759f80cf3f65ad9df4b01af5 127.0.0.1:7002@17002 master - 0 1500391717000 3 connected 10923-16383\n2e559b462362a9563f98b398af3984c05e25ef19 127.0.0.1:7003@17003 slave 63e698210378f4fda23c4070bea3b46f93952811 0 1500391716000 4 connected\nc4f8e0a95579c772c7506c195b3c7984c73312b4 127.0.0.1:7004@17004 slave 4091630217b64ece9a6fa55c26687a10c1f9a8b5 0 1500391716000 5 connected\n127.0.0.1:7000> set a a\n-> Redirected to slot [15495] located at 127.0.0.1:7002\nOK\n127.0.0.1:7002> get a\n\"a\"\n127.0.0.1:7002>\n```\n\n## 4. 特性尝鲜\n\n### 4.1 注意事项\n\n> IMPORTANT: Redis Cluster users, please note that, as specified in the list\nof incompatibilities, Redis 4.0 cluster bus protocol is not compatible with\nRedis 3.2, so in order to upgrade, a mass reboot of the instances is needed\nand rolling upgrades are not possible. This change was needed in order to\nadd compatibility for Containers/NAT, where the bus port at a fixed offset\nwas not an acceptable design, so we had to change many things, resulting\nin the incompatible protocol.\n\n4.0版本与3.2版本传输协议不兼容，更改该协议的目的是为了实现Containers/NAT\n为了升级，需要重启大量节点，无法做到滚动升级。\n\n### 4.2 升级内容\n>* Different replication fixes to PSYNC2, the new 4.0 replication engine.\n>* Modules thread safe contexts were introduced. They are an >experimental API right now, but the API is considered to be stable and usable when needed.\n>* SLOWLOG now logs the offending client name and address. Note that this is a backward compatibility breakage in case old code assumes that the slowlog entry is composed of exactly three entries.\n>* The modules native data types RDB format changed.\n>* The AOF check utility is now able to deal with RDB preambles.\n>* GEORADIUS_RO and GEORADIUSBYMEMBER_RO variants, not supporting the STORE option, were added in order to allow read-only scaling of such queries.\n>* HSET is now variadic, and HMSET is considered deprecated (but will be supported for years to come). Please use HSET in new code.\n>* GEORADIUS huge radius (>= ~6000 km) corner cases fixed, certain elements near the edges were not returned.\n>* DEBUG DIGEST modules API added.\n>* HyperLogLog commands no longer crash on certain input (non HLL) strings.\n>* Fixed SLAVEOF inside MULTI/EXEC blocks.\n>* Many other minor bug fixes and improvements.\n\n### 4.3 模块系统\nRedis 4.0 发生的最大变化就是加入了模块系统， 这个系统可以让用户通过自己编写的代码来扩展和实现 Redis 本身并不具备的功能， 具体使用方法可以参考 antirez 的博文《Redis Loadable Module System》： http://antirez.com/news/106\n因为模块系统是通过高层次 API 实现的， 它与 Redis 内核本身完全分离、互不干扰， 所以用户可以在有需要的情况下才启用这个功能， 以下是 redis.conf 中记载的模块载入方法：\n```\n################################## MODULES #####################################\n\n# Load modules at startup. If the server is not able to load modules\n# it will abort. It is possible to use multiple loadmodule directives.\n#\n# loadmodule /path/to/my_module.so\n# loadmodule /path/to/other_module.so\n```\n目前已经有人使用这个功能开发了各种各样的模块， 比如 Redis Labs 开发的一些模块就可以在 http://redismodules.com 看到， 此外 antirez 自己也使用这个功能开发了一个神经网络模块： https://github.com/antirez/neural-redis\n模块功能使得用户可以将 Redis 用作基础设施， 并在上面构建更多功能， 这给 Redis 带来了无数新的可能性。\n\n### 4.4 PSYNC 2.0\n\n新版本的 PSYNC 命令解决了旧版本的 Redis 在复制时的一些不够优化的地方：\n- 在旧版本 Redis 中， 如果一个从服务器在 FAILOVER 之后成为了新的主节点， 那么其他从节点在复制这个新主的时候就必须进行全量复制。 在 Redis 4.0 中， 新主和从服务器在处理这种情况时， 将在条件允许的情况下使用部分复制。\n- 在旧版本 Redis 中， 一个从服务器如果重启了， 那么它就必须与主服务器重新进行全量复制， 在 Redis 4.0 中， 只要条件允许， 主从在处理这种情况时将使用部分复制。\n\n### 4.5 缓存驱逐策略优化\n\n新添加了 Last Frequently Used 缓存驱逐策略， 具体信息见 antirez 的博文《Random notes on improving the Redis LRU algorithm》： http://antirez.com/news/109\n另外 Redis 4.0 还对已有的缓存驱逐策略进行了优化， 使得它们能够更健壮、高效、快速和精确。\n\n### 4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL\n\n在 Redis 4.0 之前， 用户在使用 DEL 命令删除体积较大的键， 又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时， 都可能会造成服务器阻塞。\n为了解决以上问题， Redis 4.0 新添加了 UNLINK 命令， 这个命令是 DEL 命令的异步版本， 它可以将删除指定键的操作放在后台线程里面执行， 从而尽可能地避免服务器阻塞：\n```\n\n127.0.0.1:7002> set a test\nOK\n127.0.0.1:7002> get a\n\"test\"\n127.0.0.1:7002> unlink a\n(integer) 1\n127.0.0.1:7002> get a\n(nil)\n\n```\n因为一些历史原因， 执行同步删除操作的 DEL 命令将会继续保留。\n此外， Redis 4.0 中的 FLUSHDB 和 FLUSHALL 这两个命令都新添加了 ASYNC 选项， 带有这个选项的数据库删除操作将在后台线程进行：\n```\nredis> FLUSHDB ASYNC\nOK\n\nredis> FLUSHALL ASYNC\nOK\n```\n\n### 4.7 交换数据库\nRedis 4.0 对数据库命令的另外一个修改是新增了 SWAPDB 命令， 这个命令可以对指定的两个数据库进行互换： 比如说， 通过执行命令 SWAPDB 0 1 ， 我们可以将原来的数据库 0 变成数据库 1 ， 而原来的数据库 1 则变成数据库 0 。这种场景是在非集群模式下使用的，在集群模式是不支持切换数据库的。默认所有的节点使用index 0。\n以下是一个使用 SWAPDB 的例子：\n```\nredis> SET your_name \"aibibang\"  -- 在数据库 0 中设置一个键\nOK\n\nredis> GET your_name\n\"aibibang\"\n\nredis> SWAPDB 0 1  -- 互换数据库 0 和数据库 1\nOK\n\nredis> GET your_name  -- 现在的数据库 0 已经没有之前设置的键了\n(nil)\n\nredis> SELECT 1  -- 切换到数据库 1\nOK\n\nredis[1]> GET your_name  -- 之前在数据库 0 设置的键现在可以在数据库 1 找到\n\"aibibang\"                 -- 证明两个数据库已经互换\n\n```\n\n### 4.8 混合 RDB-AOF 持久化格式\n\nRedis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。\n这个功能可以通过 aof-use-rdb-preamble 选项进行开启， redis.conf 文件中记录了这个选项的使用方法：\n```\n When rewriting the AOF file, Redis is able to use an RDB preamble in the\n# AOF file for faster rewrites and recoveries. When this option is turned\n# on the rewritten AOF file is composed of two different stanzas:\n#\n#   [RDB file][AOF tail]\n#\n# When loading Redis recognizes that the AOF file starts with the \"REDIS\"\n# string and loads the prefixed RDB file, and continues loading the AOF\n# tail.\n#\n# This is currently turned off by default in order to avoid the surprise\n# of a format change, but will at some point be used as the default.\naof-use-rdb-preamble no\n\n```\n\n### 4.9 内存命令\n新添加了一个 MEMORY 命令， 这个命令可以用于视察内存使用情况， 并进行相应的内存管理操作：\n```\n127.0.0.1:7002> memory help\n1) \"MEMORY USAGE <key> [SAMPLES <count>] - Estimate memory usage of key\"\n2) \"MEMORY STATS                         - Show memory usage details\"\n3) \"MEMORY PURGE                         - Ask the allocator to release memory\"\n4) \"MEMORY MALLOC-STATS                  - Show allocator internal stats\"\n\n```\n其中， 使用 MEMORY USAGE 子命令可以估算储存给定键所需的内存：\n```\n\n127.0.0.1:7002> set truman aibibang\n-> Redirected to slot [2113] located at 127.0.0.1:7000\nOK\n127.0.0.1:7000> memory usage truman\n(integer) 59\n```\n使用 MEMORY STATS 子命令可以查看 Redis 当前的内存使用情况：\n```\n127.0.0.1:7000> memory stats\n 1) \"peak.allocated\"\n 2) (integer) 2228415\n 3) \"total.allocated\"\n 4) (integer) 2228474\n 5) \"startup.allocated\"\n 6) (integer) 1054396\n 7) \"replication.backlog\"\n 8) (integer) 1048584\n 9) \"clients.slaves\"\n10) (integer) 16858\n11) \"clients.normal\"\n12) (integer) 49630\n13) \"aof.buffer\"\n14) (integer) 0\n15) \"db.0\"\n16) 1) \"overhead.hashtable.main\"\n    2) (integer) 112\n    3) \"overhead.hashtable.expires\"\n    4) (integer) 0\n17) \"overhead.total\"\n18) (integer) 2169580\n19) \"keys.count\"\n20) (integer) 2\n21) \"keys.bytes-per-key\"\n22) (integer) 587039\n23) \"dataset.bytes\"\n24) (integer) 58894\n25) \"dataset.percentage\"\n26) \"5.0161914825439453\"\n27) \"peak.percentage\"\n28) \"100.00264739990234\"\n29) \"fragmentation\"\n30) \"1.2773568630218506\"\n```\n使用 MEMORY PURGE 子命令可以要求分配器释放更多内存：\n```\nredis> MEMORY PURGE\nOK\n```\n使用 MEMORY MALLOC-STATS 子命令可以展示分配器内部状态：\n```\nredis> MEMORY MALLOC-STATS\nStats not supported for the current allocator\n```\n\n### 4.10 兼容NAT和Docker\nRedis 4.0 将兼容 NAT 和 Docker ， 具体的使用方法在 redis.conf 中有记载：\n\n```\n########################## CLUSTER DOCKER/NAT support  ########################\n# In certain deployments, Redis Cluster nodes address discovery fails, because\n# addresses are NAT-ted or because ports are forwarded (the typical case is\n# Docker and other containers).\n#\n# In order to make Redis Cluster working in such environments, a static\n# configuration where each node known its public address is needed. The\n# following two options are used for this scope, and are:\n#\n# * cluster-announce-ip\n# * cluster-announce-port\n# * cluster-announce-bus-port\n#\n# Each instruct the node about its address, client port, and cluster message\n# bus port. The information is then published in the header of the bus packets\n# so that other nodes will be able to correctly map the address of the node\n# publishing the information.\n#\n# If the above options are not used, the normal Redis Cluster auto-detection\n# will be used instead.\n#\n# Note that when remapped, the bus port may not be at the fixed offset of\n# clients port + 10000, so you can specify any port and bus-port depending\n# on how they get remapped. If the bus-port is not set, a fixed offset of\n# 10000 will be used as usually.\n#\n# Example:\n#\n# cluster-announce-ip 10.1.1.5\n# cluster-announce-port 6379\n# cluster-announce-bus-port 6380\n```\n## 5. 引用\n\n1. [Redis 4.0新功能介绍](http://blog.csdn.net/yin767833376/article/details/53518272)\n2. [Redis 4.0 release note](https://github.com/antirez/redis/blob/05b81d2b02578d432329c87c93f975e582d14c0e/00-RELEASENOTES)\n","source":"_posts/Redis4-0-新特性尝鲜.md","raw":"---\ntitle: Redis4.0 新特性尝鲜\ndate: 2017-07-19 22:17:00\ntags: nosql\ncategories:\n- redis\n---\n# Redis4.0 新特性尝鲜\n\n## 1.目录结构\n\n1. [目录](#1-目录结构)\n2. [前言](#2-前言)\n3. [环境搭建](#3-环境搭建)\n   1. [下载](#31-下载)\n   2. [安装](#32-编译安装)\n   3. [集群搭建](#33-新建集群)\n4. [特性尝鲜](#4-特性尝鲜)\n   1. [注意事项](#41-注意事项)\n   2. [升级内容](#42-升级内容)\n   3. [模块系统](#43-模块系统)\n   4. [PSYNC 2.0](#44-PSYNC)\n   5. [缓存驱逐策略优化](#45-缓存驱逐策略优化)\n   6. [非阻塞 DEL 、 FLUSHDB 和 FLUSHALL](#46-)\n   7. [交换数据库](#47-交换数据库)\n   8. [混合RDBAOF持久化格式](#48-)\n   9. [内存命令](#49-内存命令)\n   10. [兼容 NAT 和 Docker](#410-兼容NAT和Docker)\n5. [引用](#5-引用)\n\n## 2. 前言\n\n**2017-07-14 redis 4.0  Stable  version release**,新增了许多新功能，此次专门抽出时间，探索一些功能，把握Redis未来的发展方向，同时积累经验，为未来升级Redis打下坚实基础。学习新的东西，如果不将它记录下来，过上两周，基本上就忘记做过什么了，对知识的掌握不利，以后尽量所有的学习都能产生文字记录，便于自己总结学习各种技术，也能给新人带去一点便利。\n\n## 3. 环境搭建\n\n安装方式，较之前没有多大变化，还是写一下，便于新手学习。\n\n### 3.1. 下载\n```\n$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz\n$ tar xzf redis-4.0.0.tar.gz\n```\n\n### 3.2. 编译安装\n```\n$ cd redis-4.0.0\n$ make\n```\n如果需要将redis-cli,redis-server等相关命令安装到/bin目录下，全局使用的话，可以使用如下命令：\n```\n$ make install\n```\n或者利用软连接实现：\n```\n$ make\n$ ln -s  redis-4.0.0/src/redis-server  /bin/redis-server\n$ ln -s  redis-4.0.0/src/redis-cli  /bin/redis-cli\n```\n在编译中可能会遇到如下问题：\n```\n zmalloc.h:50:31: 错误：jemalloc/jemalloc.h：没有那个文件或目录\n```\n出现这个问题是libc 并不是默认的 分配器， 默认的是 jemalloc, 因为 jemalloc 被证明 有更少的 fragmentation problems 比libc，关于更详细信息可以查看redis中REAME,md，其中有详细介绍，此处不再细说。\n\n解决办法：\n```\nmake MALLOC=libc\n```\n\n### 3.3. 新建集群\n\n按照官方指导，搭建3m+3s集群，端口号7000-7005\n\n以下为创建脚本：\n\n```\n#!/bin/bash\n nodes=(7000 7001 7002 7003 7004 7005)\n HOME_DIR=`pwd`\nfunction create(){\n echo \"create\"\n if [ -d redis_cluster ];then\n rm -rf  redis_cluster\n fi\n\n mkdir  redis_cluster\n cd redis_cluster\n\n mkdir `echo ${nodes[*]}`\n\n for var in ${nodes[*]}\n do\n  cd $var\n  touch $var-redis.conf\n  echo \"port\" $var >> $var-redis.conf\n  echo \"cluster-enabled yes\" >> $var-redis.conf\n  echo \"cluster-config-file nodes.conf\" >> $var-redis.conf\n  echo \"cluster-node-timeout 5000\" >> $var-redis.conf\n  echo \"appendonly yes\" >> $var-redis.conf\n  echo \"daemonize yes\" >> $var-redis.conf\n  echo \"pidfile redis.pid\" >> $var-redis.conf\n  cd ../\n done\n run;\n src/redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005\n\n}\n\nfunction run(){\necho run\n for var in ${nodes[*]}\n do\n   cd $HOME_DIR/redis_cluster/$var/\n   redis-server $var-redis.conf\n   cd $HOME_DIR/\n   echo Start redis ports ${var} finish.\n done\n}\nfunction stop(){\n cd $HOME_DIR/redis_cluster/\n DIRS=`ls -l | grep \"^d\" | awk '{print $NF}'`\n  for d in $DIRS\n  do\n    PID=`cat $d/redis.pid`\n    if [ -n \"$PID\" ];then\n       kill -9 $PID\n       rm -f $d/redis.pid\n    fi\n  done\n  cd  $HOME_DIR/\n  echo Stop redis ports ${DIRS} finish.\n}\n\n\ncase $1 in\n     create)\n      echo \" create redis cluster 锛?000,7001,7002,7003,7004.7005,7006,7007\"\n      create\n      ;;\n     run)\n      run\n      ;;\n     stop)\n      stop\n     ;;\n     *)\n      echo \"Usage:[create|run|stop]\"\n      ;;\nesac\n```\n\n执行\n\n```\nsh createCluster.sh create\n```\n一路回车即可，注意构建集群需要使用redis-trib.rb，需要额外执行\n\n```\ngem install redis\n```\n结果如下：\n\n```\n[bigdata@trumanlab1 redis-4.0.0]$ redis-cli -c -h 127.0.0.1  -p 7000\n127.0.0.1:7000> cluster nodes\n63e698210378f4fda23c4070bea3b46f93952811 127.0.0.1:7000@17000 myself,master - 0 1500391716000 1 connected 0-5460\n4091630217b64ece9a6fa55c26687a10c1f9a8b5 127.0.0.1:7001@17001 master - 0 1500391717501 2 connected 5461-10922\n0b23853c3aa4f5b942a6239bc7bca71df36e121c 127.0.0.1:7005@17005 slave 12d05fb7c20bf001759f80cf3f65ad9df4b01af5 0 1500391716498 6 connected\n12d05fb7c20bf001759f80cf3f65ad9df4b01af5 127.0.0.1:7002@17002 master - 0 1500391717000 3 connected 10923-16383\n2e559b462362a9563f98b398af3984c05e25ef19 127.0.0.1:7003@17003 slave 63e698210378f4fda23c4070bea3b46f93952811 0 1500391716000 4 connected\nc4f8e0a95579c772c7506c195b3c7984c73312b4 127.0.0.1:7004@17004 slave 4091630217b64ece9a6fa55c26687a10c1f9a8b5 0 1500391716000 5 connected\n127.0.0.1:7000> set a a\n-> Redirected to slot [15495] located at 127.0.0.1:7002\nOK\n127.0.0.1:7002> get a\n\"a\"\n127.0.0.1:7002>\n```\n\n## 4. 特性尝鲜\n\n### 4.1 注意事项\n\n> IMPORTANT: Redis Cluster users, please note that, as specified in the list\nof incompatibilities, Redis 4.0 cluster bus protocol is not compatible with\nRedis 3.2, so in order to upgrade, a mass reboot of the instances is needed\nand rolling upgrades are not possible. This change was needed in order to\nadd compatibility for Containers/NAT, where the bus port at a fixed offset\nwas not an acceptable design, so we had to change many things, resulting\nin the incompatible protocol.\n\n4.0版本与3.2版本传输协议不兼容，更改该协议的目的是为了实现Containers/NAT\n为了升级，需要重启大量节点，无法做到滚动升级。\n\n### 4.2 升级内容\n>* Different replication fixes to PSYNC2, the new 4.0 replication engine.\n>* Modules thread safe contexts were introduced. They are an >experimental API right now, but the API is considered to be stable and usable when needed.\n>* SLOWLOG now logs the offending client name and address. Note that this is a backward compatibility breakage in case old code assumes that the slowlog entry is composed of exactly three entries.\n>* The modules native data types RDB format changed.\n>* The AOF check utility is now able to deal with RDB preambles.\n>* GEORADIUS_RO and GEORADIUSBYMEMBER_RO variants, not supporting the STORE option, were added in order to allow read-only scaling of such queries.\n>* HSET is now variadic, and HMSET is considered deprecated (but will be supported for years to come). Please use HSET in new code.\n>* GEORADIUS huge radius (>= ~6000 km) corner cases fixed, certain elements near the edges were not returned.\n>* DEBUG DIGEST modules API added.\n>* HyperLogLog commands no longer crash on certain input (non HLL) strings.\n>* Fixed SLAVEOF inside MULTI/EXEC blocks.\n>* Many other minor bug fixes and improvements.\n\n### 4.3 模块系统\nRedis 4.0 发生的最大变化就是加入了模块系统， 这个系统可以让用户通过自己编写的代码来扩展和实现 Redis 本身并不具备的功能， 具体使用方法可以参考 antirez 的博文《Redis Loadable Module System》： http://antirez.com/news/106\n因为模块系统是通过高层次 API 实现的， 它与 Redis 内核本身完全分离、互不干扰， 所以用户可以在有需要的情况下才启用这个功能， 以下是 redis.conf 中记载的模块载入方法：\n```\n################################## MODULES #####################################\n\n# Load modules at startup. If the server is not able to load modules\n# it will abort. It is possible to use multiple loadmodule directives.\n#\n# loadmodule /path/to/my_module.so\n# loadmodule /path/to/other_module.so\n```\n目前已经有人使用这个功能开发了各种各样的模块， 比如 Redis Labs 开发的一些模块就可以在 http://redismodules.com 看到， 此外 antirez 自己也使用这个功能开发了一个神经网络模块： https://github.com/antirez/neural-redis\n模块功能使得用户可以将 Redis 用作基础设施， 并在上面构建更多功能， 这给 Redis 带来了无数新的可能性。\n\n### 4.4 PSYNC 2.0\n\n新版本的 PSYNC 命令解决了旧版本的 Redis 在复制时的一些不够优化的地方：\n- 在旧版本 Redis 中， 如果一个从服务器在 FAILOVER 之后成为了新的主节点， 那么其他从节点在复制这个新主的时候就必须进行全量复制。 在 Redis 4.0 中， 新主和从服务器在处理这种情况时， 将在条件允许的情况下使用部分复制。\n- 在旧版本 Redis 中， 一个从服务器如果重启了， 那么它就必须与主服务器重新进行全量复制， 在 Redis 4.0 中， 只要条件允许， 主从在处理这种情况时将使用部分复制。\n\n### 4.5 缓存驱逐策略优化\n\n新添加了 Last Frequently Used 缓存驱逐策略， 具体信息见 antirez 的博文《Random notes on improving the Redis LRU algorithm》： http://antirez.com/news/109\n另外 Redis 4.0 还对已有的缓存驱逐策略进行了优化， 使得它们能够更健壮、高效、快速和精确。\n\n### 4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL\n\n在 Redis 4.0 之前， 用户在使用 DEL 命令删除体积较大的键， 又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时， 都可能会造成服务器阻塞。\n为了解决以上问题， Redis 4.0 新添加了 UNLINK 命令， 这个命令是 DEL 命令的异步版本， 它可以将删除指定键的操作放在后台线程里面执行， 从而尽可能地避免服务器阻塞：\n```\n\n127.0.0.1:7002> set a test\nOK\n127.0.0.1:7002> get a\n\"test\"\n127.0.0.1:7002> unlink a\n(integer) 1\n127.0.0.1:7002> get a\n(nil)\n\n```\n因为一些历史原因， 执行同步删除操作的 DEL 命令将会继续保留。\n此外， Redis 4.0 中的 FLUSHDB 和 FLUSHALL 这两个命令都新添加了 ASYNC 选项， 带有这个选项的数据库删除操作将在后台线程进行：\n```\nredis> FLUSHDB ASYNC\nOK\n\nredis> FLUSHALL ASYNC\nOK\n```\n\n### 4.7 交换数据库\nRedis 4.0 对数据库命令的另外一个修改是新增了 SWAPDB 命令， 这个命令可以对指定的两个数据库进行互换： 比如说， 通过执行命令 SWAPDB 0 1 ， 我们可以将原来的数据库 0 变成数据库 1 ， 而原来的数据库 1 则变成数据库 0 。这种场景是在非集群模式下使用的，在集群模式是不支持切换数据库的。默认所有的节点使用index 0。\n以下是一个使用 SWAPDB 的例子：\n```\nredis> SET your_name \"aibibang\"  -- 在数据库 0 中设置一个键\nOK\n\nredis> GET your_name\n\"aibibang\"\n\nredis> SWAPDB 0 1  -- 互换数据库 0 和数据库 1\nOK\n\nredis> GET your_name  -- 现在的数据库 0 已经没有之前设置的键了\n(nil)\n\nredis> SELECT 1  -- 切换到数据库 1\nOK\n\nredis[1]> GET your_name  -- 之前在数据库 0 设置的键现在可以在数据库 1 找到\n\"aibibang\"                 -- 证明两个数据库已经互换\n\n```\n\n### 4.8 混合 RDB-AOF 持久化格式\n\nRedis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。\n这个功能可以通过 aof-use-rdb-preamble 选项进行开启， redis.conf 文件中记录了这个选项的使用方法：\n```\n When rewriting the AOF file, Redis is able to use an RDB preamble in the\n# AOF file for faster rewrites and recoveries. When this option is turned\n# on the rewritten AOF file is composed of two different stanzas:\n#\n#   [RDB file][AOF tail]\n#\n# When loading Redis recognizes that the AOF file starts with the \"REDIS\"\n# string and loads the prefixed RDB file, and continues loading the AOF\n# tail.\n#\n# This is currently turned off by default in order to avoid the surprise\n# of a format change, but will at some point be used as the default.\naof-use-rdb-preamble no\n\n```\n\n### 4.9 内存命令\n新添加了一个 MEMORY 命令， 这个命令可以用于视察内存使用情况， 并进行相应的内存管理操作：\n```\n127.0.0.1:7002> memory help\n1) \"MEMORY USAGE <key> [SAMPLES <count>] - Estimate memory usage of key\"\n2) \"MEMORY STATS                         - Show memory usage details\"\n3) \"MEMORY PURGE                         - Ask the allocator to release memory\"\n4) \"MEMORY MALLOC-STATS                  - Show allocator internal stats\"\n\n```\n其中， 使用 MEMORY USAGE 子命令可以估算储存给定键所需的内存：\n```\n\n127.0.0.1:7002> set truman aibibang\n-> Redirected to slot [2113] located at 127.0.0.1:7000\nOK\n127.0.0.1:7000> memory usage truman\n(integer) 59\n```\n使用 MEMORY STATS 子命令可以查看 Redis 当前的内存使用情况：\n```\n127.0.0.1:7000> memory stats\n 1) \"peak.allocated\"\n 2) (integer) 2228415\n 3) \"total.allocated\"\n 4) (integer) 2228474\n 5) \"startup.allocated\"\n 6) (integer) 1054396\n 7) \"replication.backlog\"\n 8) (integer) 1048584\n 9) \"clients.slaves\"\n10) (integer) 16858\n11) \"clients.normal\"\n12) (integer) 49630\n13) \"aof.buffer\"\n14) (integer) 0\n15) \"db.0\"\n16) 1) \"overhead.hashtable.main\"\n    2) (integer) 112\n    3) \"overhead.hashtable.expires\"\n    4) (integer) 0\n17) \"overhead.total\"\n18) (integer) 2169580\n19) \"keys.count\"\n20) (integer) 2\n21) \"keys.bytes-per-key\"\n22) (integer) 587039\n23) \"dataset.bytes\"\n24) (integer) 58894\n25) \"dataset.percentage\"\n26) \"5.0161914825439453\"\n27) \"peak.percentage\"\n28) \"100.00264739990234\"\n29) \"fragmentation\"\n30) \"1.2773568630218506\"\n```\n使用 MEMORY PURGE 子命令可以要求分配器释放更多内存：\n```\nredis> MEMORY PURGE\nOK\n```\n使用 MEMORY MALLOC-STATS 子命令可以展示分配器内部状态：\n```\nredis> MEMORY MALLOC-STATS\nStats not supported for the current allocator\n```\n\n### 4.10 兼容NAT和Docker\nRedis 4.0 将兼容 NAT 和 Docker ， 具体的使用方法在 redis.conf 中有记载：\n\n```\n########################## CLUSTER DOCKER/NAT support  ########################\n# In certain deployments, Redis Cluster nodes address discovery fails, because\n# addresses are NAT-ted or because ports are forwarded (the typical case is\n# Docker and other containers).\n#\n# In order to make Redis Cluster working in such environments, a static\n# configuration where each node known its public address is needed. The\n# following two options are used for this scope, and are:\n#\n# * cluster-announce-ip\n# * cluster-announce-port\n# * cluster-announce-bus-port\n#\n# Each instruct the node about its address, client port, and cluster message\n# bus port. The information is then published in the header of the bus packets\n# so that other nodes will be able to correctly map the address of the node\n# publishing the information.\n#\n# If the above options are not used, the normal Redis Cluster auto-detection\n# will be used instead.\n#\n# Note that when remapped, the bus port may not be at the fixed offset of\n# clients port + 10000, so you can specify any port and bus-port depending\n# on how they get remapped. If the bus-port is not set, a fixed offset of\n# 10000 will be used as usually.\n#\n# Example:\n#\n# cluster-announce-ip 10.1.1.5\n# cluster-announce-port 6379\n# cluster-announce-bus-port 6380\n```\n## 5. 引用\n\n1. [Redis 4.0新功能介绍](http://blog.csdn.net/yin767833376/article/details/53518272)\n2. [Redis 4.0 release note](https://github.com/antirez/redis/blob/05b81d2b02578d432329c87c93f975e582d14c0e/00-RELEASENOTES)\n","slug":"Redis4-0-新特性尝鲜","published":1,"updated":"2019-08-22T12:31:46.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgd00ac8ceelq5czgsv","content":"<h1 id=\"Redis4-0-新特性尝鲜\"><a href=\"#Redis4-0-新特性尝鲜\" class=\"headerlink\" title=\"Redis4.0 新特性尝鲜\"></a>Redis4.0 新特性尝鲜</h1><h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1.目录结构\"></a>1.目录结构</h2><ol>\n<li><a href=\"#1-目录结构\">目录</a></li>\n<li><a href=\"#2-前言\">前言</a></li>\n<li><a href=\"#3-环境搭建\">环境搭建</a><ol>\n<li><a href=\"#31-下载\">下载</a></li>\n<li><a href=\"#32-编译安装\">安装</a></li>\n<li><a href=\"#33-新建集群\">集群搭建</a></li>\n</ol>\n</li>\n<li><a href=\"#4-特性尝鲜\">特性尝鲜</a><ol>\n<li><a href=\"#41-注意事项\">注意事项</a></li>\n<li><a href=\"#42-升级内容\">升级内容</a></li>\n<li><a href=\"#43-模块系统\">模块系统</a></li>\n<li><a href=\"#44-PSYNC\">PSYNC 2.0</a></li>\n<li><a href=\"#45-缓存驱逐策略优化\">缓存驱逐策略优化</a></li>\n<li><a href=\"#46-\">非阻塞 DEL 、 FLUSHDB 和 FLUSHALL</a></li>\n<li><a href=\"#47-交换数据库\">交换数据库</a></li>\n<li><a href=\"#48-\">混合RDBAOF持久化格式</a></li>\n<li><a href=\"#49-内存命令\">内存命令</a></li>\n<li><a href=\"#410-兼容NAT和Docker\">兼容 NAT 和 Docker</a></li>\n</ol>\n</li>\n<li><a href=\"#5-引用\">引用</a></li>\n</ol>\n<h2 id=\"2-前言\"><a href=\"#2-前言\" class=\"headerlink\" title=\"2. 前言\"></a>2. 前言</h2><p><strong>2017-07-14 redis 4.0  Stable  version release</strong>,新增了许多新功能，此次专门抽出时间，探索一些功能，把握Redis未来的发展方向，同时积累经验，为未来升级Redis打下坚实基础。学习新的东西，如果不将它记录下来，过上两周，基本上就忘记做过什么了，对知识的掌握不利，以后尽量所有的学习都能产生文字记录，便于自己总结学习各种技术，也能给新人带去一点便利。</p>\n<h2 id=\"3-环境搭建\"><a href=\"#3-环境搭建\" class=\"headerlink\" title=\"3. 环境搭建\"></a>3. 环境搭建</h2><p>安装方式，较之前没有多大变化，还是写一下，便于新手学习。</p>\n<h3 id=\"3-1-下载\"><a href=\"#3-1-下载\" class=\"headerlink\" title=\"3.1. 下载\"></a>3.1. 下载</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz</span><br><span class=\"line\">$ tar xzf redis-4.0.0.tar.gz</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-编译安装\"><a href=\"#3-2-编译安装\" class=\"headerlink\" title=\"3.2. 编译安装\"></a>3.2. 编译安装</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd redis-4.0.0</span><br><span class=\"line\">$ make</span><br></pre></td></tr></table></figure>\n<p>如果需要将redis-cli,redis-server等相关命令安装到/bin目录下，全局使用的话，可以使用如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ make install</span><br></pre></td></tr></table></figure></p>\n<p>或者利用软连接实现：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ make</span><br><span class=\"line\">$ ln -s  redis-4.0.0/src/redis-server  /bin/redis-server</span><br><span class=\"line\">$ ln -s  redis-4.0.0/src/redis-cli  /bin/redis-cli</span><br></pre></td></tr></table></figure></p>\n<p>在编译中可能会遇到如下问题：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zmalloc.h:50:31: 错误：jemalloc/jemalloc.h：没有那个文件或目录</span><br></pre></td></tr></table></figure></p>\n<p>出现这个问题是libc 并不是默认的 分配器， 默认的是 jemalloc, 因为 jemalloc 被证明 有更少的 fragmentation problems 比libc，关于更详细信息可以查看redis中REAME,md，其中有详细介绍，此处不再细说。</p>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make MALLOC=libc</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-3-新建集群\"><a href=\"#3-3-新建集群\" class=\"headerlink\" title=\"3.3. 新建集群\"></a>3.3. 新建集群</h3><p>按照官方指导，搭建3m+3s集群，端口号7000-7005</p>\n<p>以下为创建脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"> nodes=(7000 7001 7002 7003 7004 7005)</span><br><span class=\"line\"> HOME_DIR=`pwd`</span><br><span class=\"line\">function create()&#123;</span><br><span class=\"line\"> echo &quot;create&quot;</span><br><span class=\"line\"> if [ -d redis_cluster ];then</span><br><span class=\"line\"> rm -rf  redis_cluster</span><br><span class=\"line\"> fi</span><br><span class=\"line\"></span><br><span class=\"line\"> mkdir  redis_cluster</span><br><span class=\"line\"> cd redis_cluster</span><br><span class=\"line\"></span><br><span class=\"line\"> mkdir `echo $&#123;nodes[*]&#125;`</span><br><span class=\"line\"></span><br><span class=\"line\"> for var in $&#123;nodes[*]&#125;</span><br><span class=\"line\"> do</span><br><span class=\"line\">  cd $var</span><br><span class=\"line\">  touch $var-redis.conf</span><br><span class=\"line\">  echo &quot;port&quot; $var &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-enabled yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-config-file nodes.conf&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-node-timeout 5000&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;appendonly yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;daemonize yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;pidfile redis.pid&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  cd ../</span><br><span class=\"line\"> done</span><br><span class=\"line\"> run;</span><br><span class=\"line\"> src/redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">function run()&#123;</span><br><span class=\"line\">echo run</span><br><span class=\"line\"> for var in $&#123;nodes[*]&#125;</span><br><span class=\"line\"> do</span><br><span class=\"line\">   cd $HOME_DIR/redis_cluster/$var/</span><br><span class=\"line\">   redis-server $var-redis.conf</span><br><span class=\"line\">   cd $HOME_DIR/</span><br><span class=\"line\">   echo Start redis ports $&#123;var&#125; finish.</span><br><span class=\"line\"> done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">function stop()&#123;</span><br><span class=\"line\"> cd $HOME_DIR/redis_cluster/</span><br><span class=\"line\"> DIRS=`ls -l | grep &quot;^d&quot; | awk &apos;&#123;print $NF&#125;&apos;`</span><br><span class=\"line\">  for d in $DIRS</span><br><span class=\"line\">  do</span><br><span class=\"line\">    PID=`cat $d/redis.pid`</span><br><span class=\"line\">    if [ -n &quot;$PID&quot; ];then</span><br><span class=\"line\">       kill -9 $PID</span><br><span class=\"line\">       rm -f $d/redis.pid</span><br><span class=\"line\">    fi</span><br><span class=\"line\">  done</span><br><span class=\"line\">  cd  $HOME_DIR/</span><br><span class=\"line\">  echo Stop redis ports $&#123;DIRS&#125; finish.</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">     create)</span><br><span class=\"line\">      echo &quot; create redis cluster 锛?000,7001,7002,7003,7004.7005,7006,7007&quot;</span><br><span class=\"line\">      create</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">     run)</span><br><span class=\"line\">      run</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">     stop)</span><br><span class=\"line\">      stop</span><br><span class=\"line\">     ;;</span><br><span class=\"line\">     *)</span><br><span class=\"line\">      echo &quot;Usage:[create|run|stop]&quot;</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh createCluster.sh create</span><br></pre></td></tr></table></figure>\n<p>一路回车即可，注意构建集群需要使用redis-trib.rb，需要额外执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem install redis</span><br></pre></td></tr></table></figure>\n<p>结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[bigdata@trumanlab1 redis-4.0.0]$ redis-cli -c -h 127.0.0.1  -p 7000</span><br><span class=\"line\">127.0.0.1:7000&gt; cluster nodes</span><br><span class=\"line\">63e698210378f4fda23c4070bea3b46f93952811 127.0.0.1:7000@17000 myself,master - 0 1500391716000 1 connected 0-5460</span><br><span class=\"line\">4091630217b64ece9a6fa55c26687a10c1f9a8b5 127.0.0.1:7001@17001 master - 0 1500391717501 2 connected 5461-10922</span><br><span class=\"line\">0b23853c3aa4f5b942a6239bc7bca71df36e121c 127.0.0.1:7005@17005 slave 12d05fb7c20bf001759f80cf3f65ad9df4b01af5 0 1500391716498 6 connected</span><br><span class=\"line\">12d05fb7c20bf001759f80cf3f65ad9df4b01af5 127.0.0.1:7002@17002 master - 0 1500391717000 3 connected 10923-16383</span><br><span class=\"line\">2e559b462362a9563f98b398af3984c05e25ef19 127.0.0.1:7003@17003 slave 63e698210378f4fda23c4070bea3b46f93952811 0 1500391716000 4 connected</span><br><span class=\"line\">c4f8e0a95579c772c7506c195b3c7984c73312b4 127.0.0.1:7004@17004 slave 4091630217b64ece9a6fa55c26687a10c1f9a8b5 0 1500391716000 5 connected</span><br><span class=\"line\">127.0.0.1:7000&gt; set a a</span><br><span class=\"line\">-&gt; Redirected to slot [15495] located at 127.0.0.1:7002</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">&quot;a&quot;</span><br><span class=\"line\">127.0.0.1:7002&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-特性尝鲜\"><a href=\"#4-特性尝鲜\" class=\"headerlink\" title=\"4. 特性尝鲜\"></a>4. 特性尝鲜</h2><h3 id=\"4-1-注意事项\"><a href=\"#4-1-注意事项\" class=\"headerlink\" title=\"4.1 注意事项\"></a>4.1 注意事项</h3><blockquote>\n<p>IMPORTANT: Redis Cluster users, please note that, as specified in the list<br>of incompatibilities, Redis 4.0 cluster bus protocol is not compatible with<br>Redis 3.2, so in order to upgrade, a mass reboot of the instances is needed<br>and rolling upgrades are not possible. This change was needed in order to<br>add compatibility for Containers/NAT, where the bus port at a fixed offset<br>was not an acceptable design, so we had to change many things, resulting<br>in the incompatible protocol.</p>\n</blockquote>\n<p>4.0版本与3.2版本传输协议不兼容，更改该协议的目的是为了实现Containers/NAT<br>为了升级，需要重启大量节点，无法做到滚动升级。</p>\n<h3 id=\"4-2-升级内容\"><a href=\"#4-2-升级内容\" class=\"headerlink\" title=\"4.2 升级内容\"></a>4.2 升级内容</h3><blockquote>\n<ul>\n<li>Different replication fixes to PSYNC2, the new 4.0 replication engine.</li>\n<li>Modules thread safe contexts were introduced. They are an &gt;experimental API right now, but the API is considered to be stable and usable when needed.</li>\n<li>SLOWLOG now logs the offending client name and address. Note that this is a backward compatibility breakage in case old code assumes that the slowlog entry is composed of exactly three entries.</li>\n<li>The modules native data types RDB format changed.</li>\n<li>The AOF check utility is now able to deal with RDB preambles.</li>\n<li>GEORADIUS_RO and GEORADIUSBYMEMBER_RO variants, not supporting the STORE option, were added in order to allow read-only scaling of such queries.</li>\n<li>HSET is now variadic, and HMSET is considered deprecated (but will be supported for years to come). Please use HSET in new code.</li>\n<li>GEORADIUS huge radius (&gt;= ~6000 km) corner cases fixed, certain elements near the edges were not returned.</li>\n<li>DEBUG DIGEST modules API added.</li>\n<li>HyperLogLog commands no longer crash on certain input (non HLL) strings.</li>\n<li>Fixed SLAVEOF inside MULTI/EXEC blocks.</li>\n<li>Many other minor bug fixes and improvements.</li>\n</ul>\n</blockquote>\n<h3 id=\"4-3-模块系统\"><a href=\"#4-3-模块系统\" class=\"headerlink\" title=\"4.3 模块系统\"></a>4.3 模块系统</h3><p>Redis 4.0 发生的最大变化就是加入了模块系统， 这个系统可以让用户通过自己编写的代码来扩展和实现 Redis 本身并不具备的功能， 具体使用方法可以参考 antirez 的博文《Redis Loadable Module System》： <a href=\"http://antirez.com/news/106\" target=\"_blank\" rel=\"noopener\">http://antirez.com/news/106</a><br>因为模块系统是通过高层次 API 实现的， 它与 Redis 内核本身完全分离、互不干扰， 所以用户可以在有需要的情况下才启用这个功能， 以下是 redis.conf 中记载的模块载入方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">################################## MODULES #####################################</span><br><span class=\"line\"></span><br><span class=\"line\"># Load modules at startup. If the server is not able to load modules</span><br><span class=\"line\"># it will abort. It is possible to use multiple loadmodule directives.</span><br><span class=\"line\">#</span><br><span class=\"line\"># loadmodule /path/to/my_module.so</span><br><span class=\"line\"># loadmodule /path/to/other_module.so</span><br></pre></td></tr></table></figure></p>\n<p>目前已经有人使用这个功能开发了各种各样的模块， 比如 Redis Labs 开发的一些模块就可以在 <a href=\"http://redismodules.com\" target=\"_blank\" rel=\"noopener\">http://redismodules.com</a> 看到， 此外 antirez 自己也使用这个功能开发了一个神经网络模块： <a href=\"https://github.com/antirez/neural-redis\" target=\"_blank\" rel=\"noopener\">https://github.com/antirez/neural-redis</a><br>模块功能使得用户可以将 Redis 用作基础设施， 并在上面构建更多功能， 这给 Redis 带来了无数新的可能性。</p>\n<h3 id=\"4-4-PSYNC-2-0\"><a href=\"#4-4-PSYNC-2-0\" class=\"headerlink\" title=\"4.4 PSYNC 2.0\"></a>4.4 PSYNC 2.0</h3><p>新版本的 PSYNC 命令解决了旧版本的 Redis 在复制时的一些不够优化的地方：</p>\n<ul>\n<li>在旧版本 Redis 中， 如果一个从服务器在 FAILOVER 之后成为了新的主节点， 那么其他从节点在复制这个新主的时候就必须进行全量复制。 在 Redis 4.0 中， 新主和从服务器在处理这种情况时， 将在条件允许的情况下使用部分复制。</li>\n<li>在旧版本 Redis 中， 一个从服务器如果重启了， 那么它就必须与主服务器重新进行全量复制， 在 Redis 4.0 中， 只要条件允许， 主从在处理这种情况时将使用部分复制。</li>\n</ul>\n<h3 id=\"4-5-缓存驱逐策略优化\"><a href=\"#4-5-缓存驱逐策略优化\" class=\"headerlink\" title=\"4.5 缓存驱逐策略优化\"></a>4.5 缓存驱逐策略优化</h3><p>新添加了 Last Frequently Used 缓存驱逐策略， 具体信息见 antirez 的博文《Random notes on improving the Redis LRU algorithm》： <a href=\"http://antirez.com/news/109\" target=\"_blank\" rel=\"noopener\">http://antirez.com/news/109</a><br>另外 Redis 4.0 还对已有的缓存驱逐策略进行了优化， 使得它们能够更健壮、高效、快速和精确。</p>\n<h3 id=\"4-6-非阻塞-DEL-、-FLUSHDB-和-FLUSHALL\"><a href=\"#4-6-非阻塞-DEL-、-FLUSHDB-和-FLUSHALL\" class=\"headerlink\" title=\"4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL\"></a>4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL</h3><p>在 Redis 4.0 之前， 用户在使用 DEL 命令删除体积较大的键， 又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时， 都可能会造成服务器阻塞。<br>为了解决以上问题， Redis 4.0 新添加了 UNLINK 命令， 这个命令是 DEL 命令的异步版本， 它可以将删除指定键的操作放在后台线程里面执行， 从而尽可能地避免服务器阻塞：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">127.0.0.1:7002&gt; set a test</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">&quot;test&quot;</span><br><span class=\"line\">127.0.0.1:7002&gt; unlink a</span><br><span class=\"line\">(integer) 1</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">(nil)</span><br></pre></td></tr></table></figure></p>\n<p>因为一些历史原因， 执行同步删除操作的 DEL 命令将会继续保留。<br>此外， Redis 4.0 中的 FLUSHDB 和 FLUSHALL 这两个命令都新添加了 ASYNC 选项， 带有这个选项的数据库删除操作将在后台线程进行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; FLUSHDB ASYNC</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; FLUSHALL ASYNC</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-7-交换数据库\"><a href=\"#4-7-交换数据库\" class=\"headerlink\" title=\"4.7 交换数据库\"></a>4.7 交换数据库</h3><p>Redis 4.0 对数据库命令的另外一个修改是新增了 SWAPDB 命令， 这个命令可以对指定的两个数据库进行互换： 比如说， 通过执行命令 SWAPDB 0 1 ， 我们可以将原来的数据库 0 变成数据库 1 ， 而原来的数据库 1 则变成数据库 0 。这种场景是在非集群模式下使用的，在集群模式是不支持切换数据库的。默认所有的节点使用index 0。<br>以下是一个使用 SWAPDB 的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; SET your_name &quot;aibibang&quot;  -- 在数据库 0 中设置一个键</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; GET your_name</span><br><span class=\"line\">&quot;aibibang&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; SWAPDB 0 1  -- 互换数据库 0 和数据库 1</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; GET your_name  -- 现在的数据库 0 已经没有之前设置的键了</span><br><span class=\"line\">(nil)</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; SELECT 1  -- 切换到数据库 1</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis[1]&gt; GET your_name  -- 之前在数据库 0 设置的键现在可以在数据库 1 找到</span><br><span class=\"line\">&quot;aibibang&quot;                 -- 证明两个数据库已经互换</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-8-混合-RDB-AOF-持久化格式\"><a href=\"#4-8-混合-RDB-AOF-持久化格式\" class=\"headerlink\" title=\"4.8 混合 RDB-AOF 持久化格式\"></a>4.8 混合 RDB-AOF 持久化格式</h3><p>Redis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。<br>这个功能可以通过 aof-use-rdb-preamble 选项进行开启， redis.conf 文件中记录了这个选项的使用方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> When rewriting the AOF file, Redis is able to use an RDB preamble in the</span><br><span class=\"line\"># AOF file for faster rewrites and recoveries. When this option is turned</span><br><span class=\"line\"># on the rewritten AOF file is composed of two different stanzas:</span><br><span class=\"line\">#</span><br><span class=\"line\">#   [RDB file][AOF tail]</span><br><span class=\"line\">#</span><br><span class=\"line\"># When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;</span><br><span class=\"line\"># string and loads the prefixed RDB file, and continues loading the AOF</span><br><span class=\"line\"># tail.</span><br><span class=\"line\">#</span><br><span class=\"line\"># This is currently turned off by default in order to avoid the surprise</span><br><span class=\"line\"># of a format change, but will at some point be used as the default.</span><br><span class=\"line\">aof-use-rdb-preamble no</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-9-内存命令\"><a href=\"#4-9-内存命令\" class=\"headerlink\" title=\"4.9 内存命令\"></a>4.9 内存命令</h3><p>新添加了一个 MEMORY 命令， 这个命令可以用于视察内存使用情况， 并进行相应的内存管理操作：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:7002&gt; memory help</span><br><span class=\"line\">1) &quot;MEMORY USAGE &lt;key&gt; [SAMPLES &lt;count&gt;] - Estimate memory usage of key&quot;</span><br><span class=\"line\">2) &quot;MEMORY STATS                         - Show memory usage details&quot;</span><br><span class=\"line\">3) &quot;MEMORY PURGE                         - Ask the allocator to release memory&quot;</span><br><span class=\"line\">4) &quot;MEMORY MALLOC-STATS                  - Show allocator internal stats&quot;</span><br></pre></td></tr></table></figure></p>\n<p>其中， 使用 MEMORY USAGE 子命令可以估算储存给定键所需的内存：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">127.0.0.1:7002&gt; set truman aibibang</span><br><span class=\"line\">-&gt; Redirected to slot [2113] located at 127.0.0.1:7000</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7000&gt; memory usage truman</span><br><span class=\"line\">(integer) 59</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY STATS 子命令可以查看 Redis 当前的内存使用情况：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:7000&gt; memory stats</span><br><span class=\"line\"> 1) &quot;peak.allocated&quot;</span><br><span class=\"line\"> 2) (integer) 2228415</span><br><span class=\"line\"> 3) &quot;total.allocated&quot;</span><br><span class=\"line\"> 4) (integer) 2228474</span><br><span class=\"line\"> 5) &quot;startup.allocated&quot;</span><br><span class=\"line\"> 6) (integer) 1054396</span><br><span class=\"line\"> 7) &quot;replication.backlog&quot;</span><br><span class=\"line\"> 8) (integer) 1048584</span><br><span class=\"line\"> 9) &quot;clients.slaves&quot;</span><br><span class=\"line\">10) (integer) 16858</span><br><span class=\"line\">11) &quot;clients.normal&quot;</span><br><span class=\"line\">12) (integer) 49630</span><br><span class=\"line\">13) &quot;aof.buffer&quot;</span><br><span class=\"line\">14) (integer) 0</span><br><span class=\"line\">15) &quot;db.0&quot;</span><br><span class=\"line\">16) 1) &quot;overhead.hashtable.main&quot;</span><br><span class=\"line\">    2) (integer) 112</span><br><span class=\"line\">    3) &quot;overhead.hashtable.expires&quot;</span><br><span class=\"line\">    4) (integer) 0</span><br><span class=\"line\">17) &quot;overhead.total&quot;</span><br><span class=\"line\">18) (integer) 2169580</span><br><span class=\"line\">19) &quot;keys.count&quot;</span><br><span class=\"line\">20) (integer) 2</span><br><span class=\"line\">21) &quot;keys.bytes-per-key&quot;</span><br><span class=\"line\">22) (integer) 587039</span><br><span class=\"line\">23) &quot;dataset.bytes&quot;</span><br><span class=\"line\">24) (integer) 58894</span><br><span class=\"line\">25) &quot;dataset.percentage&quot;</span><br><span class=\"line\">26) &quot;5.0161914825439453&quot;</span><br><span class=\"line\">27) &quot;peak.percentage&quot;</span><br><span class=\"line\">28) &quot;100.00264739990234&quot;</span><br><span class=\"line\">29) &quot;fragmentation&quot;</span><br><span class=\"line\">30) &quot;1.2773568630218506&quot;</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY PURGE 子命令可以要求分配器释放更多内存：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; MEMORY PURGE</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY MALLOC-STATS 子命令可以展示分配器内部状态：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; MEMORY MALLOC-STATS</span><br><span class=\"line\">Stats not supported for the current allocator</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-10-兼容NAT和Docker\"><a href=\"#4-10-兼容NAT和Docker\" class=\"headerlink\" title=\"4.10 兼容NAT和Docker\"></a>4.10 兼容NAT和Docker</h3><p>Redis 4.0 将兼容 NAT 和 Docker ， 具体的使用方法在 redis.conf 中有记载：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">########################## CLUSTER DOCKER/NAT support  ########################</span><br><span class=\"line\"># In certain deployments, Redis Cluster nodes address discovery fails, because</span><br><span class=\"line\"># addresses are NAT-ted or because ports are forwarded (the typical case is</span><br><span class=\"line\"># Docker and other containers).</span><br><span class=\"line\">#</span><br><span class=\"line\"># In order to make Redis Cluster working in such environments, a static</span><br><span class=\"line\"># configuration where each node known its public address is needed. The</span><br><span class=\"line\"># following two options are used for this scope, and are:</span><br><span class=\"line\">#</span><br><span class=\"line\"># * cluster-announce-ip</span><br><span class=\"line\"># * cluster-announce-port</span><br><span class=\"line\"># * cluster-announce-bus-port</span><br><span class=\"line\">#</span><br><span class=\"line\"># Each instruct the node about its address, client port, and cluster message</span><br><span class=\"line\"># bus port. The information is then published in the header of the bus packets</span><br><span class=\"line\"># so that other nodes will be able to correctly map the address of the node</span><br><span class=\"line\"># publishing the information.</span><br><span class=\"line\">#</span><br><span class=\"line\"># If the above options are not used, the normal Redis Cluster auto-detection</span><br><span class=\"line\"># will be used instead.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Note that when remapped, the bus port may not be at the fixed offset of</span><br><span class=\"line\"># clients port + 10000, so you can specify any port and bus-port depending</span><br><span class=\"line\"># on how they get remapped. If the bus-port is not set, a fixed offset of</span><br><span class=\"line\"># 10000 will be used as usually.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Example:</span><br><span class=\"line\">#</span><br><span class=\"line\"># cluster-announce-ip 10.1.1.5</span><br><span class=\"line\"># cluster-announce-port 6379</span><br><span class=\"line\"># cluster-announce-bus-port 6380</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-引用\"><a href=\"#5-引用\" class=\"headerlink\" title=\"5. 引用\"></a>5. 引用</h2><ol>\n<li><a href=\"http://blog.csdn.net/yin767833376/article/details/53518272\" target=\"_blank\" rel=\"noopener\">Redis 4.0新功能介绍</a></li>\n<li><a href=\"https://github.com/antirez/redis/blob/05b81d2b02578d432329c87c93f975e582d14c0e/00-RELEASENOTES\" target=\"_blank\" rel=\"noopener\">Redis 4.0 release note</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Redis4-0-新特性尝鲜\"><a href=\"#Redis4-0-新特性尝鲜\" class=\"headerlink\" title=\"Redis4.0 新特性尝鲜\"></a>Redis4.0 新特性尝鲜</h1><h2 id=\"1-目录结构\"><a href=\"#1-目录结构\" class=\"headerlink\" title=\"1.目录结构\"></a>1.目录结构</h2><ol>\n<li><a href=\"#1-目录结构\">目录</a></li>\n<li><a href=\"#2-前言\">前言</a></li>\n<li><a href=\"#3-环境搭建\">环境搭建</a><ol>\n<li><a href=\"#31-下载\">下载</a></li>\n<li><a href=\"#32-编译安装\">安装</a></li>\n<li><a href=\"#33-新建集群\">集群搭建</a></li>\n</ol>\n</li>\n<li><a href=\"#4-特性尝鲜\">特性尝鲜</a><ol>\n<li><a href=\"#41-注意事项\">注意事项</a></li>\n<li><a href=\"#42-升级内容\">升级内容</a></li>\n<li><a href=\"#43-模块系统\">模块系统</a></li>\n<li><a href=\"#44-PSYNC\">PSYNC 2.0</a></li>\n<li><a href=\"#45-缓存驱逐策略优化\">缓存驱逐策略优化</a></li>\n<li><a href=\"#46-\">非阻塞 DEL 、 FLUSHDB 和 FLUSHALL</a></li>\n<li><a href=\"#47-交换数据库\">交换数据库</a></li>\n<li><a href=\"#48-\">混合RDBAOF持久化格式</a></li>\n<li><a href=\"#49-内存命令\">内存命令</a></li>\n<li><a href=\"#410-兼容NAT和Docker\">兼容 NAT 和 Docker</a></li>\n</ol>\n</li>\n<li><a href=\"#5-引用\">引用</a></li>\n</ol>\n<h2 id=\"2-前言\"><a href=\"#2-前言\" class=\"headerlink\" title=\"2. 前言\"></a>2. 前言</h2><p><strong>2017-07-14 redis 4.0  Stable  version release</strong>,新增了许多新功能，此次专门抽出时间，探索一些功能，把握Redis未来的发展方向，同时积累经验，为未来升级Redis打下坚实基础。学习新的东西，如果不将它记录下来，过上两周，基本上就忘记做过什么了，对知识的掌握不利，以后尽量所有的学习都能产生文字记录，便于自己总结学习各种技术，也能给新人带去一点便利。</p>\n<h2 id=\"3-环境搭建\"><a href=\"#3-环境搭建\" class=\"headerlink\" title=\"3. 环境搭建\"></a>3. 环境搭建</h2><p>安装方式，较之前没有多大变化，还是写一下，便于新手学习。</p>\n<h3 id=\"3-1-下载\"><a href=\"#3-1-下载\" class=\"headerlink\" title=\"3.1. 下载\"></a>3.1. 下载</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz</span><br><span class=\"line\">$ tar xzf redis-4.0.0.tar.gz</span><br></pre></td></tr></table></figure>\n<h3 id=\"3-2-编译安装\"><a href=\"#3-2-编译安装\" class=\"headerlink\" title=\"3.2. 编译安装\"></a>3.2. 编译安装</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cd redis-4.0.0</span><br><span class=\"line\">$ make</span><br></pre></td></tr></table></figure>\n<p>如果需要将redis-cli,redis-server等相关命令安装到/bin目录下，全局使用的话，可以使用如下命令：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ make install</span><br></pre></td></tr></table></figure></p>\n<p>或者利用软连接实现：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ make</span><br><span class=\"line\">$ ln -s  redis-4.0.0/src/redis-server  /bin/redis-server</span><br><span class=\"line\">$ ln -s  redis-4.0.0/src/redis-cli  /bin/redis-cli</span><br></pre></td></tr></table></figure></p>\n<p>在编译中可能会遇到如下问题：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zmalloc.h:50:31: 错误：jemalloc/jemalloc.h：没有那个文件或目录</span><br></pre></td></tr></table></figure></p>\n<p>出现这个问题是libc 并不是默认的 分配器， 默认的是 jemalloc, 因为 jemalloc 被证明 有更少的 fragmentation problems 比libc，关于更详细信息可以查看redis中REAME,md，其中有详细介绍，此处不再细说。</p>\n<p>解决办法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make MALLOC=libc</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"3-3-新建集群\"><a href=\"#3-3-新建集群\" class=\"headerlink\" title=\"3.3. 新建集群\"></a>3.3. 新建集群</h3><p>按照官方指导，搭建3m+3s集群，端口号7000-7005</p>\n<p>以下为创建脚本：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"> nodes=(7000 7001 7002 7003 7004 7005)</span><br><span class=\"line\"> HOME_DIR=`pwd`</span><br><span class=\"line\">function create()&#123;</span><br><span class=\"line\"> echo &quot;create&quot;</span><br><span class=\"line\"> if [ -d redis_cluster ];then</span><br><span class=\"line\"> rm -rf  redis_cluster</span><br><span class=\"line\"> fi</span><br><span class=\"line\"></span><br><span class=\"line\"> mkdir  redis_cluster</span><br><span class=\"line\"> cd redis_cluster</span><br><span class=\"line\"></span><br><span class=\"line\"> mkdir `echo $&#123;nodes[*]&#125;`</span><br><span class=\"line\"></span><br><span class=\"line\"> for var in $&#123;nodes[*]&#125;</span><br><span class=\"line\"> do</span><br><span class=\"line\">  cd $var</span><br><span class=\"line\">  touch $var-redis.conf</span><br><span class=\"line\">  echo &quot;port&quot; $var &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-enabled yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-config-file nodes.conf&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;cluster-node-timeout 5000&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;appendonly yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;daemonize yes&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  echo &quot;pidfile redis.pid&quot; &gt;&gt; $var-redis.conf</span><br><span class=\"line\">  cd ../</span><br><span class=\"line\"> done</span><br><span class=\"line\"> run;</span><br><span class=\"line\"> src/redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">function run()&#123;</span><br><span class=\"line\">echo run</span><br><span class=\"line\"> for var in $&#123;nodes[*]&#125;</span><br><span class=\"line\"> do</span><br><span class=\"line\">   cd $HOME_DIR/redis_cluster/$var/</span><br><span class=\"line\">   redis-server $var-redis.conf</span><br><span class=\"line\">   cd $HOME_DIR/</span><br><span class=\"line\">   echo Start redis ports $&#123;var&#125; finish.</span><br><span class=\"line\"> done</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">function stop()&#123;</span><br><span class=\"line\"> cd $HOME_DIR/redis_cluster/</span><br><span class=\"line\"> DIRS=`ls -l | grep &quot;^d&quot; | awk &apos;&#123;print $NF&#125;&apos;`</span><br><span class=\"line\">  for d in $DIRS</span><br><span class=\"line\">  do</span><br><span class=\"line\">    PID=`cat $d/redis.pid`</span><br><span class=\"line\">    if [ -n &quot;$PID&quot; ];then</span><br><span class=\"line\">       kill -9 $PID</span><br><span class=\"line\">       rm -f $d/redis.pid</span><br><span class=\"line\">    fi</span><br><span class=\"line\">  done</span><br><span class=\"line\">  cd  $HOME_DIR/</span><br><span class=\"line\">  echo Stop redis ports $&#123;DIRS&#125; finish.</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">case $1 in</span><br><span class=\"line\">     create)</span><br><span class=\"line\">      echo &quot; create redis cluster 锛?000,7001,7002,7003,7004.7005,7006,7007&quot;</span><br><span class=\"line\">      create</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">     run)</span><br><span class=\"line\">      run</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">     stop)</span><br><span class=\"line\">      stop</span><br><span class=\"line\">     ;;</span><br><span class=\"line\">     *)</span><br><span class=\"line\">      echo &quot;Usage:[create|run|stop]&quot;</span><br><span class=\"line\">      ;;</span><br><span class=\"line\">esac</span><br></pre></td></tr></table></figure>\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh createCluster.sh create</span><br></pre></td></tr></table></figure>\n<p>一路回车即可，注意构建集群需要使用redis-trib.rb，需要额外执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem install redis</span><br></pre></td></tr></table></figure>\n<p>结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[bigdata@trumanlab1 redis-4.0.0]$ redis-cli -c -h 127.0.0.1  -p 7000</span><br><span class=\"line\">127.0.0.1:7000&gt; cluster nodes</span><br><span class=\"line\">63e698210378f4fda23c4070bea3b46f93952811 127.0.0.1:7000@17000 myself,master - 0 1500391716000 1 connected 0-5460</span><br><span class=\"line\">4091630217b64ece9a6fa55c26687a10c1f9a8b5 127.0.0.1:7001@17001 master - 0 1500391717501 2 connected 5461-10922</span><br><span class=\"line\">0b23853c3aa4f5b942a6239bc7bca71df36e121c 127.0.0.1:7005@17005 slave 12d05fb7c20bf001759f80cf3f65ad9df4b01af5 0 1500391716498 6 connected</span><br><span class=\"line\">12d05fb7c20bf001759f80cf3f65ad9df4b01af5 127.0.0.1:7002@17002 master - 0 1500391717000 3 connected 10923-16383</span><br><span class=\"line\">2e559b462362a9563f98b398af3984c05e25ef19 127.0.0.1:7003@17003 slave 63e698210378f4fda23c4070bea3b46f93952811 0 1500391716000 4 connected</span><br><span class=\"line\">c4f8e0a95579c772c7506c195b3c7984c73312b4 127.0.0.1:7004@17004 slave 4091630217b64ece9a6fa55c26687a10c1f9a8b5 0 1500391716000 5 connected</span><br><span class=\"line\">127.0.0.1:7000&gt; set a a</span><br><span class=\"line\">-&gt; Redirected to slot [15495] located at 127.0.0.1:7002</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">&quot;a&quot;</span><br><span class=\"line\">127.0.0.1:7002&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-特性尝鲜\"><a href=\"#4-特性尝鲜\" class=\"headerlink\" title=\"4. 特性尝鲜\"></a>4. 特性尝鲜</h2><h3 id=\"4-1-注意事项\"><a href=\"#4-1-注意事项\" class=\"headerlink\" title=\"4.1 注意事项\"></a>4.1 注意事项</h3><blockquote>\n<p>IMPORTANT: Redis Cluster users, please note that, as specified in the list<br>of incompatibilities, Redis 4.0 cluster bus protocol is not compatible with<br>Redis 3.2, so in order to upgrade, a mass reboot of the instances is needed<br>and rolling upgrades are not possible. This change was needed in order to<br>add compatibility for Containers/NAT, where the bus port at a fixed offset<br>was not an acceptable design, so we had to change many things, resulting<br>in the incompatible protocol.</p>\n</blockquote>\n<p>4.0版本与3.2版本传输协议不兼容，更改该协议的目的是为了实现Containers/NAT<br>为了升级，需要重启大量节点，无法做到滚动升级。</p>\n<h3 id=\"4-2-升级内容\"><a href=\"#4-2-升级内容\" class=\"headerlink\" title=\"4.2 升级内容\"></a>4.2 升级内容</h3><blockquote>\n<ul>\n<li>Different replication fixes to PSYNC2, the new 4.0 replication engine.</li>\n<li>Modules thread safe contexts were introduced. They are an &gt;experimental API right now, but the API is considered to be stable and usable when needed.</li>\n<li>SLOWLOG now logs the offending client name and address. Note that this is a backward compatibility breakage in case old code assumes that the slowlog entry is composed of exactly three entries.</li>\n<li>The modules native data types RDB format changed.</li>\n<li>The AOF check utility is now able to deal with RDB preambles.</li>\n<li>GEORADIUS_RO and GEORADIUSBYMEMBER_RO variants, not supporting the STORE option, were added in order to allow read-only scaling of such queries.</li>\n<li>HSET is now variadic, and HMSET is considered deprecated (but will be supported for years to come). Please use HSET in new code.</li>\n<li>GEORADIUS huge radius (&gt;= ~6000 km) corner cases fixed, certain elements near the edges were not returned.</li>\n<li>DEBUG DIGEST modules API added.</li>\n<li>HyperLogLog commands no longer crash on certain input (non HLL) strings.</li>\n<li>Fixed SLAVEOF inside MULTI/EXEC blocks.</li>\n<li>Many other minor bug fixes and improvements.</li>\n</ul>\n</blockquote>\n<h3 id=\"4-3-模块系统\"><a href=\"#4-3-模块系统\" class=\"headerlink\" title=\"4.3 模块系统\"></a>4.3 模块系统</h3><p>Redis 4.0 发生的最大变化就是加入了模块系统， 这个系统可以让用户通过自己编写的代码来扩展和实现 Redis 本身并不具备的功能， 具体使用方法可以参考 antirez 的博文《Redis Loadable Module System》： <a href=\"http://antirez.com/news/106\" target=\"_blank\" rel=\"noopener\">http://antirez.com/news/106</a><br>因为模块系统是通过高层次 API 实现的， 它与 Redis 内核本身完全分离、互不干扰， 所以用户可以在有需要的情况下才启用这个功能， 以下是 redis.conf 中记载的模块载入方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">################################## MODULES #####################################</span><br><span class=\"line\"></span><br><span class=\"line\"># Load modules at startup. If the server is not able to load modules</span><br><span class=\"line\"># it will abort. It is possible to use multiple loadmodule directives.</span><br><span class=\"line\">#</span><br><span class=\"line\"># loadmodule /path/to/my_module.so</span><br><span class=\"line\"># loadmodule /path/to/other_module.so</span><br></pre></td></tr></table></figure></p>\n<p>目前已经有人使用这个功能开发了各种各样的模块， 比如 Redis Labs 开发的一些模块就可以在 <a href=\"http://redismodules.com\" target=\"_blank\" rel=\"noopener\">http://redismodules.com</a> 看到， 此外 antirez 自己也使用这个功能开发了一个神经网络模块： <a href=\"https://github.com/antirez/neural-redis\" target=\"_blank\" rel=\"noopener\">https://github.com/antirez/neural-redis</a><br>模块功能使得用户可以将 Redis 用作基础设施， 并在上面构建更多功能， 这给 Redis 带来了无数新的可能性。</p>\n<h3 id=\"4-4-PSYNC-2-0\"><a href=\"#4-4-PSYNC-2-0\" class=\"headerlink\" title=\"4.4 PSYNC 2.0\"></a>4.4 PSYNC 2.0</h3><p>新版本的 PSYNC 命令解决了旧版本的 Redis 在复制时的一些不够优化的地方：</p>\n<ul>\n<li>在旧版本 Redis 中， 如果一个从服务器在 FAILOVER 之后成为了新的主节点， 那么其他从节点在复制这个新主的时候就必须进行全量复制。 在 Redis 4.0 中， 新主和从服务器在处理这种情况时， 将在条件允许的情况下使用部分复制。</li>\n<li>在旧版本 Redis 中， 一个从服务器如果重启了， 那么它就必须与主服务器重新进行全量复制， 在 Redis 4.0 中， 只要条件允许， 主从在处理这种情况时将使用部分复制。</li>\n</ul>\n<h3 id=\"4-5-缓存驱逐策略优化\"><a href=\"#4-5-缓存驱逐策略优化\" class=\"headerlink\" title=\"4.5 缓存驱逐策略优化\"></a>4.5 缓存驱逐策略优化</h3><p>新添加了 Last Frequently Used 缓存驱逐策略， 具体信息见 antirez 的博文《Random notes on improving the Redis LRU algorithm》： <a href=\"http://antirez.com/news/109\" target=\"_blank\" rel=\"noopener\">http://antirez.com/news/109</a><br>另外 Redis 4.0 还对已有的缓存驱逐策略进行了优化， 使得它们能够更健壮、高效、快速和精确。</p>\n<h3 id=\"4-6-非阻塞-DEL-、-FLUSHDB-和-FLUSHALL\"><a href=\"#4-6-非阻塞-DEL-、-FLUSHDB-和-FLUSHALL\" class=\"headerlink\" title=\"4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL\"></a>4.6 非阻塞 DEL 、 FLUSHDB 和 FLUSHALL</h3><p>在 Redis 4.0 之前， 用户在使用 DEL 命令删除体积较大的键， 又或者在使用 FLUSHDB 和 FLUSHALL 删除包含大量键的数据库时， 都可能会造成服务器阻塞。<br>为了解决以上问题， Redis 4.0 新添加了 UNLINK 命令， 这个命令是 DEL 命令的异步版本， 它可以将删除指定键的操作放在后台线程里面执行， 从而尽可能地避免服务器阻塞：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">127.0.0.1:7002&gt; set a test</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">&quot;test&quot;</span><br><span class=\"line\">127.0.0.1:7002&gt; unlink a</span><br><span class=\"line\">(integer) 1</span><br><span class=\"line\">127.0.0.1:7002&gt; get a</span><br><span class=\"line\">(nil)</span><br></pre></td></tr></table></figure></p>\n<p>因为一些历史原因， 执行同步删除操作的 DEL 命令将会继续保留。<br>此外， Redis 4.0 中的 FLUSHDB 和 FLUSHALL 这两个命令都新添加了 ASYNC 选项， 带有这个选项的数据库删除操作将在后台线程进行：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; FLUSHDB ASYNC</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; FLUSHALL ASYNC</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-7-交换数据库\"><a href=\"#4-7-交换数据库\" class=\"headerlink\" title=\"4.7 交换数据库\"></a>4.7 交换数据库</h3><p>Redis 4.0 对数据库命令的另外一个修改是新增了 SWAPDB 命令， 这个命令可以对指定的两个数据库进行互换： 比如说， 通过执行命令 SWAPDB 0 1 ， 我们可以将原来的数据库 0 变成数据库 1 ， 而原来的数据库 1 则变成数据库 0 。这种场景是在非集群模式下使用的，在集群模式是不支持切换数据库的。默认所有的节点使用index 0。<br>以下是一个使用 SWAPDB 的例子：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; SET your_name &quot;aibibang&quot;  -- 在数据库 0 中设置一个键</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; GET your_name</span><br><span class=\"line\">&quot;aibibang&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; SWAPDB 0 1  -- 互换数据库 0 和数据库 1</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; GET your_name  -- 现在的数据库 0 已经没有之前设置的键了</span><br><span class=\"line\">(nil)</span><br><span class=\"line\"></span><br><span class=\"line\">redis&gt; SELECT 1  -- 切换到数据库 1</span><br><span class=\"line\">OK</span><br><span class=\"line\"></span><br><span class=\"line\">redis[1]&gt; GET your_name  -- 之前在数据库 0 设置的键现在可以在数据库 1 找到</span><br><span class=\"line\">&quot;aibibang&quot;                 -- 证明两个数据库已经互换</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-8-混合-RDB-AOF-持久化格式\"><a href=\"#4-8-混合-RDB-AOF-持久化格式\" class=\"headerlink\" title=\"4.8 混合 RDB-AOF 持久化格式\"></a>4.8 混合 RDB-AOF 持久化格式</h3><p>Redis 4.0 新增了 RDB-AOF 混合持久化格式， 这是一个可选的功能， 在开启了这个功能之后， AOF 重写产生的文件将同时包含 RDB 格式的内容和 AOF 格式的内容， 其中 RDB 格式的内容用于记录已有的数据， 而 AOF 格式的内存则用于记录最近发生了变化的数据， 这样 Redis 就可以同时兼有 RDB 持久化和 AOF 持久化的优点 —— 既能够快速地生成重写文件， 也能够在出现问题时， 快速地载入数据。<br>这个功能可以通过 aof-use-rdb-preamble 选项进行开启， redis.conf 文件中记录了这个选项的使用方法：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> When rewriting the AOF file, Redis is able to use an RDB preamble in the</span><br><span class=\"line\"># AOF file for faster rewrites and recoveries. When this option is turned</span><br><span class=\"line\"># on the rewritten AOF file is composed of two different stanzas:</span><br><span class=\"line\">#</span><br><span class=\"line\">#   [RDB file][AOF tail]</span><br><span class=\"line\">#</span><br><span class=\"line\"># When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;</span><br><span class=\"line\"># string and loads the prefixed RDB file, and continues loading the AOF</span><br><span class=\"line\"># tail.</span><br><span class=\"line\">#</span><br><span class=\"line\"># This is currently turned off by default in order to avoid the surprise</span><br><span class=\"line\"># of a format change, but will at some point be used as the default.</span><br><span class=\"line\">aof-use-rdb-preamble no</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-9-内存命令\"><a href=\"#4-9-内存命令\" class=\"headerlink\" title=\"4.9 内存命令\"></a>4.9 内存命令</h3><p>新添加了一个 MEMORY 命令， 这个命令可以用于视察内存使用情况， 并进行相应的内存管理操作：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:7002&gt; memory help</span><br><span class=\"line\">1) &quot;MEMORY USAGE &lt;key&gt; [SAMPLES &lt;count&gt;] - Estimate memory usage of key&quot;</span><br><span class=\"line\">2) &quot;MEMORY STATS                         - Show memory usage details&quot;</span><br><span class=\"line\">3) &quot;MEMORY PURGE                         - Ask the allocator to release memory&quot;</span><br><span class=\"line\">4) &quot;MEMORY MALLOC-STATS                  - Show allocator internal stats&quot;</span><br></pre></td></tr></table></figure></p>\n<p>其中， 使用 MEMORY USAGE 子命令可以估算储存给定键所需的内存：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">127.0.0.1:7002&gt; set truman aibibang</span><br><span class=\"line\">-&gt; Redirected to slot [2113] located at 127.0.0.1:7000</span><br><span class=\"line\">OK</span><br><span class=\"line\">127.0.0.1:7000&gt; memory usage truman</span><br><span class=\"line\">(integer) 59</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY STATS 子命令可以查看 Redis 当前的内存使用情况：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">127.0.0.1:7000&gt; memory stats</span><br><span class=\"line\"> 1) &quot;peak.allocated&quot;</span><br><span class=\"line\"> 2) (integer) 2228415</span><br><span class=\"line\"> 3) &quot;total.allocated&quot;</span><br><span class=\"line\"> 4) (integer) 2228474</span><br><span class=\"line\"> 5) &quot;startup.allocated&quot;</span><br><span class=\"line\"> 6) (integer) 1054396</span><br><span class=\"line\"> 7) &quot;replication.backlog&quot;</span><br><span class=\"line\"> 8) (integer) 1048584</span><br><span class=\"line\"> 9) &quot;clients.slaves&quot;</span><br><span class=\"line\">10) (integer) 16858</span><br><span class=\"line\">11) &quot;clients.normal&quot;</span><br><span class=\"line\">12) (integer) 49630</span><br><span class=\"line\">13) &quot;aof.buffer&quot;</span><br><span class=\"line\">14) (integer) 0</span><br><span class=\"line\">15) &quot;db.0&quot;</span><br><span class=\"line\">16) 1) &quot;overhead.hashtable.main&quot;</span><br><span class=\"line\">    2) (integer) 112</span><br><span class=\"line\">    3) &quot;overhead.hashtable.expires&quot;</span><br><span class=\"line\">    4) (integer) 0</span><br><span class=\"line\">17) &quot;overhead.total&quot;</span><br><span class=\"line\">18) (integer) 2169580</span><br><span class=\"line\">19) &quot;keys.count&quot;</span><br><span class=\"line\">20) (integer) 2</span><br><span class=\"line\">21) &quot;keys.bytes-per-key&quot;</span><br><span class=\"line\">22) (integer) 587039</span><br><span class=\"line\">23) &quot;dataset.bytes&quot;</span><br><span class=\"line\">24) (integer) 58894</span><br><span class=\"line\">25) &quot;dataset.percentage&quot;</span><br><span class=\"line\">26) &quot;5.0161914825439453&quot;</span><br><span class=\"line\">27) &quot;peak.percentage&quot;</span><br><span class=\"line\">28) &quot;100.00264739990234&quot;</span><br><span class=\"line\">29) &quot;fragmentation&quot;</span><br><span class=\"line\">30) &quot;1.2773568630218506&quot;</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY PURGE 子命令可以要求分配器释放更多内存：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; MEMORY PURGE</span><br><span class=\"line\">OK</span><br></pre></td></tr></table></figure></p>\n<p>使用 MEMORY MALLOC-STATS 子命令可以展示分配器内部状态：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redis&gt; MEMORY MALLOC-STATS</span><br><span class=\"line\">Stats not supported for the current allocator</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"4-10-兼容NAT和Docker\"><a href=\"#4-10-兼容NAT和Docker\" class=\"headerlink\" title=\"4.10 兼容NAT和Docker\"></a>4.10 兼容NAT和Docker</h3><p>Redis 4.0 将兼容 NAT 和 Docker ， 具体的使用方法在 redis.conf 中有记载：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">########################## CLUSTER DOCKER/NAT support  ########################</span><br><span class=\"line\"># In certain deployments, Redis Cluster nodes address discovery fails, because</span><br><span class=\"line\"># addresses are NAT-ted or because ports are forwarded (the typical case is</span><br><span class=\"line\"># Docker and other containers).</span><br><span class=\"line\">#</span><br><span class=\"line\"># In order to make Redis Cluster working in such environments, a static</span><br><span class=\"line\"># configuration where each node known its public address is needed. The</span><br><span class=\"line\"># following two options are used for this scope, and are:</span><br><span class=\"line\">#</span><br><span class=\"line\"># * cluster-announce-ip</span><br><span class=\"line\"># * cluster-announce-port</span><br><span class=\"line\"># * cluster-announce-bus-port</span><br><span class=\"line\">#</span><br><span class=\"line\"># Each instruct the node about its address, client port, and cluster message</span><br><span class=\"line\"># bus port. The information is then published in the header of the bus packets</span><br><span class=\"line\"># so that other nodes will be able to correctly map the address of the node</span><br><span class=\"line\"># publishing the information.</span><br><span class=\"line\">#</span><br><span class=\"line\"># If the above options are not used, the normal Redis Cluster auto-detection</span><br><span class=\"line\"># will be used instead.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Note that when remapped, the bus port may not be at the fixed offset of</span><br><span class=\"line\"># clients port + 10000, so you can specify any port and bus-port depending</span><br><span class=\"line\"># on how they get remapped. If the bus-port is not set, a fixed offset of</span><br><span class=\"line\"># 10000 will be used as usually.</span><br><span class=\"line\">#</span><br><span class=\"line\"># Example:</span><br><span class=\"line\">#</span><br><span class=\"line\"># cluster-announce-ip 10.1.1.5</span><br><span class=\"line\"># cluster-announce-port 6379</span><br><span class=\"line\"># cluster-announce-bus-port 6380</span><br></pre></td></tr></table></figure>\n<h2 id=\"5-引用\"><a href=\"#5-引用\" class=\"headerlink\" title=\"5. 引用\"></a>5. 引用</h2><ol>\n<li><a href=\"http://blog.csdn.net/yin767833376/article/details/53518272\" target=\"_blank\" rel=\"noopener\">Redis 4.0新功能介绍</a></li>\n<li><a href=\"https://github.com/antirez/redis/blob/05b81d2b02578d432329c87c93f975e582d14c0e/00-RELEASENOTES\" target=\"_blank\" rel=\"noopener\">Redis 4.0 release note</a></li>\n</ol>\n"},{"title":"Spring Cloud简介","date":"2018-07-22T10:46:44.000Z","_content":"# Spring Cloud\n\n## 什么是Spring Cloud\nSpring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如**配置管理**，**服务发现**，**断路器**，**智能路由**，**微代理**，**控制总线**等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。\n\n微服务是可以独立部署、水平扩展、独立访问（或者有独立的数据库）的服务单元，Spring Cloud就是这些微服务的大管家，采用了微服务这种架构之后，项目的数量会非常多，Spring Cloud做为大管家就需要提供各种方案来维护整个生态。\n\nSpring Cloud就是一套分布式服务治理的框架，既然它是一套服务治理的框架，那么它本身不会提供具体功能性的操作，更专注于服务之间的通讯、熔断、监控等。因此就需要很多的组件来支持一套功能。\n## 特性\nSpring Cloud专注于提供良好的开箱即用经验的典型用例和可扩展性机制覆盖。\n\n- 分布式/版本化配置\n- 服务注册和发现\n- 路由\n- service - to - service调用\n- 负载均衡\n- 断路器\n- 全局锁\n- 决策竞选\n- 分布式消息传递\n## Spring Cloud组件架构\n![image](http://www.ityouknow.com/assets/images/2017/springcloud/spring-cloud-architecture.png)\n\n1、外部或者内部的非Spring Cloud项目都统一通过API网关（Zuul）来访问内部服务.\n2、网关接收到请求后，从注册中心（Eureka）获取可用服务\n3、由Ribbon进行均衡负载后，分发到后端的具体实例\n4、微服务之间通过Feign进行通信处理业务\n5、Hystrix负责处理服务超时熔断\n6、Turbine监控服务间的调用和熔断相关指标\n\n\n## 核心成员\n### Spring Cloud Netflix\n这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka, Hystrix, Zuul, Archaius… 太多了\n\n### Netflix Eureka\n\n服务中心，云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。这个可是springcloud最牛鼻的小弟，服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。\n\n### Netflix Hystrix\n\n熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。\n\n### Netflix Zuul\n\nZuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。\n\n### Netflix Archaius\n\n配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。可以实现动态获取配置， 原理是每隔60s（默认，可配置）从配置源读取一次内容，这样修改了配置文件后不需要重启服务就可以使修改后的内容生效，前提使用archaius的API来读取。\n\n### Spring Cloud Config\n俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。\n\n### Spring Cloud Bus\n事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。\n\n### Spring Cloud for Cloud Foundry\nCloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题\n\n其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。\n\n### Spring Cloud Cluster\nSpring Cloud Cluster将取代Spring Integration。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。\n\n如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。\n\n### Spring Cloud Consul\nConsul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.\n\nSpring Cloud Consul 封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。\n\n## 其它小弟\n### Spring Cloud Security\n\n基于spring security的安全工具包，为你的应用程序添加安全控制。这个小弟很牛鼻专门负责整个帮派的安全问题，设置不同的门派访问特定的资源，不能把秘籍葵花宝典泄漏了。\n\n### Spring Cloud Sleuth\n\n日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。\n\n### Spring Cloud Data Flow\n\nData flow 是一个用于开发和执行大范围数据处理其模式包括ETL，批量运算和持续运算的统一编程模型和托管服务。\n\n对于在现代运行环境中可组合的微服务程序来说，Spring Cloud data flow是一个原生云可编配的服务。使用Spring Cloud data flow，开发者可以为像数据抽取，实时分析，和数据导入/导出这种常见用例创建和编配数据通道 （data pipelines）。\n\nSpring Cloud data flow 是基于原生云对 spring XD的重新设计，该项目目标是简化大数据应用的开发。Spring XD 的流处理和批处理模块的重构分别是基于 Spring Boot的stream 和 task/batch 的微服务程序。这些程序现在都是自动部署单元而且他们原生的支持像 Cloud Foundry、Apache YARN、Apache Mesos和Kubernetes 等现代运行环境。\n\nSpring Cloud data flow 为基于微服务的分布式流处理和批处理数据通道提供了一系列模型和最佳实践。\n\n### Spring Cloud Stream\n\nSpring Cloud Stream是创建消息驱动微服务应用的框架。Spring Cloud Stream是基于Spring Boot创建，用来建立单独的／工业级spring应用，使用spring integration提供与消息代理之间的连接。数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。\n\n一个业务会牵扯到多个任务，任务之间是通过事件触发的，这就是Spring Cloud stream要干的事了\n\n### Spring Cloud Task\n\nSpring Cloud Task 主要解决短命微服务的任务管理，任务调度的工作，比如说某些定时任务晚上就跑一次，或者某项数据分析临时就跑几次。\n\n### Spring Cloud Zookeeper\n\nZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。\n\n操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理，抱了Zookeeper的大腿。\n\n### Spring Cloud Connectors\n\nSpring Cloud Connectors 简化了连接到服务的过程和从云平台获取操作的过程，有很强的扩展性，可以利用Spring Cloud Connectors来构建你自己的云平台。\n\n便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。\n\n### Spring Cloud Starters\n\nSpring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。\n\n### Spring Cloud CLI\n\n基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。\n\n\n\n## 参考\n1.[Spring Cloud在国内中小型公司能用起来吗？](https://www.cnblogs.com/ityouknow/p/7508306.html)\n\n2.[Spring Cloud Dalston中文文档](https://springcloud.cc/spring-cloud-dalston.html)\n\n3.[spring-cloud](http://projects.spring.io/spring-cloud/)","source":"_posts/Spring-Cloud简介.md","raw":"---\ntitle: Spring Cloud简介\ndate: 2018-07-22 18:46:44\ntags: 开发笔记\ncategories:\n         - springcloud\n         - java\n         - 分布式框架\n---\n# Spring Cloud\n\n## 什么是Spring Cloud\nSpring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如**配置管理**，**服务发现**，**断路器**，**智能路由**，**微代理**，**控制总线**等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。\n\n微服务是可以独立部署、水平扩展、独立访问（或者有独立的数据库）的服务单元，Spring Cloud就是这些微服务的大管家，采用了微服务这种架构之后，项目的数量会非常多，Spring Cloud做为大管家就需要提供各种方案来维护整个生态。\n\nSpring Cloud就是一套分布式服务治理的框架，既然它是一套服务治理的框架，那么它本身不会提供具体功能性的操作，更专注于服务之间的通讯、熔断、监控等。因此就需要很多的组件来支持一套功能。\n## 特性\nSpring Cloud专注于提供良好的开箱即用经验的典型用例和可扩展性机制覆盖。\n\n- 分布式/版本化配置\n- 服务注册和发现\n- 路由\n- service - to - service调用\n- 负载均衡\n- 断路器\n- 全局锁\n- 决策竞选\n- 分布式消息传递\n## Spring Cloud组件架构\n![image](http://www.ityouknow.com/assets/images/2017/springcloud/spring-cloud-architecture.png)\n\n1、外部或者内部的非Spring Cloud项目都统一通过API网关（Zuul）来访问内部服务.\n2、网关接收到请求后，从注册中心（Eureka）获取可用服务\n3、由Ribbon进行均衡负载后，分发到后端的具体实例\n4、微服务之间通过Feign进行通信处理业务\n5、Hystrix负责处理服务超时熔断\n6、Turbine监控服务间的调用和熔断相关指标\n\n\n## 核心成员\n### Spring Cloud Netflix\n这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka, Hystrix, Zuul, Archaius… 太多了\n\n### Netflix Eureka\n\n服务中心，云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。这个可是springcloud最牛鼻的小弟，服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。\n\n### Netflix Hystrix\n\n熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。\n\n### Netflix Zuul\n\nZuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。\n\n### Netflix Archaius\n\n配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。可以实现动态获取配置， 原理是每隔60s（默认，可配置）从配置源读取一次内容，这样修改了配置文件后不需要重启服务就可以使修改后的内容生效，前提使用archaius的API来读取。\n\n### Spring Cloud Config\n俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。\n\n### Spring Cloud Bus\n事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。\n\n### Spring Cloud for Cloud Foundry\nCloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题\n\n其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。\n\n### Spring Cloud Cluster\nSpring Cloud Cluster将取代Spring Integration。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。\n\n如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。\n\n### Spring Cloud Consul\nConsul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.\n\nSpring Cloud Consul 封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。\n\n## 其它小弟\n### Spring Cloud Security\n\n基于spring security的安全工具包，为你的应用程序添加安全控制。这个小弟很牛鼻专门负责整个帮派的安全问题，设置不同的门派访问特定的资源，不能把秘籍葵花宝典泄漏了。\n\n### Spring Cloud Sleuth\n\n日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。\n\n### Spring Cloud Data Flow\n\nData flow 是一个用于开发和执行大范围数据处理其模式包括ETL，批量运算和持续运算的统一编程模型和托管服务。\n\n对于在现代运行环境中可组合的微服务程序来说，Spring Cloud data flow是一个原生云可编配的服务。使用Spring Cloud data flow，开发者可以为像数据抽取，实时分析，和数据导入/导出这种常见用例创建和编配数据通道 （data pipelines）。\n\nSpring Cloud data flow 是基于原生云对 spring XD的重新设计，该项目目标是简化大数据应用的开发。Spring XD 的流处理和批处理模块的重构分别是基于 Spring Boot的stream 和 task/batch 的微服务程序。这些程序现在都是自动部署单元而且他们原生的支持像 Cloud Foundry、Apache YARN、Apache Mesos和Kubernetes 等现代运行环境。\n\nSpring Cloud data flow 为基于微服务的分布式流处理和批处理数据通道提供了一系列模型和最佳实践。\n\n### Spring Cloud Stream\n\nSpring Cloud Stream是创建消息驱动微服务应用的框架。Spring Cloud Stream是基于Spring Boot创建，用来建立单独的／工业级spring应用，使用spring integration提供与消息代理之间的连接。数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。\n\n一个业务会牵扯到多个任务，任务之间是通过事件触发的，这就是Spring Cloud stream要干的事了\n\n### Spring Cloud Task\n\nSpring Cloud Task 主要解决短命微服务的任务管理，任务调度的工作，比如说某些定时任务晚上就跑一次，或者某项数据分析临时就跑几次。\n\n### Spring Cloud Zookeeper\n\nZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。\n\n操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理，抱了Zookeeper的大腿。\n\n### Spring Cloud Connectors\n\nSpring Cloud Connectors 简化了连接到服务的过程和从云平台获取操作的过程，有很强的扩展性，可以利用Spring Cloud Connectors来构建你自己的云平台。\n\n便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。\n\n### Spring Cloud Starters\n\nSpring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。\n\n### Spring Cloud CLI\n\n基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。\n\n\n\n## 参考\n1.[Spring Cloud在国内中小型公司能用起来吗？](https://www.cnblogs.com/ityouknow/p/7508306.html)\n\n2.[Spring Cloud Dalston中文文档](https://springcloud.cc/spring-cloud-dalston.html)\n\n3.[spring-cloud](http://projects.spring.io/spring-cloud/)","slug":"Spring-Cloud简介","published":1,"updated":"2018-07-22T11:35:12.627Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgf00ae8ceem373fd1t","content":"<h1 id=\"Spring-Cloud\"><a href=\"#Spring-Cloud\" class=\"headerlink\" title=\"Spring Cloud\"></a>Spring Cloud</h1><h2 id=\"什么是Spring-Cloud\"><a href=\"#什么是Spring-Cloud\" class=\"headerlink\" title=\"什么是Spring Cloud\"></a>什么是Spring Cloud</h2><p>Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如<strong>配置管理</strong>，<strong>服务发现</strong>，<strong>断路器</strong>，<strong>智能路由</strong>，<strong>微代理</strong>，<strong>控制总线</strong>等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。</p>\n<p>微服务是可以独立部署、水平扩展、独立访问（或者有独立的数据库）的服务单元，Spring Cloud就是这些微服务的大管家，采用了微服务这种架构之后，项目的数量会非常多，Spring Cloud做为大管家就需要提供各种方案来维护整个生态。</p>\n<p>Spring Cloud就是一套分布式服务治理的框架，既然它是一套服务治理的框架，那么它本身不会提供具体功能性的操作，更专注于服务之间的通讯、熔断、监控等。因此就需要很多的组件来支持一套功能。</p>\n<h2 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h2><p>Spring Cloud专注于提供良好的开箱即用经验的典型用例和可扩展性机制覆盖。</p>\n<ul>\n<li>分布式/版本化配置</li>\n<li>服务注册和发现</li>\n<li>路由</li>\n<li>service - to - service调用</li>\n<li>负载均衡</li>\n<li>断路器</li>\n<li>全局锁</li>\n<li>决策竞选</li>\n<li>分布式消息传递<h2 id=\"Spring-Cloud组件架构\"><a href=\"#Spring-Cloud组件架构\" class=\"headerlink\" title=\"Spring Cloud组件架构\"></a>Spring Cloud组件架构</h2><img src=\"http://www.ityouknow.com/assets/images/2017/springcloud/spring-cloud-architecture.png\" alt=\"image\"></li>\n</ul>\n<p>1、外部或者内部的非Spring Cloud项目都统一通过API网关（Zuul）来访问内部服务.<br>2、网关接收到请求后，从注册中心（Eureka）获取可用服务<br>3、由Ribbon进行均衡负载后，分发到后端的具体实例<br>4、微服务之间通过Feign进行通信处理业务<br>5、Hystrix负责处理服务超时熔断<br>6、Turbine监控服务间的调用和熔断相关指标</p>\n<h2 id=\"核心成员\"><a href=\"#核心成员\" class=\"headerlink\" title=\"核心成员\"></a>核心成员</h2><h3 id=\"Spring-Cloud-Netflix\"><a href=\"#Spring-Cloud-Netflix\" class=\"headerlink\" title=\"Spring Cloud Netflix\"></a>Spring Cloud Netflix</h3><p>这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka, Hystrix, Zuul, Archaius… 太多了</p>\n<h3 id=\"Netflix-Eureka\"><a href=\"#Netflix-Eureka\" class=\"headerlink\" title=\"Netflix Eureka\"></a>Netflix Eureka</h3><p>服务中心，云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。这个可是springcloud最牛鼻的小弟，服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。</p>\n<h3 id=\"Netflix-Hystrix\"><a href=\"#Netflix-Hystrix\" class=\"headerlink\" title=\"Netflix Hystrix\"></a>Netflix Hystrix</h3><p>熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。</p>\n<h3 id=\"Netflix-Zuul\"><a href=\"#Netflix-Zuul\" class=\"headerlink\" title=\"Netflix Zuul\"></a>Netflix Zuul</h3><p>Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。</p>\n<h3 id=\"Netflix-Archaius\"><a href=\"#Netflix-Archaius\" class=\"headerlink\" title=\"Netflix Archaius\"></a>Netflix Archaius</h3><p>配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。可以实现动态获取配置， 原理是每隔60s（默认，可配置）从配置源读取一次内容，这样修改了配置文件后不需要重启服务就可以使修改后的内容生效，前提使用archaius的API来读取。</p>\n<h3 id=\"Spring-Cloud-Config\"><a href=\"#Spring-Cloud-Config\" class=\"headerlink\" title=\"Spring Cloud Config\"></a>Spring Cloud Config</h3><p>俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。</p>\n<h3 id=\"Spring-Cloud-Bus\"><a href=\"#Spring-Cloud-Bus\" class=\"headerlink\" title=\"Spring Cloud Bus\"></a>Spring Cloud Bus</h3><p>事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。</p>\n<h3 id=\"Spring-Cloud-for-Cloud-Foundry\"><a href=\"#Spring-Cloud-for-Cloud-Foundry\" class=\"headerlink\" title=\"Spring Cloud for Cloud Foundry\"></a>Spring Cloud for Cloud Foundry</h3><p>Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题</p>\n<p>其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。</p>\n<h3 id=\"Spring-Cloud-Cluster\"><a href=\"#Spring-Cloud-Cluster\" class=\"headerlink\" title=\"Spring Cloud Cluster\"></a>Spring Cloud Cluster</h3><p>Spring Cloud Cluster将取代Spring Integration。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。</p>\n<p>如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。</p>\n<h3 id=\"Spring-Cloud-Consul\"><a href=\"#Spring-Cloud-Consul\" class=\"headerlink\" title=\"Spring Cloud Consul\"></a>Spring Cloud Consul</h3><p>Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.</p>\n<p>Spring Cloud Consul 封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。</p>\n<h2 id=\"其它小弟\"><a href=\"#其它小弟\" class=\"headerlink\" title=\"其它小弟\"></a>其它小弟</h2><h3 id=\"Spring-Cloud-Security\"><a href=\"#Spring-Cloud-Security\" class=\"headerlink\" title=\"Spring Cloud Security\"></a>Spring Cloud Security</h3><p>基于spring security的安全工具包，为你的应用程序添加安全控制。这个小弟很牛鼻专门负责整个帮派的安全问题，设置不同的门派访问特定的资源，不能把秘籍葵花宝典泄漏了。</p>\n<h3 id=\"Spring-Cloud-Sleuth\"><a href=\"#Spring-Cloud-Sleuth\" class=\"headerlink\" title=\"Spring Cloud Sleuth\"></a>Spring Cloud Sleuth</h3><p>日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。</p>\n<h3 id=\"Spring-Cloud-Data-Flow\"><a href=\"#Spring-Cloud-Data-Flow\" class=\"headerlink\" title=\"Spring Cloud Data Flow\"></a>Spring Cloud Data Flow</h3><p>Data flow 是一个用于开发和执行大范围数据处理其模式包括ETL，批量运算和持续运算的统一编程模型和托管服务。</p>\n<p>对于在现代运行环境中可组合的微服务程序来说，Spring Cloud data flow是一个原生云可编配的服务。使用Spring Cloud data flow，开发者可以为像数据抽取，实时分析，和数据导入/导出这种常见用例创建和编配数据通道 （data pipelines）。</p>\n<p>Spring Cloud data flow 是基于原生云对 spring XD的重新设计，该项目目标是简化大数据应用的开发。Spring XD 的流处理和批处理模块的重构分别是基于 Spring Boot的stream 和 task/batch 的微服务程序。这些程序现在都是自动部署单元而且他们原生的支持像 Cloud Foundry、Apache YARN、Apache Mesos和Kubernetes 等现代运行环境。</p>\n<p>Spring Cloud data flow 为基于微服务的分布式流处理和批处理数据通道提供了一系列模型和最佳实践。</p>\n<h3 id=\"Spring-Cloud-Stream\"><a href=\"#Spring-Cloud-Stream\" class=\"headerlink\" title=\"Spring Cloud Stream\"></a>Spring Cloud Stream</h3><p>Spring Cloud Stream是创建消息驱动微服务应用的框架。Spring Cloud Stream是基于Spring Boot创建，用来建立单独的／工业级spring应用，使用spring integration提供与消息代理之间的连接。数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。</p>\n<p>一个业务会牵扯到多个任务，任务之间是通过事件触发的，这就是Spring Cloud stream要干的事了</p>\n<h3 id=\"Spring-Cloud-Task\"><a href=\"#Spring-Cloud-Task\" class=\"headerlink\" title=\"Spring Cloud Task\"></a>Spring Cloud Task</h3><p>Spring Cloud Task 主要解决短命微服务的任务管理，任务调度的工作，比如说某些定时任务晚上就跑一次，或者某项数据分析临时就跑几次。</p>\n<h3 id=\"Spring-Cloud-Zookeeper\"><a href=\"#Spring-Cloud-Zookeeper\" class=\"headerlink\" title=\"Spring Cloud Zookeeper\"></a>Spring Cloud Zookeeper</h3><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>\n<p>操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理，抱了Zookeeper的大腿。</p>\n<h3 id=\"Spring-Cloud-Connectors\"><a href=\"#Spring-Cloud-Connectors\" class=\"headerlink\" title=\"Spring Cloud Connectors\"></a>Spring Cloud Connectors</h3><p>Spring Cloud Connectors 简化了连接到服务的过程和从云平台获取操作的过程，有很强的扩展性，可以利用Spring Cloud Connectors来构建你自己的云平台。</p>\n<p>便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。</p>\n<h3 id=\"Spring-Cloud-Starters\"><a href=\"#Spring-Cloud-Starters\" class=\"headerlink\" title=\"Spring Cloud Starters\"></a>Spring Cloud Starters</h3><p>Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。</p>\n<h3 id=\"Spring-Cloud-CLI\"><a href=\"#Spring-Cloud-CLI\" class=\"headerlink\" title=\"Spring Cloud CLI\"></a>Spring Cloud CLI</h3><p>基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://www.cnblogs.com/ityouknow/p/7508306.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud在国内中小型公司能用起来吗？</a></p>\n<p>2.<a href=\"https://springcloud.cc/spring-cloud-dalston.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud Dalston中文文档</a></p>\n<p>3.<a href=\"http://projects.spring.io/spring-cloud/\" target=\"_blank\" rel=\"noopener\">spring-cloud</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Spring-Cloud\"><a href=\"#Spring-Cloud\" class=\"headerlink\" title=\"Spring Cloud\"></a>Spring Cloud</h1><h2 id=\"什么是Spring-Cloud\"><a href=\"#什么是Spring-Cloud\" class=\"headerlink\" title=\"什么是Spring Cloud\"></a>什么是Spring Cloud</h2><p>Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如<strong>配置管理</strong>，<strong>服务发现</strong>，<strong>断路器</strong>，<strong>智能路由</strong>，<strong>微代理</strong>，<strong>控制总线</strong>等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。</p>\n<p>微服务是可以独立部署、水平扩展、独立访问（或者有独立的数据库）的服务单元，Spring Cloud就是这些微服务的大管家，采用了微服务这种架构之后，项目的数量会非常多，Spring Cloud做为大管家就需要提供各种方案来维护整个生态。</p>\n<p>Spring Cloud就是一套分布式服务治理的框架，既然它是一套服务治理的框架，那么它本身不会提供具体功能性的操作，更专注于服务之间的通讯、熔断、监控等。因此就需要很多的组件来支持一套功能。</p>\n<h2 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h2><p>Spring Cloud专注于提供良好的开箱即用经验的典型用例和可扩展性机制覆盖。</p>\n<ul>\n<li>分布式/版本化配置</li>\n<li>服务注册和发现</li>\n<li>路由</li>\n<li>service - to - service调用</li>\n<li>负载均衡</li>\n<li>断路器</li>\n<li>全局锁</li>\n<li>决策竞选</li>\n<li>分布式消息传递<h2 id=\"Spring-Cloud组件架构\"><a href=\"#Spring-Cloud组件架构\" class=\"headerlink\" title=\"Spring Cloud组件架构\"></a>Spring Cloud组件架构</h2><img src=\"http://www.ityouknow.com/assets/images/2017/springcloud/spring-cloud-architecture.png\" alt=\"image\"></li>\n</ul>\n<p>1、外部或者内部的非Spring Cloud项目都统一通过API网关（Zuul）来访问内部服务.<br>2、网关接收到请求后，从注册中心（Eureka）获取可用服务<br>3、由Ribbon进行均衡负载后，分发到后端的具体实例<br>4、微服务之间通过Feign进行通信处理业务<br>5、Hystrix负责处理服务超时熔断<br>6、Turbine监控服务间的调用和熔断相关指标</p>\n<h2 id=\"核心成员\"><a href=\"#核心成员\" class=\"headerlink\" title=\"核心成员\"></a>核心成员</h2><h3 id=\"Spring-Cloud-Netflix\"><a href=\"#Spring-Cloud-Netflix\" class=\"headerlink\" title=\"Spring Cloud Netflix\"></a>Spring Cloud Netflix</h3><p>这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka, Hystrix, Zuul, Archaius… 太多了</p>\n<h3 id=\"Netflix-Eureka\"><a href=\"#Netflix-Eureka\" class=\"headerlink\" title=\"Netflix Eureka\"></a>Netflix Eureka</h3><p>服务中心，云端服务发现，一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。这个可是springcloud最牛鼻的小弟，服务中心，任何小弟需要其它小弟支持什么都需要从这里来拿，同样的你有什么独门武功的都赶紧过报道，方便以后其它小弟来调用；它的好处是你不需要直接找各种什么小弟支持，只需要到服务中心来领取，也不需要知道提供支持的其它小弟在哪里，还是几个小弟来支持的，反正拿来用就行，服务中心来保证稳定性和质量。</p>\n<h3 id=\"Netflix-Hystrix\"><a href=\"#Netflix-Hystrix\" class=\"headerlink\" title=\"Netflix Hystrix\"></a>Netflix Hystrix</h3><p>熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。比如突然某个小弟生病了，但是你还需要它的支持，然后调用之后它半天没有响应，你却不知道，一直在等等这个响应；有可能别的小弟也正在调用你的武功绝技，那么当请求多之后，就会发生严重的阻塞影响老大的整体计划。这个时候Hystrix就派上用场了，当Hystrix发现某个小弟不在状态不稳定立马马上让它下线，让其它小弟来顶上来，或者给你说不用等了这个小弟今天肯定不行，该干嘛赶紧干嘛去别在这排队了。</p>\n<h3 id=\"Netflix-Zuul\"><a href=\"#Netflix-Zuul\" class=\"headerlink\" title=\"Netflix Zuul\"></a>Netflix Zuul</h3><p>Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。当其它门派来找大哥办事的时候一定要先经过zuul,看下有没有带刀子什么的给拦截回去，或者是需要找那个小弟的直接给带过去。</p>\n<h3 id=\"Netflix-Archaius\"><a href=\"#Netflix-Archaius\" class=\"headerlink\" title=\"Netflix Archaius\"></a>Netflix Archaius</h3><p>配置管理API，包含一系列配置管理API，提供动态类型化属性、线程安全配置操作、轮询框架、回调机制等功能。可以实现动态获取配置， 原理是每隔60s（默认，可配置）从配置源读取一次内容，这样修改了配置文件后不需要重启服务就可以使修改后的内容生效，前提使用archaius的API来读取。</p>\n<h3 id=\"Spring-Cloud-Config\"><a href=\"#Spring-Cloud-Config\" class=\"headerlink\" title=\"Spring Cloud Config\"></a>Spring Cloud Config</h3><p>俗称的配置中心，配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion。就是以后大家武器、枪火什么的东西都集中放到一起，别随便自己带，方便以后统一管理、升级装备。</p>\n<h3 id=\"Spring-Cloud-Bus\"><a href=\"#Spring-Cloud-Bus\" class=\"headerlink\" title=\"Spring Cloud Bus\"></a>Spring Cloud Bus</h3><p>事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。相当于水浒传中日行八百里的神行太保戴宗，确保各个小弟之间消息保持畅通。</p>\n<h3 id=\"Spring-Cloud-for-Cloud-Foundry\"><a href=\"#Spring-Cloud-for-Cloud-Foundry\" class=\"headerlink\" title=\"Spring Cloud for Cloud Foundry\"></a>Spring Cloud for Cloud Foundry</h3><p>Cloud Foundry是VMware推出的业界第一个开源PaaS云平台，它支持多种框架、语言、运行时环境、云平台及应用服务，使开发人员能够在几秒钟内进行应用程序的部署和扩展，无需担心任何基础架构的问题</p>\n<p>其实就是与CloudFoundry进行集成的一套解决方案，抱了Cloud Foundry的大腿。</p>\n<h3 id=\"Spring-Cloud-Cluster\"><a href=\"#Spring-Cloud-Cluster\" class=\"headerlink\" title=\"Spring Cloud Cluster\"></a>Spring Cloud Cluster</h3><p>Spring Cloud Cluster将取代Spring Integration。提供在分布式系统中的集群所需要的基础功能支持，如：选举、集群的状态一致性、全局锁、tokens等常见状态模式的抽象和实现。</p>\n<p>如果把不同的帮派组织成统一的整体，Spring Cloud Cluster已经帮你提供了很多方便组织成统一的工具。</p>\n<h3 id=\"Spring-Cloud-Consul\"><a href=\"#Spring-Cloud-Consul\" class=\"headerlink\" title=\"Spring Cloud Consul\"></a>Spring Cloud Consul</h3><p>Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对.</p>\n<p>Spring Cloud Consul 封装了Consul操作，consul是一个服务发现与配置工具，与Docker容器可以无缝集成。</p>\n<h2 id=\"其它小弟\"><a href=\"#其它小弟\" class=\"headerlink\" title=\"其它小弟\"></a>其它小弟</h2><h3 id=\"Spring-Cloud-Security\"><a href=\"#Spring-Cloud-Security\" class=\"headerlink\" title=\"Spring Cloud Security\"></a>Spring Cloud Security</h3><p>基于spring security的安全工具包，为你的应用程序添加安全控制。这个小弟很牛鼻专门负责整个帮派的安全问题，设置不同的门派访问特定的资源，不能把秘籍葵花宝典泄漏了。</p>\n<h3 id=\"Spring-Cloud-Sleuth\"><a href=\"#Spring-Cloud-Sleuth\" class=\"headerlink\" title=\"Spring Cloud Sleuth\"></a>Spring Cloud Sleuth</h3><p>日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。</p>\n<h3 id=\"Spring-Cloud-Data-Flow\"><a href=\"#Spring-Cloud-Data-Flow\" class=\"headerlink\" title=\"Spring Cloud Data Flow\"></a>Spring Cloud Data Flow</h3><p>Data flow 是一个用于开发和执行大范围数据处理其模式包括ETL，批量运算和持续运算的统一编程模型和托管服务。</p>\n<p>对于在现代运行环境中可组合的微服务程序来说，Spring Cloud data flow是一个原生云可编配的服务。使用Spring Cloud data flow，开发者可以为像数据抽取，实时分析，和数据导入/导出这种常见用例创建和编配数据通道 （data pipelines）。</p>\n<p>Spring Cloud data flow 是基于原生云对 spring XD的重新设计，该项目目标是简化大数据应用的开发。Spring XD 的流处理和批处理模块的重构分别是基于 Spring Boot的stream 和 task/batch 的微服务程序。这些程序现在都是自动部署单元而且他们原生的支持像 Cloud Foundry、Apache YARN、Apache Mesos和Kubernetes 等现代运行环境。</p>\n<p>Spring Cloud data flow 为基于微服务的分布式流处理和批处理数据通道提供了一系列模型和最佳实践。</p>\n<h3 id=\"Spring-Cloud-Stream\"><a href=\"#Spring-Cloud-Stream\" class=\"headerlink\" title=\"Spring Cloud Stream\"></a>Spring Cloud Stream</h3><p>Spring Cloud Stream是创建消息驱动微服务应用的框架。Spring Cloud Stream是基于Spring Boot创建，用来建立单独的／工业级spring应用，使用spring integration提供与消息代理之间的连接。数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。</p>\n<p>一个业务会牵扯到多个任务，任务之间是通过事件触发的，这就是Spring Cloud stream要干的事了</p>\n<h3 id=\"Spring-Cloud-Task\"><a href=\"#Spring-Cloud-Task\" class=\"headerlink\" title=\"Spring Cloud Task\"></a>Spring Cloud Task</h3><p>Spring Cloud Task 主要解决短命微服务的任务管理，任务调度的工作，比如说某些定时任务晚上就跑一次，或者某项数据分析临时就跑几次。</p>\n<h3 id=\"Spring-Cloud-Zookeeper\"><a href=\"#Spring-Cloud-Zookeeper\" class=\"headerlink\" title=\"Spring Cloud Zookeeper\"></a>Spring Cloud Zookeeper</h3><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>\n<p>操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理，抱了Zookeeper的大腿。</p>\n<h3 id=\"Spring-Cloud-Connectors\"><a href=\"#Spring-Cloud-Connectors\" class=\"headerlink\" title=\"Spring Cloud Connectors\"></a>Spring Cloud Connectors</h3><p>Spring Cloud Connectors 简化了连接到服务的过程和从云平台获取操作的过程，有很强的扩展性，可以利用Spring Cloud Connectors来构建你自己的云平台。</p>\n<p>便于云端应用程序在各种PaaS平台连接到后端，如：数据库和消息代理服务。</p>\n<h3 id=\"Spring-Cloud-Starters\"><a href=\"#Spring-Cloud-Starters\" class=\"headerlink\" title=\"Spring Cloud Starters\"></a>Spring Cloud Starters</h3><p>Spring Boot式的启动项目，为Spring Cloud提供开箱即用的依赖管理。</p>\n<h3 id=\"Spring-Cloud-CLI\"><a href=\"#Spring-Cloud-CLI\" class=\"headerlink\" title=\"Spring Cloud CLI\"></a>Spring Cloud CLI</h3><p>基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"https://www.cnblogs.com/ityouknow/p/7508306.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud在国内中小型公司能用起来吗？</a></p>\n<p>2.<a href=\"https://springcloud.cc/spring-cloud-dalston.html\" target=\"_blank\" rel=\"noopener\">Spring Cloud Dalston中文文档</a></p>\n<p>3.<a href=\"http://projects.spring.io/spring-cloud/\" target=\"_blank\" rel=\"noopener\">spring-cloud</a></p>\n"},{"title":"kafka producer源码分析","toc":false,"date":"2019-06-14T13:47:42.000Z","_content":"\n Kafka producer源码分析\n\n## 前言\n\n在开始文章之前，需要解释是一下为什么要研究producer源码。\n\n### 为什么要研究producer源码\n\n通常producer使用都很简单，初始化一个`KafkaProducer`实例，然后调用`send`方法就好，但是我们有了解后面是如何发送到kafka集群的吗？其实我们不知道，其次，到底客户端有几个线程？我们不知道。还有producer还能做什么？我们同样不知道。本篇文章就是想回答一下上面提出的几个问题，能力有限，如有错误，欢迎指出！\n\n## 架构\n\n在介绍客户端架构之前，先回答一个问题\n\nproducer到底存在几个线程？**2个**  **Main thread** 和**sender**,其中sender线程负责发送消息，main 线程负责interceptor、序列化、分区等其他操作。\n\n![](http://blogstatic.aibibang.com/Kafka%20producer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1.jpg)\n\n\n\n- Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。\n- 进行interceptor、序列化、分区等其他操作，发送到Producer程序中RecordAccumulator中。\n- Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次的内，统一发送给对应的broker中。\n\n\n\n## 消息发送源码分析\n\nsender发送的时机是由两个指标决定的，一个是时间`linger.ms`，一个是数据量大小 `batch.size`\n\nsender线程主要代码\n\nrun->run(long now)->sendProducerData\n\n```java\n    private long sendProducerData(long now) {\n        Cluster cluster = metadata.fetch();\n        //result.nextReadyCheckDelayMs\b表示下次检查是否ready的时间，也是//selecotr会阻塞的时间\n        RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);\n        if (!result.unknownLeaderTopics.isEmpty()) {\n            for (String topic : result.unknownLeaderTopics)\n                this.metadata.add(topic);\n\n            log.debug(\"Requesting metadata update due to unknown leader topics from the batched records: {}\",\n                result.unknownLeaderTopics);\n            this.metadata.requestUpdate();\n        }\n\n        Iterator<Node> iter = result.readyNodes.iterator();\n        long notReadyTimeout = Long.MAX_VALUE;\n        while (iter.hasNext()) {\n            Node node = iter.next();\n            if (!this.client.ready(node, now)) {\n                iter.remove();\n                notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));\n            }\n        }\n\n        // create produce requests\n        Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);\n        addToInflightBatches(batches);\n        if (guaranteeMessageOrder) {\n            // Mute all the partitions drained\n            for (List<ProducerBatch> batchList : batches.values()) {\n                for (ProducerBatch batch : batchList)\n                    this.accumulator.mutePartition(batch.topicPartition);\n            }\n        }\n\n        accumulator.resetNextBatchExpiryTime();\n        List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);\n        List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);\n        expiredBatches.addAll(expiredInflightBatches);\n\n        if (!expiredBatches.isEmpty())\n            log.trace(\"Expired {} batches in accumulator\", expiredBatches.size());\n        for (ProducerBatch expiredBatch : expiredBatches) {\n            String errorMessage = \"Expiring \" + expiredBatch.recordCount + \" record(s) for \" + expiredBatch.topicPartition\n                + \":\" + (now - expiredBatch.createdMs) + \" ms has passed since batch creation\";\n            failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);\n            if (transactionManager != null && expiredBatch.inRetry()) {\n                transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);\n            }\n        }\n        sensors.updateProduceRequestMetrics(batches);\n       // 暂且只关心result.nextReadyCheckDelayMs\n        long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);\n        pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);\n        pollTimeout = Math.max(pollTimeout, 0);\n        if (!result.readyNodes.isEmpty()) {\n            log.trace(\"Nodes with data ready to send: {}\", result.readyNodes);\n\n            pollTimeout = 0;\n        }\n        sendProduceRequests(batches, now);\n        return pollTimeout;\n    }\n\n```\n\n```java\nlong pollTimeout = sendProducerData(now);\n// poll最终会调用selector,pollTimeout也就是selector阻塞的时间\n client.poll(pollTimeout, now);\n```\n\n\n\nselector\n\n```java\n    /**\n     * Check for data, waiting up to the given timeout.\n     *\n     * @param timeoutMs Length of time to wait, in milliseconds, which must be non-negative\n     * @return The number of keys ready\n     */\n    private int select(long timeoutMs) throws IOException {\n        if (timeoutMs < 0L)\n            throw new IllegalArgumentException(\"timeout should be >= 0\");\n\n        if (timeoutMs == 0L)\n            return this.nioSelector.selectNow();\n        else\n            return this.nioSelector.select(timeoutMs);\n    }\n```\n\n\n\n```java\n    public ReadyCheckResult ready(Cluster cluster, long nowMs) {\n        Set<Node> readyNodes = new HashSet<>();\n        // 初始化为最大值\n        long nextReadyCheckDelayMs = Long.MAX_VALUE;\n        Set<String> unknownLeaderTopics = new HashSet<>();\n\n        boolean exhausted = this.free.queued() > 0;\n        for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {\n            TopicPartition part = entry.getKey();\n            Deque<ProducerBatch> deque = entry.getValue();\n\n            Node leader = cluster.leaderFor(part);\n            synchronized (deque) {\n                if (leader == null && !deque.isEmpty()) {\n                    unknownLeaderTopics.add(part.topic());\n                } else if (!readyNodes.contains(leader) && !isMuted(part, nowMs)) {\n                    ProducerBatch batch = deque.peekFirst();\n                    if (batch != null) {\n                        long waitedTimeMs = batch.waitedTimeMs(nowMs);\n                        boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;\n                        // 和linger.ms有关\n                        long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;\n                        boolean full = deque.size() > 1 || batch.isFull();\n                        boolean expired = waitedTimeMs >= timeToWaitMs;\n                        boolean sendable = full || expired || exhausted || closed || flushInProgress();\n                        if (sendable && !backingOff) {\n                            readyNodes.add(leader);\n                        } else {\n                            long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);\n                            nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);\n                        }\n                    }\n                }\n            }\n        }\n        return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);\n    }\n```\n\n我们可以从实例化一个新的KafkaProducer开始分析（还没有调用send方法），在sender线程调用accumulator#ready(..)时候，会返回result，其中包含selector可能要阻塞的时间。由于还没有调用send方法，所以Deque<RecordBatch>为空，所以result中包含的nextReadyCheckDelayMs也是最大值，这个时候selector会一直阻塞。\n\n然后我们调用send方法,该方法会调用waitOnMetadata，waitOnMetadata会调用`sender.wakeup()`，所以会唤醒sender线程。\n\n这个时候会唤醒阻塞在selector#select(..)的sender线程，sender线程又运行到accumulator#ready(..)，当Deque<ProducerBatch>有值，所以返回的result包含的nextReadyCheckDelayMs不再是最大值，而是和linger.ms有关的值。也就是时候selector会最多阻塞lingger.ms后就返回，然后再次轮询。（根据源码分析，第一条消息会创建batch,因此newBatchCreated为true,同样会触发唤醒sender）\n\n如果有一个ProducerBatch满了，也会调用Sender#wakeup(..)，\n\nKafkaProducer#doSend(...)\n\n```java\n if (result.batchIsFull || result.newBatchCreated) {\n                log.trace(\"Waking up the sender since topic {} partition {} is either full or getting a new batch\", record.topic(), partition);\n                this.sender.wakeup();\n            }\n```\n\n\n\n所以综上所述：只要满足linger.ms和batch.size满了就会激活sender线程来发送消息。\n\n## 客户端顺序性保证\n\n因为消息发送是批量发送的，那么就有可能存在上一批发送失败，接下来一批发送成功。即会出现数据乱序。\n\n`max.in.flight.requests.per.connection=1`,限制客户端在单个连接上能够发送的未响应请求的个数。如果业务对顺序性有很高的要求，将该参数值设置为1。\n## 总结\n\nkafka producer大概流程如上，数据发送是**批量**的，最后是按Node发送响应的消息的。即其中Integer即为broker id。消息在RecordAccumulator中还是按TopicPartition存放的，但是在最终发送会作相应的转换。\n\nsender发送的时机是由两个指标决定的，一个是时间`linger.ms`，一个是数据量大小 `batch.size`\n\n了解producer发送流程，这样就可以让我们更改的生产消息，使用更多的特性，例如拦截器可以做各种特殊处理的场景。\n\n## 参考\n\n## 1. [kafka producer的batch.size和linger.ms](https://www.cnblogs.com/set-cookie/p/8902340.html)","source":"_posts/kafka-producer源码分析.md","raw":"---\ntitle: kafka producer源码分析\ntags:\n  - kafka\n  - 专栏\ncategories:\n  - kafka\ntoc: false\ndate: 2019-06-14 21:47:42\n---\n\n Kafka producer源码分析\n\n## 前言\n\n在开始文章之前，需要解释是一下为什么要研究producer源码。\n\n### 为什么要研究producer源码\n\n通常producer使用都很简单，初始化一个`KafkaProducer`实例，然后调用`send`方法就好，但是我们有了解后面是如何发送到kafka集群的吗？其实我们不知道，其次，到底客户端有几个线程？我们不知道。还有producer还能做什么？我们同样不知道。本篇文章就是想回答一下上面提出的几个问题，能力有限，如有错误，欢迎指出！\n\n## 架构\n\n在介绍客户端架构之前，先回答一个问题\n\nproducer到底存在几个线程？**2个**  **Main thread** 和**sender**,其中sender线程负责发送消息，main 线程负责interceptor、序列化、分区等其他操作。\n\n![](http://blogstatic.aibibang.com/Kafka%20producer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1.jpg)\n\n\n\n- Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。\n- 进行interceptor、序列化、分区等其他操作，发送到Producer程序中RecordAccumulator中。\n- Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次的内，统一发送给对应的broker中。\n\n\n\n## 消息发送源码分析\n\nsender发送的时机是由两个指标决定的，一个是时间`linger.ms`，一个是数据量大小 `batch.size`\n\nsender线程主要代码\n\nrun->run(long now)->sendProducerData\n\n```java\n    private long sendProducerData(long now) {\n        Cluster cluster = metadata.fetch();\n        //result.nextReadyCheckDelayMs\b表示下次检查是否ready的时间，也是//selecotr会阻塞的时间\n        RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);\n        if (!result.unknownLeaderTopics.isEmpty()) {\n            for (String topic : result.unknownLeaderTopics)\n                this.metadata.add(topic);\n\n            log.debug(\"Requesting metadata update due to unknown leader topics from the batched records: {}\",\n                result.unknownLeaderTopics);\n            this.metadata.requestUpdate();\n        }\n\n        Iterator<Node> iter = result.readyNodes.iterator();\n        long notReadyTimeout = Long.MAX_VALUE;\n        while (iter.hasNext()) {\n            Node node = iter.next();\n            if (!this.client.ready(node, now)) {\n                iter.remove();\n                notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));\n            }\n        }\n\n        // create produce requests\n        Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);\n        addToInflightBatches(batches);\n        if (guaranteeMessageOrder) {\n            // Mute all the partitions drained\n            for (List<ProducerBatch> batchList : batches.values()) {\n                for (ProducerBatch batch : batchList)\n                    this.accumulator.mutePartition(batch.topicPartition);\n            }\n        }\n\n        accumulator.resetNextBatchExpiryTime();\n        List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);\n        List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);\n        expiredBatches.addAll(expiredInflightBatches);\n\n        if (!expiredBatches.isEmpty())\n            log.trace(\"Expired {} batches in accumulator\", expiredBatches.size());\n        for (ProducerBatch expiredBatch : expiredBatches) {\n            String errorMessage = \"Expiring \" + expiredBatch.recordCount + \" record(s) for \" + expiredBatch.topicPartition\n                + \":\" + (now - expiredBatch.createdMs) + \" ms has passed since batch creation\";\n            failBatch(expiredBatch, -1, NO_TIMESTAMP, new TimeoutException(errorMessage), false);\n            if (transactionManager != null && expiredBatch.inRetry()) {\n                transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);\n            }\n        }\n        sensors.updateProduceRequestMetrics(batches);\n       // 暂且只关心result.nextReadyCheckDelayMs\n        long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);\n        pollTimeout = Math.min(pollTimeout, this.accumulator.nextExpiryTimeMs() - now);\n        pollTimeout = Math.max(pollTimeout, 0);\n        if (!result.readyNodes.isEmpty()) {\n            log.trace(\"Nodes with data ready to send: {}\", result.readyNodes);\n\n            pollTimeout = 0;\n        }\n        sendProduceRequests(batches, now);\n        return pollTimeout;\n    }\n\n```\n\n```java\nlong pollTimeout = sendProducerData(now);\n// poll最终会调用selector,pollTimeout也就是selector阻塞的时间\n client.poll(pollTimeout, now);\n```\n\n\n\nselector\n\n```java\n    /**\n     * Check for data, waiting up to the given timeout.\n     *\n     * @param timeoutMs Length of time to wait, in milliseconds, which must be non-negative\n     * @return The number of keys ready\n     */\n    private int select(long timeoutMs) throws IOException {\n        if (timeoutMs < 0L)\n            throw new IllegalArgumentException(\"timeout should be >= 0\");\n\n        if (timeoutMs == 0L)\n            return this.nioSelector.selectNow();\n        else\n            return this.nioSelector.select(timeoutMs);\n    }\n```\n\n\n\n```java\n    public ReadyCheckResult ready(Cluster cluster, long nowMs) {\n        Set<Node> readyNodes = new HashSet<>();\n        // 初始化为最大值\n        long nextReadyCheckDelayMs = Long.MAX_VALUE;\n        Set<String> unknownLeaderTopics = new HashSet<>();\n\n        boolean exhausted = this.free.queued() > 0;\n        for (Map.Entry<TopicPartition, Deque<ProducerBatch>> entry : this.batches.entrySet()) {\n            TopicPartition part = entry.getKey();\n            Deque<ProducerBatch> deque = entry.getValue();\n\n            Node leader = cluster.leaderFor(part);\n            synchronized (deque) {\n                if (leader == null && !deque.isEmpty()) {\n                    unknownLeaderTopics.add(part.topic());\n                } else if (!readyNodes.contains(leader) && !isMuted(part, nowMs)) {\n                    ProducerBatch batch = deque.peekFirst();\n                    if (batch != null) {\n                        long waitedTimeMs = batch.waitedTimeMs(nowMs);\n                        boolean backingOff = batch.attempts() > 0 && waitedTimeMs < retryBackoffMs;\n                        // 和linger.ms有关\n                        long timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;\n                        boolean full = deque.size() > 1 || batch.isFull();\n                        boolean expired = waitedTimeMs >= timeToWaitMs;\n                        boolean sendable = full || expired || exhausted || closed || flushInProgress();\n                        if (sendable && !backingOff) {\n                            readyNodes.add(leader);\n                        } else {\n                            long timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, 0);\n                            nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);\n                        }\n                    }\n                }\n            }\n        }\n        return new ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);\n    }\n```\n\n我们可以从实例化一个新的KafkaProducer开始分析（还没有调用send方法），在sender线程调用accumulator#ready(..)时候，会返回result，其中包含selector可能要阻塞的时间。由于还没有调用send方法，所以Deque<RecordBatch>为空，所以result中包含的nextReadyCheckDelayMs也是最大值，这个时候selector会一直阻塞。\n\n然后我们调用send方法,该方法会调用waitOnMetadata，waitOnMetadata会调用`sender.wakeup()`，所以会唤醒sender线程。\n\n这个时候会唤醒阻塞在selector#select(..)的sender线程，sender线程又运行到accumulator#ready(..)，当Deque<ProducerBatch>有值，所以返回的result包含的nextReadyCheckDelayMs不再是最大值，而是和linger.ms有关的值。也就是时候selector会最多阻塞lingger.ms后就返回，然后再次轮询。（根据源码分析，第一条消息会创建batch,因此newBatchCreated为true,同样会触发唤醒sender）\n\n如果有一个ProducerBatch满了，也会调用Sender#wakeup(..)，\n\nKafkaProducer#doSend(...)\n\n```java\n if (result.batchIsFull || result.newBatchCreated) {\n                log.trace(\"Waking up the sender since topic {} partition {} is either full or getting a new batch\", record.topic(), partition);\n                this.sender.wakeup();\n            }\n```\n\n\n\n所以综上所述：只要满足linger.ms和batch.size满了就会激活sender线程来发送消息。\n\n## 客户端顺序性保证\n\n因为消息发送是批量发送的，那么就有可能存在上一批发送失败，接下来一批发送成功。即会出现数据乱序。\n\n`max.in.flight.requests.per.connection=1`,限制客户端在单个连接上能够发送的未响应请求的个数。如果业务对顺序性有很高的要求，将该参数值设置为1。\n## 总结\n\nkafka producer大概流程如上，数据发送是**批量**的，最后是按Node发送响应的消息的。即其中Integer即为broker id。消息在RecordAccumulator中还是按TopicPartition存放的，但是在最终发送会作相应的转换。\n\nsender发送的时机是由两个指标决定的，一个是时间`linger.ms`，一个是数据量大小 `batch.size`\n\n了解producer发送流程，这样就可以让我们更改的生产消息，使用更多的特性，例如拦截器可以做各种特殊处理的场景。\n\n## 参考\n\n## 1. [kafka producer的batch.size和linger.ms](https://www.cnblogs.com/set-cookie/p/8902340.html)","slug":"kafka-producer源码分析","published":1,"updated":"2019-06-21T12:51:25.364Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgh00ai8cee2jnl17tw","content":"<p> Kafka producer源码分析</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在开始文章之前，需要解释是一下为什么要研究producer源码。</p>\n<h3 id=\"为什么要研究producer源码\"><a href=\"#为什么要研究producer源码\" class=\"headerlink\" title=\"为什么要研究producer源码\"></a>为什么要研究producer源码</h3><p>通常producer使用都很简单，初始化一个<code>KafkaProducer</code>实例，然后调用<code>send</code>方法就好，但是我们有了解后面是如何发送到kafka集群的吗？其实我们不知道，其次，到底客户端有几个线程？我们不知道。还有producer还能做什么？我们同样不知道。本篇文章就是想回答一下上面提出的几个问题，能力有限，如有错误，欢迎指出！</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p>在介绍客户端架构之前，先回答一个问题</p>\n<p>producer到底存在几个线程？<strong>2个</strong>  <strong>Main thread</strong> 和<strong>sender</strong>,其中sender线程负责发送消息，main 线程负责interceptor、序列化、分区等其他操作。</p>\n<p><img src=\"http://blogstatic.aibibang.com/Kafka%20producer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1.jpg\" alt=\"\"></p>\n<ul>\n<li>Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。</li>\n<li>进行interceptor、序列化、分区等其他操作，发送到Producer程序中RecordAccumulator中。</li>\n<li>Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次的内，统一发送给对应的broker中。</li>\n</ul>\n<h2 id=\"消息发送源码分析\"><a href=\"#消息发送源码分析\" class=\"headerlink\" title=\"消息发送源码分析\"></a>消息发送源码分析</h2><p>sender发送的时机是由两个指标决定的，一个是时间<code>linger.ms</code>，一个是数据量大小 <code>batch.size</code></p>\n<p>sender线程主要代码</p>\n<p>run-&gt;run(long now)-&gt;sendProducerData</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">long</span> <span class=\"title\">sendProducerData</span><span class=\"params\">(<span class=\"keyword\">long</span> now)</span> </span>&#123;</span><br><span class=\"line\">    Cluster cluster = metadata.fetch();</span><br><span class=\"line\">    <span class=\"comment\">//result.nextReadyCheckDelayMs\b表示下次检查是否ready的时间，也是//selecotr会阻塞的时间</span></span><br><span class=\"line\">    RecordAccumulator.ReadyCheckResult result = <span class=\"keyword\">this</span>.accumulator.ready(cluster, now);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.metadata.add(topic);</span><br><span class=\"line\"></span><br><span class=\"line\">        log.debug(<span class=\"string\">\"Requesting metadata update due to unknown leader topics from the batched records: &#123;&#125;\"</span>,</span><br><span class=\"line\">            result.unknownLeaderTopics);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.metadata.requestUpdate();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class=\"line\">    <span class=\"keyword\">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (iter.hasNext()) &#123;</span><br><span class=\"line\">        Node node = iter.next();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!<span class=\"keyword\">this</span>.client.ready(node, now)) &#123;</span><br><span class=\"line\">            iter.remove();</span><br><span class=\"line\">            notReadyTimeout = Math.min(notReadyTimeout, <span class=\"keyword\">this</span>.client.pollDelayMs(node, now));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// create produce requests</span></span><br><span class=\"line\">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class=\"keyword\">this</span>.accumulator.drain(cluster, result.readyNodes, <span class=\"keyword\">this</span>.maxRequestSize, now);</span><br><span class=\"line\">    addToInflightBatches(batches);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (guaranteeMessageOrder) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Mute all the partitions drained</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ProducerBatch batch : batchList)</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    accumulator.resetNextBatchExpiryTime();</span><br><span class=\"line\">    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);</span><br><span class=\"line\">    List&lt;ProducerBatch&gt; expiredBatches = <span class=\"keyword\">this</span>.accumulator.expiredBatches(now);</span><br><span class=\"line\">    expiredBatches.addAll(expiredInflightBatches);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!expiredBatches.isEmpty())</span><br><span class=\"line\">        log.trace(<span class=\"string\">\"Expired &#123;&#125; batches in accumulator\"</span>, expiredBatches.size());</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ProducerBatch expiredBatch : expiredBatches) &#123;</span><br><span class=\"line\">        String errorMessage = <span class=\"string\">\"Expiring \"</span> + expiredBatch.recordCount + <span class=\"string\">\" record(s) for \"</span> + expiredBatch.topicPartition</span><br><span class=\"line\">            + <span class=\"string\">\":\"</span> + (now - expiredBatch.createdMs) + <span class=\"string\">\" ms has passed since batch creation\"</span>;</span><br><span class=\"line\">        failBatch(expiredBatch, -<span class=\"number\">1</span>, NO_TIMESTAMP, <span class=\"keyword\">new</span> TimeoutException(errorMessage), <span class=\"keyword\">false</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (transactionManager != <span class=\"keyword\">null</span> &amp;&amp; expiredBatch.inRetry()) &#123;</span><br><span class=\"line\">            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    sensors.updateProduceRequestMetrics(batches);</span><br><span class=\"line\">   <span class=\"comment\">// 暂且只关心result.nextReadyCheckDelayMs</span></span><br><span class=\"line\">    <span class=\"keyword\">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class=\"line\">    pollTimeout = Math.min(pollTimeout, <span class=\"keyword\">this</span>.accumulator.nextExpiryTimeMs() - now);</span><br><span class=\"line\">    pollTimeout = Math.max(pollTimeout, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!result.readyNodes.isEmpty()) &#123;</span><br><span class=\"line\">        log.trace(<span class=\"string\">\"Nodes with data ready to send: &#123;&#125;\"</span>, result.readyNodes);</span><br><span class=\"line\"></span><br><span class=\"line\">        pollTimeout = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    sendProduceRequests(batches, now);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pollTimeout;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">long</span> pollTimeout = sendProducerData(now);</span><br><span class=\"line\"><span class=\"comment\">// poll最终会调用selector,pollTimeout也就是selector阻塞的时间</span></span><br><span class=\"line\"> client.poll(pollTimeout, now);</span><br></pre></td></tr></table></figure>\n<p>selector</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Check for data, waiting up to the given timeout.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> timeoutMs Length of time to wait, in milliseconds, which must be non-negative</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span> The number of keys ready</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">select</span><span class=\"params\">(<span class=\"keyword\">long</span> timeoutMs)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (timeoutMs &lt; <span class=\"number\">0L</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"timeout should be &gt;= 0\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (timeoutMs == <span class=\"number\">0L</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.nioSelector.selectNow();</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.nioSelector.select(timeoutMs);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ReadyCheckResult <span class=\"title\">ready</span><span class=\"params\">(Cluster cluster, <span class=\"keyword\">long</span> nowMs)</span> </span>&#123;</span><br><span class=\"line\">    Set&lt;Node&gt; readyNodes = <span class=\"keyword\">new</span> HashSet&lt;&gt;();</span><br><span class=\"line\">    <span class=\"comment\">// 初始化为最大值</span></span><br><span class=\"line\">    <span class=\"keyword\">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</span><br><span class=\"line\">    Set&lt;String&gt; unknownLeaderTopics = <span class=\"keyword\">new</span> HashSet&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> exhausted = <span class=\"keyword\">this</span>.free.queued() &gt; <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; entry : <span class=\"keyword\">this</span>.batches.entrySet()) &#123;</span><br><span class=\"line\">        TopicPartition part = entry.getKey();</span><br><span class=\"line\">        Deque&lt;ProducerBatch&gt; deque = entry.getValue();</span><br><span class=\"line\"></span><br><span class=\"line\">        Node leader = cluster.leaderFor(part);</span><br><span class=\"line\">        <span class=\"keyword\">synchronized</span> (deque) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (leader == <span class=\"keyword\">null</span> &amp;&amp; !deque.isEmpty()) &#123;</span><br><span class=\"line\">                unknownLeaderTopics.add(part.topic());</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!readyNodes.contains(leader) &amp;&amp; !isMuted(part, nowMs)) &#123;</span><br><span class=\"line\">                ProducerBatch batch = deque.peekFirst();</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (batch != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">long</span> waitedTimeMs = batch.waitedTimeMs(nowMs);</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> backingOff = batch.attempts() &gt; <span class=\"number\">0</span> &amp;&amp; waitedTimeMs &lt; retryBackoffMs;</span><br><span class=\"line\">                    <span class=\"comment\">// 和linger.ms有关</span></span><br><span class=\"line\">                    <span class=\"keyword\">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> full = deque.size() &gt; <span class=\"number\">1</span> || batch.isFull();</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs;</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> sendable = full || expired || exhausted || closed || flushInProgress();</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (sendable &amp;&amp; !backingOff) &#123;</span><br><span class=\"line\">                        readyNodes.add(leader);</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class=\"number\">0</span>);</span><br><span class=\"line\">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们可以从实例化一个新的KafkaProducer开始分析（还没有调用send方法），在sender线程调用accumulator#ready(..)时候，会返回result，其中包含selector可能要阻塞的时间。由于还没有调用send方法，所以Deque<recordbatch>为空，所以result中包含的nextReadyCheckDelayMs也是最大值，这个时候selector会一直阻塞。</recordbatch></p>\n<p>然后我们调用send方法,该方法会调用waitOnMetadata，waitOnMetadata会调用<code>sender.wakeup()</code>，所以会唤醒sender线程。</p>\n<p>这个时候会唤醒阻塞在selector#select(..)的sender线程，sender线程又运行到accumulator#ready(..)，当Deque<producerbatch>有值，所以返回的result包含的nextReadyCheckDelayMs不再是最大值，而是和linger.ms有关的值。也就是时候selector会最多阻塞lingger.ms后就返回，然后再次轮询。（根据源码分析，第一条消息会创建batch,因此newBatchCreated为true,同样会触发唤醒sender）</producerbatch></p>\n<p>如果有一个ProducerBatch满了，也会调用Sender#wakeup(..)，</p>\n<p>KafkaProducer#doSend(…)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class=\"line\">               log.trace(<span class=\"string\">\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\"</span>, record.topic(), partition);</span><br><span class=\"line\">               <span class=\"keyword\">this</span>.sender.wakeup();</span><br><span class=\"line\">           &#125;</span><br></pre></td></tr></table></figure>\n<p>所以综上所述：只要满足linger.ms和batch.size满了就会激活sender线程来发送消息。</p>\n<h2 id=\"客户端顺序性保证\"><a href=\"#客户端顺序性保证\" class=\"headerlink\" title=\"客户端顺序性保证\"></a>客户端顺序性保证</h2><p>因为消息发送是批量发送的，那么就有可能存在上一批发送失败，接下来一批发送成功。即会出现数据乱序。</p>\n<p><code>max.in.flight.requests.per.connection=1</code>,限制客户端在单个连接上能够发送的未响应请求的个数。如果业务对顺序性有很高的要求，将该参数值设置为1。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>kafka producer大概流程如上，数据发送是<strong>批量</strong>的，最后是按Node发送响应的消息的。即其中Integer即为broker id。消息在RecordAccumulator中还是按TopicPartition存放的，但是在最终发送会作相应的转换。</p>\n<p>sender发送的时机是由两个指标决定的，一个是时间<code>linger.ms</code>，一个是数据量大小 <code>batch.size</code></p>\n<p>了解producer发送流程，这样就可以让我们更改的生产消息，使用更多的特性，例如拦截器可以做各种特殊处理的场景。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><h2 id=\"1-kafka-producer的batch-size和linger-ms\"><a href=\"#1-kafka-producer的batch-size和linger-ms\" class=\"headerlink\" title=\"1. kafka producer的batch.size和linger.ms\"></a>1. <a href=\"https://www.cnblogs.com/set-cookie/p/8902340.html\" target=\"_blank\" rel=\"noopener\">kafka producer的batch.size和linger.ms</a></h2>","site":{"data":{}},"excerpt":"","more":"<p> Kafka producer源码分析</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>在开始文章之前，需要解释是一下为什么要研究producer源码。</p>\n<h3 id=\"为什么要研究producer源码\"><a href=\"#为什么要研究producer源码\" class=\"headerlink\" title=\"为什么要研究producer源码\"></a>为什么要研究producer源码</h3><p>通常producer使用都很简单，初始化一个<code>KafkaProducer</code>实例，然后调用<code>send</code>方法就好，但是我们有了解后面是如何发送到kafka集群的吗？其实我们不知道，其次，到底客户端有几个线程？我们不知道。还有producer还能做什么？我们同样不知道。本篇文章就是想回答一下上面提出的几个问题，能力有限，如有错误，欢迎指出！</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p>在介绍客户端架构之前，先回答一个问题</p>\n<p>producer到底存在几个线程？<strong>2个</strong>  <strong>Main thread</strong> 和<strong>sender</strong>,其中sender线程负责发送消息，main 线程负责interceptor、序列化、分区等其他操作。</p>\n<p><img src=\"http://blogstatic.aibibang.com/Kafka%20producer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1.jpg\" alt=\"\"></p>\n<ul>\n<li>Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。</li>\n<li>进行interceptor、序列化、分区等其他操作，发送到Producer程序中RecordAccumulator中。</li>\n<li>Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次的内，统一发送给对应的broker中。</li>\n</ul>\n<h2 id=\"消息发送源码分析\"><a href=\"#消息发送源码分析\" class=\"headerlink\" title=\"消息发送源码分析\"></a>消息发送源码分析</h2><p>sender发送的时机是由两个指标决定的，一个是时间<code>linger.ms</code>，一个是数据量大小 <code>batch.size</code></p>\n<p>sender线程主要代码</p>\n<p>run-&gt;run(long now)-&gt;sendProducerData</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">long</span> <span class=\"title\">sendProducerData</span><span class=\"params\">(<span class=\"keyword\">long</span> now)</span> </span>&#123;</span><br><span class=\"line\">    Cluster cluster = metadata.fetch();</span><br><span class=\"line\">    <span class=\"comment\">//result.nextReadyCheckDelayMs\b表示下次检查是否ready的时间，也是//selecotr会阻塞的时间</span></span><br><span class=\"line\">    RecordAccumulator.ReadyCheckResult result = <span class=\"keyword\">this</span>.accumulator.ready(cluster, now);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class=\"line\">            <span class=\"keyword\">this</span>.metadata.add(topic);</span><br><span class=\"line\"></span><br><span class=\"line\">        log.debug(<span class=\"string\">\"Requesting metadata update due to unknown leader topics from the batched records: &#123;&#125;\"</span>,</span><br><span class=\"line\">            result.unknownLeaderTopics);</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.metadata.requestUpdate();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class=\"line\">    <span class=\"keyword\">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (iter.hasNext()) &#123;</span><br><span class=\"line\">        Node node = iter.next();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!<span class=\"keyword\">this</span>.client.ready(node, now)) &#123;</span><br><span class=\"line\">            iter.remove();</span><br><span class=\"line\">            notReadyTimeout = Math.min(notReadyTimeout, <span class=\"keyword\">this</span>.client.pollDelayMs(node, now));</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// create produce requests</span></span><br><span class=\"line\">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class=\"keyword\">this</span>.accumulator.drain(cluster, result.readyNodes, <span class=\"keyword\">this</span>.maxRequestSize, now);</span><br><span class=\"line\">    addToInflightBatches(batches);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (guaranteeMessageOrder) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Mute all the partitions drained</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (ProducerBatch batch : batchList)</span><br><span class=\"line\">                <span class=\"keyword\">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    accumulator.resetNextBatchExpiryTime();</span><br><span class=\"line\">    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);</span><br><span class=\"line\">    List&lt;ProducerBatch&gt; expiredBatches = <span class=\"keyword\">this</span>.accumulator.expiredBatches(now);</span><br><span class=\"line\">    expiredBatches.addAll(expiredInflightBatches);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!expiredBatches.isEmpty())</span><br><span class=\"line\">        log.trace(<span class=\"string\">\"Expired &#123;&#125; batches in accumulator\"</span>, expiredBatches.size());</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ProducerBatch expiredBatch : expiredBatches) &#123;</span><br><span class=\"line\">        String errorMessage = <span class=\"string\">\"Expiring \"</span> + expiredBatch.recordCount + <span class=\"string\">\" record(s) for \"</span> + expiredBatch.topicPartition</span><br><span class=\"line\">            + <span class=\"string\">\":\"</span> + (now - expiredBatch.createdMs) + <span class=\"string\">\" ms has passed since batch creation\"</span>;</span><br><span class=\"line\">        failBatch(expiredBatch, -<span class=\"number\">1</span>, NO_TIMESTAMP, <span class=\"keyword\">new</span> TimeoutException(errorMessage), <span class=\"keyword\">false</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (transactionManager != <span class=\"keyword\">null</span> &amp;&amp; expiredBatch.inRetry()) &#123;</span><br><span class=\"line\">            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    sensors.updateProduceRequestMetrics(batches);</span><br><span class=\"line\">   <span class=\"comment\">// 暂且只关心result.nextReadyCheckDelayMs</span></span><br><span class=\"line\">    <span class=\"keyword\">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class=\"line\">    pollTimeout = Math.min(pollTimeout, <span class=\"keyword\">this</span>.accumulator.nextExpiryTimeMs() - now);</span><br><span class=\"line\">    pollTimeout = Math.max(pollTimeout, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!result.readyNodes.isEmpty()) &#123;</span><br><span class=\"line\">        log.trace(<span class=\"string\">\"Nodes with data ready to send: &#123;&#125;\"</span>, result.readyNodes);</span><br><span class=\"line\"></span><br><span class=\"line\">        pollTimeout = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    sendProduceRequests(batches, now);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pollTimeout;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">long</span> pollTimeout = sendProducerData(now);</span><br><span class=\"line\"><span class=\"comment\">// poll最终会调用selector,pollTimeout也就是selector阻塞的时间</span></span><br><span class=\"line\"> client.poll(pollTimeout, now);</span><br></pre></td></tr></table></figure>\n<p>selector</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Check for data, waiting up to the given timeout.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> timeoutMs Length of time to wait, in milliseconds, which must be non-negative</span></span><br><span class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span> The number of keys ready</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">int</span> <span class=\"title\">select</span><span class=\"params\">(<span class=\"keyword\">long</span> timeoutMs)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (timeoutMs &lt; <span class=\"number\">0L</span>)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(<span class=\"string\">\"timeout should be &gt;= 0\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (timeoutMs == <span class=\"number\">0L</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.nioSelector.selectNow();</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>.nioSelector.select(timeoutMs);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> ReadyCheckResult <span class=\"title\">ready</span><span class=\"params\">(Cluster cluster, <span class=\"keyword\">long</span> nowMs)</span> </span>&#123;</span><br><span class=\"line\">    Set&lt;Node&gt; readyNodes = <span class=\"keyword\">new</span> HashSet&lt;&gt;();</span><br><span class=\"line\">    <span class=\"comment\">// 初始化为最大值</span></span><br><span class=\"line\">    <span class=\"keyword\">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</span><br><span class=\"line\">    Set&lt;String&gt; unknownLeaderTopics = <span class=\"keyword\">new</span> HashSet&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> exhausted = <span class=\"keyword\">this</span>.free.queued() &gt; <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; entry : <span class=\"keyword\">this</span>.batches.entrySet()) &#123;</span><br><span class=\"line\">        TopicPartition part = entry.getKey();</span><br><span class=\"line\">        Deque&lt;ProducerBatch&gt; deque = entry.getValue();</span><br><span class=\"line\"></span><br><span class=\"line\">        Node leader = cluster.leaderFor(part);</span><br><span class=\"line\">        <span class=\"keyword\">synchronized</span> (deque) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (leader == <span class=\"keyword\">null</span> &amp;&amp; !deque.isEmpty()) &#123;</span><br><span class=\"line\">                unknownLeaderTopics.add(part.topic());</span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!readyNodes.contains(leader) &amp;&amp; !isMuted(part, nowMs)) &#123;</span><br><span class=\"line\">                ProducerBatch batch = deque.peekFirst();</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (batch != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">long</span> waitedTimeMs = batch.waitedTimeMs(nowMs);</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> backingOff = batch.attempts() &gt; <span class=\"number\">0</span> &amp;&amp; waitedTimeMs &lt; retryBackoffMs;</span><br><span class=\"line\">                    <span class=\"comment\">// 和linger.ms有关</span></span><br><span class=\"line\">                    <span class=\"keyword\">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> full = deque.size() &gt; <span class=\"number\">1</span> || batch.isFull();</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs;</span><br><span class=\"line\">                    <span class=\"keyword\">boolean</span> sendable = full || expired || exhausted || closed || flushInProgress();</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (sendable &amp;&amp; !backingOff) &#123;</span><br><span class=\"line\">                        readyNodes.add(leader);</span><br><span class=\"line\">                    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class=\"number\">0</span>);</span><br><span class=\"line\">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们可以从实例化一个新的KafkaProducer开始分析（还没有调用send方法），在sender线程调用accumulator#ready(..)时候，会返回result，其中包含selector可能要阻塞的时间。由于还没有调用send方法，所以Deque<recordbatch>为空，所以result中包含的nextReadyCheckDelayMs也是最大值，这个时候selector会一直阻塞。</recordbatch></p>\n<p>然后我们调用send方法,该方法会调用waitOnMetadata，waitOnMetadata会调用<code>sender.wakeup()</code>，所以会唤醒sender线程。</p>\n<p>这个时候会唤醒阻塞在selector#select(..)的sender线程，sender线程又运行到accumulator#ready(..)，当Deque<producerbatch>有值，所以返回的result包含的nextReadyCheckDelayMs不再是最大值，而是和linger.ms有关的值。也就是时候selector会最多阻塞lingger.ms后就返回，然后再次轮询。（根据源码分析，第一条消息会创建batch,因此newBatchCreated为true,同样会触发唤醒sender）</producerbatch></p>\n<p>如果有一个ProducerBatch满了，也会调用Sender#wakeup(..)，</p>\n<p>KafkaProducer#doSend(…)</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class=\"line\">               log.trace(<span class=\"string\">\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\"</span>, record.topic(), partition);</span><br><span class=\"line\">               <span class=\"keyword\">this</span>.sender.wakeup();</span><br><span class=\"line\">           &#125;</span><br></pre></td></tr></table></figure>\n<p>所以综上所述：只要满足linger.ms和batch.size满了就会激活sender线程来发送消息。</p>\n<h2 id=\"客户端顺序性保证\"><a href=\"#客户端顺序性保证\" class=\"headerlink\" title=\"客户端顺序性保证\"></a>客户端顺序性保证</h2><p>因为消息发送是批量发送的，那么就有可能存在上一批发送失败，接下来一批发送成功。即会出现数据乱序。</p>\n<p><code>max.in.flight.requests.per.connection=1</code>,限制客户端在单个连接上能够发送的未响应请求的个数。如果业务对顺序性有很高的要求，将该参数值设置为1。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>kafka producer大概流程如上，数据发送是<strong>批量</strong>的，最后是按Node发送响应的消息的。即其中Integer即为broker id。消息在RecordAccumulator中还是按TopicPartition存放的，但是在最终发送会作相应的转换。</p>\n<p>sender发送的时机是由两个指标决定的，一个是时间<code>linger.ms</code>，一个是数据量大小 <code>batch.size</code></p>\n<p>了解producer发送流程，这样就可以让我们更改的生产消息，使用更多的特性，例如拦截器可以做各种特殊处理的场景。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><h2 id=\"1-kafka-producer的batch-size和linger-ms\"><a href=\"#1-kafka-producer的batch-size和linger-ms\" class=\"headerlink\" title=\"1. kafka producer的batch.size和linger.ms\"></a>1. <a href=\"https://www.cnblogs.com/set-cookie/p/8902340.html\" target=\"_blank\" rel=\"noopener\">kafka producer的batch.size和linger.ms</a></h2>"},{"title":"kafka幂等性和事务使用及实现原理","originContent":"","toc":false,"date":"2019-07-08T11:13:41.000Z","_content":"\n# kafka幂等性和事务使用及实现原理\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章解决如下问题：\n\n1. 如何开启幂等性？\n2. 如何使用事务？\n3. 幂等性的原理\n4. 事务实现原理\n\n## 正文\n\n### Producer 幂等性\n\nProducer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：\n\n- 只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;\n- 幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。\n\n如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。\n\n使用方式：`props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");`\n\n当幂等性开启的时候acks即为all。如果显性的将acks设置为0，-1，那么将会报错`Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.`\n\n示例：\n\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:9092\");\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n\nKafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);\nkafkaProducer.send(new ProducerRecord<String, String>(\"truman_kafka_center\", \"1\", \"hello world.\")).get();\nkafkaProducer.close();\n```\n\n### 幂等性原理\n\n幂等性是通过两个关键信息保证的，PID(Producer ID)和sequence numbers。\n\n- PID 用来标识每个producer client\n- sequence numbers 客户端发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复\n\nproducer初始化会由server端生成一个PID,然后发送每条信息都包含该PID和sequence number，在server端，是按照partition同样存放一个sequence numbers 信息，通过判断客户端发送过来的sequence number与server端number+1差值来决定数据是否重复或者漏掉。\n\n通常情况下为了保证数据顺序性，我们可以通过`max.in.flight.requests.per.connection=1`来保证，这个也只是针对单实例。在kafka2.0+版本上，只要开启幂等性，不用设置这个参数也能保证发送数据的顺序性。\n\n### 为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5\n\n其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。\n\n假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（**相当于client 狂发错误请求**）。\n\n### Kafka 事务性\n\n#### 示例\n\n```java\n//Producer\nProperties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:9092\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"transactional_id-0\");\nKafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);\nkafkaProducer.initTransactions();\nkafkaProducer.beginTransaction();\nfor (int i = 0; i < 10; i++) {\n    kafkaProducer.send(new ProducerRecord<String, String>(\"truman_kafka_center\", \"key\"+i, \"hello world.\")).get();\n}\nkafkaProducer.commitTransaction();\nkafkaProducer.close();\n//Consumer\nProperties config = new Properties();\nconfig.put(\"group.id\", \"test11\");\nconfig.put(\"bootstrap.servers\", \"127.0.0.1:9092\");\nconfig.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nconfig.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nconfig.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, \"read_committed\");\n\nKafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(config);\nconsumer.subscribe(Arrays.asList(TOPIC));\nboolean isConsumer = true;\nwhile (isConsumer) {\n    ConsumerRecords<String, String> records = (ConsumerRecords<String, String>) consumer\n        .poll(Duration.ofMillis(100));\n    for (ConsumerRecord<String, String> record : records) {\n        System.out.println(\"consumer message: key =\" + record.key() + \" value:\" + record.value());\n    }\n}\nconsumer.close();\n}\n```\n\n#### 事务实现原理\n\n###### (1)查找TransactionCoordinator\n\n通过transaction_id 找到TransactionCoordinator，具体算法是`Utils.abs(transaction_id.hashCode %transactionTopicPartitionCount )`，获取到partition，再找到该partition的leader,即为TransactionCoordinator。\n\n###### (2)获取PID\n凡是开启幂等性都是需要生成PID(Producer ID),只不过未开启事务的PID可以在任意broker生成，而开启事务只能在TransactionCoordinator节点生成。这里只讲开启事务的情况，Producer Client的`initTransactions()`方法会向TransactionCoordinator发起InitPidRequest ，这样就能获取PID。这里面还有一些细节问题，这里不探讨，例如transaction_id 之前的事务状态什么的。但需要说明的一点是这里**会将 transaction_id  与相应的 TransactionMetadata 持久化到事务日志**（_transaction_state）中。\n\n###### (3)开启事务\n\nProducer调用`beginTransaction`开始一个事务状态，这里只是在客户端将本地事务状态转移成 IN_TRANSACTION，只有在发送第一条信息后，TransactionCoordinator才会认为该事务已经开启。\n\n###### (4)Consume-Porcess-Produce Loop\n\n这里说的是一个典型的`consume-process-produce`场景：\n\n```java\nwhile (true) {\n    ConsumerRecords records = consumer.poll(Duration.ofMillis(1000));\n    producer.beginTransaction();\n    //start\n    for (ConsumerRecord record : records){\n        producer.send(producerRecord(“outputTopic1”, record));\n        producer.send(producerRecord(“outputTopic2”, record));\n    }\n    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);\n    //end\n    producer.commitTransaction();\n}\n```\n该阶段主要经历以下几个步骤：\n1. AddPartitionsToTxnRequest\n2. ProduceRequest\n3. AddOffsetsToTxnRequest\n4. TxnOffsetsCommitRequest\n\n关于这里的详细介绍可以查看参考链接，或者直接查看官网文档！\n\n###### (5)提交或者中断事务\n\nProducer 调用 `commitTransaction()` 或者 `abortTransaction()` 方法来 commit 或者 abort 这个事务操作。\n\n基本上经历以下三个步骤，才真正结束事务。\n\n1. EndTxnRequest\n2. WriteTxnMarkerRquest\n3. Writing the Final Commit or Abort Message\n\n其中EndTxnRequest是在Producer发起的请求，其他阶段都是在TransactionCoordinator端发起完成的。WriteTxnMarkerRquest是发送请求到partition的leader上写入事务结果信息（ControlBatch）,第三步主要是在`_transaction_state`中标记事务的结束。\n\n\n\n\n## 参考\n\n1.[Kafka 事务性之幂等性实现](http://matt33.com/2018/10/24/kafka-idempotent/)\n\n2.[Kafka Exactly-Once 之事务性实现](http://matt33.com/2018/11/04/kafka-transaction/)\n\n3[KIP-98 - Exactly Once Delivery and Transactional Messaging](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\n","source":"_posts/kafka幂等性和事务使用及实现原理.md","raw":"---\ntitle: kafka幂等性和事务使用及实现原理\ntags:\n  - kafka\n  - 专栏\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-07-08 19:13:41\n---\n\n# kafka幂等性和事务使用及实现原理\n\n## 开篇\n\n在开始这篇之前，先抛出问题，这章解决如下问题：\n\n1. 如何开启幂等性？\n2. 如何使用事务？\n3. 幂等性的原理\n4. 事务实现原理\n\n## 正文\n\n### Producer 幂等性\n\nProducer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：\n\n- 只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;\n- 幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。\n\n如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。\n\n使用方式：`props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");`\n\n当幂等性开启的时候acks即为all。如果显性的将acks设置为0，-1，那么将会报错`Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.`\n\n示例：\n\n```java\nProperties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:9092\");\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n\nKafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);\nkafkaProducer.send(new ProducerRecord<String, String>(\"truman_kafka_center\", \"1\", \"hello world.\")).get();\nkafkaProducer.close();\n```\n\n### 幂等性原理\n\n幂等性是通过两个关键信息保证的，PID(Producer ID)和sequence numbers。\n\n- PID 用来标识每个producer client\n- sequence numbers 客户端发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复\n\nproducer初始化会由server端生成一个PID,然后发送每条信息都包含该PID和sequence number，在server端，是按照partition同样存放一个sequence numbers 信息，通过判断客户端发送过来的sequence number与server端number+1差值来决定数据是否重复或者漏掉。\n\n通常情况下为了保证数据顺序性，我们可以通过`max.in.flight.requests.per.connection=1`来保证，这个也只是针对单实例。在kafka2.0+版本上，只要开启幂等性，不用设置这个参数也能保证发送数据的顺序性。\n\n### 为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5\n\n其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。\n\n假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（**相当于client 狂发错误请求**）。\n\n### Kafka 事务性\n\n#### 示例\n\n```java\n//Producer\nProperties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:9092\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\nprops.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"transactional_id-0\");\nKafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);\nkafkaProducer.initTransactions();\nkafkaProducer.beginTransaction();\nfor (int i = 0; i < 10; i++) {\n    kafkaProducer.send(new ProducerRecord<String, String>(\"truman_kafka_center\", \"key\"+i, \"hello world.\")).get();\n}\nkafkaProducer.commitTransaction();\nkafkaProducer.close();\n//Consumer\nProperties config = new Properties();\nconfig.put(\"group.id\", \"test11\");\nconfig.put(\"bootstrap.servers\", \"127.0.0.1:9092\");\nconfig.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nconfig.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\nconfig.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, \"read_committed\");\n\nKafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(config);\nconsumer.subscribe(Arrays.asList(TOPIC));\nboolean isConsumer = true;\nwhile (isConsumer) {\n    ConsumerRecords<String, String> records = (ConsumerRecords<String, String>) consumer\n        .poll(Duration.ofMillis(100));\n    for (ConsumerRecord<String, String> record : records) {\n        System.out.println(\"consumer message: key =\" + record.key() + \" value:\" + record.value());\n    }\n}\nconsumer.close();\n}\n```\n\n#### 事务实现原理\n\n###### (1)查找TransactionCoordinator\n\n通过transaction_id 找到TransactionCoordinator，具体算法是`Utils.abs(transaction_id.hashCode %transactionTopicPartitionCount )`，获取到partition，再找到该partition的leader,即为TransactionCoordinator。\n\n###### (2)获取PID\n凡是开启幂等性都是需要生成PID(Producer ID),只不过未开启事务的PID可以在任意broker生成，而开启事务只能在TransactionCoordinator节点生成。这里只讲开启事务的情况，Producer Client的`initTransactions()`方法会向TransactionCoordinator发起InitPidRequest ，这样就能获取PID。这里面还有一些细节问题，这里不探讨，例如transaction_id 之前的事务状态什么的。但需要说明的一点是这里**会将 transaction_id  与相应的 TransactionMetadata 持久化到事务日志**（_transaction_state）中。\n\n###### (3)开启事务\n\nProducer调用`beginTransaction`开始一个事务状态，这里只是在客户端将本地事务状态转移成 IN_TRANSACTION，只有在发送第一条信息后，TransactionCoordinator才会认为该事务已经开启。\n\n###### (4)Consume-Porcess-Produce Loop\n\n这里说的是一个典型的`consume-process-produce`场景：\n\n```java\nwhile (true) {\n    ConsumerRecords records = consumer.poll(Duration.ofMillis(1000));\n    producer.beginTransaction();\n    //start\n    for (ConsumerRecord record : records){\n        producer.send(producerRecord(“outputTopic1”, record));\n        producer.send(producerRecord(“outputTopic2”, record));\n    }\n    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);\n    //end\n    producer.commitTransaction();\n}\n```\n该阶段主要经历以下几个步骤：\n1. AddPartitionsToTxnRequest\n2. ProduceRequest\n3. AddOffsetsToTxnRequest\n4. TxnOffsetsCommitRequest\n\n关于这里的详细介绍可以查看参考链接，或者直接查看官网文档！\n\n###### (5)提交或者中断事务\n\nProducer 调用 `commitTransaction()` 或者 `abortTransaction()` 方法来 commit 或者 abort 这个事务操作。\n\n基本上经历以下三个步骤，才真正结束事务。\n\n1. EndTxnRequest\n2. WriteTxnMarkerRquest\n3. Writing the Final Commit or Abort Message\n\n其中EndTxnRequest是在Producer发起的请求，其他阶段都是在TransactionCoordinator端发起完成的。WriteTxnMarkerRquest是发送请求到partition的leader上写入事务结果信息（ControlBatch）,第三步主要是在`_transaction_state`中标记事务的结束。\n\n\n\n\n## 参考\n\n1.[Kafka 事务性之幂等性实现](http://matt33.com/2018/10/24/kafka-idempotent/)\n\n2.[Kafka Exactly-Once 之事务性实现](http://matt33.com/2018/11/04/kafka-transaction/)\n\n3[KIP-98 - Exactly Once Delivery and Transactional Messaging](https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging)\n\n","slug":"kafka幂等性和事务使用及实现原理","published":1,"updated":"2019-07-08T11:13:41.927Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgj00al8ceewbq5xooh","content":"<h1 id=\"kafka幂等性和事务使用及实现原理\"><a href=\"#kafka幂等性和事务使用及实现原理\" class=\"headerlink\" title=\"kafka幂等性和事务使用及实现原理\"></a>kafka幂等性和事务使用及实现原理</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章解决如下问题：</p>\n<ol>\n<li>如何开启幂等性？</li>\n<li>如何使用事务？</li>\n<li>幂等性的原理</li>\n<li>事务实现原理</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Producer-幂等性\"><a href=\"#Producer-幂等性\" class=\"headerlink\" title=\"Producer 幂等性\"></a>Producer 幂等性</h3><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p>\n<ul>\n<li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li>\n<li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li>\n</ul>\n<p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p>\n<p>使用方式：<code>props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, &quot;true&quot;);</code></p>\n<p>当幂等性开启的时候acks即为all。如果显性的将acks设置为0，-1，那么将会报错<code>Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.</code></p>\n<p>示例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Properties props = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.ACKS_CONFIG, <span class=\"string\">\"all\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class=\"string\">\"true\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class=\"line\">kafkaProducer.send(<span class=\"keyword\">new</span> ProducerRecord&lt;String, String&gt;(<span class=\"string\">\"truman_kafka_center\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"hello world.\"</span>)).get();</span><br><span class=\"line\">kafkaProducer.close();</span><br></pre></td></tr></table></figure>\n<h3 id=\"幂等性原理\"><a href=\"#幂等性原理\" class=\"headerlink\" title=\"幂等性原理\"></a>幂等性原理</h3><p>幂等性是通过两个关键信息保证的，PID(Producer ID)和sequence numbers。</p>\n<ul>\n<li>PID 用来标识每个producer client</li>\n<li>sequence numbers 客户端发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复</li>\n</ul>\n<p>producer初始化会由server端生成一个PID,然后发送每条信息都包含该PID和sequence number，在server端，是按照partition同样存放一个sequence numbers 信息，通过判断客户端发送过来的sequence number与server端number+1差值来决定数据是否重复或者漏掉。</p>\n<p>通常情况下为了保证数据顺序性，我们可以通过<code>max.in.flight.requests.per.connection=1</code>来保证，这个也只是针对单实例。在kafka2.0+版本上，只要开启幂等性，不用设置这个参数也能保证发送数据的顺序性。</p>\n<h3 id=\"为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5\"><a href=\"#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5\" class=\"headerlink\" title=\"为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5\"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p>\n<p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（<strong>相当于client 狂发错误请求</strong>）。</p>\n<h3 id=\"Kafka-事务性\"><a href=\"#Kafka-事务性\" class=\"headerlink\" title=\"Kafka 事务性\"></a>Kafka 事务性</h3><h4 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//Producer</span></span><br><span class=\"line\">Properties props = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class=\"string\">\"true\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class=\"string\">\"transactional_id-0\"</span>);</span><br><span class=\"line\">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class=\"line\">kafkaProducer.initTransactions();</span><br><span class=\"line\">kafkaProducer.beginTransaction();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    kafkaProducer.send(<span class=\"keyword\">new</span> ProducerRecord&lt;String, String&gt;(<span class=\"string\">\"truman_kafka_center\"</span>, <span class=\"string\">\"key\"</span>+i, <span class=\"string\">\"hello world.\"</span>)).get();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">kafkaProducer.commitTransaction();</span><br><span class=\"line\">kafkaProducer.close();</span><br><span class=\"line\"><span class=\"comment\">//Consumer</span></span><br><span class=\"line\">Properties config = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">config.put(<span class=\"string\">\"group.id\"</span>, <span class=\"string\">\"test11\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"bootstrap.servers\"</span>, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"key.deserializer\"</span>, <span class=\"string\">\"org.apache.kafka.common.serialization.StringDeserializer\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"value.deserializer\"</span>, <span class=\"string\">\"org.apache.kafka.common.serialization.StringDeserializer\"</span>);</span><br><span class=\"line\">config.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, <span class=\"string\">\"read_committed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">KafkaConsumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;String, String&gt;(config);</span><br><span class=\"line\">consumer.subscribe(Arrays.asList(TOPIC));</span><br><span class=\"line\"><span class=\"keyword\">boolean</span> isConsumer = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span> (isConsumer) &#123;</span><br><span class=\"line\">    ConsumerRecords&lt;String, String&gt; records = (ConsumerRecords&lt;String, String&gt;) consumer</span><br><span class=\"line\">        .poll(Duration.ofMillis(<span class=\"number\">100</span>));</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"consumer message: key =\"</span> + record.key() + <span class=\"string\">\" value:\"</span> + record.value());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">consumer.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"事务实现原理\"><a href=\"#事务实现原理\" class=\"headerlink\" title=\"事务实现原理\"></a>事务实现原理</h4><h6 id=\"1-查找TransactionCoordinator\"><a href=\"#1-查找TransactionCoordinator\" class=\"headerlink\" title=\"(1)查找TransactionCoordinator\"></a>(1)查找TransactionCoordinator</h6><p>通过transaction_id 找到TransactionCoordinator，具体算法是<code>Utils.abs(transaction_id.hashCode %transactionTopicPartitionCount )</code>，获取到partition，再找到该partition的leader,即为TransactionCoordinator。</p>\n<h6 id=\"2-获取PID\"><a href=\"#2-获取PID\" class=\"headerlink\" title=\"(2)获取PID\"></a>(2)获取PID</h6><p>凡是开启幂等性都是需要生成PID(Producer ID),只不过未开启事务的PID可以在任意broker生成，而开启事务只能在TransactionCoordinator节点生成。这里只讲开启事务的情况，Producer Client的<code>initTransactions()</code>方法会向TransactionCoordinator发起InitPidRequest ，这样就能获取PID。这里面还有一些细节问题，这里不探讨，例如transaction_id 之前的事务状态什么的。但需要说明的一点是这里<strong>会将 transaction_id  与相应的 TransactionMetadata 持久化到事务日志</strong>（_transaction_state）中。</p>\n<h6 id=\"3-开启事务\"><a href=\"#3-开启事务\" class=\"headerlink\" title=\"(3)开启事务\"></a>(3)开启事务</h6><p>Producer调用<code>beginTransaction</code>开始一个事务状态，这里只是在客户端将本地事务状态转移成 IN_TRANSACTION，只有在发送第一条信息后，TransactionCoordinator才会认为该事务已经开启。</p>\n<h6 id=\"4-Consume-Porcess-Produce-Loop\"><a href=\"#4-Consume-Porcess-Produce-Loop\" class=\"headerlink\" title=\"(4)Consume-Porcess-Produce Loop\"></a>(4)Consume-Porcess-Produce Loop</h6><p>这里说的是一个典型的<code>consume-process-produce</code>场景：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">    ConsumerRecords records = consumer.poll(Duration.ofMillis(<span class=\"number\">1000</span>));</span><br><span class=\"line\">    producer.beginTransaction();</span><br><span class=\"line\">    <span class=\"comment\">//start</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ConsumerRecord record : records)&#123;</span><br><span class=\"line\">        producer.send(producerRecord(“outputTopic1”, record));</span><br><span class=\"line\">        producer.send(producerRecord(“outputTopic2”, record));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class=\"line\">    <span class=\"comment\">//end</span></span><br><span class=\"line\">    producer.commitTransaction();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该阶段主要经历以下几个步骤：</p>\n<ol>\n<li>AddPartitionsToTxnRequest</li>\n<li>ProduceRequest</li>\n<li>AddOffsetsToTxnRequest</li>\n<li>TxnOffsetsCommitRequest</li>\n</ol>\n<p>关于这里的详细介绍可以查看参考链接，或者直接查看官网文档！</p>\n<h6 id=\"5-提交或者中断事务\"><a href=\"#5-提交或者中断事务\" class=\"headerlink\" title=\"(5)提交或者中断事务\"></a>(5)提交或者中断事务</h6><p>Producer 调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p>\n<p>基本上经历以下三个步骤，才真正结束事务。</p>\n<ol>\n<li>EndTxnRequest</li>\n<li>WriteTxnMarkerRquest</li>\n<li>Writing the Final Commit or Abort Message</li>\n</ol>\n<p>其中EndTxnRequest是在Producer发起的请求，其他阶段都是在TransactionCoordinator端发起完成的。WriteTxnMarkerRquest是发送请求到partition的leader上写入事务结果信息（ControlBatch）,第三步主要是在<code>_transaction_state</code>中标记事务的结束。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://matt33.com/2018/10/24/kafka-idempotent/\" target=\"_blank\" rel=\"noopener\">Kafka 事务性之幂等性实现</a></p>\n<p>2.<a href=\"http://matt33.com/2018/11/04/kafka-transaction/\" target=\"_blank\" rel=\"noopener\">Kafka Exactly-Once 之事务性实现</a></p>\n<p>3<a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging\" target=\"_blank\" rel=\"noopener\">KIP-98 - Exactly Once Delivery and Transactional Messaging</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka幂等性和事务使用及实现原理\"><a href=\"#kafka幂等性和事务使用及实现原理\" class=\"headerlink\" title=\"kafka幂等性和事务使用及实现原理\"></a>kafka幂等性和事务使用及实现原理</h1><h2 id=\"开篇\"><a href=\"#开篇\" class=\"headerlink\" title=\"开篇\"></a>开篇</h2><p>在开始这篇之前，先抛出问题，这章解决如下问题：</p>\n<ol>\n<li>如何开启幂等性？</li>\n<li>如何使用事务？</li>\n<li>幂等性的原理</li>\n<li>事务实现原理</li>\n</ol>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"Producer-幂等性\"><a href=\"#Producer-幂等性\" class=\"headerlink\" title=\"Producer 幂等性\"></a>Producer 幂等性</h3><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p>\n<ul>\n<li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li>\n<li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li>\n</ul>\n<p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p>\n<p>使用方式：<code>props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, &quot;true&quot;);</code></p>\n<p>当幂等性开启的时候acks即为all。如果显性的将acks设置为0，-1，那么将会报错<code>Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.</code></p>\n<p>示例：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Properties props = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.ACKS_CONFIG, <span class=\"string\">\"all\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class=\"string\">\"true\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class=\"line\">kafkaProducer.send(<span class=\"keyword\">new</span> ProducerRecord&lt;String, String&gt;(<span class=\"string\">\"truman_kafka_center\"</span>, <span class=\"string\">\"1\"</span>, <span class=\"string\">\"hello world.\"</span>)).get();</span><br><span class=\"line\">kafkaProducer.close();</span><br></pre></td></tr></table></figure>\n<h3 id=\"幂等性原理\"><a href=\"#幂等性原理\" class=\"headerlink\" title=\"幂等性原理\"></a>幂等性原理</h3><p>幂等性是通过两个关键信息保证的，PID(Producer ID)和sequence numbers。</p>\n<ul>\n<li>PID 用来标识每个producer client</li>\n<li>sequence numbers 客户端发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复</li>\n</ul>\n<p>producer初始化会由server端生成一个PID,然后发送每条信息都包含该PID和sequence number，在server端，是按照partition同样存放一个sequence numbers 信息，通过判断客户端发送过来的sequence number与server端number+1差值来决定数据是否重复或者漏掉。</p>\n<p>通常情况下为了保证数据顺序性，我们可以通过<code>max.in.flight.requests.per.connection=1</code>来保证，这个也只是针对单实例。在kafka2.0+版本上，只要开启幂等性，不用设置这个参数也能保证发送数据的顺序性。</p>\n<h3 id=\"为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5\"><a href=\"#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5\" class=\"headerlink\" title=\"为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5\"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p>\n<p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（<strong>相当于client 狂发错误请求</strong>）。</p>\n<h3 id=\"Kafka-事务性\"><a href=\"#Kafka-事务性\" class=\"headerlink\" title=\"Kafka 事务性\"></a>Kafka 事务性</h3><h4 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h4><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//Producer</span></span><br><span class=\"line\">Properties props = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class=\"line\">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class=\"string\">\"true\"</span>);</span><br><span class=\"line\">props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class=\"string\">\"transactional_id-0\"</span>);</span><br><span class=\"line\">KafkaProducer&lt;String, String&gt; kafkaProducer = <span class=\"keyword\">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class=\"line\">kafkaProducer.initTransactions();</span><br><span class=\"line\">kafkaProducer.beginTransaction();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    kafkaProducer.send(<span class=\"keyword\">new</span> ProducerRecord&lt;String, String&gt;(<span class=\"string\">\"truman_kafka_center\"</span>, <span class=\"string\">\"key\"</span>+i, <span class=\"string\">\"hello world.\"</span>)).get();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">kafkaProducer.commitTransaction();</span><br><span class=\"line\">kafkaProducer.close();</span><br><span class=\"line\"><span class=\"comment\">//Consumer</span></span><br><span class=\"line\">Properties config = <span class=\"keyword\">new</span> Properties();</span><br><span class=\"line\">config.put(<span class=\"string\">\"group.id\"</span>, <span class=\"string\">\"test11\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"bootstrap.servers\"</span>, <span class=\"string\">\"127.0.0.1:9092\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"key.deserializer\"</span>, <span class=\"string\">\"org.apache.kafka.common.serialization.StringDeserializer\"</span>);</span><br><span class=\"line\">config.put(<span class=\"string\">\"value.deserializer\"</span>, <span class=\"string\">\"org.apache.kafka.common.serialization.StringDeserializer\"</span>);</span><br><span class=\"line\">config.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, <span class=\"string\">\"read_committed\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">KafkaConsumer&lt;String, String&gt; consumer = <span class=\"keyword\">new</span> KafkaConsumer&lt;String, String&gt;(config);</span><br><span class=\"line\">consumer.subscribe(Arrays.asList(TOPIC));</span><br><span class=\"line\"><span class=\"keyword\">boolean</span> isConsumer = <span class=\"keyword\">true</span>;</span><br><span class=\"line\"><span class=\"keyword\">while</span> (isConsumer) &#123;</span><br><span class=\"line\">    ConsumerRecords&lt;String, String&gt; records = (ConsumerRecords&lt;String, String&gt;) consumer</span><br><span class=\"line\">        .poll(Duration.ofMillis(<span class=\"number\">100</span>));</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"consumer message: key =\"</span> + record.key() + <span class=\"string\">\" value:\"</span> + record.value());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">consumer.close();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"事务实现原理\"><a href=\"#事务实现原理\" class=\"headerlink\" title=\"事务实现原理\"></a>事务实现原理</h4><h6 id=\"1-查找TransactionCoordinator\"><a href=\"#1-查找TransactionCoordinator\" class=\"headerlink\" title=\"(1)查找TransactionCoordinator\"></a>(1)查找TransactionCoordinator</h6><p>通过transaction_id 找到TransactionCoordinator，具体算法是<code>Utils.abs(transaction_id.hashCode %transactionTopicPartitionCount )</code>，获取到partition，再找到该partition的leader,即为TransactionCoordinator。</p>\n<h6 id=\"2-获取PID\"><a href=\"#2-获取PID\" class=\"headerlink\" title=\"(2)获取PID\"></a>(2)获取PID</h6><p>凡是开启幂等性都是需要生成PID(Producer ID),只不过未开启事务的PID可以在任意broker生成，而开启事务只能在TransactionCoordinator节点生成。这里只讲开启事务的情况，Producer Client的<code>initTransactions()</code>方法会向TransactionCoordinator发起InitPidRequest ，这样就能获取PID。这里面还有一些细节问题，这里不探讨，例如transaction_id 之前的事务状态什么的。但需要说明的一点是这里<strong>会将 transaction_id  与相应的 TransactionMetadata 持久化到事务日志</strong>（_transaction_state）中。</p>\n<h6 id=\"3-开启事务\"><a href=\"#3-开启事务\" class=\"headerlink\" title=\"(3)开启事务\"></a>(3)开启事务</h6><p>Producer调用<code>beginTransaction</code>开始一个事务状态，这里只是在客户端将本地事务状态转移成 IN_TRANSACTION，只有在发送第一条信息后，TransactionCoordinator才会认为该事务已经开启。</p>\n<h6 id=\"4-Consume-Porcess-Produce-Loop\"><a href=\"#4-Consume-Porcess-Produce-Loop\" class=\"headerlink\" title=\"(4)Consume-Porcess-Produce Loop\"></a>(4)Consume-Porcess-Produce Loop</h6><p>这里说的是一个典型的<code>consume-process-produce</code>场景：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>) &#123;</span><br><span class=\"line\">    ConsumerRecords records = consumer.poll(Duration.ofMillis(<span class=\"number\">1000</span>));</span><br><span class=\"line\">    producer.beginTransaction();</span><br><span class=\"line\">    <span class=\"comment\">//start</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (ConsumerRecord record : records)&#123;</span><br><span class=\"line\">        producer.send(producerRecord(“outputTopic1”, record));</span><br><span class=\"line\">        producer.send(producerRecord(“outputTopic2”, record));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class=\"line\">    <span class=\"comment\">//end</span></span><br><span class=\"line\">    producer.commitTransaction();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该阶段主要经历以下几个步骤：</p>\n<ol>\n<li>AddPartitionsToTxnRequest</li>\n<li>ProduceRequest</li>\n<li>AddOffsetsToTxnRequest</li>\n<li>TxnOffsetsCommitRequest</li>\n</ol>\n<p>关于这里的详细介绍可以查看参考链接，或者直接查看官网文档！</p>\n<h6 id=\"5-提交或者中断事务\"><a href=\"#5-提交或者中断事务\" class=\"headerlink\" title=\"(5)提交或者中断事务\"></a>(5)提交或者中断事务</h6><p>Producer 调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p>\n<p>基本上经历以下三个步骤，才真正结束事务。</p>\n<ol>\n<li>EndTxnRequest</li>\n<li>WriteTxnMarkerRquest</li>\n<li>Writing the Final Commit or Abort Message</li>\n</ol>\n<p>其中EndTxnRequest是在Producer发起的请求，其他阶段都是在TransactionCoordinator端发起完成的。WriteTxnMarkerRquest是发送请求到partition的leader上写入事务结果信息（ControlBatch）,第三步主要是在<code>_transaction_state</code>中标记事务的结束。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>1.<a href=\"http://matt33.com/2018/10/24/kafka-idempotent/\" target=\"_blank\" rel=\"noopener\">Kafka 事务性之幂等性实现</a></p>\n<p>2.<a href=\"http://matt33.com/2018/11/04/kafka-transaction/\" target=\"_blank\" rel=\"noopener\">Kafka Exactly-Once 之事务性实现</a></p>\n<p>3<a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging\" target=\"_blank\" rel=\"noopener\">KIP-98 - Exactly Once Delivery and Transactional Messaging</a></p>\n"},{"title":"logstash插件开发","date":"2016-10-24T13:23:14.000Z","_content":"## 背景\nlogstash强大魅力在于它的插件体系，虽然官方插件很多，但不可能满足所有的要求，因此就需要定制化个性化插件，本次结合Logstash Monitor Redis需求开发专用插件，以实现动态化获取master 实例中info 信息。\n## logstash插件介绍\n### 体系结构\n```\n$ tree logstash-input-example\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── inputs\n│           └── example.rb\n├── logstash-input-example.gemspec\n└── spec\n    └── inputs\n        └── example_spec.rb\n```\n其实只需要这logstash-input-example.gemspec,example.rb两个文件即可。\nmypluginname_spec.rb 是测试类。\n\n先看看logstash-input-example.gemspec都做了什么吧！\n```\nGem::Specification.new do |s|\n  s.name = 'logstash-input-example'\n  s.version         = '2.0.4'\n  s.licenses = ['Apache License (2.0)']\n  s.summary = \"This example input streams a string at a definable interval.\"\n  s.description     = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\n  s.authors = [\"Elastic\"]\n  s.email = 'info@elastic.co'\n  s.homepage = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\n  s.require_paths = [\"lib\"]\n\n  # Files\n  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']\n   # Tests\n  s.test_files = s.files.grep(%r{^(test|spec|features)/})\n\n  # Special flag to let us know this is actually a logstash plugin\n  s.metadata = { \"logstash_plugin\" => \"true\", \"logstash_group\" => \"input\" }\n\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core\", \">= 2.0.0\", \"< 3.0.0\"\n  s.add_runtime_dependency 'logstash-codec-plain'\n  s.add_runtime_dependency 'stud', '>= 0.0.22'\n  s.add_development_dependency 'logstash-devutils', '>= 0.0.16'\nend\n```\n上面的信息，只要改改版本和名字，其他的信息基本不需要动。\n\n关键的信息还有：\n\n- s.require_paths定义了插件核心文件的位置\n- s.add_runtime_dependency 定义了插件运行的环境\n然后再看看example.rb\n这个文件就需要详细说说了，基本的框架如下，\n```\n# encoding: utf-8\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\nrequire \"stud/interval\"\nrequire \"socket\" # for Socket.gethostname\n\n# Generate a repeating message.\n#\n# This plugin is intented only as an example.\n\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n\n  # If undefined, Logstash will complain, even if codec is unused.\n  default :codec, \"plain\"\n\n  # The message string to use in the event.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n  # Set how frequently messages should be sent.\n  #\n  # The default, `1`, means send a message every second.\n  config :interval, :validate => :number, :default => 1\n\n  public\n  def register\n    @host = Socket.gethostname\n  end # def register\n\n  def run(queue)\n    # we can abort the loop if stop? becomes true\n    while !stop?\n      event = LogStash::Event.new(\"message\" => @message, \"host\" => @host)\n      decorate(event)\n      queue << event\n      # because the sleep interval can be big, when shutdown happens\n      # we want to be able to abort the sleep\n      # Stud.stoppable_sleep will frequently evaluate the given block\n      # and abort the sleep(@interval) if the return value is true\n      Stud.stoppable_sleep(@interval) { stop? }\n    end # loop\n  end # def run\n\n  def stop\n    # nothing to do in this case so it is not necessary to define stop\n    # examples of common \"stop\" tasks:\n    #  * close sockets (unblocking blocking reads/accepts)\n    #  * cleanup temporary files\n    #  * terminate spawned threads\n  end\nend # class LogStash::Inputs::Example\n```\n挨行看看！\n\n首先第一行的# encoding: utf-8,不要以为是注释就没什么作用。它定义了插件的编码方式。\n\n下面两行：\n\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\n引入了插件必备的包。\n```\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n```\n插件继承自Base基类，并配置插件的使用名称。\n\n下面的一行对参数做了配置，参数有很多的配置属性，完整的如下：\n```\n config :variable_name,:validate =>:variable_type,:default =>\"Default value\",:required => boolean,:deprecated => boolean\n```\n其中\n\nvariable_name就是参数的名称了。\nvalidate 定义是否进行校验，如果不是指定的类型，在logstash -f xxx --configtest的时候就会报错。它支持多种数据类型，比如:string, :password, :boolean, :number, :array, :hash, :path (a file-system path), :codec (since 1.2.0), :bytes.\ndefault 定义参数的默认值\nrequired 定义参数是否是必须值\ndeprecated 定义参数的额外信息，比如一个参数不再推荐使用了，就可以通过它给出提示！典型的就是es-output里面的Index_type，当使用这个参数时，就会给出提示\n### 插件安装\n1. 便捷安装方式\n\n第一步，首先把这个插件文件夹拷贝到下面的目录中\n```\nlogstash-2.1.0\\vendor\\bundle\\jruby\\1.9\\gems\n```\n第二步，修改logstash根目录下的Gemfile,添加如下的内容：\n```\ngem \"logstash-filter-example\", :path => \"vendor/bundle/jruby/1.9/gems/logstash-filter-example-1.0.0\"\n```\n第三步，编写配置文件，test.conf：\n```\ninput{\n    example{} \n}\nfilter{\n    \n}\noutput{\n    stdout{\n        codec => rubydebug\n    }\n}\n```\n第四步，输入logstash -f test.conf时，输入任意字符，回车~~~大功告成！\n```\n{\n       \"message\" => \"Hello World!\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2016-01-27T19:17:18.932Z\",\n          \"host\" => \"cadenza\"\n}\n```\n2. 官方指导方式\n\n第一步，build\n```\ngem build logstash-input-example.gemspec\n```\n会在当前路径下生成logstash-input-example-2.0.4.gem\n第二步，install\n\n```\nbin/logstash-plugin install /logstash-input-example/logstash-input-example-2.0.4.gem\n```\n验证\n```\nvalidating /logstash-input-example/logstash-input-example-2.0.4.gem >= 0\nValid logstash plugin. Continuing...\nSuccessfully installed 'logstash-input-example' with version '2.0.4'\n```\n第三步，查看plugin：\n```\nbin/logstash-plugin list\n```\n第四步，使用\n\n略\n\n## 开发案例\n开发插件实现根据cluster nodes信息获取redis cluster 中master节点 info信息。使用该插件只用输入一条命令，即可动态获取相关信息。\n### 插件开发\n此插件是基于exec基础上封装的，主要修改内容为：\n```\n  def execute(command, queue)\n    @logger.debug? && @logger.debug(\"Running exec\", :command => command)\n    begin\n\t  @io = IO.popen(command)\n\t  fields = (@io.read).split(/\\r\\n|\\n/)\n\t  puts fields\n\t  length = fields.length-1\n      \tfor i in 0..length do \n\t\t  if fields[i].include?':' then\n\t\t\tfield = fields[i].split(':')\n\t\t\tnewcommand = \"redis-cli -c -h #{field[0]} -p #{field[1]} info\"\n\t\t\t@io = IO.popen(newcommand)\n\t\t\t@codec.decode(@io.read) do |event|\n\t\t\tdecorate(event)\n\t\t\tevent.set(\"host\", @hostname)\n\t\t\tevent.set(\"command\", newcommand)\n\t\t\tqueue << event\n\t\t  end\n\t\tend\n\t\t\n      end\n    rescue StandardError => e\n      @logger.error(\"Error while running command\",\n        :command => command, :e => e, :backtrace => e.backtrace)\n    rescue Exception => e\n      @logger.error(\"Exception while running command\",\n        :command => command, :e => e, :backtrace => e.backtrace)\n    ensure\n      stop\n    end\n  end\n```\n### 使用Demo\n使用方式\n```\nredisexec {\ncommand => \"redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk '{print $2}'\"\ninterval => 20\ntype => \"info\"\n}\n```\n完整使用案例\n\n将info 信息存储到 ElasticSerach中\n```\ninput {\nredisexec {\ncommand => \"redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk '{print $2}'\"\ninterval => 20\ntype => \"info\"\n}\n}\nfilter {\n        grok {\n            match => {\"command\" => \"redis-cli -c -h %{IP:node:} -p %{NUMBER:port}%{DATA:data}\" }\n      remove_field => [ \"host\" ]\n        }\n    ruby {\n        code => \"fields = event['message'].split(/\\r\\n|\\n/)\n        length = fields.length-1\n        for i in 1..length do\n          if fields[i].include?':' then\n            field = fields[i].split(':')\n            event[field[0]] = field[1].to_f\n          end\n        end\n        \"\n        remove_field => [ \"message\" ]\n    }\n}\noutput {\n#stdout {  codec => rubydebug }\nelasticsearch {\nhosts => [\"127.0.0.1:9200\"]\ntemplate_overwrite => true\nindex => \"rediscluster-%{+YYYY.MM.dd}\"\nworkers => 5\n}\n}\n```\n## 参考\n1. http://www.cnblogs.com/xing901022/p/5259750.html\n2. https://github.com/logstash-plugins?utf8=%E2%9C%93&query=example\n3. https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html","source":"_posts/logstash插件开发.md","raw":"---\ntitle: logstash插件开发\ndate: 2016-10-24 21:23:14\ntags: research\ncategories:\n- elasticsearch\n---\n## 背景\nlogstash强大魅力在于它的插件体系，虽然官方插件很多，但不可能满足所有的要求，因此就需要定制化个性化插件，本次结合Logstash Monitor Redis需求开发专用插件，以实现动态化获取master 实例中info 信息。\n## logstash插件介绍\n### 体系结构\n```\n$ tree logstash-input-example\n├── Gemfile\n├── LICENSE\n├── README.md\n├── Rakefile\n├── lib\n│   └── logstash\n│       └── inputs\n│           └── example.rb\n├── logstash-input-example.gemspec\n└── spec\n    └── inputs\n        └── example_spec.rb\n```\n其实只需要这logstash-input-example.gemspec,example.rb两个文件即可。\nmypluginname_spec.rb 是测试类。\n\n先看看logstash-input-example.gemspec都做了什么吧！\n```\nGem::Specification.new do |s|\n  s.name = 'logstash-input-example'\n  s.version         = '2.0.4'\n  s.licenses = ['Apache License (2.0)']\n  s.summary = \"This example input streams a string at a definable interval.\"\n  s.description     = \"This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program\"\n  s.authors = [\"Elastic\"]\n  s.email = 'info@elastic.co'\n  s.homepage = \"http://www.elastic.co/guide/en/logstash/current/index.html\"\n  s.require_paths = [\"lib\"]\n\n  # Files\n  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']\n   # Tests\n  s.test_files = s.files.grep(%r{^(test|spec|features)/})\n\n  # Special flag to let us know this is actually a logstash plugin\n  s.metadata = { \"logstash_plugin\" => \"true\", \"logstash_group\" => \"input\" }\n\n  # Gem dependencies\n  s.add_runtime_dependency \"logstash-core\", \">= 2.0.0\", \"< 3.0.0\"\n  s.add_runtime_dependency 'logstash-codec-plain'\n  s.add_runtime_dependency 'stud', '>= 0.0.22'\n  s.add_development_dependency 'logstash-devutils', '>= 0.0.16'\nend\n```\n上面的信息，只要改改版本和名字，其他的信息基本不需要动。\n\n关键的信息还有：\n\n- s.require_paths定义了插件核心文件的位置\n- s.add_runtime_dependency 定义了插件运行的环境\n然后再看看example.rb\n这个文件就需要详细说说了，基本的框架如下，\n```\n# encoding: utf-8\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\nrequire \"stud/interval\"\nrequire \"socket\" # for Socket.gethostname\n\n# Generate a repeating message.\n#\n# This plugin is intented only as an example.\n\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n\n  # If undefined, Logstash will complain, even if codec is unused.\n  default :codec, \"plain\"\n\n  # The message string to use in the event.\n  config :message, :validate => :string, :default => \"Hello World!\"\n\n  # Set how frequently messages should be sent.\n  #\n  # The default, `1`, means send a message every second.\n  config :interval, :validate => :number, :default => 1\n\n  public\n  def register\n    @host = Socket.gethostname\n  end # def register\n\n  def run(queue)\n    # we can abort the loop if stop? becomes true\n    while !stop?\n      event = LogStash::Event.new(\"message\" => @message, \"host\" => @host)\n      decorate(event)\n      queue << event\n      # because the sleep interval can be big, when shutdown happens\n      # we want to be able to abort the sleep\n      # Stud.stoppable_sleep will frequently evaluate the given block\n      # and abort the sleep(@interval) if the return value is true\n      Stud.stoppable_sleep(@interval) { stop? }\n    end # loop\n  end # def run\n\n  def stop\n    # nothing to do in this case so it is not necessary to define stop\n    # examples of common \"stop\" tasks:\n    #  * close sockets (unblocking blocking reads/accepts)\n    #  * cleanup temporary files\n    #  * terminate spawned threads\n  end\nend # class LogStash::Inputs::Example\n```\n挨行看看！\n\n首先第一行的# encoding: utf-8,不要以为是注释就没什么作用。它定义了插件的编码方式。\n\n下面两行：\n\nrequire \"logstash/inputs/base\"\nrequire \"logstash/namespace\"\n引入了插件必备的包。\n```\nclass LogStash::Inputs::Example < LogStash::Inputs::Base\n  config_name \"example\"\n```\n插件继承自Base基类，并配置插件的使用名称。\n\n下面的一行对参数做了配置，参数有很多的配置属性，完整的如下：\n```\n config :variable_name,:validate =>:variable_type,:default =>\"Default value\",:required => boolean,:deprecated => boolean\n```\n其中\n\nvariable_name就是参数的名称了。\nvalidate 定义是否进行校验，如果不是指定的类型，在logstash -f xxx --configtest的时候就会报错。它支持多种数据类型，比如:string, :password, :boolean, :number, :array, :hash, :path (a file-system path), :codec (since 1.2.0), :bytes.\ndefault 定义参数的默认值\nrequired 定义参数是否是必须值\ndeprecated 定义参数的额外信息，比如一个参数不再推荐使用了，就可以通过它给出提示！典型的就是es-output里面的Index_type，当使用这个参数时，就会给出提示\n### 插件安装\n1. 便捷安装方式\n\n第一步，首先把这个插件文件夹拷贝到下面的目录中\n```\nlogstash-2.1.0\\vendor\\bundle\\jruby\\1.9\\gems\n```\n第二步，修改logstash根目录下的Gemfile,添加如下的内容：\n```\ngem \"logstash-filter-example\", :path => \"vendor/bundle/jruby/1.9/gems/logstash-filter-example-1.0.0\"\n```\n第三步，编写配置文件，test.conf：\n```\ninput{\n    example{} \n}\nfilter{\n    \n}\noutput{\n    stdout{\n        codec => rubydebug\n    }\n}\n```\n第四步，输入logstash -f test.conf时，输入任意字符，回车~~~大功告成！\n```\n{\n       \"message\" => \"Hello World!\",\n      \"@version\" => \"1\",\n    \"@timestamp\" => \"2016-01-27T19:17:18.932Z\",\n          \"host\" => \"cadenza\"\n}\n```\n2. 官方指导方式\n\n第一步，build\n```\ngem build logstash-input-example.gemspec\n```\n会在当前路径下生成logstash-input-example-2.0.4.gem\n第二步，install\n\n```\nbin/logstash-plugin install /logstash-input-example/logstash-input-example-2.0.4.gem\n```\n验证\n```\nvalidating /logstash-input-example/logstash-input-example-2.0.4.gem >= 0\nValid logstash plugin. Continuing...\nSuccessfully installed 'logstash-input-example' with version '2.0.4'\n```\n第三步，查看plugin：\n```\nbin/logstash-plugin list\n```\n第四步，使用\n\n略\n\n## 开发案例\n开发插件实现根据cluster nodes信息获取redis cluster 中master节点 info信息。使用该插件只用输入一条命令，即可动态获取相关信息。\n### 插件开发\n此插件是基于exec基础上封装的，主要修改内容为：\n```\n  def execute(command, queue)\n    @logger.debug? && @logger.debug(\"Running exec\", :command => command)\n    begin\n\t  @io = IO.popen(command)\n\t  fields = (@io.read).split(/\\r\\n|\\n/)\n\t  puts fields\n\t  length = fields.length-1\n      \tfor i in 0..length do \n\t\t  if fields[i].include?':' then\n\t\t\tfield = fields[i].split(':')\n\t\t\tnewcommand = \"redis-cli -c -h #{field[0]} -p #{field[1]} info\"\n\t\t\t@io = IO.popen(newcommand)\n\t\t\t@codec.decode(@io.read) do |event|\n\t\t\tdecorate(event)\n\t\t\tevent.set(\"host\", @hostname)\n\t\t\tevent.set(\"command\", newcommand)\n\t\t\tqueue << event\n\t\t  end\n\t\tend\n\t\t\n      end\n    rescue StandardError => e\n      @logger.error(\"Error while running command\",\n        :command => command, :e => e, :backtrace => e.backtrace)\n    rescue Exception => e\n      @logger.error(\"Exception while running command\",\n        :command => command, :e => e, :backtrace => e.backtrace)\n    ensure\n      stop\n    end\n  end\n```\n### 使用Demo\n使用方式\n```\nredisexec {\ncommand => \"redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk '{print $2}'\"\ninterval => 20\ntype => \"info\"\n}\n```\n完整使用案例\n\n将info 信息存储到 ElasticSerach中\n```\ninput {\nredisexec {\ncommand => \"redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk '{print $2}'\"\ninterval => 20\ntype => \"info\"\n}\n}\nfilter {\n        grok {\n            match => {\"command\" => \"redis-cli -c -h %{IP:node:} -p %{NUMBER:port}%{DATA:data}\" }\n      remove_field => [ \"host\" ]\n        }\n    ruby {\n        code => \"fields = event['message'].split(/\\r\\n|\\n/)\n        length = fields.length-1\n        for i in 1..length do\n          if fields[i].include?':' then\n            field = fields[i].split(':')\n            event[field[0]] = field[1].to_f\n          end\n        end\n        \"\n        remove_field => [ \"message\" ]\n    }\n}\noutput {\n#stdout {  codec => rubydebug }\nelasticsearch {\nhosts => [\"127.0.0.1:9200\"]\ntemplate_overwrite => true\nindex => \"rediscluster-%{+YYYY.MM.dd}\"\nworkers => 5\n}\n}\n```\n## 参考\n1. http://www.cnblogs.com/xing901022/p/5259750.html\n2. https://github.com/logstash-plugins?utf8=%E2%9C%93&query=example\n3. https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html","slug":"logstash插件开发","published":1,"updated":"2016-10-24T13:23:57.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgl00ao8cee8yh58d0l","content":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>logstash强大魅力在于它的插件体系，虽然官方插件很多，但不可能满足所有的要求，因此就需要定制化个性化插件，本次结合Logstash Monitor Redis需求开发专用插件，以实现动态化获取master 实例中info 信息。</p>\n<h2 id=\"logstash插件介绍\"><a href=\"#logstash插件介绍\" class=\"headerlink\" title=\"logstash插件介绍\"></a>logstash插件介绍</h2><h3 id=\"体系结构\"><a href=\"#体系结构\" class=\"headerlink\" title=\"体系结构\"></a>体系结构</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ tree logstash-input-example</span><br><span class=\"line\">├── Gemfile</span><br><span class=\"line\">├── LICENSE</span><br><span class=\"line\">├── README.md</span><br><span class=\"line\">├── Rakefile</span><br><span class=\"line\">├── lib</span><br><span class=\"line\">│   └── logstash</span><br><span class=\"line\">│       └── inputs</span><br><span class=\"line\">│           └── example.rb</span><br><span class=\"line\">├── logstash-input-example.gemspec</span><br><span class=\"line\">└── spec</span><br><span class=\"line\">    └── inputs</span><br><span class=\"line\">        └── example_spec.rb</span><br></pre></td></tr></table></figure>\n<p>其实只需要这logstash-input-example.gemspec,example.rb两个文件即可。<br>mypluginname_spec.rb 是测试类。</p>\n<p>先看看logstash-input-example.gemspec都做了什么吧！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Gem::Specification.new do |s|</span><br><span class=\"line\">  s.name = &apos;logstash-input-example&apos;</span><br><span class=\"line\">  s.version         = &apos;2.0.4&apos;</span><br><span class=\"line\">  s.licenses = [&apos;Apache License (2.0)&apos;]</span><br><span class=\"line\">  s.summary = &quot;This example input streams a string at a definable interval.&quot;</span><br><span class=\"line\">  s.description     = &quot;This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program&quot;</span><br><span class=\"line\">  s.authors = [&quot;Elastic&quot;]</span><br><span class=\"line\">  s.email = &apos;info@elastic.co&apos;</span><br><span class=\"line\">  s.homepage = &quot;http://www.elastic.co/guide/en/logstash/current/index.html&quot;</span><br><span class=\"line\">  s.require_paths = [&quot;lib&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">  # Files</span><br><span class=\"line\">  s.files = Dir[&apos;lib/**/*&apos;,&apos;spec/**/*&apos;,&apos;vendor/**/*&apos;,&apos;*.gemspec&apos;,&apos;*.md&apos;,&apos;CONTRIBUTORS&apos;,&apos;Gemfile&apos;,&apos;LICENSE&apos;,&apos;NOTICE.TXT&apos;]</span><br><span class=\"line\">   # Tests</span><br><span class=\"line\">  s.test_files = s.files.grep(%r&#123;^(test|spec|features)/&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  # Special flag to let us know this is actually a logstash plugin</span><br><span class=\"line\">  s.metadata = &#123; &quot;logstash_plugin&quot; =&gt; &quot;true&quot;, &quot;logstash_group&quot; =&gt; &quot;input&quot; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Gem dependencies</span><br><span class=\"line\">  s.add_runtime_dependency &quot;logstash-core&quot;, &quot;&gt;= 2.0.0&quot;, &quot;&lt; 3.0.0&quot;</span><br><span class=\"line\">  s.add_runtime_dependency &apos;logstash-codec-plain&apos;</span><br><span class=\"line\">  s.add_runtime_dependency &apos;stud&apos;, &apos;&gt;= 0.0.22&apos;</span><br><span class=\"line\">  s.add_development_dependency &apos;logstash-devutils&apos;, &apos;&gt;= 0.0.16&apos;</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></p>\n<p>上面的信息，只要改改版本和名字，其他的信息基本不需要动。</p>\n<p>关键的信息还有：</p>\n<ul>\n<li>s.require_paths定义了插件核心文件的位置</li>\n<li>s.add_runtime_dependency 定义了插件运行的环境<br>然后再看看example.rb<br>这个文件就需要详细说说了，基本的框架如下，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># encoding: utf-8</span><br><span class=\"line\">require &quot;logstash/inputs/base&quot;</span><br><span class=\"line\">require &quot;logstash/namespace&quot;</span><br><span class=\"line\">require &quot;stud/interval&quot;</span><br><span class=\"line\">require &quot;socket&quot; # for Socket.gethostname</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate a repeating message.</span><br><span class=\"line\">#</span><br><span class=\"line\"># This plugin is intented only as an example.</span><br><span class=\"line\"></span><br><span class=\"line\">class LogStash::Inputs::Example &lt; LogStash::Inputs::Base</span><br><span class=\"line\">  config_name &quot;example&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # If undefined, Logstash will complain, even if codec is unused.</span><br><span class=\"line\">  default :codec, &quot;plain&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # The message string to use in the event.</span><br><span class=\"line\">  config :message, :validate =&gt; :string, :default =&gt; &quot;Hello World!&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Set how frequently messages should be sent.</span><br><span class=\"line\">  #</span><br><span class=\"line\">  # The default, `1`, means send a message every second.</span><br><span class=\"line\">  config :interval, :validate =&gt; :number, :default =&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">  public</span><br><span class=\"line\">  def register</span><br><span class=\"line\">    @host = Socket.gethostname</span><br><span class=\"line\">  end # def register</span><br><span class=\"line\"></span><br><span class=\"line\">  def run(queue)</span><br><span class=\"line\">    # we can abort the loop if stop? becomes true</span><br><span class=\"line\">    while !stop?</span><br><span class=\"line\">      event = LogStash::Event.new(&quot;message&quot; =&gt; @message, &quot;host&quot; =&gt; @host)</span><br><span class=\"line\">      decorate(event)</span><br><span class=\"line\">      queue &lt;&lt; event</span><br><span class=\"line\">      # because the sleep interval can be big, when shutdown happens</span><br><span class=\"line\">      # we want to be able to abort the sleep</span><br><span class=\"line\">      # Stud.stoppable_sleep will frequently evaluate the given block</span><br><span class=\"line\">      # and abort the sleep(@interval) if the return value is true</span><br><span class=\"line\">      Stud.stoppable_sleep(@interval) &#123; stop? &#125;</span><br><span class=\"line\">    end # loop</span><br><span class=\"line\">  end # def run</span><br><span class=\"line\"></span><br><span class=\"line\">  def stop</span><br><span class=\"line\">    # nothing to do in this case so it is not necessary to define stop</span><br><span class=\"line\">    # examples of common &quot;stop&quot; tasks:</span><br><span class=\"line\">    #  * close sockets (unblocking blocking reads/accepts)</span><br><span class=\"line\">    #  * cleanup temporary files</span><br><span class=\"line\">    #  * terminate spawned threads</span><br><span class=\"line\">  end</span><br><span class=\"line\">end # class LogStash::Inputs::Example</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>挨行看看！</p>\n<p>首先第一行的# encoding: utf-8,不要以为是注释就没什么作用。它定义了插件的编码方式。</p>\n<p>下面两行：</p>\n<p>require “logstash/inputs/base”<br>require “logstash/namespace”<br>引入了插件必备的包。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class LogStash::Inputs::Example &lt; LogStash::Inputs::Base</span><br><span class=\"line\">  config_name &quot;example&quot;</span><br></pre></td></tr></table></figure></p>\n<p>插件继承自Base基类，并配置插件的使用名称。</p>\n<p>下面的一行对参数做了配置，参数有很多的配置属性，完整的如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config :variable_name,:validate =&gt;:variable_type,:default =&gt;&quot;Default value&quot;,:required =&gt; boolean,:deprecated =&gt; boolean</span><br></pre></td></tr></table></figure></p>\n<p>其中</p>\n<p>variable_name就是参数的名称了。<br>validate 定义是否进行校验，如果不是指定的类型，在logstash -f xxx –configtest的时候就会报错。它支持多种数据类型，比如:string, :password, :boolean, :number, :array, :hash, :path (a file-system path), :codec (since 1.2.0), :bytes.<br>default 定义参数的默认值<br>required 定义参数是否是必须值<br>deprecated 定义参数的额外信息，比如一个参数不再推荐使用了，就可以通过它给出提示！典型的就是es-output里面的Index_type，当使用这个参数时，就会给出提示</p>\n<h3 id=\"插件安装\"><a href=\"#插件安装\" class=\"headerlink\" title=\"插件安装\"></a>插件安装</h3><ol>\n<li>便捷安装方式</li>\n</ol>\n<p>第一步，首先把这个插件文件夹拷贝到下面的目录中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logstash-2.1.0\\vendor\\bundle\\jruby\\1.9\\gems</span><br></pre></td></tr></table></figure></p>\n<p>第二步，修改logstash根目录下的Gemfile,添加如下的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem &quot;logstash-filter-example&quot;, :path =&gt; &quot;vendor/bundle/jruby/1.9/gems/logstash-filter-example-1.0.0&quot;</span><br></pre></td></tr></table></figure></p>\n<p>第三步，编写配置文件，test.conf：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input&#123;</span><br><span class=\"line\">    example&#123;&#125; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output&#123;</span><br><span class=\"line\">    stdout&#123;</span><br><span class=\"line\">        codec =&gt; rubydebug</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>第四步，输入logstash -f test.conf时，输入任意字符，回车~~~大功告成！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">       &quot;message&quot; =&gt; &quot;Hello World!&quot;,</span><br><span class=\"line\">      &quot;@version&quot; =&gt; &quot;1&quot;,</span><br><span class=\"line\">    &quot;@timestamp&quot; =&gt; &quot;2016-01-27T19:17:18.932Z&quot;,</span><br><span class=\"line\">          &quot;host&quot; =&gt; &quot;cadenza&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>官方指导方式</li>\n</ol>\n<p>第一步，build<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem build logstash-input-example.gemspec</span><br></pre></td></tr></table></figure></p>\n<p>会在当前路径下生成logstash-input-example-2.0.4.gem<br>第二步，install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash-plugin install /logstash-input-example/logstash-input-example-2.0.4.gem</span><br></pre></td></tr></table></figure>\n<p>验证<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">validating /logstash-input-example/logstash-input-example-2.0.4.gem &gt;= 0</span><br><span class=\"line\">Valid logstash plugin. Continuing...</span><br><span class=\"line\">Successfully installed &apos;logstash-input-example&apos; with version &apos;2.0.4&apos;</span><br></pre></td></tr></table></figure></p>\n<p>第三步，查看plugin：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash-plugin list</span><br></pre></td></tr></table></figure></p>\n<p>第四步，使用</p>\n<p>略</p>\n<h2 id=\"开发案例\"><a href=\"#开发案例\" class=\"headerlink\" title=\"开发案例\"></a>开发案例</h2><p>开发插件实现根据cluster nodes信息获取redis cluster 中master节点 info信息。使用该插件只用输入一条命令，即可动态获取相关信息。</p>\n<h3 id=\"插件开发\"><a href=\"#插件开发\" class=\"headerlink\" title=\"插件开发\"></a>插件开发</h3><p>此插件是基于exec基础上封装的，主要修改内容为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def execute(command, queue)</span><br><span class=\"line\">  @logger.debug? &amp;&amp; @logger.debug(&quot;Running exec&quot;, :command =&gt; command)</span><br><span class=\"line\">  begin</span><br><span class=\"line\"> @io = IO.popen(command)</span><br><span class=\"line\"> fields = (@io.read).split(/\\r\\n|\\n/)</span><br><span class=\"line\"> puts fields</span><br><span class=\"line\"> length = fields.length-1</span><br><span class=\"line\">    \tfor i in 0..length do </span><br><span class=\"line\">  if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">\tfield = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">\tnewcommand = &quot;redis-cli -c -h #&#123;field[0]&#125; -p #&#123;field[1]&#125; info&quot;</span><br><span class=\"line\">\t@io = IO.popen(newcommand)</span><br><span class=\"line\">\t@codec.decode(@io.read) do |event|</span><br><span class=\"line\">\tdecorate(event)</span><br><span class=\"line\">\tevent.set(&quot;host&quot;, @hostname)</span><br><span class=\"line\">\tevent.set(&quot;command&quot;, newcommand)</span><br><span class=\"line\">\tqueue &lt;&lt; event</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br><span class=\"line\"></span><br><span class=\"line\">    end</span><br><span class=\"line\">  rescue StandardError =&gt; e</span><br><span class=\"line\">    @logger.error(&quot;Error while running command&quot;,</span><br><span class=\"line\">      :command =&gt; command, :e =&gt; e, :backtrace =&gt; e.backtrace)</span><br><span class=\"line\">  rescue Exception =&gt; e</span><br><span class=\"line\">    @logger.error(&quot;Exception while running command&quot;,</span><br><span class=\"line\">      :command =&gt; command, :e =&gt; e, :backtrace =&gt; e.backtrace)</span><br><span class=\"line\">  ensure</span><br><span class=\"line\">    stop</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用Demo\"><a href=\"#使用Demo\" class=\"headerlink\" title=\"使用Demo\"></a>使用Demo</h3><p>使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redisexec &#123;</span><br><span class=\"line\">command =&gt; &quot;redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk &apos;&#123;print $2&#125;&apos;&quot;</span><br><span class=\"line\">interval =&gt; 20</span><br><span class=\"line\">type =&gt; &quot;info&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>完整使用案例</p>\n<p>将info 信息存储到 ElasticSerach中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">redisexec &#123;</span><br><span class=\"line\">command =&gt; &quot;redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk &apos;&#123;print $2&#125;&apos;&quot;</span><br><span class=\"line\">interval =&gt; 20</span><br><span class=\"line\">type =&gt; &quot;info&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter &#123;</span><br><span class=\"line\">        grok &#123;</span><br><span class=\"line\">            match =&gt; &#123;&quot;command&quot; =&gt; &quot;redis-cli -c -h %&#123;IP:node:&#125; -p %&#123;NUMBER:port&#125;%&#123;DATA:data&#125;&quot; &#125;</span><br><span class=\"line\">      remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ruby &#123;</span><br><span class=\"line\">        code =&gt; &quot;fields = event[&apos;message&apos;].split(/\\r\\n|\\n/)</span><br><span class=\"line\">        length = fields.length-1</span><br><span class=\"line\">        for i in 1..length do</span><br><span class=\"line\">          if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">            field = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">            event[field[0]] = field[1].to_f</span><br><span class=\"line\">          end</span><br><span class=\"line\">        end</span><br><span class=\"line\">        &quot;</span><br><span class=\"line\">        remove_field =&gt; [ &quot;message&quot; ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output &#123;</span><br><span class=\"line\">#stdout &#123;  codec =&gt; rubydebug &#125;</span><br><span class=\"line\">elasticsearch &#123;</span><br><span class=\"line\">hosts =&gt; [&quot;127.0.0.1:9200&quot;]</span><br><span class=\"line\">template_overwrite =&gt; true</span><br><span class=\"line\">index =&gt; &quot;rediscluster-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class=\"line\">workers =&gt; 5</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.cnblogs.com/xing901022/p/5259750.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/xing901022/p/5259750.html</a></li>\n<li><a href=\"https://github.com/logstash-plugins?utf8=%E2%9C%93&amp;query=example\" target=\"_blank\" rel=\"noopener\">https://github.com/logstash-plugins?utf8=%E2%9C%93&amp;query=example</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>logstash强大魅力在于它的插件体系，虽然官方插件很多，但不可能满足所有的要求，因此就需要定制化个性化插件，本次结合Logstash Monitor Redis需求开发专用插件，以实现动态化获取master 实例中info 信息。</p>\n<h2 id=\"logstash插件介绍\"><a href=\"#logstash插件介绍\" class=\"headerlink\" title=\"logstash插件介绍\"></a>logstash插件介绍</h2><h3 id=\"体系结构\"><a href=\"#体系结构\" class=\"headerlink\" title=\"体系结构\"></a>体系结构</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ tree logstash-input-example</span><br><span class=\"line\">├── Gemfile</span><br><span class=\"line\">├── LICENSE</span><br><span class=\"line\">├── README.md</span><br><span class=\"line\">├── Rakefile</span><br><span class=\"line\">├── lib</span><br><span class=\"line\">│   └── logstash</span><br><span class=\"line\">│       └── inputs</span><br><span class=\"line\">│           └── example.rb</span><br><span class=\"line\">├── logstash-input-example.gemspec</span><br><span class=\"line\">└── spec</span><br><span class=\"line\">    └── inputs</span><br><span class=\"line\">        └── example_spec.rb</span><br></pre></td></tr></table></figure>\n<p>其实只需要这logstash-input-example.gemspec,example.rb两个文件即可。<br>mypluginname_spec.rb 是测试类。</p>\n<p>先看看logstash-input-example.gemspec都做了什么吧！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Gem::Specification.new do |s|</span><br><span class=\"line\">  s.name = &apos;logstash-input-example&apos;</span><br><span class=\"line\">  s.version         = &apos;2.0.4&apos;</span><br><span class=\"line\">  s.licenses = [&apos;Apache License (2.0)&apos;]</span><br><span class=\"line\">  s.summary = &quot;This example input streams a string at a definable interval.&quot;</span><br><span class=\"line\">  s.description     = &quot;This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program&quot;</span><br><span class=\"line\">  s.authors = [&quot;Elastic&quot;]</span><br><span class=\"line\">  s.email = &apos;info@elastic.co&apos;</span><br><span class=\"line\">  s.homepage = &quot;http://www.elastic.co/guide/en/logstash/current/index.html&quot;</span><br><span class=\"line\">  s.require_paths = [&quot;lib&quot;]</span><br><span class=\"line\"></span><br><span class=\"line\">  # Files</span><br><span class=\"line\">  s.files = Dir[&apos;lib/**/*&apos;,&apos;spec/**/*&apos;,&apos;vendor/**/*&apos;,&apos;*.gemspec&apos;,&apos;*.md&apos;,&apos;CONTRIBUTORS&apos;,&apos;Gemfile&apos;,&apos;LICENSE&apos;,&apos;NOTICE.TXT&apos;]</span><br><span class=\"line\">   # Tests</span><br><span class=\"line\">  s.test_files = s.files.grep(%r&#123;^(test|spec|features)/&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  # Special flag to let us know this is actually a logstash plugin</span><br><span class=\"line\">  s.metadata = &#123; &quot;logstash_plugin&quot; =&gt; &quot;true&quot;, &quot;logstash_group&quot; =&gt; &quot;input&quot; &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Gem dependencies</span><br><span class=\"line\">  s.add_runtime_dependency &quot;logstash-core&quot;, &quot;&gt;= 2.0.0&quot;, &quot;&lt; 3.0.0&quot;</span><br><span class=\"line\">  s.add_runtime_dependency &apos;logstash-codec-plain&apos;</span><br><span class=\"line\">  s.add_runtime_dependency &apos;stud&apos;, &apos;&gt;= 0.0.22&apos;</span><br><span class=\"line\">  s.add_development_dependency &apos;logstash-devutils&apos;, &apos;&gt;= 0.0.16&apos;</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></p>\n<p>上面的信息，只要改改版本和名字，其他的信息基本不需要动。</p>\n<p>关键的信息还有：</p>\n<ul>\n<li>s.require_paths定义了插件核心文件的位置</li>\n<li>s.add_runtime_dependency 定义了插件运行的环境<br>然后再看看example.rb<br>这个文件就需要详细说说了，基本的框架如下，<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># encoding: utf-8</span><br><span class=\"line\">require &quot;logstash/inputs/base&quot;</span><br><span class=\"line\">require &quot;logstash/namespace&quot;</span><br><span class=\"line\">require &quot;stud/interval&quot;</span><br><span class=\"line\">require &quot;socket&quot; # for Socket.gethostname</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate a repeating message.</span><br><span class=\"line\">#</span><br><span class=\"line\"># This plugin is intented only as an example.</span><br><span class=\"line\"></span><br><span class=\"line\">class LogStash::Inputs::Example &lt; LogStash::Inputs::Base</span><br><span class=\"line\">  config_name &quot;example&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # If undefined, Logstash will complain, even if codec is unused.</span><br><span class=\"line\">  default :codec, &quot;plain&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # The message string to use in the event.</span><br><span class=\"line\">  config :message, :validate =&gt; :string, :default =&gt; &quot;Hello World!&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Set how frequently messages should be sent.</span><br><span class=\"line\">  #</span><br><span class=\"line\">  # The default, `1`, means send a message every second.</span><br><span class=\"line\">  config :interval, :validate =&gt; :number, :default =&gt; 1</span><br><span class=\"line\"></span><br><span class=\"line\">  public</span><br><span class=\"line\">  def register</span><br><span class=\"line\">    @host = Socket.gethostname</span><br><span class=\"line\">  end # def register</span><br><span class=\"line\"></span><br><span class=\"line\">  def run(queue)</span><br><span class=\"line\">    # we can abort the loop if stop? becomes true</span><br><span class=\"line\">    while !stop?</span><br><span class=\"line\">      event = LogStash::Event.new(&quot;message&quot; =&gt; @message, &quot;host&quot; =&gt; @host)</span><br><span class=\"line\">      decorate(event)</span><br><span class=\"line\">      queue &lt;&lt; event</span><br><span class=\"line\">      # because the sleep interval can be big, when shutdown happens</span><br><span class=\"line\">      # we want to be able to abort the sleep</span><br><span class=\"line\">      # Stud.stoppable_sleep will frequently evaluate the given block</span><br><span class=\"line\">      # and abort the sleep(@interval) if the return value is true</span><br><span class=\"line\">      Stud.stoppable_sleep(@interval) &#123; stop? &#125;</span><br><span class=\"line\">    end # loop</span><br><span class=\"line\">  end # def run</span><br><span class=\"line\"></span><br><span class=\"line\">  def stop</span><br><span class=\"line\">    # nothing to do in this case so it is not necessary to define stop</span><br><span class=\"line\">    # examples of common &quot;stop&quot; tasks:</span><br><span class=\"line\">    #  * close sockets (unblocking blocking reads/accepts)</span><br><span class=\"line\">    #  * cleanup temporary files</span><br><span class=\"line\">    #  * terminate spawned threads</span><br><span class=\"line\">  end</span><br><span class=\"line\">end # class LogStash::Inputs::Example</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>挨行看看！</p>\n<p>首先第一行的# encoding: utf-8,不要以为是注释就没什么作用。它定义了插件的编码方式。</p>\n<p>下面两行：</p>\n<p>require “logstash/inputs/base”<br>require “logstash/namespace”<br>引入了插件必备的包。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class LogStash::Inputs::Example &lt; LogStash::Inputs::Base</span><br><span class=\"line\">  config_name &quot;example&quot;</span><br></pre></td></tr></table></figure></p>\n<p>插件继承自Base基类，并配置插件的使用名称。</p>\n<p>下面的一行对参数做了配置，参数有很多的配置属性，完整的如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config :variable_name,:validate =&gt;:variable_type,:default =&gt;&quot;Default value&quot;,:required =&gt; boolean,:deprecated =&gt; boolean</span><br></pre></td></tr></table></figure></p>\n<p>其中</p>\n<p>variable_name就是参数的名称了。<br>validate 定义是否进行校验，如果不是指定的类型，在logstash -f xxx –configtest的时候就会报错。它支持多种数据类型，比如:string, :password, :boolean, :number, :array, :hash, :path (a file-system path), :codec (since 1.2.0), :bytes.<br>default 定义参数的默认值<br>required 定义参数是否是必须值<br>deprecated 定义参数的额外信息，比如一个参数不再推荐使用了，就可以通过它给出提示！典型的就是es-output里面的Index_type，当使用这个参数时，就会给出提示</p>\n<h3 id=\"插件安装\"><a href=\"#插件安装\" class=\"headerlink\" title=\"插件安装\"></a>插件安装</h3><ol>\n<li>便捷安装方式</li>\n</ol>\n<p>第一步，首先把这个插件文件夹拷贝到下面的目录中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logstash-2.1.0\\vendor\\bundle\\jruby\\1.9\\gems</span><br></pre></td></tr></table></figure></p>\n<p>第二步，修改logstash根目录下的Gemfile,添加如下的内容：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem &quot;logstash-filter-example&quot;, :path =&gt; &quot;vendor/bundle/jruby/1.9/gems/logstash-filter-example-1.0.0&quot;</span><br></pre></td></tr></table></figure></p>\n<p>第三步，编写配置文件，test.conf：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input&#123;</span><br><span class=\"line\">    example&#123;&#125; </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output&#123;</span><br><span class=\"line\">    stdout&#123;</span><br><span class=\"line\">        codec =&gt; rubydebug</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>第四步，输入logstash -f test.conf时，输入任意字符，回车~~~大功告成！<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">       &quot;message&quot; =&gt; &quot;Hello World!&quot;,</span><br><span class=\"line\">      &quot;@version&quot; =&gt; &quot;1&quot;,</span><br><span class=\"line\">    &quot;@timestamp&quot; =&gt; &quot;2016-01-27T19:17:18.932Z&quot;,</span><br><span class=\"line\">          &quot;host&quot; =&gt; &quot;cadenza&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"2\">\n<li>官方指导方式</li>\n</ol>\n<p>第一步，build<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gem build logstash-input-example.gemspec</span><br></pre></td></tr></table></figure></p>\n<p>会在当前路径下生成logstash-input-example-2.0.4.gem<br>第二步，install</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash-plugin install /logstash-input-example/logstash-input-example-2.0.4.gem</span><br></pre></td></tr></table></figure>\n<p>验证<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">validating /logstash-input-example/logstash-input-example-2.0.4.gem &gt;= 0</span><br><span class=\"line\">Valid logstash plugin. Continuing...</span><br><span class=\"line\">Successfully installed &apos;logstash-input-example&apos; with version &apos;2.0.4&apos;</span><br></pre></td></tr></table></figure></p>\n<p>第三步，查看plugin：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bin/logstash-plugin list</span><br></pre></td></tr></table></figure></p>\n<p>第四步，使用</p>\n<p>略</p>\n<h2 id=\"开发案例\"><a href=\"#开发案例\" class=\"headerlink\" title=\"开发案例\"></a>开发案例</h2><p>开发插件实现根据cluster nodes信息获取redis cluster 中master节点 info信息。使用该插件只用输入一条命令，即可动态获取相关信息。</p>\n<h3 id=\"插件开发\"><a href=\"#插件开发\" class=\"headerlink\" title=\"插件开发\"></a>插件开发</h3><p>此插件是基于exec基础上封装的，主要修改内容为：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def execute(command, queue)</span><br><span class=\"line\">  @logger.debug? &amp;&amp; @logger.debug(&quot;Running exec&quot;, :command =&gt; command)</span><br><span class=\"line\">  begin</span><br><span class=\"line\"> @io = IO.popen(command)</span><br><span class=\"line\"> fields = (@io.read).split(/\\r\\n|\\n/)</span><br><span class=\"line\"> puts fields</span><br><span class=\"line\"> length = fields.length-1</span><br><span class=\"line\">    \tfor i in 0..length do </span><br><span class=\"line\">  if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">\tfield = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">\tnewcommand = &quot;redis-cli -c -h #&#123;field[0]&#125; -p #&#123;field[1]&#125; info&quot;</span><br><span class=\"line\">\t@io = IO.popen(newcommand)</span><br><span class=\"line\">\t@codec.decode(@io.read) do |event|</span><br><span class=\"line\">\tdecorate(event)</span><br><span class=\"line\">\tevent.set(&quot;host&quot;, @hostname)</span><br><span class=\"line\">\tevent.set(&quot;command&quot;, newcommand)</span><br><span class=\"line\">\tqueue &lt;&lt; event</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br><span class=\"line\"></span><br><span class=\"line\">    end</span><br><span class=\"line\">  rescue StandardError =&gt; e</span><br><span class=\"line\">    @logger.error(&quot;Error while running command&quot;,</span><br><span class=\"line\">      :command =&gt; command, :e =&gt; e, :backtrace =&gt; e.backtrace)</span><br><span class=\"line\">  rescue Exception =&gt; e</span><br><span class=\"line\">    @logger.error(&quot;Exception while running command&quot;,</span><br><span class=\"line\">      :command =&gt; command, :e =&gt; e, :backtrace =&gt; e.backtrace)</span><br><span class=\"line\">  ensure</span><br><span class=\"line\">    stop</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用Demo\"><a href=\"#使用Demo\" class=\"headerlink\" title=\"使用Demo\"></a>使用Demo</h3><p>使用方式<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">redisexec &#123;</span><br><span class=\"line\">command =&gt; &quot;redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk &apos;&#123;print $2&#125;&apos;&quot;</span><br><span class=\"line\">interval =&gt; 20</span><br><span class=\"line\">type =&gt; &quot;info&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>完整使用案例</p>\n<p>将info 信息存储到 ElasticSerach中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input &#123;</span><br><span class=\"line\">redisexec &#123;</span><br><span class=\"line\">command =&gt; &quot;redis-cli -h 127.0.0.1 -p 6379 cluster nodes|grep master|awk &apos;&#123;print $2&#125;&apos;&quot;</span><br><span class=\"line\">interval =&gt; 20</span><br><span class=\"line\">type =&gt; &quot;info&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">filter &#123;</span><br><span class=\"line\">        grok &#123;</span><br><span class=\"line\">            match =&gt; &#123;&quot;command&quot; =&gt; &quot;redis-cli -c -h %&#123;IP:node:&#125; -p %&#123;NUMBER:port&#125;%&#123;DATA:data&#125;&quot; &#125;</span><br><span class=\"line\">      remove_field =&gt; [ &quot;host&quot; ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ruby &#123;</span><br><span class=\"line\">        code =&gt; &quot;fields = event[&apos;message&apos;].split(/\\r\\n|\\n/)</span><br><span class=\"line\">        length = fields.length-1</span><br><span class=\"line\">        for i in 1..length do</span><br><span class=\"line\">          if fields[i].include?&apos;:&apos; then</span><br><span class=\"line\">            field = fields[i].split(&apos;:&apos;)</span><br><span class=\"line\">            event[field[0]] = field[1].to_f</span><br><span class=\"line\">          end</span><br><span class=\"line\">        end</span><br><span class=\"line\">        &quot;</span><br><span class=\"line\">        remove_field =&gt; [ &quot;message&quot; ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">output &#123;</span><br><span class=\"line\">#stdout &#123;  codec =&gt; rubydebug &#125;</span><br><span class=\"line\">elasticsearch &#123;</span><br><span class=\"line\">hosts =&gt; [&quot;127.0.0.1:9200&quot;]</span><br><span class=\"line\">template_overwrite =&gt; true</span><br><span class=\"line\">index =&gt; &quot;rediscluster-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class=\"line\">workers =&gt; 5</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.cnblogs.com/xing901022/p/5259750.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/xing901022/p/5259750.html</a></li>\n<li><a href=\"https://github.com/logstash-plugins?utf8=%E2%9C%93&amp;query=example\" target=\"_blank\" rel=\"noopener\">https://github.com/logstash-plugins?utf8=%E2%9C%93&amp;query=example</a></li>\n<li><a href=\"https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html\" target=\"_blank\" rel=\"noopener\">https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html</a></li>\n</ol>\n"},{"title":"x-pack之graph研究","date":"2018-07-22T10:44:57.000Z","_content":"# x-pack之graph研究\n## 简介\nGraph 功能是一种基于 API 和基于 UI 的工具，能够让您数据中存在的相关关系浮现出来，同时能够在任何规模下利用各项 Elasticsearch 功能，例如分布式查询执行、实时数据可用性和索引等。\n\n- **反欺诈**：挖掘海量购物行为数据和用户画像，探究由哪位店主对一组信用卡被盗事件负责。\n- **个性化推荐**：根据听众偏好为喜欢莫扎特的听众推荐下一首最佳歌曲， 从而吸引和取悦听众。\n- **安全分析**：发现潜在的破坏分子和其他意想不到的相关事件， 如挖掘网络中的主机与之进行通信的外部 IP的关联关系。\n\n## 场景实战\n\n### 业务场景\n\n针对反欺诈场景，对商家打分作弊，马甲风险评估\n\n### 准备分析数据\n\n运行python IndexReviews.py 将review 数据写入到es 中\n\n\n```\n{\n      \"date\": \"2006-04-22 13:48\",\n      \"reviewer\": \"15300\",\n      \"rating\": 5,\n      \"hour\": \"2006-04-22 13\",\n      \"seller\": \"74\"\n}\n```\n\nIndexReviews.py 代码如下：\n```\nimport csv\nfrom elasticsearch import helpers\nfrom elasticsearch.client import Elasticsearch\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nes = Elasticsearch(\n[\n        'http://192.168.0.101:9200/'\n]\n)\nindexName = \"reviews\"\nes.indices.delete(index=indexName, ignore=[400, 404])\nindexSettings = {\n    \"settings\": {\n        \"index.number_of_replicas\": 0,\n        \"index.number_of_shards\": 1\n    },\n    \"mappings\": {\n\n        \"review\": {\n            \"properties\": {\n                \"reviewer\": {\n                    \"type\": \"keyword\"\n                },\n                \"seller\": {\n                    \"type\": \"keyword\"\n                },\n                \"rating\": {\n                    \"type\": \"integer\"\n                },\n                \"date\": {\n                    \"type\": \"date\",\n                    \"format\": \"yyyy-MM-dd HH:mm\"\n                },\n                \"hour\": {\n                    \"type\": \"keyword\"\n                }\n            }\n        }\n\n    }\n}\nes.indices.create(index=indexName, body=indexSettings)\n\nactions = []\nrowNum = 0\ncsvFilename = \"reviews.csv\"\nwith open(csvFilename, 'rb') as csvfile:\n    csvreader = csv.reader(csvfile)\n    for row in csvreader:\n        rowNum += 1\n        if rowNum == 1:\n            continue\n\n        action = {\n            \"_index\": indexName,\n            '_op_type': 'index',\n            \"_type\": \"review\",\n            \"_source\": {\n                \"reviewer\": row[0],\n                \"seller\": row[1],\n                \"rating\": int(row[2]),\n                \"date\": row[3],\n                \"hour\": row[3][:-3],\n            }\n        }\n        actions.append(action)\n        # Flush bulk indexing action if necessary\n        if len(actions) >= 5000:\n            print rowNum\n            helpers.bulk(es, actions)\n            del actions[0:len(actions)]\n\nif len(actions) >= 0:\n    helpers.bulk(es, actions)\n    del actions[0:len(actions)]\n```\n### 使用xpack graph 分析\n\n1. 创建index\n创建用于输出结果的index,\n2. 使用es api 获取前50000个seller \n3. 调用graph 针对每个seller进行关系分析\n例如\n```\nPOST review/_xpack/graph/_explore\n{\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"term\": {\"seller\": \"A9JN\"}\n                    },\n                    {\n                        \"match\": {\"rating\": 5}\n                    }\n                ]\n            }\n        },\n        \"controls\": {\n            \"sample_size\": 20,\n            \"use_significance\": true\n        },\n        \"vertices\": [\n            {\n                \"field\": \"customer\",\n                \"size\": 50,\n                \"min_doc_count\": 1\n            }\n        ],\n        \"connections\": {\n            \"query\": {\n                \"term\": {\n                    \"seller\": \"A9JN\"\n                }\n            },\n            \"vertices\": [\n                {\n                    \"field\": \"hour\",\n                    \"size\": 500,\n                    \"min_doc_count\": 1\n                }\n            ]\n        }\n    }\n```\n\n4. 根据查询的图关系进行客户端图计算，查找出reviewer ---->hour---->reviewer出现一次以上的路径相框，将该结果汇总。\n5. 将第四步的结果组织存储到es 中\n```\n{\n            \"url\": workspaceUrl,\n            \"seller\": sellerId,\n            \"riskType\": \"SockPuppetry\",\n            \"riskRating\": numCoincidences,\n            \"numDocs\": sellerDetails[\"numReviews\"]\n}\n```\n\n\n完整代码参考一下：\n```\nimport networkx as nx\nfrom elasticsearch import helpers\nfrom elasticsearch.client import Elasticsearch\nimport sys\nimport urllib\nimport json\n\nes = Elasticsearch(\n[\n        'http://192.168.0.101:9200/'\n]\n)\n\nalertsIndexName = \"alerts\"\n\ndef createAlertsIndex():\n    alertsIndexSettings = {\n        \"settings\": {\n            \"number_of_replicas\": \"0\",\n            \"number_of_shards\": \"1\"\n        },\n        \"mappings\": {\n            \"task\": {\n                \"properties\": {\n                    \"riskType\": {\n                        \"type\": \"keyword\"\n                    },\n                    \"url\": {\n                        \"type\": \"text\",\n                        \"index\": \"false\"\n                    }\n                }\n            }\n        }\n    }\n    if not es.indices.exists(index=alertsIndexName):\n        es.indices.create(index=alertsIndexName, body=alertsIndexSettings)\n\ndef getSellersList():\n    sellers=[]\n    sellersQuery = {\n        \"size\": 0,\n        \"aggs\": {\n            \"topTerms\": {\n                \"terms\": {\n                    \"field\": \"seller\",\n                    \"size\": 50000\n                }\n            }\n        }\n    }\n    results = es.search(index=\"reviews\", body=sellersQuery)[\"aggregations\"][\"topTerms\"][\"buckets\"]\n    for bucket in results:\n        sellers.append({\n            \"seller\" : bucket[\"key\"],\n            \"numReviews\" : bucket[\"doc_count\"]\n        })\n    return sellers\n\ndef nodeId(node):\n    return node[\"field\"] + \":\" + node[\"term\"]\n    \ndef erasePrint(msg):\n    sys.stdout.write('\\r')\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n\ncreateAlertsIndex()\nsellers=getSellersList()\nrowNum = 0;\ntotalNumReviews=0\nfor sellerDetails in sellers:\n    rowNum +=1\n    totalNumReviews+=sellerDetails[\"numReviews\"]\n    sellerId=sellerDetails[\"seller\"]\n    q = {\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"term\": {\"seller\": sellerId}\n                    },\n                    {\n                        \"match\": {\"rating\": 5}\n                    }\n                ]\n            }\n        },\n        \"controls\": {\n            \"sample_size\": 2000,\n            \"use_significance\": True\n        },\n        \"vertices\": [\n            {\n                # Find most loyal reviewers - significantly connected to the current seller\n                \"field\": \"reviewer\",\n                \"size\": 50,\n                \"min_doc_count\": 1\n            }\n        ],\n        \"connections\": {\n            # Find date/times of loyal reviewers' activity - guide the exploration so only current-seller-related reviews\n            \"query\": {\n                \"term\": {\n                    \"seller\": sellerId\n                }\n            },\n            # This will naturally favour date/times that are common to multiple reviewers\n            \"vertices\": [\n                {\n                    \"field\": \"hour\",\n                    \"size\": 500,\n                    \"min_doc_count\": 1\n                }\n            ]\n        }\n    }\n\n    erasePrint(\"Examining seller \" + str(rowNum) + \" of \" + str(len(sellers)))\n\n    results = es.transport.perform_request('POST', \"/reviews/_xpack/graph/_explore\", body=q)\n\n    # Use NetworkX to create a client-side graph of reviewers and date/times we can analyze\n    G = nx.Graph()\n\n    for node in results[\"vertices\"]:\n        G.add_node(nodeId(node), type=node[\"field\"])\n    for edge in results[\"connections\"]:\n        n1 = results[\"vertices\"][int(edge[\"source\"])]\n        n2 = results[\"vertices\"][int(edge[\"target\"])]\n        G.add_edge(nodeId(n1), nodeId(n2))\n\n    # Examine all \"islands\" of reviewers connected by same date/time\n    subgraphs = nx.connected_component_subgraphs(G)\n\n    numCoincidences = 0\n    for subgraph in subgraphs:\n        numHours = 0\n        reviewers = []\n        for n, d in subgraph.nodes(data=True):\n            if d[\"type\"] == \"hour\":\n                numHours += 1\n            if d[\"type\"] == \"reviewer\":\n                reviewers.append(n)\n        if numHours > 1:\n            if len(reviewers) > 1:\n                for srcI in range(0, len(reviewers)):\n                    reviewer1 = reviewers[srcI]\n                    for targetI in range(srcI + 1, len(reviewers)):\n                        reviewer2 = reviewers[targetI]\n                        paths = nx.all_simple_paths(subgraph, source=reviewer1, target=reviewer2, cutoff=2)\n                        #  Each path is a reviewer <-> date/time <-> reviewer triple\n                        sameTimeReviews = 0\n                        for path in paths:\n                            sameTimeReviews += 1\n                        # Two reviewers reviewing at same date/time is not a coincidence - however\n                        # repeated synchronized reviews *are* a coincidence\n                        if sameTimeReviews > 1:\n                            numCoincidences += sameTimeReviews - 1\n\n    if numCoincidences > 0:\n        erasePrint(\"\")\n        print \"seller:\" + str(sellerId) + \" has \", numCoincidences, \" reviewer coincidences in\", sellerDetails[\"numReviews\"], \"reviews\"\n        gq = json.dumps(q)\n        workspaceUrl = \"graph#/workspace/32935810-5e4d-11e8-811e-7b68199c5a31?query=\" + urllib.quote_plus(gq)\n        doc = {\n            \"url\": workspaceUrl,\n            \"seller\": sellerId,\n            \"riskType\": \"SockPuppetry\",\n            \"riskRating\": numCoincidences,\n            \"numDocs\": sellerDetails[\"numReviews\"]\n        }\n        res = es.index(index=alertsIndexName, doc_type='task', id=sellerId, body=doc)\nerasePrint(\"\")\nprint \"Completed analysis of\", len(sellers), \"sellers and\",totalNumReviews,\"reviews\"\n\n```\n\n## 注意\n本文代码适用于python2.7,其他版本，请自行参照修改\n## 参考\n1. [Fraud detection using the elastic stack](https://www.youtube.com/watch?v=liMhiiyQ9co)\n","source":"_posts/x-pack之graph研究.md","raw":"---\ntitle: x-pack之graph研究\ndate: 2018-07-22 18:44:57\ntags: research\ncategories:\n- elasticsearch\n---\n# x-pack之graph研究\n## 简介\nGraph 功能是一种基于 API 和基于 UI 的工具，能够让您数据中存在的相关关系浮现出来，同时能够在任何规模下利用各项 Elasticsearch 功能，例如分布式查询执行、实时数据可用性和索引等。\n\n- **反欺诈**：挖掘海量购物行为数据和用户画像，探究由哪位店主对一组信用卡被盗事件负责。\n- **个性化推荐**：根据听众偏好为喜欢莫扎特的听众推荐下一首最佳歌曲， 从而吸引和取悦听众。\n- **安全分析**：发现潜在的破坏分子和其他意想不到的相关事件， 如挖掘网络中的主机与之进行通信的外部 IP的关联关系。\n\n## 场景实战\n\n### 业务场景\n\n针对反欺诈场景，对商家打分作弊，马甲风险评估\n\n### 准备分析数据\n\n运行python IndexReviews.py 将review 数据写入到es 中\n\n\n```\n{\n      \"date\": \"2006-04-22 13:48\",\n      \"reviewer\": \"15300\",\n      \"rating\": 5,\n      \"hour\": \"2006-04-22 13\",\n      \"seller\": \"74\"\n}\n```\n\nIndexReviews.py 代码如下：\n```\nimport csv\nfrom elasticsearch import helpers\nfrom elasticsearch.client import Elasticsearch\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nes = Elasticsearch(\n[\n        'http://192.168.0.101:9200/'\n]\n)\nindexName = \"reviews\"\nes.indices.delete(index=indexName, ignore=[400, 404])\nindexSettings = {\n    \"settings\": {\n        \"index.number_of_replicas\": 0,\n        \"index.number_of_shards\": 1\n    },\n    \"mappings\": {\n\n        \"review\": {\n            \"properties\": {\n                \"reviewer\": {\n                    \"type\": \"keyword\"\n                },\n                \"seller\": {\n                    \"type\": \"keyword\"\n                },\n                \"rating\": {\n                    \"type\": \"integer\"\n                },\n                \"date\": {\n                    \"type\": \"date\",\n                    \"format\": \"yyyy-MM-dd HH:mm\"\n                },\n                \"hour\": {\n                    \"type\": \"keyword\"\n                }\n            }\n        }\n\n    }\n}\nes.indices.create(index=indexName, body=indexSettings)\n\nactions = []\nrowNum = 0\ncsvFilename = \"reviews.csv\"\nwith open(csvFilename, 'rb') as csvfile:\n    csvreader = csv.reader(csvfile)\n    for row in csvreader:\n        rowNum += 1\n        if rowNum == 1:\n            continue\n\n        action = {\n            \"_index\": indexName,\n            '_op_type': 'index',\n            \"_type\": \"review\",\n            \"_source\": {\n                \"reviewer\": row[0],\n                \"seller\": row[1],\n                \"rating\": int(row[2]),\n                \"date\": row[3],\n                \"hour\": row[3][:-3],\n            }\n        }\n        actions.append(action)\n        # Flush bulk indexing action if necessary\n        if len(actions) >= 5000:\n            print rowNum\n            helpers.bulk(es, actions)\n            del actions[0:len(actions)]\n\nif len(actions) >= 0:\n    helpers.bulk(es, actions)\n    del actions[0:len(actions)]\n```\n### 使用xpack graph 分析\n\n1. 创建index\n创建用于输出结果的index,\n2. 使用es api 获取前50000个seller \n3. 调用graph 针对每个seller进行关系分析\n例如\n```\nPOST review/_xpack/graph/_explore\n{\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"term\": {\"seller\": \"A9JN\"}\n                    },\n                    {\n                        \"match\": {\"rating\": 5}\n                    }\n                ]\n            }\n        },\n        \"controls\": {\n            \"sample_size\": 20,\n            \"use_significance\": true\n        },\n        \"vertices\": [\n            {\n                \"field\": \"customer\",\n                \"size\": 50,\n                \"min_doc_count\": 1\n            }\n        ],\n        \"connections\": {\n            \"query\": {\n                \"term\": {\n                    \"seller\": \"A9JN\"\n                }\n            },\n            \"vertices\": [\n                {\n                    \"field\": \"hour\",\n                    \"size\": 500,\n                    \"min_doc_count\": 1\n                }\n            ]\n        }\n    }\n```\n\n4. 根据查询的图关系进行客户端图计算，查找出reviewer ---->hour---->reviewer出现一次以上的路径相框，将该结果汇总。\n5. 将第四步的结果组织存储到es 中\n```\n{\n            \"url\": workspaceUrl,\n            \"seller\": sellerId,\n            \"riskType\": \"SockPuppetry\",\n            \"riskRating\": numCoincidences,\n            \"numDocs\": sellerDetails[\"numReviews\"]\n}\n```\n\n\n完整代码参考一下：\n```\nimport networkx as nx\nfrom elasticsearch import helpers\nfrom elasticsearch.client import Elasticsearch\nimport sys\nimport urllib\nimport json\n\nes = Elasticsearch(\n[\n        'http://192.168.0.101:9200/'\n]\n)\n\nalertsIndexName = \"alerts\"\n\ndef createAlertsIndex():\n    alertsIndexSettings = {\n        \"settings\": {\n            \"number_of_replicas\": \"0\",\n            \"number_of_shards\": \"1\"\n        },\n        \"mappings\": {\n            \"task\": {\n                \"properties\": {\n                    \"riskType\": {\n                        \"type\": \"keyword\"\n                    },\n                    \"url\": {\n                        \"type\": \"text\",\n                        \"index\": \"false\"\n                    }\n                }\n            }\n        }\n    }\n    if not es.indices.exists(index=alertsIndexName):\n        es.indices.create(index=alertsIndexName, body=alertsIndexSettings)\n\ndef getSellersList():\n    sellers=[]\n    sellersQuery = {\n        \"size\": 0,\n        \"aggs\": {\n            \"topTerms\": {\n                \"terms\": {\n                    \"field\": \"seller\",\n                    \"size\": 50000\n                }\n            }\n        }\n    }\n    results = es.search(index=\"reviews\", body=sellersQuery)[\"aggregations\"][\"topTerms\"][\"buckets\"]\n    for bucket in results:\n        sellers.append({\n            \"seller\" : bucket[\"key\"],\n            \"numReviews\" : bucket[\"doc_count\"]\n        })\n    return sellers\n\ndef nodeId(node):\n    return node[\"field\"] + \":\" + node[\"term\"]\n    \ndef erasePrint(msg):\n    sys.stdout.write('\\r')\n    sys.stdout.write(msg)\n    sys.stdout.flush()\n\ncreateAlertsIndex()\nsellers=getSellersList()\nrowNum = 0;\ntotalNumReviews=0\nfor sellerDetails in sellers:\n    rowNum +=1\n    totalNumReviews+=sellerDetails[\"numReviews\"]\n    sellerId=sellerDetails[\"seller\"]\n    q = {\n        \"query\": {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"term\": {\"seller\": sellerId}\n                    },\n                    {\n                        \"match\": {\"rating\": 5}\n                    }\n                ]\n            }\n        },\n        \"controls\": {\n            \"sample_size\": 2000,\n            \"use_significance\": True\n        },\n        \"vertices\": [\n            {\n                # Find most loyal reviewers - significantly connected to the current seller\n                \"field\": \"reviewer\",\n                \"size\": 50,\n                \"min_doc_count\": 1\n            }\n        ],\n        \"connections\": {\n            # Find date/times of loyal reviewers' activity - guide the exploration so only current-seller-related reviews\n            \"query\": {\n                \"term\": {\n                    \"seller\": sellerId\n                }\n            },\n            # This will naturally favour date/times that are common to multiple reviewers\n            \"vertices\": [\n                {\n                    \"field\": \"hour\",\n                    \"size\": 500,\n                    \"min_doc_count\": 1\n                }\n            ]\n        }\n    }\n\n    erasePrint(\"Examining seller \" + str(rowNum) + \" of \" + str(len(sellers)))\n\n    results = es.transport.perform_request('POST', \"/reviews/_xpack/graph/_explore\", body=q)\n\n    # Use NetworkX to create a client-side graph of reviewers and date/times we can analyze\n    G = nx.Graph()\n\n    for node in results[\"vertices\"]:\n        G.add_node(nodeId(node), type=node[\"field\"])\n    for edge in results[\"connections\"]:\n        n1 = results[\"vertices\"][int(edge[\"source\"])]\n        n2 = results[\"vertices\"][int(edge[\"target\"])]\n        G.add_edge(nodeId(n1), nodeId(n2))\n\n    # Examine all \"islands\" of reviewers connected by same date/time\n    subgraphs = nx.connected_component_subgraphs(G)\n\n    numCoincidences = 0\n    for subgraph in subgraphs:\n        numHours = 0\n        reviewers = []\n        for n, d in subgraph.nodes(data=True):\n            if d[\"type\"] == \"hour\":\n                numHours += 1\n            if d[\"type\"] == \"reviewer\":\n                reviewers.append(n)\n        if numHours > 1:\n            if len(reviewers) > 1:\n                for srcI in range(0, len(reviewers)):\n                    reviewer1 = reviewers[srcI]\n                    for targetI in range(srcI + 1, len(reviewers)):\n                        reviewer2 = reviewers[targetI]\n                        paths = nx.all_simple_paths(subgraph, source=reviewer1, target=reviewer2, cutoff=2)\n                        #  Each path is a reviewer <-> date/time <-> reviewer triple\n                        sameTimeReviews = 0\n                        for path in paths:\n                            sameTimeReviews += 1\n                        # Two reviewers reviewing at same date/time is not a coincidence - however\n                        # repeated synchronized reviews *are* a coincidence\n                        if sameTimeReviews > 1:\n                            numCoincidences += sameTimeReviews - 1\n\n    if numCoincidences > 0:\n        erasePrint(\"\")\n        print \"seller:\" + str(sellerId) + \" has \", numCoincidences, \" reviewer coincidences in\", sellerDetails[\"numReviews\"], \"reviews\"\n        gq = json.dumps(q)\n        workspaceUrl = \"graph#/workspace/32935810-5e4d-11e8-811e-7b68199c5a31?query=\" + urllib.quote_plus(gq)\n        doc = {\n            \"url\": workspaceUrl,\n            \"seller\": sellerId,\n            \"riskType\": \"SockPuppetry\",\n            \"riskRating\": numCoincidences,\n            \"numDocs\": sellerDetails[\"numReviews\"]\n        }\n        res = es.index(index=alertsIndexName, doc_type='task', id=sellerId, body=doc)\nerasePrint(\"\")\nprint \"Completed analysis of\", len(sellers), \"sellers and\",totalNumReviews,\"reviews\"\n\n```\n\n## 注意\n本文代码适用于python2.7,其他版本，请自行参照修改\n## 参考\n1. [Fraud detection using the elastic stack](https://www.youtube.com/watch?v=liMhiiyQ9co)\n","slug":"x-pack之graph研究","published":1,"updated":"2019-08-22T12:31:46.166Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvgn00ar8ceet23wokk0","content":"<h1 id=\"x-pack之graph研究\"><a href=\"#x-pack之graph研究\" class=\"headerlink\" title=\"x-pack之graph研究\"></a>x-pack之graph研究</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Graph 功能是一种基于 API 和基于 UI 的工具，能够让您数据中存在的相关关系浮现出来，同时能够在任何规模下利用各项 Elasticsearch 功能，例如分布式查询执行、实时数据可用性和索引等。</p>\n<ul>\n<li><strong>反欺诈</strong>：挖掘海量购物行为数据和用户画像，探究由哪位店主对一组信用卡被盗事件负责。</li>\n<li><strong>个性化推荐</strong>：根据听众偏好为喜欢莫扎特的听众推荐下一首最佳歌曲， 从而吸引和取悦听众。</li>\n<li><strong>安全分析</strong>：发现潜在的破坏分子和其他意想不到的相关事件， 如挖掘网络中的主机与之进行通信的外部 IP的关联关系。</li>\n</ul>\n<h2 id=\"场景实战\"><a href=\"#场景实战\" class=\"headerlink\" title=\"场景实战\"></a>场景实战</h2><h3 id=\"业务场景\"><a href=\"#业务场景\" class=\"headerlink\" title=\"业务场景\"></a>业务场景</h3><p>针对反欺诈场景，对商家打分作弊，马甲风险评估</p>\n<h3 id=\"准备分析数据\"><a href=\"#准备分析数据\" class=\"headerlink\" title=\"准备分析数据\"></a>准备分析数据</h3><p>运行python IndexReviews.py 将review 数据写入到es 中</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;date&quot;: &quot;2006-04-22 13:48&quot;,</span><br><span class=\"line\">      &quot;reviewer&quot;: &quot;15300&quot;,</span><br><span class=\"line\">      &quot;rating&quot;: 5,</span><br><span class=\"line\">      &quot;hour&quot;: &quot;2006-04-22 13&quot;,</span><br><span class=\"line\">      &quot;seller&quot;: &quot;74&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>IndexReviews.py 代码如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">from elasticsearch import helpers</span><br><span class=\"line\">from elasticsearch.client import Elasticsearch</span><br><span class=\"line\">import sys</span><br><span class=\"line\">reload(sys)</span><br><span class=\"line\">sys.setdefaultencoding(&apos;utf8&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">es = Elasticsearch(</span><br><span class=\"line\">[</span><br><span class=\"line\">        &apos;http://192.168.0.101:9200/&apos;</span><br><span class=\"line\">]</span><br><span class=\"line\">)</span><br><span class=\"line\">indexName = &quot;reviews&quot;</span><br><span class=\"line\">es.indices.delete(index=indexName, ignore=[400, 404])</span><br><span class=\"line\">indexSettings = &#123;</span><br><span class=\"line\">    &quot;settings&quot;: &#123;</span><br><span class=\"line\">        &quot;index.number_of_replicas&quot;: 0,</span><br><span class=\"line\">        &quot;index.number_of_shards&quot;: 1</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;mappings&quot;: &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &quot;review&quot;: &#123;</span><br><span class=\"line\">            &quot;properties&quot;: &#123;</span><br><span class=\"line\">                &quot;reviewer&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;seller&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;rating&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;integer&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;date&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">                    &quot;format&quot;: &quot;yyyy-MM-dd HH:mm&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;hour&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">es.indices.create(index=indexName, body=indexSettings)</span><br><span class=\"line\"></span><br><span class=\"line\">actions = []</span><br><span class=\"line\">rowNum = 0</span><br><span class=\"line\">csvFilename = &quot;reviews.csv&quot;</span><br><span class=\"line\">with open(csvFilename, &apos;rb&apos;) as csvfile:</span><br><span class=\"line\">    csvreader = csv.reader(csvfile)</span><br><span class=\"line\">    for row in csvreader:</span><br><span class=\"line\">        rowNum += 1</span><br><span class=\"line\">        if rowNum == 1:</span><br><span class=\"line\">            continue</span><br><span class=\"line\"></span><br><span class=\"line\">        action = &#123;</span><br><span class=\"line\">            &quot;_index&quot;: indexName,</span><br><span class=\"line\">            &apos;_op_type&apos;: &apos;index&apos;,</span><br><span class=\"line\">            &quot;_type&quot;: &quot;review&quot;,</span><br><span class=\"line\">            &quot;_source&quot;: &#123;</span><br><span class=\"line\">                &quot;reviewer&quot;: row[0],</span><br><span class=\"line\">                &quot;seller&quot;: row[1],</span><br><span class=\"line\">                &quot;rating&quot;: int(row[2]),</span><br><span class=\"line\">                &quot;date&quot;: row[3],</span><br><span class=\"line\">                &quot;hour&quot;: row[3][:-3],</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        actions.append(action)</span><br><span class=\"line\">        # Flush bulk indexing action if necessary</span><br><span class=\"line\">        if len(actions) &gt;= 5000:</span><br><span class=\"line\">            print rowNum</span><br><span class=\"line\">            helpers.bulk(es, actions)</span><br><span class=\"line\">            del actions[0:len(actions)]</span><br><span class=\"line\"></span><br><span class=\"line\">if len(actions) &gt;= 0:</span><br><span class=\"line\">    helpers.bulk(es, actions)</span><br><span class=\"line\">    del actions[0:len(actions)]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用xpack-graph-分析\"><a href=\"#使用xpack-graph-分析\" class=\"headerlink\" title=\"使用xpack graph 分析\"></a>使用xpack graph 分析</h3><ol>\n<li>创建index<br>创建用于输出结果的index,</li>\n<li>使用es api 获取前50000个seller </li>\n<li><p>调用graph 针对每个seller进行关系分析<br>例如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST review/_xpack/graph/_explore</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">            &quot;bool&quot;: &#123;</span><br><span class=\"line\">                &quot;must&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;term&quot;: &#123;&quot;seller&quot;: &quot;A9JN&quot;&#125;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;match&quot;: &#123;&quot;rating&quot;: 5&#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;controls&quot;: &#123;</span><br><span class=\"line\">            &quot;sample_size&quot;: 20,</span><br><span class=\"line\">            &quot;use_significance&quot;: true</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;vertices&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;field&quot;: &quot;customer&quot;,</span><br><span class=\"line\">                &quot;size&quot;: 50,</span><br><span class=\"line\">                &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;connections&quot;: &#123;</span><br><span class=\"line\">            &quot;query&quot;: &#123;</span><br><span class=\"line\">                &quot;term&quot;: &#123;</span><br><span class=\"line\">                    &quot;seller&quot;: &quot;A9JN&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;vertices&quot;: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;hour&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 500,</span><br><span class=\"line\">                    &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据查询的图关系进行客户端图计算，查找出reviewer —-&gt;hour—-&gt;reviewer出现一次以上的路径相框，将该结果汇总。</p>\n</li>\n<li>将第四步的结果组织存储到es 中<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">            &quot;url&quot;: workspaceUrl,</span><br><span class=\"line\">            &quot;seller&quot;: sellerId,</span><br><span class=\"line\">            &quot;riskType&quot;: &quot;SockPuppetry&quot;,</span><br><span class=\"line\">            &quot;riskRating&quot;: numCoincidences,</span><br><span class=\"line\">            &quot;numDocs&quot;: sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>完整代码参考一下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import networkx as nx</span><br><span class=\"line\">from elasticsearch import helpers</span><br><span class=\"line\">from elasticsearch.client import Elasticsearch</span><br><span class=\"line\">import sys</span><br><span class=\"line\">import urllib</span><br><span class=\"line\">import json</span><br><span class=\"line\"></span><br><span class=\"line\">es = Elasticsearch(</span><br><span class=\"line\">[</span><br><span class=\"line\">        &apos;http://192.168.0.101:9200/&apos;</span><br><span class=\"line\">]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">alertsIndexName = &quot;alerts&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">def createAlertsIndex():</span><br><span class=\"line\">    alertsIndexSettings = &#123;</span><br><span class=\"line\">        &quot;settings&quot;: &#123;</span><br><span class=\"line\">            &quot;number_of_replicas&quot;: &quot;0&quot;,</span><br><span class=\"line\">            &quot;number_of_shards&quot;: &quot;1&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;mappings&quot;: &#123;</span><br><span class=\"line\">            &quot;task&quot;: &#123;</span><br><span class=\"line\">                &quot;properties&quot;: &#123;</span><br><span class=\"line\">                    &quot;riskType&quot;: &#123;</span><br><span class=\"line\">                        &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &quot;url&quot;: &#123;</span><br><span class=\"line\">                        &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">                        &quot;index&quot;: &quot;false&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if not es.indices.exists(index=alertsIndexName):</span><br><span class=\"line\">        es.indices.create(index=alertsIndexName, body=alertsIndexSettings)</span><br><span class=\"line\"></span><br><span class=\"line\">def getSellersList():</span><br><span class=\"line\">    sellers=[]</span><br><span class=\"line\">    sellersQuery = &#123;</span><br><span class=\"line\">        &quot;size&quot;: 0,</span><br><span class=\"line\">        &quot;aggs&quot;: &#123;</span><br><span class=\"line\">            &quot;topTerms&quot;: &#123;</span><br><span class=\"line\">                &quot;terms&quot;: &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;seller&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 50000</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    results = es.search(index=&quot;reviews&quot;, body=sellersQuery)[&quot;aggregations&quot;][&quot;topTerms&quot;][&quot;buckets&quot;]</span><br><span class=\"line\">    for bucket in results:</span><br><span class=\"line\">        sellers.append(&#123;</span><br><span class=\"line\">            &quot;seller&quot; : bucket[&quot;key&quot;],</span><br><span class=\"line\">            &quot;numReviews&quot; : bucket[&quot;doc_count&quot;]</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    return sellers</span><br><span class=\"line\"></span><br><span class=\"line\">def nodeId(node):</span><br><span class=\"line\">    return node[&quot;field&quot;] + &quot;:&quot; + node[&quot;term&quot;]</span><br><span class=\"line\">    </span><br><span class=\"line\">def erasePrint(msg):</span><br><span class=\"line\">    sys.stdout.write(&apos;\\r&apos;)</span><br><span class=\"line\">    sys.stdout.write(msg)</span><br><span class=\"line\">    sys.stdout.flush()</span><br><span class=\"line\"></span><br><span class=\"line\">createAlertsIndex()</span><br><span class=\"line\">sellers=getSellersList()</span><br><span class=\"line\">rowNum = 0;</span><br><span class=\"line\">totalNumReviews=0</span><br><span class=\"line\">for sellerDetails in sellers:</span><br><span class=\"line\">    rowNum +=1</span><br><span class=\"line\">    totalNumReviews+=sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">    sellerId=sellerDetails[&quot;seller&quot;]</span><br><span class=\"line\">    q = &#123;</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">            &quot;bool&quot;: &#123;</span><br><span class=\"line\">                &quot;must&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;term&quot;: &#123;&quot;seller&quot;: sellerId&#125;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;match&quot;: &#123;&quot;rating&quot;: 5&#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;controls&quot;: &#123;</span><br><span class=\"line\">            &quot;sample_size&quot;: 2000,</span><br><span class=\"line\">            &quot;use_significance&quot;: True</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;vertices&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                # Find most loyal reviewers - significantly connected to the current seller</span><br><span class=\"line\">                &quot;field&quot;: &quot;reviewer&quot;,</span><br><span class=\"line\">                &quot;size&quot;: 50,</span><br><span class=\"line\">                &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;connections&quot;: &#123;</span><br><span class=\"line\">            # Find date/times of loyal reviewers&apos; activity - guide the exploration so only current-seller-related reviews</span><br><span class=\"line\">            &quot;query&quot;: &#123;</span><br><span class=\"line\">                &quot;term&quot;: &#123;</span><br><span class=\"line\">                    &quot;seller&quot;: sellerId</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            # This will naturally favour date/times that are common to multiple reviewers</span><br><span class=\"line\">            &quot;vertices&quot;: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;hour&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 500,</span><br><span class=\"line\">                    &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    erasePrint(&quot;Examining seller &quot; + str(rowNum) + &quot; of &quot; + str(len(sellers)))</span><br><span class=\"line\"></span><br><span class=\"line\">    results = es.transport.perform_request(&apos;POST&apos;, &quot;/reviews/_xpack/graph/_explore&quot;, body=q)</span><br><span class=\"line\"></span><br><span class=\"line\">    # Use NetworkX to create a client-side graph of reviewers and date/times we can analyze</span><br><span class=\"line\">    G = nx.Graph()</span><br><span class=\"line\"></span><br><span class=\"line\">    for node in results[&quot;vertices&quot;]:</span><br><span class=\"line\">        G.add_node(nodeId(node), type=node[&quot;field&quot;])</span><br><span class=\"line\">    for edge in results[&quot;connections&quot;]:</span><br><span class=\"line\">        n1 = results[&quot;vertices&quot;][int(edge[&quot;source&quot;])]</span><br><span class=\"line\">        n2 = results[&quot;vertices&quot;][int(edge[&quot;target&quot;])]</span><br><span class=\"line\">        G.add_edge(nodeId(n1), nodeId(n2))</span><br><span class=\"line\"></span><br><span class=\"line\">    # Examine all &quot;islands&quot; of reviewers connected by same date/time</span><br><span class=\"line\">    subgraphs = nx.connected_component_subgraphs(G)</span><br><span class=\"line\"></span><br><span class=\"line\">    numCoincidences = 0</span><br><span class=\"line\">    for subgraph in subgraphs:</span><br><span class=\"line\">        numHours = 0</span><br><span class=\"line\">        reviewers = []</span><br><span class=\"line\">        for n, d in subgraph.nodes(data=True):</span><br><span class=\"line\">            if d[&quot;type&quot;] == &quot;hour&quot;:</span><br><span class=\"line\">                numHours += 1</span><br><span class=\"line\">            if d[&quot;type&quot;] == &quot;reviewer&quot;:</span><br><span class=\"line\">                reviewers.append(n)</span><br><span class=\"line\">        if numHours &gt; 1:</span><br><span class=\"line\">            if len(reviewers) &gt; 1:</span><br><span class=\"line\">                for srcI in range(0, len(reviewers)):</span><br><span class=\"line\">                    reviewer1 = reviewers[srcI]</span><br><span class=\"line\">                    for targetI in range(srcI + 1, len(reviewers)):</span><br><span class=\"line\">                        reviewer2 = reviewers[targetI]</span><br><span class=\"line\">                        paths = nx.all_simple_paths(subgraph, source=reviewer1, target=reviewer2, cutoff=2)</span><br><span class=\"line\">                        #  Each path is a reviewer &lt;-&gt; date/time &lt;-&gt; reviewer triple</span><br><span class=\"line\">                        sameTimeReviews = 0</span><br><span class=\"line\">                        for path in paths:</span><br><span class=\"line\">                            sameTimeReviews += 1</span><br><span class=\"line\">                        # Two reviewers reviewing at same date/time is not a coincidence - however</span><br><span class=\"line\">                        # repeated synchronized reviews *are* a coincidence</span><br><span class=\"line\">                        if sameTimeReviews &gt; 1:</span><br><span class=\"line\">                            numCoincidences += sameTimeReviews - 1</span><br><span class=\"line\"></span><br><span class=\"line\">    if numCoincidences &gt; 0:</span><br><span class=\"line\">        erasePrint(&quot;&quot;)</span><br><span class=\"line\">        print &quot;seller:&quot; + str(sellerId) + &quot; has &quot;, numCoincidences, &quot; reviewer coincidences in&quot;, sellerDetails[&quot;numReviews&quot;], &quot;reviews&quot;</span><br><span class=\"line\">        gq = json.dumps(q)</span><br><span class=\"line\">        workspaceUrl = &quot;graph#/workspace/32935810-5e4d-11e8-811e-7b68199c5a31?query=&quot; + urllib.quote_plus(gq)</span><br><span class=\"line\">        doc = &#123;</span><br><span class=\"line\">            &quot;url&quot;: workspaceUrl,</span><br><span class=\"line\">            &quot;seller&quot;: sellerId,</span><br><span class=\"line\">            &quot;riskType&quot;: &quot;SockPuppetry&quot;,</span><br><span class=\"line\">            &quot;riskRating&quot;: numCoincidences,</span><br><span class=\"line\">            &quot;numDocs&quot;: sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        res = es.index(index=alertsIndexName, doc_type=&apos;task&apos;, id=sellerId, body=doc)</span><br><span class=\"line\">erasePrint(&quot;&quot;)</span><br><span class=\"line\">print &quot;Completed analysis of&quot;, len(sellers), &quot;sellers and&quot;,totalNumReviews,&quot;reviews&quot;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h2><p>本文代码适用于python2.7,其他版本，请自行参照修改</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.youtube.com/watch?v=liMhiiyQ9co\" target=\"_blank\" rel=\"noopener\">Fraud detection using the elastic stack</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"x-pack之graph研究\"><a href=\"#x-pack之graph研究\" class=\"headerlink\" title=\"x-pack之graph研究\"></a>x-pack之graph研究</h1><h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>Graph 功能是一种基于 API 和基于 UI 的工具，能够让您数据中存在的相关关系浮现出来，同时能够在任何规模下利用各项 Elasticsearch 功能，例如分布式查询执行、实时数据可用性和索引等。</p>\n<ul>\n<li><strong>反欺诈</strong>：挖掘海量购物行为数据和用户画像，探究由哪位店主对一组信用卡被盗事件负责。</li>\n<li><strong>个性化推荐</strong>：根据听众偏好为喜欢莫扎特的听众推荐下一首最佳歌曲， 从而吸引和取悦听众。</li>\n<li><strong>安全分析</strong>：发现潜在的破坏分子和其他意想不到的相关事件， 如挖掘网络中的主机与之进行通信的外部 IP的关联关系。</li>\n</ul>\n<h2 id=\"场景实战\"><a href=\"#场景实战\" class=\"headerlink\" title=\"场景实战\"></a>场景实战</h2><h3 id=\"业务场景\"><a href=\"#业务场景\" class=\"headerlink\" title=\"业务场景\"></a>业务场景</h3><p>针对反欺诈场景，对商家打分作弊，马甲风险评估</p>\n<h3 id=\"准备分析数据\"><a href=\"#准备分析数据\" class=\"headerlink\" title=\"准备分析数据\"></a>准备分析数据</h3><p>运行python IndexReviews.py 将review 数据写入到es 中</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">      &quot;date&quot;: &quot;2006-04-22 13:48&quot;,</span><br><span class=\"line\">      &quot;reviewer&quot;: &quot;15300&quot;,</span><br><span class=\"line\">      &quot;rating&quot;: 5,</span><br><span class=\"line\">      &quot;hour&quot;: &quot;2006-04-22 13&quot;,</span><br><span class=\"line\">      &quot;seller&quot;: &quot;74&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>IndexReviews.py 代码如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">from elasticsearch import helpers</span><br><span class=\"line\">from elasticsearch.client import Elasticsearch</span><br><span class=\"line\">import sys</span><br><span class=\"line\">reload(sys)</span><br><span class=\"line\">sys.setdefaultencoding(&apos;utf8&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">es = Elasticsearch(</span><br><span class=\"line\">[</span><br><span class=\"line\">        &apos;http://192.168.0.101:9200/&apos;</span><br><span class=\"line\">]</span><br><span class=\"line\">)</span><br><span class=\"line\">indexName = &quot;reviews&quot;</span><br><span class=\"line\">es.indices.delete(index=indexName, ignore=[400, 404])</span><br><span class=\"line\">indexSettings = &#123;</span><br><span class=\"line\">    &quot;settings&quot;: &#123;</span><br><span class=\"line\">        &quot;index.number_of_replicas&quot;: 0,</span><br><span class=\"line\">        &quot;index.number_of_shards&quot;: 1</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;mappings&quot;: &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        &quot;review&quot;: &#123;</span><br><span class=\"line\">            &quot;properties&quot;: &#123;</span><br><span class=\"line\">                &quot;reviewer&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;seller&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;rating&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;integer&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;date&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">                    &quot;format&quot;: &quot;yyyy-MM-dd HH:mm&quot;</span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                &quot;hour&quot;: &#123;</span><br><span class=\"line\">                    &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">es.indices.create(index=indexName, body=indexSettings)</span><br><span class=\"line\"></span><br><span class=\"line\">actions = []</span><br><span class=\"line\">rowNum = 0</span><br><span class=\"line\">csvFilename = &quot;reviews.csv&quot;</span><br><span class=\"line\">with open(csvFilename, &apos;rb&apos;) as csvfile:</span><br><span class=\"line\">    csvreader = csv.reader(csvfile)</span><br><span class=\"line\">    for row in csvreader:</span><br><span class=\"line\">        rowNum += 1</span><br><span class=\"line\">        if rowNum == 1:</span><br><span class=\"line\">            continue</span><br><span class=\"line\"></span><br><span class=\"line\">        action = &#123;</span><br><span class=\"line\">            &quot;_index&quot;: indexName,</span><br><span class=\"line\">            &apos;_op_type&apos;: &apos;index&apos;,</span><br><span class=\"line\">            &quot;_type&quot;: &quot;review&quot;,</span><br><span class=\"line\">            &quot;_source&quot;: &#123;</span><br><span class=\"line\">                &quot;reviewer&quot;: row[0],</span><br><span class=\"line\">                &quot;seller&quot;: row[1],</span><br><span class=\"line\">                &quot;rating&quot;: int(row[2]),</span><br><span class=\"line\">                &quot;date&quot;: row[3],</span><br><span class=\"line\">                &quot;hour&quot;: row[3][:-3],</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        actions.append(action)</span><br><span class=\"line\">        # Flush bulk indexing action if necessary</span><br><span class=\"line\">        if len(actions) &gt;= 5000:</span><br><span class=\"line\">            print rowNum</span><br><span class=\"line\">            helpers.bulk(es, actions)</span><br><span class=\"line\">            del actions[0:len(actions)]</span><br><span class=\"line\"></span><br><span class=\"line\">if len(actions) &gt;= 0:</span><br><span class=\"line\">    helpers.bulk(es, actions)</span><br><span class=\"line\">    del actions[0:len(actions)]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用xpack-graph-分析\"><a href=\"#使用xpack-graph-分析\" class=\"headerlink\" title=\"使用xpack graph 分析\"></a>使用xpack graph 分析</h3><ol>\n<li>创建index<br>创建用于输出结果的index,</li>\n<li>使用es api 获取前50000个seller </li>\n<li><p>调用graph 针对每个seller进行关系分析<br>例如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST review/_xpack/graph/_explore</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">            &quot;bool&quot;: &#123;</span><br><span class=\"line\">                &quot;must&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;term&quot;: &#123;&quot;seller&quot;: &quot;A9JN&quot;&#125;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;match&quot;: &#123;&quot;rating&quot;: 5&#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;controls&quot;: &#123;</span><br><span class=\"line\">            &quot;sample_size&quot;: 20,</span><br><span class=\"line\">            &quot;use_significance&quot;: true</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;vertices&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;field&quot;: &quot;customer&quot;,</span><br><span class=\"line\">                &quot;size&quot;: 50,</span><br><span class=\"line\">                &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;connections&quot;: &#123;</span><br><span class=\"line\">            &quot;query&quot;: &#123;</span><br><span class=\"line\">                &quot;term&quot;: &#123;</span><br><span class=\"line\">                    &quot;seller&quot;: &quot;A9JN&quot;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;vertices&quot;: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;hour&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 500,</span><br><span class=\"line\">                    &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>根据查询的图关系进行客户端图计算，查找出reviewer —-&gt;hour—-&gt;reviewer出现一次以上的路径相框，将该结果汇总。</p>\n</li>\n<li>将第四步的结果组织存储到es 中<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">            &quot;url&quot;: workspaceUrl,</span><br><span class=\"line\">            &quot;seller&quot;: sellerId,</span><br><span class=\"line\">            &quot;riskType&quot;: &quot;SockPuppetry&quot;,</span><br><span class=\"line\">            &quot;riskRating&quot;: numCoincidences,</span><br><span class=\"line\">            &quot;numDocs&quot;: sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>完整代码参考一下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import networkx as nx</span><br><span class=\"line\">from elasticsearch import helpers</span><br><span class=\"line\">from elasticsearch.client import Elasticsearch</span><br><span class=\"line\">import sys</span><br><span class=\"line\">import urllib</span><br><span class=\"line\">import json</span><br><span class=\"line\"></span><br><span class=\"line\">es = Elasticsearch(</span><br><span class=\"line\">[</span><br><span class=\"line\">        &apos;http://192.168.0.101:9200/&apos;</span><br><span class=\"line\">]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">alertsIndexName = &quot;alerts&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">def createAlertsIndex():</span><br><span class=\"line\">    alertsIndexSettings = &#123;</span><br><span class=\"line\">        &quot;settings&quot;: &#123;</span><br><span class=\"line\">            &quot;number_of_replicas&quot;: &quot;0&quot;,</span><br><span class=\"line\">            &quot;number_of_shards&quot;: &quot;1&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;mappings&quot;: &#123;</span><br><span class=\"line\">            &quot;task&quot;: &#123;</span><br><span class=\"line\">                &quot;properties&quot;: &#123;</span><br><span class=\"line\">                    &quot;riskType&quot;: &#123;</span><br><span class=\"line\">                        &quot;type&quot;: &quot;keyword&quot;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &quot;url&quot;: &#123;</span><br><span class=\"line\">                        &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">                        &quot;index&quot;: &quot;false&quot;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if not es.indices.exists(index=alertsIndexName):</span><br><span class=\"line\">        es.indices.create(index=alertsIndexName, body=alertsIndexSettings)</span><br><span class=\"line\"></span><br><span class=\"line\">def getSellersList():</span><br><span class=\"line\">    sellers=[]</span><br><span class=\"line\">    sellersQuery = &#123;</span><br><span class=\"line\">        &quot;size&quot;: 0,</span><br><span class=\"line\">        &quot;aggs&quot;: &#123;</span><br><span class=\"line\">            &quot;topTerms&quot;: &#123;</span><br><span class=\"line\">                &quot;terms&quot;: &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;seller&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 50000</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    results = es.search(index=&quot;reviews&quot;, body=sellersQuery)[&quot;aggregations&quot;][&quot;topTerms&quot;][&quot;buckets&quot;]</span><br><span class=\"line\">    for bucket in results:</span><br><span class=\"line\">        sellers.append(&#123;</span><br><span class=\"line\">            &quot;seller&quot; : bucket[&quot;key&quot;],</span><br><span class=\"line\">            &quot;numReviews&quot; : bucket[&quot;doc_count&quot;]</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    return sellers</span><br><span class=\"line\"></span><br><span class=\"line\">def nodeId(node):</span><br><span class=\"line\">    return node[&quot;field&quot;] + &quot;:&quot; + node[&quot;term&quot;]</span><br><span class=\"line\">    </span><br><span class=\"line\">def erasePrint(msg):</span><br><span class=\"line\">    sys.stdout.write(&apos;\\r&apos;)</span><br><span class=\"line\">    sys.stdout.write(msg)</span><br><span class=\"line\">    sys.stdout.flush()</span><br><span class=\"line\"></span><br><span class=\"line\">createAlertsIndex()</span><br><span class=\"line\">sellers=getSellersList()</span><br><span class=\"line\">rowNum = 0;</span><br><span class=\"line\">totalNumReviews=0</span><br><span class=\"line\">for sellerDetails in sellers:</span><br><span class=\"line\">    rowNum +=1</span><br><span class=\"line\">    totalNumReviews+=sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">    sellerId=sellerDetails[&quot;seller&quot;]</span><br><span class=\"line\">    q = &#123;</span><br><span class=\"line\">        &quot;query&quot;: &#123;</span><br><span class=\"line\">            &quot;bool&quot;: &#123;</span><br><span class=\"line\">                &quot;must&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;term&quot;: &#123;&quot;seller&quot;: sellerId&#125;</span><br><span class=\"line\">                    &#125;,</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;match&quot;: &#123;&quot;rating&quot;: 5&#125;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;controls&quot;: &#123;</span><br><span class=\"line\">            &quot;sample_size&quot;: 2000,</span><br><span class=\"line\">            &quot;use_significance&quot;: True</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;vertices&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                # Find most loyal reviewers - significantly connected to the current seller</span><br><span class=\"line\">                &quot;field&quot;: &quot;reviewer&quot;,</span><br><span class=\"line\">                &quot;size&quot;: 50,</span><br><span class=\"line\">                &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;connections&quot;: &#123;</span><br><span class=\"line\">            # Find date/times of loyal reviewers&apos; activity - guide the exploration so only current-seller-related reviews</span><br><span class=\"line\">            &quot;query&quot;: &#123;</span><br><span class=\"line\">                &quot;term&quot;: &#123;</span><br><span class=\"line\">                    &quot;seller&quot;: sellerId</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            # This will naturally favour date/times that are common to multiple reviewers</span><br><span class=\"line\">            &quot;vertices&quot;: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    &quot;field&quot;: &quot;hour&quot;,</span><br><span class=\"line\">                    &quot;size&quot;: 500,</span><br><span class=\"line\">                    &quot;min_doc_count&quot;: 1</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    erasePrint(&quot;Examining seller &quot; + str(rowNum) + &quot; of &quot; + str(len(sellers)))</span><br><span class=\"line\"></span><br><span class=\"line\">    results = es.transport.perform_request(&apos;POST&apos;, &quot;/reviews/_xpack/graph/_explore&quot;, body=q)</span><br><span class=\"line\"></span><br><span class=\"line\">    # Use NetworkX to create a client-side graph of reviewers and date/times we can analyze</span><br><span class=\"line\">    G = nx.Graph()</span><br><span class=\"line\"></span><br><span class=\"line\">    for node in results[&quot;vertices&quot;]:</span><br><span class=\"line\">        G.add_node(nodeId(node), type=node[&quot;field&quot;])</span><br><span class=\"line\">    for edge in results[&quot;connections&quot;]:</span><br><span class=\"line\">        n1 = results[&quot;vertices&quot;][int(edge[&quot;source&quot;])]</span><br><span class=\"line\">        n2 = results[&quot;vertices&quot;][int(edge[&quot;target&quot;])]</span><br><span class=\"line\">        G.add_edge(nodeId(n1), nodeId(n2))</span><br><span class=\"line\"></span><br><span class=\"line\">    # Examine all &quot;islands&quot; of reviewers connected by same date/time</span><br><span class=\"line\">    subgraphs = nx.connected_component_subgraphs(G)</span><br><span class=\"line\"></span><br><span class=\"line\">    numCoincidences = 0</span><br><span class=\"line\">    for subgraph in subgraphs:</span><br><span class=\"line\">        numHours = 0</span><br><span class=\"line\">        reviewers = []</span><br><span class=\"line\">        for n, d in subgraph.nodes(data=True):</span><br><span class=\"line\">            if d[&quot;type&quot;] == &quot;hour&quot;:</span><br><span class=\"line\">                numHours += 1</span><br><span class=\"line\">            if d[&quot;type&quot;] == &quot;reviewer&quot;:</span><br><span class=\"line\">                reviewers.append(n)</span><br><span class=\"line\">        if numHours &gt; 1:</span><br><span class=\"line\">            if len(reviewers) &gt; 1:</span><br><span class=\"line\">                for srcI in range(0, len(reviewers)):</span><br><span class=\"line\">                    reviewer1 = reviewers[srcI]</span><br><span class=\"line\">                    for targetI in range(srcI + 1, len(reviewers)):</span><br><span class=\"line\">                        reviewer2 = reviewers[targetI]</span><br><span class=\"line\">                        paths = nx.all_simple_paths(subgraph, source=reviewer1, target=reviewer2, cutoff=2)</span><br><span class=\"line\">                        #  Each path is a reviewer &lt;-&gt; date/time &lt;-&gt; reviewer triple</span><br><span class=\"line\">                        sameTimeReviews = 0</span><br><span class=\"line\">                        for path in paths:</span><br><span class=\"line\">                            sameTimeReviews += 1</span><br><span class=\"line\">                        # Two reviewers reviewing at same date/time is not a coincidence - however</span><br><span class=\"line\">                        # repeated synchronized reviews *are* a coincidence</span><br><span class=\"line\">                        if sameTimeReviews &gt; 1:</span><br><span class=\"line\">                            numCoincidences += sameTimeReviews - 1</span><br><span class=\"line\"></span><br><span class=\"line\">    if numCoincidences &gt; 0:</span><br><span class=\"line\">        erasePrint(&quot;&quot;)</span><br><span class=\"line\">        print &quot;seller:&quot; + str(sellerId) + &quot; has &quot;, numCoincidences, &quot; reviewer coincidences in&quot;, sellerDetails[&quot;numReviews&quot;], &quot;reviews&quot;</span><br><span class=\"line\">        gq = json.dumps(q)</span><br><span class=\"line\">        workspaceUrl = &quot;graph#/workspace/32935810-5e4d-11e8-811e-7b68199c5a31?query=&quot; + urllib.quote_plus(gq)</span><br><span class=\"line\">        doc = &#123;</span><br><span class=\"line\">            &quot;url&quot;: workspaceUrl,</span><br><span class=\"line\">            &quot;seller&quot;: sellerId,</span><br><span class=\"line\">            &quot;riskType&quot;: &quot;SockPuppetry&quot;,</span><br><span class=\"line\">            &quot;riskRating&quot;: numCoincidences,</span><br><span class=\"line\">            &quot;numDocs&quot;: sellerDetails[&quot;numReviews&quot;]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        res = es.index(index=alertsIndexName, doc_type=&apos;task&apos;, id=sellerId, body=doc)</span><br><span class=\"line\">erasePrint(&quot;&quot;)</span><br><span class=\"line\">print &quot;Completed analysis of&quot;, len(sellers), &quot;sellers and&quot;,totalNumReviews,&quot;reviews&quot;</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h2><p>本文代码适用于python2.7,其他版本，请自行参照修改</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://www.youtube.com/watch?v=liMhiiyQ9co\" target=\"_blank\" rel=\"noopener\">Fraud detection using the elastic stack</a></li>\n</ol>\n"},{"title":"如何优雅的分析redis中的内存数据","date":"2019-03-04T13:37:12.000Z","_content":"## 前言\n目前我们EC Bigdata team 运维公司 4个 Redis 集群，300+ Redis 实例，500G+ 的内存数据，我们想要分析业务是否有误用，以提高资源利用率。伴随着业务team的广泛使用，近期数据增                长比较快，我们紧迫需要一个工具分析一下各种业务存储的数据有多大，是否存入僵死数据浪费资源；同时E4 WWW redis 集群有业务方反馈近期有比较明显的慢查询发生，所以我们需要针对 slow log 和存入的常用数据类型Hash,List,Set分析，是否有big key引起慢查询,是否有team存在超大的 big key和不合理设置ttl的情况\n\n那有没有什么办法让我们安全高效的看到 Redis 内存消耗的详细报表呢？办法总比问题多，有需求就有解决方案。EC Bigdata team针对这个问题实现了一个 Redis 内存数据可视化分析平台 RCT (Redis Computed Tomography)。\n\nRCT可以非常方便的对 Reids 的内存进行分析，了解一个 Redis 实例里都有哪些 key，哪类 key 占用的空间是多少，最耗内存的 key 有哪些，占比如何，非常直观,除此之外，我们还可以针对Redis slowlog/clientlist进行分钟级别监控，直观监控集群效应状况。\n## 同类产品\n1. [redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)\n2. [rdr](https://github.com/xueqiu/rdr)\n3. [redis-rdb-cli](https://github.com/leonchen83/redis-rdb-cli)\n\n市面上已经存在这么多开源的产品，我们为什么还要重新做一个呢？主要还是没有满足我们的需求，以上的都是redis rdb解析工具，最接近我们需求的就是雪球开源的rdr,但是也只限于离线分析，而我们节点众多，不可能一个一个去线上机器copy,或者开发copy工具，这样也会给相应机器带来网络热点（总有办法解决这个问题），带来很多繁重无意义的劳动，么不是专门的运维工程师，还有更多的开发任务等着我们去做。\n\n我们想要一个每天都会给我们产生报表，自动推送到我们的邮箱，或者在网页上就能看到最近redis内存的变化，它不仅仅是一个工具，还是常态化运行的服务，基于此我们打造了属于最近的redis内存分析平台RCT。\n## 理论\n### 设计思路\n使用 bgsave，获取 rdb 文件，解析后获取数据。\n\n优点：机制成熟，可靠性好；文件相对小，传输、解析效率高；\n\n缺点：bgsave 虽然会 fork 子进程，但还是有可能导致主进程卡住一段时间，对业务有产生影响的风险；\n\n采用**低峰期在从节点做 bgsave 获取 rdb 文件**，相对安全可靠。拿到了 rdb 文件就相当于拿到了 Redis 实例的所有数据，接下来就是生成报表的过程了：\n\n解析 rdb 文件，获取到 Key 和 Value 的内容；根据相对应的数据结构及内容，估算内存消耗等;统计并生成报表；逻辑很简单，所以设计思路很清晰。\n### 数据流图\n![](https://github.com/xaecbd/RCT/blob/master/doc/screenshots/%E6%95%B0%E6%8D%AE%E6%B5%81.png?raw=true)\n### slave节点均分计算\n为了使对线上机器不产生影响，我们选择是在slave节点进行rdb文件分析，该任务是分布式的。为了均衡对每个机器的影响，通过算法去保证slave分配算法均匀的落在不同的机器上。\n\n#### 算法思路\n1. slave数量最小的优先分配\n2. 通过map存储不同IP分配的数量，按照规则，优先分配数量最小的IP\n3. staticsResult里面不存在的IP优先分配\n4. 如果上面未分配,则选择staticsResult中数值最小的那个slave\n#### 算法代码实现\n```\n/**\n\t * 根据<master:slaves>获取执行分析任务ports规则\n\t * 即获取其中一个slave,尽量保持均衡在不同机器上\n\t * \n\t * @param clusterNodesMap\n\t * @return <ip:ports>\n\t */\n\tpublic static Map<String, Set<String>> generateAnalyzeRule(Map<String, List<String>> clusterNodesMap) {\n\t\t\n\t\t// 通过该map存储不同IP分配的数量，按照规则，优先分配数量最小的IP\n\t\tMap<String, Integer> staticsResult = new HashMap<>();\n\t\tMap<String, Set<String>> generateRule = new HashMap<>();\n\n\t\t// 此处排序是为了将slave数量最小的优先分配\n\t\tList<Map.Entry<String, List<String>>> sortList = new LinkedList<>(clusterNodesMap.entrySet());\n\t\tCollections.sort(sortList, new Comparator<Entry<String, List<String>>>() {\n\t\t\t@Override\n\t\t\tpublic int compare(Entry<String, List<String>> o1, Entry<String, List<String>> o2) {\n\t\t\t\treturn o1.getValue().size() - o2.getValue().size();\n\t\t\t}\n\t\t});\n\n\t\tfor (Entry<String, List<String>> entry : sortList) {\n\t\t\tList<String> slaves = entry.getValue();\n\t\t\tboolean isSelected = false;\n\t\t\tString tempPort = null;\n\t\t\tString tempIP = null;\n\t\t\tint num = 0;\n\t\t\tfor (String slave : slaves) {\n\t\t\t\tString ip = slave.split(\":\")[0];\n\t\t\t\tString port = slave.split(\":\")[1];\n\t\t\t\t// 统计组里面不存在的IP优先分配\n\t\t\t\tif (!staticsResult.containsKey(ip)) {\n\t\t\t\t\tstaticsResult.put(ip, 1);\n\t\t\t\t\tSet<String> generatePorts = generateRule.get(ip);\n\t\t\t\t\tif (generatePorts == null) {\n\t\t\t\t\t\tgeneratePorts = new HashSet<>();\n\t\t\t\t\t}\n\t\t\t\t\tgeneratePorts.add(port);\n\t\t\t\t\tgenerateRule.put(ip, generatePorts);\n\t\t\t\t\tisSelected = true;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// 此处是为了求出被使用最少的IP\n\t\t\t\t\tInteger staticsNum = staticsResult.get(ip);\n\t\t\t\t\tif (num == 0) {\n\t\t\t\t\t\tnum = staticsNum;\n\t\t\t\t\t\ttempPort = port;\n\t\t\t\t\t\ttempIP = ip;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (staticsNum < num) {\n\t\t\t\t\t\ttempPort = port;\n\t\t\t\t\t\ttempIP = ip;\n\t\t\t\t\t\tnum = staticsNum;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\t// 如果上面未分配,则选择staticsResult中数值最小的那个slave\n\t\t\tif (!isSelected) {\n\t\t\t\tif (slaves != null && slaves.size() > 0) {\n\t\t\t\t\tif (tempPort != null) {\n\t\t\t\t\t\tSet<String> generatePorts = generateRule.get(tempIP);\n\t\t\t\t\t\tif (generatePorts == null) {\n\t\t\t\t\t\t\tgeneratePorts = new HashSet<>();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tgeneratePorts.add(tempPort);\n\t\t\t\t\t\tgenerateRule.put(tempIP, generatePorts);\n\t\t\t\t\t\tstaticsResult.put(tempIP, staticsResult.get(tempIP) + 1);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn generateRule;\n\t}\n```\n### Jemalloc内存预分配\n#### 算法思路\nredis中支持多种内存分配算法，推荐Jemalloc，因此我们选择该算法来预估内存大小。因为这个算法比较复杂，我们参考雪球做法使用约定数组，根据不同数据大小分配不同内存空间，为了提高查找效率，这块采用了变种二分查找。\n#### 算法代码\n```\npublic class Jemalloc {\n\n\tprivate static long[] array16;\n\n\tprivate static long[] array192;\n\n\tprivate static long[] array768;\n\n\tprivate static long[] array4096;\n\n\tprivate static long[] array4194304;\n\n\tstatic {\n\t\tarray16 = range(16, 128 + 1, 16);\n\n\t\tarray192 = range(192, 512 + 1, 64);\n\n\t\tarray768 = range(768, 4096 + 1, 256);\n\n\t\tarray4096 = range(4096, 4194304 + 1, 4096);\n\n\t\tarray4194304 = range(4194304, 536870912 + 1, 4194304);\n\n\t}\n\n\t/**\n\t * 根据Jemalloc 估算分配内存大小 Small: All 2^n-aligned allocations of size 2^n will incur\n\t * no additional overhead, due to how small allocations are aligned and packed.\n\t * Small: [8], [16, 32, 48, ..., 128], [192, 256, 320, ..., 512], [768, 1024,\n\t * 1280, ..., 3840] Large: The worst case size is half the chunk size, in which\n\t * case only one allocation per chunk can be allocated. If the remaining\n\t * (nearly) half of the chunk isn't otherwise useful for smaller allocations,\n\t * the overhead will essentially be 50%. However, assuming you use a diverse\n\t * mixture of size classes, the actual overhead shouldn't be a significant issue\n\t * in practice. Large: [4 KiB, 8 KiB, 12 KiB, ..., 4072 KiB] Huge: Extra virtual\n\t * memory is mapped, then the excess is trimmed and unmapped. This can leave\n\t * virtual memory holes, but it incurs no physical memory overhead. Earlier\n\t * versions of jemalloc heuristically attempted to optimistically map chunks\n\t * without excess that would need to be trimmed, but it didn't save much system\n\t * call overhead in practice. Huge: [4 MiB, 8 MiB, 12 MiB, ..., 512 MiB]\n\t * \n\t * @param size\n\t * @return\n\t */\n\tpublic static long assign(long size) {\n\t\tif (size <= 4096) {\n\t\t\t// Small\n\t\t\tif (is_power2(size)) {\n\t\t\t\treturn size;\n\t\t\t} else if (size < 128) {\n\t\t\t\treturn min_ge(array16, size);\n\t\t\t} else if (size < 512) {\n\t\t\t\treturn min_ge(array192, size);\n\t\t\t} else {\n\t\t\t\treturn min_ge(array768, size);\n\t\t\t}\n\t\t} else if (size < 4194304) {\n\t\t\t// Large\n\t\t\treturn min_ge(array4096, size);\n\t\t} else {\n\t\t\t// Huge\n\t\t\treturn min_ge(array4194304, size);\n\t\t}\n\t}\n\n\t/**\n\t * 创建一个long数组\n\t * \n\t * @param start\n\t * @param stop\n\t * @param step\n\t * @return\n\t */\n\tpublic static long[] range(int start, int stop, int step) {\n\t\tint size = (stop - 1 - start) / step + 1;\n\t\tlong[] array = new long[size];\n\t\tint index = 0;\n\t\tfor (int i = start; i < stop; i = i + step) {\n\t\t\tarray[index] = i;\n\t\t\tindex++;\n\t\t}\n\t\treturn array;\n\t}\n\n\tpublic static long min_ge(long[] srcArray, long key) {\n\t\tint index = binarySearch(srcArray, key);\n\t\treturn srcArray[index];\n\t}\n\n\t// 二分查找最小值，即最接近要查找的值，但是要大于该值\n\tpublic static int binarySearch(long srcArray[], long key) {\n\t\tint mid = (0+srcArray.length-1) / 2;\n\t\tif (key == srcArray[mid]) {\n\t\t\treturn mid;\n\t\t}\n\n\t\tif (key > srcArray[mid] && key <= srcArray[mid + 1]) {\n\t\t\treturn mid + 1;\n\t\t}\n\n\t\tint start = 0;\n\t\tint end = srcArray.length - 1;\n\t\twhile (start <= end) {\n\t\t\tmid = (end - start) / 2 + start;\n\t\t\tif (key == srcArray[mid]) {\n\t\t\t\treturn mid;\n\t\t\t}\n\t\t\tif (key > srcArray[mid] && key <= srcArray[mid + 1]) {\n\t\t\t\treturn mid + 1;\n\t\t\t}\n\t\t\tif (key < srcArray[mid]) {\n\t\t\t\tend = mid - 1;\n\t\t\t}\n\t\t\tif (key > srcArray[mid]) {\n\t\t\t\tstart = mid + 1;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tpublic static boolean is_power2(long size) {\n\t\tif (size == 0) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif ((size & (size - 1)) == 0) {\n\t\t\treturn true;\n\t\t}\n\n\t\treturn false;\n\t}\n}\n```\n### Redis不同数据结构预估\n#### 算法思路\n详见[redis源码](https://github.com/antirez/redis)\n#### 算法代码\n```\n\tprivate static final long redisObject = (long)(8 + 8);\n\t// 一个dictEntry，24字节，jemalloc会分配32字节的内存块\n\tprivate static final long dicEntry = (long)(2 * 8 + 8 + 8);\n\tprivate static final String patternString = \"^[-\\\\\\\\+]?[\\\\\\\\d]*$\";\n\n\tprivate static long skiplistMaxLevel = 32;\n\tprivate static long redisSharedInterges = 10000;\n\tprivate static long longSize = 8;\n\tprivate static long pointerSize = 8;\n\n\t/**\n\t * 一个SDS结构占据的空间为：free所占长度+len所占长度+ buf数组的长度=4+4+len+1=len+9\n\t * \n\t * @param length\n\t * @return\n\t */\n\tprivate static long sds(long length) {\n\t\tlong mem = 9 + length;\n\t\treturn mem;\n\t}\n\n\t/**\n\t * \n\t * 计算 string byte 大小\n\t * \n\t * @param kv\n\t *            https://searchdatabase.techtarget.com.cn/wp-content/uploads/res/database/article/2011/2011-11-14-16-56-18.jpg\n\t * @return\n\t */\n\tpublic static long CalculateString(KeyStringValueString kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = dicEntry + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject + SizeofString(kv.getValueAsString());\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateLinkedList(KeyStringValueList kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsStringList().size();\n\t\tmem = mem + LinkedListEntryOverhead() * length;\n\t\tmem = mem + LinkedlistOverhead();\n\t\tmem = mem + redisObject * length;\n\t\tfor (String value : kv.getValueAsStringList()) {\n\t\t\tmem = mem + SizeofString(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\t\n\tpublic static long CalculateZipList(KeyStringValueList kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + dicEntry;\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tlong length = kv.getValueAsStringList().size();\n\t\tmem = mem + ZiplistOverhead(length);\n        \n\t\tfor (String value : kv.getValueAsStringList()) {\n\t\t\tmem = mem + ZiplistAlignedStringOverhead(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateHash(KeyStringValueHash kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsHash().size();\n\t\tmem = mem + HashtableOverhead(length);\n\n\t\tfor (String key : kv.getValueAsHash().keySet()) {\n\t\t\tString value = kv.getValueAsHash().get(key);\n\t\t\tmem = mem + SizeofString(key);\n\t\t\tmem = mem + SizeofString(value);\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + HashtableEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateSet(KeyStringValueSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + HashtableOverhead(length);\n\t\tmem = mem + redisObject * length;\n\n\t\tfor (String value : kv.getValueAsSet()) {\n\t\t\tmem = mem + SizeofString(value);\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + HashtableEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\t\n\tpublic static long CalculateIntSet(KeyStringValueSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + dicEntry;\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\t\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + IntsetOverhead(length);\n\n\t\tfor (String value : kv.getValueAsSet()) {\n\t\t\tmem = mem + ZiplistAlignedStringOverhead(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateZSet(KeyStringValueZSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + SkiplistOverhead(length);\n\t\tmem = mem + redisObject * length;\n\n\t\tfor (ZSetEntry value : kv.getValueAsZSet()) {\n\t\t\tmem = mem + 8;\n\t\t\tmem = mem + SizeofString(value.getElement());\n\t\t\t// TODO 还有个 score\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + SkiplistEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\t// TopLevelObjOverhead get memory use of a top level object\n\t// Each top level object is an entry in a dictionary, and so we have to include\n\t// the overhead of a dictionary entry\n\tpublic static long TopLevelObjOverhead() {\n\t\treturn HashtableEntryOverhead();\n\t}\n\n\t/**\n\t * SizeofString get memory use of a string\n\t * https://github.com/antirez/redis/blob/unstable/src/sds.h\n\t * \n\t * @param bytes\n\t * @return\n\t */\n\tpublic static long SizeofString(byte[] bytes) {\n\t\tString value = new String(bytes);\n\n\t\tif (isInteger(value)) {\n\t\t\ttry {\n\t\t\t\tLong num = Long.parseLong(value);\n\t\t\t\tif (num < redisSharedInterges && num > 0) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\treturn 8;\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t}\n\t\t}\n\n\t\treturn Jemalloc.assign(sds(bytes.length));\n\t}\n\t\n\t\n\tpublic static long SizeofString(String value) {\n\t\tif (isInteger(value)) {\n\t\t\ttry {\n\t\t\t\tLong num = Long.parseLong(value);\n\t\t\t\tif (num < redisSharedInterges && num > 0) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\treturn 8;\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t}\n\t\t}\n\n\t\treturn Jemalloc.assign(sds(value.length()));\n\t}\n\t\n\tpublic static long DictOverhead(long size) {\n\t\t return Jemalloc.assign(56 + 2*pointerSize + nextPower(size) * 3*8);\n\t}\n\n\tpublic static boolean isInteger(String str) {\n\t\tPattern pattern = Pattern.compile(patternString);\n\t\treturn pattern.matcher(str).matches();\n\t}\n\n\t/**\n\t * 过期时间也是存储为一个 dictEntry，时间戳为 int64；\n\t * \n\t * @param kv\n\t * @return\n\t */\n\t// KeyExpiryOverhead get memory useage of a key expiry\n\t// Key expiry is stored in a hashtable, so we have to pay for the cost of a\n\t// hashtable entry\n\t// The timestamp itself is stored as an int64, which is a 8 bytes\n\t@SuppressWarnings(\"rawtypes\")\n\tpublic static long KeyExpiryOverhead(KeyValuePair kv) {\n\t\t// If there is no expiry, there isn't any overhead\n\t\tif (kv.getExpiredType() == ExpiredType.NONE) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn HashtableEntryOverhead() + 8;\n\t}\n\n\tpublic static long HashtableOverhead(long size) {\n\t\treturn 4 + 7 * longSize + 4 * pointerSize + nextPower(size) * pointerSize * 3 / 2;\n\t}\n\n\t// HashtableEntryOverhead get memory use of hashtable entry\n\t// See https://github.com/antirez/redis/blob/unstable/src/dict.h\n\t// Each dictEntry has 2 pointers + int64\n\tpublic static long HashtableEntryOverhead() {\n\t\treturn 2 * pointerSize + 8;\n\t}\n\n\tpublic static long ZiplistOverhead(long size) {\n\t\treturn Jemalloc.assign(12 + 21 * size);\n\t}\n\n\tpublic static long ZiplistAlignedStringOverhead(String value) {\n\t\ttry {\n\t\t\tLong.parseLong(value);\n\t\t\treturn 8;\n\t\t} catch (NumberFormatException e) {\n\t\t}\n\t\treturn Jemalloc.assign(value.length());\n\t}\n\n\t// LinkedlistOverhead get memory use of a linked list\n\t// See https://github.com/antirez/redis/blob/unstable/src/adlist.h\n\t// A list has 5 pointers + an unsigned long\n\tpublic static long LinkedlistOverhead() {\n\t\treturn longSize + 5 * pointerSize;\n\t}\n\n\t// LinkedListEntryOverhead get memory use of a linked list entry\n\t// See https://github.com/antirez/redis/blob/unstable/src/adlist.h\n\t// A node has 3 pointers\n\tpublic static long LinkedListEntryOverhead() {\n\t\treturn 3 * pointerSize;\n\t}\n\n\t// SkiplistOverhead get memory use of a skiplist\n\tpublic static long SkiplistOverhead(long size) {\n\t\treturn 2 * pointerSize + HashtableOverhead(size) + (2 * pointerSize + 16);\n\t}\n\n\t// SkiplistEntryOverhead get memory use of a skiplist entry\n\tpublic static long SkiplistEntryOverhead() {\n\t\treturn HashtableEntryOverhead() + 2 * pointerSize + 8 + (pointerSize + 8) * zsetRandLevel();\n\t}\n\n\tpublic static long nextPower(long size) {\n\t\tlong power = 1;\n\t\twhile (power <= size) {\n\t\t\tpower = power << 1;\n\t\t}\n\t\treturn power;\n\t}\n\n\tpublic static long zsetRandLevel() {\n\t\tlong level = 1;\n\t\tint rint = new Random().nextInt(65536);\n\t\tint flag = 65535 / 4;\n\t\twhile (rint < flag) {// skiplistP\n\t\t\tlevel++;\n\t\t\trint = new Random().nextInt(65536);\n\t\t}\n\t\tif (level < skiplistMaxLevel) {\n\t\t\treturn level;\n\t\t}\n\t\treturn skiplistMaxLevel;\n\t}\n\t\n\tpublic static long  IntsetOverhead(long size) {\n\t    //     typedef struct intset {\n\t    //     uint32_t encoding;\n\t    //     uint32_t length;\n\t    //     int8_t contents[];\n\t    //      } intset;\n\t    return (4 + 4) * size;\n\t}\n```\n## RCT分析redis rdb\nRCT是一个一站式redis内存分析分析平台，分析任务是分布式的，需要将RCT-Analyze部署到rdb所在机器上，RCT-Dashboard部署在任何机器，只要能保持和RCT-Analyze通信即可。不同于以上列举的工具，我们最初定位RCT就是一个可以长期运行，尽可能每天分析redis中数据，为redis运维人员提供运维依据，便于做出更好的规范，高效的使用redis。\n### 部署\n部署过程略，详见[官方文档](https://github.com/xaecbd/RCT/blob/master/README_zh.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B)，推荐使用docker方式，这样也是我们目前采用的方式。\n### 配置分析任务\n#### 新增RDB分析配置\n只有先创建了redis节点之后，才能进入到RCT工具导航页面。\n1.  点击导航RDB Analyze\n2.  如若一直没有添加过RDB信息，则可在页面弹出的框中进行完善信息，或者点击下方的Add按钮进行添加\n3.  点击edit则可以对RDB 信息进行修改\n4.  如若已经完善了RDB信息，则点击打开switch开关，则是对RDB直接进行定时任务的开启和关闭 \n5.  点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态 \n\n#### RDB Add页面参数说明\n1.  **Automatic Analyze**：是否开启定时任务\n2.  **Schedule**：cron 表达式（填写完成之后，可以点击右侧的图标进行查看定时表达式执行的时间）\n3.  **Analyzer**:分析器 （依次是生成报表，根据filter导key到elasticsearch中，根据preix导key到elasticsearch中）\n4.  **Data Path**:rdb文件的目录（eg:/opt/app/redis/9002/dump.rdb,data path应为/opt/app/redis）\n5.  **Prefixes**:key的前缀，可为空，但是在选择了分析器中的根据preix导key，则必须填写prefixes。我们强烈建议将已知前缀填入，可以提高分析效率，节省时间。\n6.  **Report**：是否生成报表\n7.  **Mail**：生成报表之后的收件人，平台将会将报表做为附件发送给收件人，如果收件人有多个，请用;隔开\n\n#### RDB 主页面参数说明\nRDB主页面参数与add页面参数基本相同，在此不再做赘述，唯一有区别的是：\nStatus：分析RDB文件的进度状态，分为成功，正在分析，失败三种状态，status为正在分析状态时，可通过点击后面的链接进入到实时分析页面，查看实时状态。\n\n#### 分析器介绍\n1. 生成报表：对rdb文件的数据进行分析并将结果写入数据库中，如果配置了Report，结果会以excel报表的形式发送邮件给用户\n2. 根据filter导key：在对rdb文件分析的时候，根据过滤器导出相应的key，将数据写入到EleasticSearch中\n3. 根据prefix导key：在对rdb文件分析的时候，根据制定的前缀key导出相应的key，将数据写入到EleasticSearch中\n\n### 手动分析\n点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态\n### 查看报表\n目前RCT支持dashboard/email两种方式，这里我仅展示dashboard.\n![image](https://github.com/xaecbd/RCT/raw/master/doc/screenshots/rct.jpg?raw=true)\n\n## 项目地址\n[https://github.com/xaecbd/RCT](https://github.com/xaecbd/RCT),欢迎使用，欢迎加入我们，帮我们提高RCT。如果你有问题，可以前往github新建issue。\n## 参考\n1. [Redis持久化文件RDB的格式解析](http://www.innereye.cn/2017/01/17/redis/Redis_RDB_File_Format_20170117/)\n2. [redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)\n3. [rdr](https://github.com/xueqiu/rdr)\n4. [analysis-redis](https://infoq.cn/article/analysis-redis)\n5. [Redis内存模型](http://www.cnblogs.com/kismetv/p/8654978.html)","source":"_posts/如何优雅的分析redis中的内存数据.md","raw":"---\ntitle: 如何优雅的分析redis中的内存数据\ndate: 2019-03-04 21:37:12\ntags: 技术分享\ncategories:\n- redis\n---\n## 前言\n目前我们EC Bigdata team 运维公司 4个 Redis 集群，300+ Redis 实例，500G+ 的内存数据，我们想要分析业务是否有误用，以提高资源利用率。伴随着业务team的广泛使用，近期数据增                长比较快，我们紧迫需要一个工具分析一下各种业务存储的数据有多大，是否存入僵死数据浪费资源；同时E4 WWW redis 集群有业务方反馈近期有比较明显的慢查询发生，所以我们需要针对 slow log 和存入的常用数据类型Hash,List,Set分析，是否有big key引起慢查询,是否有team存在超大的 big key和不合理设置ttl的情况\n\n那有没有什么办法让我们安全高效的看到 Redis 内存消耗的详细报表呢？办法总比问题多，有需求就有解决方案。EC Bigdata team针对这个问题实现了一个 Redis 内存数据可视化分析平台 RCT (Redis Computed Tomography)。\n\nRCT可以非常方便的对 Reids 的内存进行分析，了解一个 Redis 实例里都有哪些 key，哪类 key 占用的空间是多少，最耗内存的 key 有哪些，占比如何，非常直观,除此之外，我们还可以针对Redis slowlog/clientlist进行分钟级别监控，直观监控集群效应状况。\n## 同类产品\n1. [redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)\n2. [rdr](https://github.com/xueqiu/rdr)\n3. [redis-rdb-cli](https://github.com/leonchen83/redis-rdb-cli)\n\n市面上已经存在这么多开源的产品，我们为什么还要重新做一个呢？主要还是没有满足我们的需求，以上的都是redis rdb解析工具，最接近我们需求的就是雪球开源的rdr,但是也只限于离线分析，而我们节点众多，不可能一个一个去线上机器copy,或者开发copy工具，这样也会给相应机器带来网络热点（总有办法解决这个问题），带来很多繁重无意义的劳动，么不是专门的运维工程师，还有更多的开发任务等着我们去做。\n\n我们想要一个每天都会给我们产生报表，自动推送到我们的邮箱，或者在网页上就能看到最近redis内存的变化，它不仅仅是一个工具，还是常态化运行的服务，基于此我们打造了属于最近的redis内存分析平台RCT。\n## 理论\n### 设计思路\n使用 bgsave，获取 rdb 文件，解析后获取数据。\n\n优点：机制成熟，可靠性好；文件相对小，传输、解析效率高；\n\n缺点：bgsave 虽然会 fork 子进程，但还是有可能导致主进程卡住一段时间，对业务有产生影响的风险；\n\n采用**低峰期在从节点做 bgsave 获取 rdb 文件**，相对安全可靠。拿到了 rdb 文件就相当于拿到了 Redis 实例的所有数据，接下来就是生成报表的过程了：\n\n解析 rdb 文件，获取到 Key 和 Value 的内容；根据相对应的数据结构及内容，估算内存消耗等;统计并生成报表；逻辑很简单，所以设计思路很清晰。\n### 数据流图\n![](https://github.com/xaecbd/RCT/blob/master/doc/screenshots/%E6%95%B0%E6%8D%AE%E6%B5%81.png?raw=true)\n### slave节点均分计算\n为了使对线上机器不产生影响，我们选择是在slave节点进行rdb文件分析，该任务是分布式的。为了均衡对每个机器的影响，通过算法去保证slave分配算法均匀的落在不同的机器上。\n\n#### 算法思路\n1. slave数量最小的优先分配\n2. 通过map存储不同IP分配的数量，按照规则，优先分配数量最小的IP\n3. staticsResult里面不存在的IP优先分配\n4. 如果上面未分配,则选择staticsResult中数值最小的那个slave\n#### 算法代码实现\n```\n/**\n\t * 根据<master:slaves>获取执行分析任务ports规则\n\t * 即获取其中一个slave,尽量保持均衡在不同机器上\n\t * \n\t * @param clusterNodesMap\n\t * @return <ip:ports>\n\t */\n\tpublic static Map<String, Set<String>> generateAnalyzeRule(Map<String, List<String>> clusterNodesMap) {\n\t\t\n\t\t// 通过该map存储不同IP分配的数量，按照规则，优先分配数量最小的IP\n\t\tMap<String, Integer> staticsResult = new HashMap<>();\n\t\tMap<String, Set<String>> generateRule = new HashMap<>();\n\n\t\t// 此处排序是为了将slave数量最小的优先分配\n\t\tList<Map.Entry<String, List<String>>> sortList = new LinkedList<>(clusterNodesMap.entrySet());\n\t\tCollections.sort(sortList, new Comparator<Entry<String, List<String>>>() {\n\t\t\t@Override\n\t\t\tpublic int compare(Entry<String, List<String>> o1, Entry<String, List<String>> o2) {\n\t\t\t\treturn o1.getValue().size() - o2.getValue().size();\n\t\t\t}\n\t\t});\n\n\t\tfor (Entry<String, List<String>> entry : sortList) {\n\t\t\tList<String> slaves = entry.getValue();\n\t\t\tboolean isSelected = false;\n\t\t\tString tempPort = null;\n\t\t\tString tempIP = null;\n\t\t\tint num = 0;\n\t\t\tfor (String slave : slaves) {\n\t\t\t\tString ip = slave.split(\":\")[0];\n\t\t\t\tString port = slave.split(\":\")[1];\n\t\t\t\t// 统计组里面不存在的IP优先分配\n\t\t\t\tif (!staticsResult.containsKey(ip)) {\n\t\t\t\t\tstaticsResult.put(ip, 1);\n\t\t\t\t\tSet<String> generatePorts = generateRule.get(ip);\n\t\t\t\t\tif (generatePorts == null) {\n\t\t\t\t\t\tgeneratePorts = new HashSet<>();\n\t\t\t\t\t}\n\t\t\t\t\tgeneratePorts.add(port);\n\t\t\t\t\tgenerateRule.put(ip, generatePorts);\n\t\t\t\t\tisSelected = true;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\t// 此处是为了求出被使用最少的IP\n\t\t\t\t\tInteger staticsNum = staticsResult.get(ip);\n\t\t\t\t\tif (num == 0) {\n\t\t\t\t\t\tnum = staticsNum;\n\t\t\t\t\t\ttempPort = port;\n\t\t\t\t\t\ttempIP = ip;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif (staticsNum < num) {\n\t\t\t\t\t\ttempPort = port;\n\t\t\t\t\t\ttempIP = ip;\n\t\t\t\t\t\tnum = staticsNum;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\t// 如果上面未分配,则选择staticsResult中数值最小的那个slave\n\t\t\tif (!isSelected) {\n\t\t\t\tif (slaves != null && slaves.size() > 0) {\n\t\t\t\t\tif (tempPort != null) {\n\t\t\t\t\t\tSet<String> generatePorts = generateRule.get(tempIP);\n\t\t\t\t\t\tif (generatePorts == null) {\n\t\t\t\t\t\t\tgeneratePorts = new HashSet<>();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tgeneratePorts.add(tempPort);\n\t\t\t\t\t\tgenerateRule.put(tempIP, generatePorts);\n\t\t\t\t\t\tstaticsResult.put(tempIP, staticsResult.get(tempIP) + 1);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn generateRule;\n\t}\n```\n### Jemalloc内存预分配\n#### 算法思路\nredis中支持多种内存分配算法，推荐Jemalloc，因此我们选择该算法来预估内存大小。因为这个算法比较复杂，我们参考雪球做法使用约定数组，根据不同数据大小分配不同内存空间，为了提高查找效率，这块采用了变种二分查找。\n#### 算法代码\n```\npublic class Jemalloc {\n\n\tprivate static long[] array16;\n\n\tprivate static long[] array192;\n\n\tprivate static long[] array768;\n\n\tprivate static long[] array4096;\n\n\tprivate static long[] array4194304;\n\n\tstatic {\n\t\tarray16 = range(16, 128 + 1, 16);\n\n\t\tarray192 = range(192, 512 + 1, 64);\n\n\t\tarray768 = range(768, 4096 + 1, 256);\n\n\t\tarray4096 = range(4096, 4194304 + 1, 4096);\n\n\t\tarray4194304 = range(4194304, 536870912 + 1, 4194304);\n\n\t}\n\n\t/**\n\t * 根据Jemalloc 估算分配内存大小 Small: All 2^n-aligned allocations of size 2^n will incur\n\t * no additional overhead, due to how small allocations are aligned and packed.\n\t * Small: [8], [16, 32, 48, ..., 128], [192, 256, 320, ..., 512], [768, 1024,\n\t * 1280, ..., 3840] Large: The worst case size is half the chunk size, in which\n\t * case only one allocation per chunk can be allocated. If the remaining\n\t * (nearly) half of the chunk isn't otherwise useful for smaller allocations,\n\t * the overhead will essentially be 50%. However, assuming you use a diverse\n\t * mixture of size classes, the actual overhead shouldn't be a significant issue\n\t * in practice. Large: [4 KiB, 8 KiB, 12 KiB, ..., 4072 KiB] Huge: Extra virtual\n\t * memory is mapped, then the excess is trimmed and unmapped. This can leave\n\t * virtual memory holes, but it incurs no physical memory overhead. Earlier\n\t * versions of jemalloc heuristically attempted to optimistically map chunks\n\t * without excess that would need to be trimmed, but it didn't save much system\n\t * call overhead in practice. Huge: [4 MiB, 8 MiB, 12 MiB, ..., 512 MiB]\n\t * \n\t * @param size\n\t * @return\n\t */\n\tpublic static long assign(long size) {\n\t\tif (size <= 4096) {\n\t\t\t// Small\n\t\t\tif (is_power2(size)) {\n\t\t\t\treturn size;\n\t\t\t} else if (size < 128) {\n\t\t\t\treturn min_ge(array16, size);\n\t\t\t} else if (size < 512) {\n\t\t\t\treturn min_ge(array192, size);\n\t\t\t} else {\n\t\t\t\treturn min_ge(array768, size);\n\t\t\t}\n\t\t} else if (size < 4194304) {\n\t\t\t// Large\n\t\t\treturn min_ge(array4096, size);\n\t\t} else {\n\t\t\t// Huge\n\t\t\treturn min_ge(array4194304, size);\n\t\t}\n\t}\n\n\t/**\n\t * 创建一个long数组\n\t * \n\t * @param start\n\t * @param stop\n\t * @param step\n\t * @return\n\t */\n\tpublic static long[] range(int start, int stop, int step) {\n\t\tint size = (stop - 1 - start) / step + 1;\n\t\tlong[] array = new long[size];\n\t\tint index = 0;\n\t\tfor (int i = start; i < stop; i = i + step) {\n\t\t\tarray[index] = i;\n\t\t\tindex++;\n\t\t}\n\t\treturn array;\n\t}\n\n\tpublic static long min_ge(long[] srcArray, long key) {\n\t\tint index = binarySearch(srcArray, key);\n\t\treturn srcArray[index];\n\t}\n\n\t// 二分查找最小值，即最接近要查找的值，但是要大于该值\n\tpublic static int binarySearch(long srcArray[], long key) {\n\t\tint mid = (0+srcArray.length-1) / 2;\n\t\tif (key == srcArray[mid]) {\n\t\t\treturn mid;\n\t\t}\n\n\t\tif (key > srcArray[mid] && key <= srcArray[mid + 1]) {\n\t\t\treturn mid + 1;\n\t\t}\n\n\t\tint start = 0;\n\t\tint end = srcArray.length - 1;\n\t\twhile (start <= end) {\n\t\t\tmid = (end - start) / 2 + start;\n\t\t\tif (key == srcArray[mid]) {\n\t\t\t\treturn mid;\n\t\t\t}\n\t\t\tif (key > srcArray[mid] && key <= srcArray[mid + 1]) {\n\t\t\t\treturn mid + 1;\n\t\t\t}\n\t\t\tif (key < srcArray[mid]) {\n\t\t\t\tend = mid - 1;\n\t\t\t}\n\t\t\tif (key > srcArray[mid]) {\n\t\t\t\tstart = mid + 1;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tpublic static boolean is_power2(long size) {\n\t\tif (size == 0) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif ((size & (size - 1)) == 0) {\n\t\t\treturn true;\n\t\t}\n\n\t\treturn false;\n\t}\n}\n```\n### Redis不同数据结构预估\n#### 算法思路\n详见[redis源码](https://github.com/antirez/redis)\n#### 算法代码\n```\n\tprivate static final long redisObject = (long)(8 + 8);\n\t// 一个dictEntry，24字节，jemalloc会分配32字节的内存块\n\tprivate static final long dicEntry = (long)(2 * 8 + 8 + 8);\n\tprivate static final String patternString = \"^[-\\\\\\\\+]?[\\\\\\\\d]*$\";\n\n\tprivate static long skiplistMaxLevel = 32;\n\tprivate static long redisSharedInterges = 10000;\n\tprivate static long longSize = 8;\n\tprivate static long pointerSize = 8;\n\n\t/**\n\t * 一个SDS结构占据的空间为：free所占长度+len所占长度+ buf数组的长度=4+4+len+1=len+9\n\t * \n\t * @param length\n\t * @return\n\t */\n\tprivate static long sds(long length) {\n\t\tlong mem = 9 + length;\n\t\treturn mem;\n\t}\n\n\t/**\n\t * \n\t * 计算 string byte 大小\n\t * \n\t * @param kv\n\t *            https://searchdatabase.techtarget.com.cn/wp-content/uploads/res/database/article/2011/2011-11-14-16-56-18.jpg\n\t * @return\n\t */\n\tpublic static long CalculateString(KeyStringValueString kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = dicEntry + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject + SizeofString(kv.getValueAsString());\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateLinkedList(KeyStringValueList kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsStringList().size();\n\t\tmem = mem + LinkedListEntryOverhead() * length;\n\t\tmem = mem + LinkedlistOverhead();\n\t\tmem = mem + redisObject * length;\n\t\tfor (String value : kv.getValueAsStringList()) {\n\t\t\tmem = mem + SizeofString(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\t\n\tpublic static long CalculateZipList(KeyStringValueList kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + dicEntry;\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tlong length = kv.getValueAsStringList().size();\n\t\tmem = mem + ZiplistOverhead(length);\n        \n\t\tfor (String value : kv.getValueAsStringList()) {\n\t\t\tmem = mem + ZiplistAlignedStringOverhead(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateHash(KeyStringValueHash kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsHash().size();\n\t\tmem = mem + HashtableOverhead(length);\n\n\t\tfor (String key : kv.getValueAsHash().keySet()) {\n\t\t\tString value = kv.getValueAsHash().get(key);\n\t\t\tmem = mem + SizeofString(key);\n\t\t\tmem = mem + SizeofString(value);\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + HashtableEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateSet(KeyStringValueSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + HashtableOverhead(length);\n\t\tmem = mem + redisObject * length;\n\n\t\tfor (String value : kv.getValueAsSet()) {\n\t\t\tmem = mem + SizeofString(value);\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + HashtableEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\t\n\tpublic static long CalculateIntSet(KeyStringValueSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\t\tmem = mem + dicEntry;\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\t\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + IntsetOverhead(length);\n\n\t\tfor (String value : kv.getValueAsSet()) {\n\t\t\tmem = mem + ZiplistAlignedStringOverhead(value);\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\tpublic static long CalculateZSet(KeyStringValueZSet kv) {\n\t\tlong mem = KeyExpiryOverhead(kv);\n\n\t\tmem = mem + SizeofString(kv.getRawKey());\n\t\tmem = mem + redisObject;\n\t\tmem = mem + dicEntry;\n\t\tlong length = kv.getValueAsSet().size();\n\t\tmem = mem + SkiplistOverhead(length);\n\t\tmem = mem + redisObject * length;\n\n\t\tfor (ZSetEntry value : kv.getValueAsZSet()) {\n\t\t\tmem = mem + 8;\n\t\t\tmem = mem + SizeofString(value.getElement());\n\t\t\t// TODO 还有个 score\n\t\t\tmem = mem + 2 * redisObject;\n\t\t\tmem = mem + SkiplistEntryOverhead();\n\t\t}\n\n\t\treturn mem;\n\t}\n\n\t// TopLevelObjOverhead get memory use of a top level object\n\t// Each top level object is an entry in a dictionary, and so we have to include\n\t// the overhead of a dictionary entry\n\tpublic static long TopLevelObjOverhead() {\n\t\treturn HashtableEntryOverhead();\n\t}\n\n\t/**\n\t * SizeofString get memory use of a string\n\t * https://github.com/antirez/redis/blob/unstable/src/sds.h\n\t * \n\t * @param bytes\n\t * @return\n\t */\n\tpublic static long SizeofString(byte[] bytes) {\n\t\tString value = new String(bytes);\n\n\t\tif (isInteger(value)) {\n\t\t\ttry {\n\t\t\t\tLong num = Long.parseLong(value);\n\t\t\t\tif (num < redisSharedInterges && num > 0) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\treturn 8;\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t}\n\t\t}\n\n\t\treturn Jemalloc.assign(sds(bytes.length));\n\t}\n\t\n\t\n\tpublic static long SizeofString(String value) {\n\t\tif (isInteger(value)) {\n\t\t\ttry {\n\t\t\t\tLong num = Long.parseLong(value);\n\t\t\t\tif (num < redisSharedInterges && num > 0) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\treturn 8;\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t}\n\t\t}\n\n\t\treturn Jemalloc.assign(sds(value.length()));\n\t}\n\t\n\tpublic static long DictOverhead(long size) {\n\t\t return Jemalloc.assign(56 + 2*pointerSize + nextPower(size) * 3*8);\n\t}\n\n\tpublic static boolean isInteger(String str) {\n\t\tPattern pattern = Pattern.compile(patternString);\n\t\treturn pattern.matcher(str).matches();\n\t}\n\n\t/**\n\t * 过期时间也是存储为一个 dictEntry，时间戳为 int64；\n\t * \n\t * @param kv\n\t * @return\n\t */\n\t// KeyExpiryOverhead get memory useage of a key expiry\n\t// Key expiry is stored in a hashtable, so we have to pay for the cost of a\n\t// hashtable entry\n\t// The timestamp itself is stored as an int64, which is a 8 bytes\n\t@SuppressWarnings(\"rawtypes\")\n\tpublic static long KeyExpiryOverhead(KeyValuePair kv) {\n\t\t// If there is no expiry, there isn't any overhead\n\t\tif (kv.getExpiredType() == ExpiredType.NONE) {\n\t\t\treturn 0;\n\t\t}\n\t\treturn HashtableEntryOverhead() + 8;\n\t}\n\n\tpublic static long HashtableOverhead(long size) {\n\t\treturn 4 + 7 * longSize + 4 * pointerSize + nextPower(size) * pointerSize * 3 / 2;\n\t}\n\n\t// HashtableEntryOverhead get memory use of hashtable entry\n\t// See https://github.com/antirez/redis/blob/unstable/src/dict.h\n\t// Each dictEntry has 2 pointers + int64\n\tpublic static long HashtableEntryOverhead() {\n\t\treturn 2 * pointerSize + 8;\n\t}\n\n\tpublic static long ZiplistOverhead(long size) {\n\t\treturn Jemalloc.assign(12 + 21 * size);\n\t}\n\n\tpublic static long ZiplistAlignedStringOverhead(String value) {\n\t\ttry {\n\t\t\tLong.parseLong(value);\n\t\t\treturn 8;\n\t\t} catch (NumberFormatException e) {\n\t\t}\n\t\treturn Jemalloc.assign(value.length());\n\t}\n\n\t// LinkedlistOverhead get memory use of a linked list\n\t// See https://github.com/antirez/redis/blob/unstable/src/adlist.h\n\t// A list has 5 pointers + an unsigned long\n\tpublic static long LinkedlistOverhead() {\n\t\treturn longSize + 5 * pointerSize;\n\t}\n\n\t// LinkedListEntryOverhead get memory use of a linked list entry\n\t// See https://github.com/antirez/redis/blob/unstable/src/adlist.h\n\t// A node has 3 pointers\n\tpublic static long LinkedListEntryOverhead() {\n\t\treturn 3 * pointerSize;\n\t}\n\n\t// SkiplistOverhead get memory use of a skiplist\n\tpublic static long SkiplistOverhead(long size) {\n\t\treturn 2 * pointerSize + HashtableOverhead(size) + (2 * pointerSize + 16);\n\t}\n\n\t// SkiplistEntryOverhead get memory use of a skiplist entry\n\tpublic static long SkiplistEntryOverhead() {\n\t\treturn HashtableEntryOverhead() + 2 * pointerSize + 8 + (pointerSize + 8) * zsetRandLevel();\n\t}\n\n\tpublic static long nextPower(long size) {\n\t\tlong power = 1;\n\t\twhile (power <= size) {\n\t\t\tpower = power << 1;\n\t\t}\n\t\treturn power;\n\t}\n\n\tpublic static long zsetRandLevel() {\n\t\tlong level = 1;\n\t\tint rint = new Random().nextInt(65536);\n\t\tint flag = 65535 / 4;\n\t\twhile (rint < flag) {// skiplistP\n\t\t\tlevel++;\n\t\t\trint = new Random().nextInt(65536);\n\t\t}\n\t\tif (level < skiplistMaxLevel) {\n\t\t\treturn level;\n\t\t}\n\t\treturn skiplistMaxLevel;\n\t}\n\t\n\tpublic static long  IntsetOverhead(long size) {\n\t    //     typedef struct intset {\n\t    //     uint32_t encoding;\n\t    //     uint32_t length;\n\t    //     int8_t contents[];\n\t    //      } intset;\n\t    return (4 + 4) * size;\n\t}\n```\n## RCT分析redis rdb\nRCT是一个一站式redis内存分析分析平台，分析任务是分布式的，需要将RCT-Analyze部署到rdb所在机器上，RCT-Dashboard部署在任何机器，只要能保持和RCT-Analyze通信即可。不同于以上列举的工具，我们最初定位RCT就是一个可以长期运行，尽可能每天分析redis中数据，为redis运维人员提供运维依据，便于做出更好的规范，高效的使用redis。\n### 部署\n部署过程略，详见[官方文档](https://github.com/xaecbd/RCT/blob/master/README_zh.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B)，推荐使用docker方式，这样也是我们目前采用的方式。\n### 配置分析任务\n#### 新增RDB分析配置\n只有先创建了redis节点之后，才能进入到RCT工具导航页面。\n1.  点击导航RDB Analyze\n2.  如若一直没有添加过RDB信息，则可在页面弹出的框中进行完善信息，或者点击下方的Add按钮进行添加\n3.  点击edit则可以对RDB 信息进行修改\n4.  如若已经完善了RDB信息，则点击打开switch开关，则是对RDB直接进行定时任务的开启和关闭 \n5.  点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态 \n\n#### RDB Add页面参数说明\n1.  **Automatic Analyze**：是否开启定时任务\n2.  **Schedule**：cron 表达式（填写完成之后，可以点击右侧的图标进行查看定时表达式执行的时间）\n3.  **Analyzer**:分析器 （依次是生成报表，根据filter导key到elasticsearch中，根据preix导key到elasticsearch中）\n4.  **Data Path**:rdb文件的目录（eg:/opt/app/redis/9002/dump.rdb,data path应为/opt/app/redis）\n5.  **Prefixes**:key的前缀，可为空，但是在选择了分析器中的根据preix导key，则必须填写prefixes。我们强烈建议将已知前缀填入，可以提高分析效率，节省时间。\n6.  **Report**：是否生成报表\n7.  **Mail**：生成报表之后的收件人，平台将会将报表做为附件发送给收件人，如果收件人有多个，请用;隔开\n\n#### RDB 主页面参数说明\nRDB主页面参数与add页面参数基本相同，在此不再做赘述，唯一有区别的是：\nStatus：分析RDB文件的进度状态，分为成功，正在分析，失败三种状态，status为正在分析状态时，可通过点击后面的链接进入到实时分析页面，查看实时状态。\n\n#### 分析器介绍\n1. 生成报表：对rdb文件的数据进行分析并将结果写入数据库中，如果配置了Report，结果会以excel报表的形式发送邮件给用户\n2. 根据filter导key：在对rdb文件分析的时候，根据过滤器导出相应的key，将数据写入到EleasticSearch中\n3. 根据prefix导key：在对rdb文件分析的时候，根据制定的前缀key导出相应的key，将数据写入到EleasticSearch中\n\n### 手动分析\n点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态\n### 查看报表\n目前RCT支持dashboard/email两种方式，这里我仅展示dashboard.\n![image](https://github.com/xaecbd/RCT/raw/master/doc/screenshots/rct.jpg?raw=true)\n\n## 项目地址\n[https://github.com/xaecbd/RCT](https://github.com/xaecbd/RCT),欢迎使用，欢迎加入我们，帮我们提高RCT。如果你有问题，可以前往github新建issue。\n## 参考\n1. [Redis持久化文件RDB的格式解析](http://www.innereye.cn/2017/01/17/redis/Redis_RDB_File_Format_20170117/)\n2. [redis-rdb-tools](https://github.com/sripathikrishnan/redis-rdb-tools)\n3. [rdr](https://github.com/xueqiu/rdr)\n4. [analysis-redis](https://infoq.cn/article/analysis-redis)\n5. [Redis内存模型](http://www.cnblogs.com/kismetv/p/8654978.html)","slug":"如何优雅的分析redis中的内存数据","published":1,"updated":"2019-03-13T12:40:11.950Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvhm00b88cee7rqvkh0q","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>目前我们EC Bigdata team 运维公司 4个 Redis 集群，300+ Redis 实例，500G+ 的内存数据，我们想要分析业务是否有误用，以提高资源利用率。伴随着业务team的广泛使用，近期数据增                长比较快，我们紧迫需要一个工具分析一下各种业务存储的数据有多大，是否存入僵死数据浪费资源；同时E4 WWW redis 集群有业务方反馈近期有比较明显的慢查询发生，所以我们需要针对 slow log 和存入的常用数据类型Hash,List,Set分析，是否有big key引起慢查询,是否有team存在超大的 big key和不合理设置ttl的情况</p>\n<p>那有没有什么办法让我们安全高效的看到 Redis 内存消耗的详细报表呢？办法总比问题多，有需求就有解决方案。EC Bigdata team针对这个问题实现了一个 Redis 内存数据可视化分析平台 RCT (Redis Computed Tomography)。</p>\n<p>RCT可以非常方便的对 Reids 的内存进行分析，了解一个 Redis 实例里都有哪些 key，哪类 key 占用的空间是多少，最耗内存的 key 有哪些，占比如何，非常直观,除此之外，我们还可以针对Redis slowlog/clientlist进行分钟级别监控，直观监控集群效应状况。</p>\n<h2 id=\"同类产品\"><a href=\"#同类产品\" class=\"headerlink\" title=\"同类产品\"></a>同类产品</h2><ol>\n<li><a href=\"https://github.com/sripathikrishnan/redis-rdb-tools\" target=\"_blank\" rel=\"noopener\">redis-rdb-tools</a></li>\n<li><a href=\"https://github.com/xueqiu/rdr\" target=\"_blank\" rel=\"noopener\">rdr</a></li>\n<li><a href=\"https://github.com/leonchen83/redis-rdb-cli\" target=\"_blank\" rel=\"noopener\">redis-rdb-cli</a></li>\n</ol>\n<p>市面上已经存在这么多开源的产品，我们为什么还要重新做一个呢？主要还是没有满足我们的需求，以上的都是redis rdb解析工具，最接近我们需求的就是雪球开源的rdr,但是也只限于离线分析，而我们节点众多，不可能一个一个去线上机器copy,或者开发copy工具，这样也会给相应机器带来网络热点（总有办法解决这个问题），带来很多繁重无意义的劳动，么不是专门的运维工程师，还有更多的开发任务等着我们去做。</p>\n<p>我们想要一个每天都会给我们产生报表，自动推送到我们的邮箱，或者在网页上就能看到最近redis内存的变化，它不仅仅是一个工具，还是常态化运行的服务，基于此我们打造了属于最近的redis内存分析平台RCT。</p>\n<h2 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h2><h3 id=\"设计思路\"><a href=\"#设计思路\" class=\"headerlink\" title=\"设计思路\"></a>设计思路</h3><p>使用 bgsave，获取 rdb 文件，解析后获取数据。</p>\n<p>优点：机制成熟，可靠性好；文件相对小，传输、解析效率高；</p>\n<p>缺点：bgsave 虽然会 fork 子进程，但还是有可能导致主进程卡住一段时间，对业务有产生影响的风险；</p>\n<p>采用<strong>低峰期在从节点做 bgsave 获取 rdb 文件</strong>，相对安全可靠。拿到了 rdb 文件就相当于拿到了 Redis 实例的所有数据，接下来就是生成报表的过程了：</p>\n<p>解析 rdb 文件，获取到 Key 和 Value 的内容；根据相对应的数据结构及内容，估算内存消耗等;统计并生成报表；逻辑很简单，所以设计思路很清晰。</p>\n<h3 id=\"数据流图\"><a href=\"#数据流图\" class=\"headerlink\" title=\"数据流图\"></a>数据流图</h3><p><img src=\"https://github.com/xaecbd/RCT/blob/master/doc/screenshots/%E6%95%B0%E6%8D%AE%E6%B5%81.png?raw=true\" alt=\"\"></p>\n<h3 id=\"slave节点均分计算\"><a href=\"#slave节点均分计算\" class=\"headerlink\" title=\"slave节点均分计算\"></a>slave节点均分计算</h3><p>为了使对线上机器不产生影响，我们选择是在slave节点进行rdb文件分析，该任务是分布式的。为了均衡对每个机器的影响，通过算法去保证slave分配算法均匀的落在不同的机器上。</p>\n<h4 id=\"算法思路\"><a href=\"#算法思路\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><ol>\n<li>slave数量最小的优先分配</li>\n<li>通过map存储不同IP分配的数量，按照规则，优先分配数量最小的IP</li>\n<li>staticsResult里面不存在的IP优先分配</li>\n<li>如果上面未分配,则选择staticsResult中数值最小的那个slave<h4 id=\"算法代码实现\"><a href=\"#算法代码实现\" class=\"headerlink\" title=\"算法代码实现\"></a>算法代码实现</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">\t * 根据&lt;master:slaves&gt;获取执行分析任务ports规则</span><br><span class=\"line\">\t * 即获取其中一个slave,尽量保持均衡在不同机器上</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param clusterNodesMap</span><br><span class=\"line\">\t * @return &lt;ip:ports&gt;</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static Map&lt;String, Set&lt;String&gt;&gt; generateAnalyzeRule(Map&lt;String, List&lt;String&gt;&gt; clusterNodesMap) &#123;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t// 通过该map存储不同IP分配的数量，按照规则，优先分配数量最小的IP</span><br><span class=\"line\">\t\tMap&lt;String, Integer&gt; staticsResult = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;String&gt;&gt; generateRule = new HashMap&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 此处排序是为了将slave数量最小的优先分配</span><br><span class=\"line\">\t\tList&lt;Map.Entry&lt;String, List&lt;String&gt;&gt;&gt; sortList = new LinkedList&lt;&gt;(clusterNodesMap.entrySet());</span><br><span class=\"line\">\t\tCollections.sort(sortList, new Comparator&lt;Entry&lt;String, List&lt;String&gt;&gt;&gt;() &#123;</span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic int compare(Entry&lt;String, List&lt;String&gt;&gt; o1, Entry&lt;String, List&lt;String&gt;&gt; o2) &#123;</span><br><span class=\"line\">\t\t\t\treturn o1.getValue().size() - o2.getValue().size();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor (Entry&lt;String, List&lt;String&gt;&gt; entry : sortList) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; slaves = entry.getValue();</span><br><span class=\"line\">\t\t\tboolean isSelected = false;</span><br><span class=\"line\">\t\t\tString tempPort = null;</span><br><span class=\"line\">\t\t\tString tempIP = null;</span><br><span class=\"line\">\t\t\tint num = 0;</span><br><span class=\"line\">\t\t\tfor (String slave : slaves) &#123;</span><br><span class=\"line\">\t\t\t\tString ip = slave.split(&quot;:&quot;)[0];</span><br><span class=\"line\">\t\t\t\tString port = slave.split(&quot;:&quot;)[1];</span><br><span class=\"line\">\t\t\t\t// 统计组里面不存在的IP优先分配</span><br><span class=\"line\">\t\t\t\tif (!staticsResult.containsKey(ip)) &#123;</span><br><span class=\"line\">\t\t\t\t\tstaticsResult.put(ip, 1);</span><br><span class=\"line\">\t\t\t\t\tSet&lt;String&gt; generatePorts = generateRule.get(ip);</span><br><span class=\"line\">\t\t\t\t\tif (generatePorts == null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tgeneratePorts = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tgeneratePorts.add(port);</span><br><span class=\"line\">\t\t\t\t\tgenerateRule.put(ip, generatePorts);</span><br><span class=\"line\">\t\t\t\t\tisSelected = true;</span><br><span class=\"line\">\t\t\t\t\tbreak;</span><br><span class=\"line\">\t\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\t\t// 此处是为了求出被使用最少的IP</span><br><span class=\"line\">\t\t\t\t\tInteger staticsNum = staticsResult.get(ip);</span><br><span class=\"line\">\t\t\t\t\tif (num == 0) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tnum = staticsNum;</span><br><span class=\"line\">\t\t\t\t\t\ttempPort = port;</span><br><span class=\"line\">\t\t\t\t\t\ttempIP = ip;</span><br><span class=\"line\">\t\t\t\t\t\tcontinue;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tif (staticsNum &lt; num) &#123;</span><br><span class=\"line\">\t\t\t\t\t\ttempPort = port;</span><br><span class=\"line\">\t\t\t\t\t\ttempIP = ip;</span><br><span class=\"line\">\t\t\t\t\t\tnum = staticsNum;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t// 如果上面未分配,则选择staticsResult中数值最小的那个slave</span><br><span class=\"line\">\t\t\tif (!isSelected) &#123;</span><br><span class=\"line\">\t\t\t\tif (slaves != null &amp;&amp; slaves.size() &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\t\tif (tempPort != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tSet&lt;String&gt; generatePorts = generateRule.get(tempIP);</span><br><span class=\"line\">\t\t\t\t\t\tif (generatePorts == null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tgeneratePorts = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tgeneratePorts.add(tempPort);</span><br><span class=\"line\">\t\t\t\t\t\tgenerateRule.put(tempIP, generatePorts);</span><br><span class=\"line\">\t\t\t\t\t\tstaticsResult.put(tempIP, staticsResult.get(tempIP) + 1);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn generateRule;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"Jemalloc内存预分配\"><a href=\"#Jemalloc内存预分配\" class=\"headerlink\" title=\"Jemalloc内存预分配\"></a>Jemalloc内存预分配</h3><h4 id=\"算法思路-1\"><a href=\"#算法思路-1\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><p>redis中支持多种内存分配算法，推荐Jemalloc，因此我们选择该算法来预估内存大小。因为这个算法比较复杂，我们参考雪球做法使用约定数组，根据不同数据大小分配不同内存空间，为了提高查找效率，这块采用了变种二分查找。</p>\n<h4 id=\"算法代码\"><a href=\"#算法代码\" class=\"headerlink\" title=\"算法代码\"></a>算法代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class Jemalloc &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array16;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array192;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array768;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array4096;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array4194304;</span><br><span class=\"line\"></span><br><span class=\"line\">\tstatic &#123;</span><br><span class=\"line\">\t\tarray16 = range(16, 128 + 1, 16);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray192 = range(192, 512 + 1, 64);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray768 = range(768, 4096 + 1, 256);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray4096 = range(4096, 4194304 + 1, 4096);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray4194304 = range(4194304, 536870912 + 1, 4194304);</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据Jemalloc 估算分配内存大小 Small: All 2^n-aligned allocations of size 2^n will incur</span><br><span class=\"line\">\t * no additional overhead, due to how small allocations are aligned and packed.</span><br><span class=\"line\">\t * Small: [8], [16, 32, 48, ..., 128], [192, 256, 320, ..., 512], [768, 1024,</span><br><span class=\"line\">\t * 1280, ..., 3840] Large: The worst case size is half the chunk size, in which</span><br><span class=\"line\">\t * case only one allocation per chunk can be allocated. If the remaining</span><br><span class=\"line\">\t * (nearly) half of the chunk isn&apos;t otherwise useful for smaller allocations,</span><br><span class=\"line\">\t * the overhead will essentially be 50%. However, assuming you use a diverse</span><br><span class=\"line\">\t * mixture of size classes, the actual overhead shouldn&apos;t be a significant issue</span><br><span class=\"line\">\t * in practice. Large: [4 KiB, 8 KiB, 12 KiB, ..., 4072 KiB] Huge: Extra virtual</span><br><span class=\"line\">\t * memory is mapped, then the excess is trimmed and unmapped. This can leave</span><br><span class=\"line\">\t * virtual memory holes, but it incurs no physical memory overhead. Earlier</span><br><span class=\"line\">\t * versions of jemalloc heuristically attempted to optimistically map chunks</span><br><span class=\"line\">\t * without excess that would need to be trimmed, but it didn&apos;t save much system</span><br><span class=\"line\">\t * call overhead in practice. Huge: [4 MiB, 8 MiB, 12 MiB, ..., 512 MiB]</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param size</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static long assign(long size) &#123;</span><br><span class=\"line\">\t\tif (size &lt;= 4096) &#123;</span><br><span class=\"line\">\t\t\t// Small</span><br><span class=\"line\">\t\t\tif (is_power2(size)) &#123;</span><br><span class=\"line\">\t\t\t\treturn size;</span><br><span class=\"line\">\t\t\t&#125; else if (size &lt; 128) &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array16, size);</span><br><span class=\"line\">\t\t\t&#125; else if (size &lt; 512) &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array192, size);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array768, size);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else if (size &lt; 4194304) &#123;</span><br><span class=\"line\">\t\t\t// Large</span><br><span class=\"line\">\t\t\treturn min_ge(array4096, size);</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t// Huge</span><br><span class=\"line\">\t\t\treturn min_ge(array4194304, size);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 创建一个long数组</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param start</span><br><span class=\"line\">\t * @param stop</span><br><span class=\"line\">\t * @param step</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static long[] range(int start, int stop, int step) &#123;</span><br><span class=\"line\">\t\tint size = (stop - 1 - start) / step + 1;</span><br><span class=\"line\">\t\tlong[] array = new long[size];</span><br><span class=\"line\">\t\tint index = 0;</span><br><span class=\"line\">\t\tfor (int i = start; i &lt; stop; i = i + step) &#123;</span><br><span class=\"line\">\t\t\tarray[index] = i;</span><br><span class=\"line\">\t\t\tindex++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn array;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static long min_ge(long[] srcArray, long key) &#123;</span><br><span class=\"line\">\t\tint index = binarySearch(srcArray, key);</span><br><span class=\"line\">\t\treturn srcArray[index];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 二分查找最小值，即最接近要查找的值，但是要大于该值</span><br><span class=\"line\">\tpublic static int binarySearch(long srcArray[], long key) &#123;</span><br><span class=\"line\">\t\tint mid = (0+srcArray.length-1) / 2;</span><br><span class=\"line\">\t\tif (key == srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\treturn mid;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif (key &gt; srcArray[mid] &amp;&amp; key &lt;= srcArray[mid + 1]) &#123;</span><br><span class=\"line\">\t\t\treturn mid + 1;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tint start = 0;</span><br><span class=\"line\">\t\tint end = srcArray.length - 1;</span><br><span class=\"line\">\t\twhile (start &lt;= end) &#123;</span><br><span class=\"line\">\t\t\tmid = (end - start) / 2 + start;</span><br><span class=\"line\">\t\t\tif (key == srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\treturn mid;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &gt; srcArray[mid] &amp;&amp; key &lt;= srcArray[mid + 1]) &#123;</span><br><span class=\"line\">\t\t\t\treturn mid + 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &lt; srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\tend = mid - 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &gt; srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\tstart = mid + 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn 0;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static boolean is_power2(long size) &#123;</span><br><span class=\"line\">\t\tif (size == 0) &#123;</span><br><span class=\"line\">\t\t\treturn false;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif ((size &amp; (size - 1)) == 0) &#123;</span><br><span class=\"line\">\t\t\treturn true;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn false;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Redis不同数据结构预估\"><a href=\"#Redis不同数据结构预估\" class=\"headerlink\" title=\"Redis不同数据结构预估\"></a>Redis不同数据结构预估</h3><h4 id=\"算法思路-2\"><a href=\"#算法思路-2\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><p>详见<a href=\"https://github.com/antirez/redis\" target=\"_blank\" rel=\"noopener\">redis源码</a></p>\n<h4 id=\"算法代码-1\"><a href=\"#算法代码-1\" class=\"headerlink\" title=\"算法代码\"></a>算法代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static final long redisObject = (long)(8 + 8);</span><br><span class=\"line\">// 一个dictEntry，24字节，jemalloc会分配32字节的内存块</span><br><span class=\"line\">private static final long dicEntry = (long)(2 * 8 + 8 + 8);</span><br><span class=\"line\">private static final String patternString = &quot;^[-\\\\\\\\+]?[\\\\\\\\d]*$&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">private static long skiplistMaxLevel = 32;</span><br><span class=\"line\">private static long redisSharedInterges = 10000;</span><br><span class=\"line\">private static long longSize = 8;</span><br><span class=\"line\">private static long pointerSize = 8;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 一个SDS结构占据的空间为：free所占长度+len所占长度+ buf数组的长度=4+4+len+1=len+9</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param length</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">private static long sds(long length) &#123;</span><br><span class=\"line\">\tlong mem = 9 + length;</span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * 计算 string byte 大小</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param kv</span><br><span class=\"line\"> *            https://searchdatabase.techtarget.com.cn/wp-content/uploads/res/database/article/2011/2011-11-14-16-56-18.jpg</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static long CalculateString(KeyStringValueString kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = dicEntry + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject + SizeofString(kv.getValueAsString());</span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateLinkedList(KeyStringValueList kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsStringList().size();</span><br><span class=\"line\">\tmem = mem + LinkedListEntryOverhead() * length;</span><br><span class=\"line\">\tmem = mem + LinkedlistOverhead();</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\">\tfor (String value : kv.getValueAsStringList()) &#123;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateZipList(KeyStringValueList kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tlong length = kv.getValueAsStringList().size();</span><br><span class=\"line\">\tmem = mem + ZiplistOverhead(length);</span><br><span class=\"line\">       </span><br><span class=\"line\">\tfor (String value : kv.getValueAsStringList()) &#123;</span><br><span class=\"line\">\t\tmem = mem + ZiplistAlignedStringOverhead(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateHash(KeyStringValueHash kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsHash().size();</span><br><span class=\"line\">\tmem = mem + HashtableOverhead(length);</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String key : kv.getValueAsHash().keySet()) &#123;</span><br><span class=\"line\">\t\tString value = kv.getValueAsHash().get(key);</span><br><span class=\"line\">\t\tmem = mem + SizeofString(key);</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + HashtableEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateSet(KeyStringValueSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\"></span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + HashtableOverhead(length);</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String value : kv.getValueAsSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + HashtableEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateIntSet(KeyStringValueSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + IntsetOverhead(length);</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String value : kv.getValueAsSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + ZiplistAlignedStringOverhead(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateZSet(KeyStringValueZSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\"></span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + SkiplistOverhead(length);</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (ZSetEntry value : kv.getValueAsZSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + 8;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value.getElement());</span><br><span class=\"line\">\t\t// TODO 还有个 score</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + SkiplistEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// TopLevelObjOverhead get memory use of a top level object</span><br><span class=\"line\">// Each top level object is an entry in a dictionary, and so we have to include</span><br><span class=\"line\">// the overhead of a dictionary entry</span><br><span class=\"line\">public static long TopLevelObjOverhead() &#123;</span><br><span class=\"line\">\treturn HashtableEntryOverhead();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * SizeofString get memory use of a string</span><br><span class=\"line\"> * https://github.com/antirez/redis/blob/unstable/src/sds.h</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param bytes</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static long SizeofString(byte[] bytes) &#123;</span><br><span class=\"line\">\tString value = new String(bytes);</span><br><span class=\"line\"></span><br><span class=\"line\">\tif (isInteger(value)) &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tLong num = Long.parseLong(value);</span><br><span class=\"line\">\t\t\tif (num &lt; redisSharedInterges &amp;&amp; num &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\treturn 0;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn 8;</span><br><span class=\"line\">\t\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn Jemalloc.assign(sds(bytes.length));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">public static long SizeofString(String value) &#123;</span><br><span class=\"line\">\tif (isInteger(value)) &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tLong num = Long.parseLong(value);</span><br><span class=\"line\">\t\t\tif (num &lt; redisSharedInterges &amp;&amp; num &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\treturn 0;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn 8;</span><br><span class=\"line\">\t\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn Jemalloc.assign(sds(value.length()));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long DictOverhead(long size) &#123;</span><br><span class=\"line\">\t return Jemalloc.assign(56 + 2*pointerSize + nextPower(size) * 3*8);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static boolean isInteger(String str) &#123;</span><br><span class=\"line\">\tPattern pattern = Pattern.compile(patternString);</span><br><span class=\"line\">\treturn pattern.matcher(str).matches();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 过期时间也是存储为一个 dictEntry，时间戳为 int64；</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param kv</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">// KeyExpiryOverhead get memory useage of a key expiry</span><br><span class=\"line\">// Key expiry is stored in a hashtable, so we have to pay for the cost of a</span><br><span class=\"line\">// hashtable entry</span><br><span class=\"line\">// The timestamp itself is stored as an int64, which is a 8 bytes</span><br><span class=\"line\">@SuppressWarnings(&quot;rawtypes&quot;)</span><br><span class=\"line\">public static long KeyExpiryOverhead(KeyValuePair kv) &#123;</span><br><span class=\"line\">\t// If there is no expiry, there isn&apos;t any overhead</span><br><span class=\"line\">\tif (kv.getExpiredType() == ExpiredType.NONE) &#123;</span><br><span class=\"line\">\t\treturn 0;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn HashtableEntryOverhead() + 8;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long HashtableOverhead(long size) &#123;</span><br><span class=\"line\">\treturn 4 + 7 * longSize + 4 * pointerSize + nextPower(size) * pointerSize * 3 / 2;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// HashtableEntryOverhead get memory use of hashtable entry</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/dict.h</span><br><span class=\"line\">// Each dictEntry has 2 pointers + int64</span><br><span class=\"line\">public static long HashtableEntryOverhead() &#123;</span><br><span class=\"line\">\treturn 2 * pointerSize + 8;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long ZiplistOverhead(long size) &#123;</span><br><span class=\"line\">\treturn Jemalloc.assign(12 + 21 * size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long ZiplistAlignedStringOverhead(String value) &#123;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tLong.parseLong(value);</span><br><span class=\"line\">\t\treturn 8;</span><br><span class=\"line\">\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn Jemalloc.assign(value.length());</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// LinkedlistOverhead get memory use of a linked list</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/adlist.h</span><br><span class=\"line\">// A list has 5 pointers + an unsigned long</span><br><span class=\"line\">public static long LinkedlistOverhead() &#123;</span><br><span class=\"line\">\treturn longSize + 5 * pointerSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// LinkedListEntryOverhead get memory use of a linked list entry</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/adlist.h</span><br><span class=\"line\">// A node has 3 pointers</span><br><span class=\"line\">public static long LinkedListEntryOverhead() &#123;</span><br><span class=\"line\">\treturn 3 * pointerSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// SkiplistOverhead get memory use of a skiplist</span><br><span class=\"line\">public static long SkiplistOverhead(long size) &#123;</span><br><span class=\"line\">\treturn 2 * pointerSize + HashtableOverhead(size) + (2 * pointerSize + 16);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// SkiplistEntryOverhead get memory use of a skiplist entry</span><br><span class=\"line\">public static long SkiplistEntryOverhead() &#123;</span><br><span class=\"line\">\treturn HashtableEntryOverhead() + 2 * pointerSize + 8 + (pointerSize + 8) * zsetRandLevel();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long nextPower(long size) &#123;</span><br><span class=\"line\">\tlong power = 1;</span><br><span class=\"line\">\twhile (power &lt;= size) &#123;</span><br><span class=\"line\">\t\tpower = power &lt;&lt; 1;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn power;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long zsetRandLevel() &#123;</span><br><span class=\"line\">\tlong level = 1;</span><br><span class=\"line\">\tint rint = new Random().nextInt(65536);</span><br><span class=\"line\">\tint flag = 65535 / 4;</span><br><span class=\"line\">\twhile (rint &lt; flag) &#123;// skiplistP</span><br><span class=\"line\">\t\tlevel++;</span><br><span class=\"line\">\t\trint = new Random().nextInt(65536);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif (level &lt; skiplistMaxLevel) &#123;</span><br><span class=\"line\">\t\treturn level;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn skiplistMaxLevel;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long  IntsetOverhead(long size) &#123;</span><br><span class=\"line\">    //     typedef struct intset &#123;</span><br><span class=\"line\">    //     uint32_t encoding;</span><br><span class=\"line\">    //     uint32_t length;</span><br><span class=\"line\">    //     int8_t contents[];</span><br><span class=\"line\">    //      &#125; intset;</span><br><span class=\"line\">    return (4 + 4) * size;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"RCT分析redis-rdb\"><a href=\"#RCT分析redis-rdb\" class=\"headerlink\" title=\"RCT分析redis rdb\"></a>RCT分析redis rdb</h2><p>RCT是一个一站式redis内存分析分析平台，分析任务是分布式的，需要将RCT-Analyze部署到rdb所在机器上，RCT-Dashboard部署在任何机器，只要能保持和RCT-Analyze通信即可。不同于以上列举的工具，我们最初定位RCT就是一个可以长期运行，尽可能每天分析redis中数据，为redis运维人员提供运维依据，便于做出更好的规范，高效的使用redis。</p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><p>部署过程略，详见<a href=\"https://github.com/xaecbd/RCT/blob/master/README_zh.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B\" target=\"_blank\" rel=\"noopener\">官方文档</a>，推荐使用docker方式，这样也是我们目前采用的方式。</p>\n<h3 id=\"配置分析任务\"><a href=\"#配置分析任务\" class=\"headerlink\" title=\"配置分析任务\"></a>配置分析任务</h3><h4 id=\"新增RDB分析配置\"><a href=\"#新增RDB分析配置\" class=\"headerlink\" title=\"新增RDB分析配置\"></a>新增RDB分析配置</h4><p>只有先创建了redis节点之后，才能进入到RCT工具导航页面。</p>\n<ol>\n<li>点击导航RDB Analyze</li>\n<li>如若一直没有添加过RDB信息，则可在页面弹出的框中进行完善信息，或者点击下方的Add按钮进行添加</li>\n<li>点击edit则可以对RDB 信息进行修改</li>\n<li>如若已经完善了RDB信息，则点击打开switch开关，则是对RDB直接进行定时任务的开启和关闭 </li>\n<li>点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态 </li>\n</ol>\n<h4 id=\"RDB-Add页面参数说明\"><a href=\"#RDB-Add页面参数说明\" class=\"headerlink\" title=\"RDB Add页面参数说明\"></a>RDB Add页面参数说明</h4><ol>\n<li><strong>Automatic Analyze</strong>：是否开启定时任务</li>\n<li><strong>Schedule</strong>：cron 表达式（填写完成之后，可以点击右侧的图标进行查看定时表达式执行的时间）</li>\n<li><strong>Analyzer</strong>:分析器 （依次是生成报表，根据filter导key到elasticsearch中，根据preix导key到elasticsearch中）</li>\n<li><strong>Data Path</strong>:rdb文件的目录（eg:/opt/app/redis/9002/dump.rdb,data path应为/opt/app/redis）</li>\n<li><strong>Prefixes</strong>:key的前缀，可为空，但是在选择了分析器中的根据preix导key，则必须填写prefixes。我们强烈建议将已知前缀填入，可以提高分析效率，节省时间。</li>\n<li><strong>Report</strong>：是否生成报表</li>\n<li><strong>Mail</strong>：生成报表之后的收件人，平台将会将报表做为附件发送给收件人，如果收件人有多个，请用;隔开</li>\n</ol>\n<h4 id=\"RDB-主页面参数说明\"><a href=\"#RDB-主页面参数说明\" class=\"headerlink\" title=\"RDB 主页面参数说明\"></a>RDB 主页面参数说明</h4><p>RDB主页面参数与add页面参数基本相同，在此不再做赘述，唯一有区别的是：<br>Status：分析RDB文件的进度状态，分为成功，正在分析，失败三种状态，status为正在分析状态时，可通过点击后面的链接进入到实时分析页面，查看实时状态。</p>\n<h4 id=\"分析器介绍\"><a href=\"#分析器介绍\" class=\"headerlink\" title=\"分析器介绍\"></a>分析器介绍</h4><ol>\n<li>生成报表：对rdb文件的数据进行分析并将结果写入数据库中，如果配置了Report，结果会以excel报表的形式发送邮件给用户</li>\n<li>根据filter导key：在对rdb文件分析的时候，根据过滤器导出相应的key，将数据写入到EleasticSearch中</li>\n<li>根据prefix导key：在对rdb文件分析的时候，根据制定的前缀key导出相应的key，将数据写入到EleasticSearch中</li>\n</ol>\n<h3 id=\"手动分析\"><a href=\"#手动分析\" class=\"headerlink\" title=\"手动分析\"></a>手动分析</h3><p>点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态</p>\n<h3 id=\"查看报表\"><a href=\"#查看报表\" class=\"headerlink\" title=\"查看报表\"></a>查看报表</h3><p>目前RCT支持dashboard/email两种方式，这里我仅展示dashboard.<br><img src=\"https://github.com/xaecbd/RCT/raw/master/doc/screenshots/rct.jpg?raw=true\" alt=\"image\"></p>\n<h2 id=\"项目地址\"><a href=\"#项目地址\" class=\"headerlink\" title=\"项目地址\"></a>项目地址</h2><p><a href=\"https://github.com/xaecbd/RCT\" target=\"_blank\" rel=\"noopener\">https://github.com/xaecbd/RCT</a>,欢迎使用，欢迎加入我们，帮我们提高RCT。如果你有问题，可以前往github新建issue。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.innereye.cn/2017/01/17/redis/Redis_RDB_File_Format_20170117/\" target=\"_blank\" rel=\"noopener\">Redis持久化文件RDB的格式解析</a></li>\n<li><a href=\"https://github.com/sripathikrishnan/redis-rdb-tools\" target=\"_blank\" rel=\"noopener\">redis-rdb-tools</a></li>\n<li><a href=\"https://github.com/xueqiu/rdr\" target=\"_blank\" rel=\"noopener\">rdr</a></li>\n<li><a href=\"https://infoq.cn/article/analysis-redis\" target=\"_blank\" rel=\"noopener\">analysis-redis</a></li>\n<li><a href=\"http://www.cnblogs.com/kismetv/p/8654978.html\" target=\"_blank\" rel=\"noopener\">Redis内存模型</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>目前我们EC Bigdata team 运维公司 4个 Redis 集群，300+ Redis 实例，500G+ 的内存数据，我们想要分析业务是否有误用，以提高资源利用率。伴随着业务team的广泛使用，近期数据增                长比较快，我们紧迫需要一个工具分析一下各种业务存储的数据有多大，是否存入僵死数据浪费资源；同时E4 WWW redis 集群有业务方反馈近期有比较明显的慢查询发生，所以我们需要针对 slow log 和存入的常用数据类型Hash,List,Set分析，是否有big key引起慢查询,是否有team存在超大的 big key和不合理设置ttl的情况</p>\n<p>那有没有什么办法让我们安全高效的看到 Redis 内存消耗的详细报表呢？办法总比问题多，有需求就有解决方案。EC Bigdata team针对这个问题实现了一个 Redis 内存数据可视化分析平台 RCT (Redis Computed Tomography)。</p>\n<p>RCT可以非常方便的对 Reids 的内存进行分析，了解一个 Redis 实例里都有哪些 key，哪类 key 占用的空间是多少，最耗内存的 key 有哪些，占比如何，非常直观,除此之外，我们还可以针对Redis slowlog/clientlist进行分钟级别监控，直观监控集群效应状况。</p>\n<h2 id=\"同类产品\"><a href=\"#同类产品\" class=\"headerlink\" title=\"同类产品\"></a>同类产品</h2><ol>\n<li><a href=\"https://github.com/sripathikrishnan/redis-rdb-tools\" target=\"_blank\" rel=\"noopener\">redis-rdb-tools</a></li>\n<li><a href=\"https://github.com/xueqiu/rdr\" target=\"_blank\" rel=\"noopener\">rdr</a></li>\n<li><a href=\"https://github.com/leonchen83/redis-rdb-cli\" target=\"_blank\" rel=\"noopener\">redis-rdb-cli</a></li>\n</ol>\n<p>市面上已经存在这么多开源的产品，我们为什么还要重新做一个呢？主要还是没有满足我们的需求，以上的都是redis rdb解析工具，最接近我们需求的就是雪球开源的rdr,但是也只限于离线分析，而我们节点众多，不可能一个一个去线上机器copy,或者开发copy工具，这样也会给相应机器带来网络热点（总有办法解决这个问题），带来很多繁重无意义的劳动，么不是专门的运维工程师，还有更多的开发任务等着我们去做。</p>\n<p>我们想要一个每天都会给我们产生报表，自动推送到我们的邮箱，或者在网页上就能看到最近redis内存的变化，它不仅仅是一个工具，还是常态化运行的服务，基于此我们打造了属于最近的redis内存分析平台RCT。</p>\n<h2 id=\"理论\"><a href=\"#理论\" class=\"headerlink\" title=\"理论\"></a>理论</h2><h3 id=\"设计思路\"><a href=\"#设计思路\" class=\"headerlink\" title=\"设计思路\"></a>设计思路</h3><p>使用 bgsave，获取 rdb 文件，解析后获取数据。</p>\n<p>优点：机制成熟，可靠性好；文件相对小，传输、解析效率高；</p>\n<p>缺点：bgsave 虽然会 fork 子进程，但还是有可能导致主进程卡住一段时间，对业务有产生影响的风险；</p>\n<p>采用<strong>低峰期在从节点做 bgsave 获取 rdb 文件</strong>，相对安全可靠。拿到了 rdb 文件就相当于拿到了 Redis 实例的所有数据，接下来就是生成报表的过程了：</p>\n<p>解析 rdb 文件，获取到 Key 和 Value 的内容；根据相对应的数据结构及内容，估算内存消耗等;统计并生成报表；逻辑很简单，所以设计思路很清晰。</p>\n<h3 id=\"数据流图\"><a href=\"#数据流图\" class=\"headerlink\" title=\"数据流图\"></a>数据流图</h3><p><img src=\"https://github.com/xaecbd/RCT/blob/master/doc/screenshots/%E6%95%B0%E6%8D%AE%E6%B5%81.png?raw=true\" alt=\"\"></p>\n<h3 id=\"slave节点均分计算\"><a href=\"#slave节点均分计算\" class=\"headerlink\" title=\"slave节点均分计算\"></a>slave节点均分计算</h3><p>为了使对线上机器不产生影响，我们选择是在slave节点进行rdb文件分析，该任务是分布式的。为了均衡对每个机器的影响，通过算法去保证slave分配算法均匀的落在不同的机器上。</p>\n<h4 id=\"算法思路\"><a href=\"#算法思路\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><ol>\n<li>slave数量最小的优先分配</li>\n<li>通过map存储不同IP分配的数量，按照规则，优先分配数量最小的IP</li>\n<li>staticsResult里面不存在的IP优先分配</li>\n<li>如果上面未分配,则选择staticsResult中数值最小的那个slave<h4 id=\"算法代码实现\"><a href=\"#算法代码实现\" class=\"headerlink\" title=\"算法代码实现\"></a>算法代码实现</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">\t * 根据&lt;master:slaves&gt;获取执行分析任务ports规则</span><br><span class=\"line\">\t * 即获取其中一个slave,尽量保持均衡在不同机器上</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param clusterNodesMap</span><br><span class=\"line\">\t * @return &lt;ip:ports&gt;</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static Map&lt;String, Set&lt;String&gt;&gt; generateAnalyzeRule(Map&lt;String, List&lt;String&gt;&gt; clusterNodesMap) &#123;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t// 通过该map存储不同IP分配的数量，按照规则，优先分配数量最小的IP</span><br><span class=\"line\">\t\tMap&lt;String, Integer&gt; staticsResult = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;String&gt;&gt; generateRule = new HashMap&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 此处排序是为了将slave数量最小的优先分配</span><br><span class=\"line\">\t\tList&lt;Map.Entry&lt;String, List&lt;String&gt;&gt;&gt; sortList = new LinkedList&lt;&gt;(clusterNodesMap.entrySet());</span><br><span class=\"line\">\t\tCollections.sort(sortList, new Comparator&lt;Entry&lt;String, List&lt;String&gt;&gt;&gt;() &#123;</span><br><span class=\"line\">\t\t\t@Override</span><br><span class=\"line\">\t\t\tpublic int compare(Entry&lt;String, List&lt;String&gt;&gt; o1, Entry&lt;String, List&lt;String&gt;&gt; o2) &#123;</span><br><span class=\"line\">\t\t\t\treturn o1.getValue().size() - o2.getValue().size();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfor (Entry&lt;String, List&lt;String&gt;&gt; entry : sortList) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; slaves = entry.getValue();</span><br><span class=\"line\">\t\t\tboolean isSelected = false;</span><br><span class=\"line\">\t\t\tString tempPort = null;</span><br><span class=\"line\">\t\t\tString tempIP = null;</span><br><span class=\"line\">\t\t\tint num = 0;</span><br><span class=\"line\">\t\t\tfor (String slave : slaves) &#123;</span><br><span class=\"line\">\t\t\t\tString ip = slave.split(&quot;:&quot;)[0];</span><br><span class=\"line\">\t\t\t\tString port = slave.split(&quot;:&quot;)[1];</span><br><span class=\"line\">\t\t\t\t// 统计组里面不存在的IP优先分配</span><br><span class=\"line\">\t\t\t\tif (!staticsResult.containsKey(ip)) &#123;</span><br><span class=\"line\">\t\t\t\t\tstaticsResult.put(ip, 1);</span><br><span class=\"line\">\t\t\t\t\tSet&lt;String&gt; generatePorts = generateRule.get(ip);</span><br><span class=\"line\">\t\t\t\t\tif (generatePorts == null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tgeneratePorts = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tgeneratePorts.add(port);</span><br><span class=\"line\">\t\t\t\t\tgenerateRule.put(ip, generatePorts);</span><br><span class=\"line\">\t\t\t\t\tisSelected = true;</span><br><span class=\"line\">\t\t\t\t\tbreak;</span><br><span class=\"line\">\t\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\t\t// 此处是为了求出被使用最少的IP</span><br><span class=\"line\">\t\t\t\t\tInteger staticsNum = staticsResult.get(ip);</span><br><span class=\"line\">\t\t\t\t\tif (num == 0) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tnum = staticsNum;</span><br><span class=\"line\">\t\t\t\t\t\ttempPort = port;</span><br><span class=\"line\">\t\t\t\t\t\ttempIP = ip;</span><br><span class=\"line\">\t\t\t\t\t\tcontinue;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\tif (staticsNum &lt; num) &#123;</span><br><span class=\"line\">\t\t\t\t\t\ttempPort = port;</span><br><span class=\"line\">\t\t\t\t\t\ttempIP = ip;</span><br><span class=\"line\">\t\t\t\t\t\tnum = staticsNum;</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t// 如果上面未分配,则选择staticsResult中数值最小的那个slave</span><br><span class=\"line\">\t\t\tif (!isSelected) &#123;</span><br><span class=\"line\">\t\t\t\tif (slaves != null &amp;&amp; slaves.size() &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\t\tif (tempPort != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tSet&lt;String&gt; generatePorts = generateRule.get(tempIP);</span><br><span class=\"line\">\t\t\t\t\t\tif (generatePorts == null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tgeneratePorts = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t\tgeneratePorts.add(tempPort);</span><br><span class=\"line\">\t\t\t\t\t\tgenerateRule.put(tempIP, generatePorts);</span><br><span class=\"line\">\t\t\t\t\t\tstaticsResult.put(tempIP, staticsResult.get(tempIP) + 1);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn generateRule;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"Jemalloc内存预分配\"><a href=\"#Jemalloc内存预分配\" class=\"headerlink\" title=\"Jemalloc内存预分配\"></a>Jemalloc内存预分配</h3><h4 id=\"算法思路-1\"><a href=\"#算法思路-1\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><p>redis中支持多种内存分配算法，推荐Jemalloc，因此我们选择该算法来预估内存大小。因为这个算法比较复杂，我们参考雪球做法使用约定数组，根据不同数据大小分配不同内存空间，为了提高查找效率，这块采用了变种二分查找。</p>\n<h4 id=\"算法代码\"><a href=\"#算法代码\" class=\"headerlink\" title=\"算法代码\"></a>算法代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class Jemalloc &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array16;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array192;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array768;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array4096;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate static long[] array4194304;</span><br><span class=\"line\"></span><br><span class=\"line\">\tstatic &#123;</span><br><span class=\"line\">\t\tarray16 = range(16, 128 + 1, 16);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray192 = range(192, 512 + 1, 64);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray768 = range(768, 4096 + 1, 256);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray4096 = range(4096, 4194304 + 1, 4096);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tarray4194304 = range(4194304, 536870912 + 1, 4194304);</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据Jemalloc 估算分配内存大小 Small: All 2^n-aligned allocations of size 2^n will incur</span><br><span class=\"line\">\t * no additional overhead, due to how small allocations are aligned and packed.</span><br><span class=\"line\">\t * Small: [8], [16, 32, 48, ..., 128], [192, 256, 320, ..., 512], [768, 1024,</span><br><span class=\"line\">\t * 1280, ..., 3840] Large: The worst case size is half the chunk size, in which</span><br><span class=\"line\">\t * case only one allocation per chunk can be allocated. If the remaining</span><br><span class=\"line\">\t * (nearly) half of the chunk isn&apos;t otherwise useful for smaller allocations,</span><br><span class=\"line\">\t * the overhead will essentially be 50%. However, assuming you use a diverse</span><br><span class=\"line\">\t * mixture of size classes, the actual overhead shouldn&apos;t be a significant issue</span><br><span class=\"line\">\t * in practice. Large: [4 KiB, 8 KiB, 12 KiB, ..., 4072 KiB] Huge: Extra virtual</span><br><span class=\"line\">\t * memory is mapped, then the excess is trimmed and unmapped. This can leave</span><br><span class=\"line\">\t * virtual memory holes, but it incurs no physical memory overhead. Earlier</span><br><span class=\"line\">\t * versions of jemalloc heuristically attempted to optimistically map chunks</span><br><span class=\"line\">\t * without excess that would need to be trimmed, but it didn&apos;t save much system</span><br><span class=\"line\">\t * call overhead in practice. Huge: [4 MiB, 8 MiB, 12 MiB, ..., 512 MiB]</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param size</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static long assign(long size) &#123;</span><br><span class=\"line\">\t\tif (size &lt;= 4096) &#123;</span><br><span class=\"line\">\t\t\t// Small</span><br><span class=\"line\">\t\t\tif (is_power2(size)) &#123;</span><br><span class=\"line\">\t\t\t\treturn size;</span><br><span class=\"line\">\t\t\t&#125; else if (size &lt; 128) &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array16, size);</span><br><span class=\"line\">\t\t\t&#125; else if (size &lt; 512) &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array192, size);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\treturn min_ge(array768, size);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else if (size &lt; 4194304) &#123;</span><br><span class=\"line\">\t\t\t// Large</span><br><span class=\"line\">\t\t\treturn min_ge(array4096, size);</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t// Huge</span><br><span class=\"line\">\t\t\treturn min_ge(array4194304, size);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 创建一个long数组</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param start</span><br><span class=\"line\">\t * @param stop</span><br><span class=\"line\">\t * @param step</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic static long[] range(int start, int stop, int step) &#123;</span><br><span class=\"line\">\t\tint size = (stop - 1 - start) / step + 1;</span><br><span class=\"line\">\t\tlong[] array = new long[size];</span><br><span class=\"line\">\t\tint index = 0;</span><br><span class=\"line\">\t\tfor (int i = start; i &lt; stop; i = i + step) &#123;</span><br><span class=\"line\">\t\t\tarray[index] = i;</span><br><span class=\"line\">\t\t\tindex++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn array;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static long min_ge(long[] srcArray, long key) &#123;</span><br><span class=\"line\">\t\tint index = binarySearch(srcArray, key);</span><br><span class=\"line\">\t\treturn srcArray[index];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 二分查找最小值，即最接近要查找的值，但是要大于该值</span><br><span class=\"line\">\tpublic static int binarySearch(long srcArray[], long key) &#123;</span><br><span class=\"line\">\t\tint mid = (0+srcArray.length-1) / 2;</span><br><span class=\"line\">\t\tif (key == srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\treturn mid;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif (key &gt; srcArray[mid] &amp;&amp; key &lt;= srcArray[mid + 1]) &#123;</span><br><span class=\"line\">\t\t\treturn mid + 1;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tint start = 0;</span><br><span class=\"line\">\t\tint end = srcArray.length - 1;</span><br><span class=\"line\">\t\twhile (start &lt;= end) &#123;</span><br><span class=\"line\">\t\t\tmid = (end - start) / 2 + start;</span><br><span class=\"line\">\t\t\tif (key == srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\treturn mid;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &gt; srcArray[mid] &amp;&amp; key &lt;= srcArray[mid + 1]) &#123;</span><br><span class=\"line\">\t\t\t\treturn mid + 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &lt; srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\tend = mid - 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tif (key &gt; srcArray[mid]) &#123;</span><br><span class=\"line\">\t\t\t\tstart = mid + 1;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn 0;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpublic static boolean is_power2(long size) &#123;</span><br><span class=\"line\">\t\tif (size == 0) &#123;</span><br><span class=\"line\">\t\t\treturn false;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif ((size &amp; (size - 1)) == 0) &#123;</span><br><span class=\"line\">\t\t\treturn true;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn false;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Redis不同数据结构预估\"><a href=\"#Redis不同数据结构预估\" class=\"headerlink\" title=\"Redis不同数据结构预估\"></a>Redis不同数据结构预估</h3><h4 id=\"算法思路-2\"><a href=\"#算法思路-2\" class=\"headerlink\" title=\"算法思路\"></a>算法思路</h4><p>详见<a href=\"https://github.com/antirez/redis\" target=\"_blank\" rel=\"noopener\">redis源码</a></p>\n<h4 id=\"算法代码-1\"><a href=\"#算法代码-1\" class=\"headerlink\" title=\"算法代码\"></a>算法代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private static final long redisObject = (long)(8 + 8);</span><br><span class=\"line\">// 一个dictEntry，24字节，jemalloc会分配32字节的内存块</span><br><span class=\"line\">private static final long dicEntry = (long)(2 * 8 + 8 + 8);</span><br><span class=\"line\">private static final String patternString = &quot;^[-\\\\\\\\+]?[\\\\\\\\d]*$&quot;;</span><br><span class=\"line\"></span><br><span class=\"line\">private static long skiplistMaxLevel = 32;</span><br><span class=\"line\">private static long redisSharedInterges = 10000;</span><br><span class=\"line\">private static long longSize = 8;</span><br><span class=\"line\">private static long pointerSize = 8;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 一个SDS结构占据的空间为：free所占长度+len所占长度+ buf数组的长度=4+4+len+1=len+9</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param length</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">private static long sds(long length) &#123;</span><br><span class=\"line\">\tlong mem = 9 + length;</span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * 计算 string byte 大小</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param kv</span><br><span class=\"line\"> *            https://searchdatabase.techtarget.com.cn/wp-content/uploads/res/database/article/2011/2011-11-14-16-56-18.jpg</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static long CalculateString(KeyStringValueString kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = dicEntry + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject + SizeofString(kv.getValueAsString());</span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateLinkedList(KeyStringValueList kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsStringList().size();</span><br><span class=\"line\">\tmem = mem + LinkedListEntryOverhead() * length;</span><br><span class=\"line\">\tmem = mem + LinkedlistOverhead();</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\">\tfor (String value : kv.getValueAsStringList()) &#123;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateZipList(KeyStringValueList kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tlong length = kv.getValueAsStringList().size();</span><br><span class=\"line\">\tmem = mem + ZiplistOverhead(length);</span><br><span class=\"line\">       </span><br><span class=\"line\">\tfor (String value : kv.getValueAsStringList()) &#123;</span><br><span class=\"line\">\t\tmem = mem + ZiplistAlignedStringOverhead(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateHash(KeyStringValueHash kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsHash().size();</span><br><span class=\"line\">\tmem = mem + HashtableOverhead(length);</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String key : kv.getValueAsHash().keySet()) &#123;</span><br><span class=\"line\">\t\tString value = kv.getValueAsHash().get(key);</span><br><span class=\"line\">\t\tmem = mem + SizeofString(key);</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + HashtableEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateSet(KeyStringValueSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\"></span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + HashtableOverhead(length);</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String value : kv.getValueAsSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value);</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + HashtableEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateIntSet(KeyStringValueSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + IntsetOverhead(length);</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (String value : kv.getValueAsSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + ZiplistAlignedStringOverhead(value);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long CalculateZSet(KeyStringValueZSet kv) &#123;</span><br><span class=\"line\">\tlong mem = KeyExpiryOverhead(kv);</span><br><span class=\"line\"></span><br><span class=\"line\">\tmem = mem + SizeofString(kv.getRawKey());</span><br><span class=\"line\">\tmem = mem + redisObject;</span><br><span class=\"line\">\tmem = mem + dicEntry;</span><br><span class=\"line\">\tlong length = kv.getValueAsSet().size();</span><br><span class=\"line\">\tmem = mem + SkiplistOverhead(length);</span><br><span class=\"line\">\tmem = mem + redisObject * length;</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor (ZSetEntry value : kv.getValueAsZSet()) &#123;</span><br><span class=\"line\">\t\tmem = mem + 8;</span><br><span class=\"line\">\t\tmem = mem + SizeofString(value.getElement());</span><br><span class=\"line\">\t\t// TODO 还有个 score</span><br><span class=\"line\">\t\tmem = mem + 2 * redisObject;</span><br><span class=\"line\">\t\tmem = mem + SkiplistEntryOverhead();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn mem;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// TopLevelObjOverhead get memory use of a top level object</span><br><span class=\"line\">// Each top level object is an entry in a dictionary, and so we have to include</span><br><span class=\"line\">// the overhead of a dictionary entry</span><br><span class=\"line\">public static long TopLevelObjOverhead() &#123;</span><br><span class=\"line\">\treturn HashtableEntryOverhead();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * SizeofString get memory use of a string</span><br><span class=\"line\"> * https://github.com/antirez/redis/blob/unstable/src/sds.h</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param bytes</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">public static long SizeofString(byte[] bytes) &#123;</span><br><span class=\"line\">\tString value = new String(bytes);</span><br><span class=\"line\"></span><br><span class=\"line\">\tif (isInteger(value)) &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tLong num = Long.parseLong(value);</span><br><span class=\"line\">\t\t\tif (num &lt; redisSharedInterges &amp;&amp; num &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\treturn 0;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn 8;</span><br><span class=\"line\">\t\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn Jemalloc.assign(sds(bytes.length));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">public static long SizeofString(String value) &#123;</span><br><span class=\"line\">\tif (isInteger(value)) &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tLong num = Long.parseLong(value);</span><br><span class=\"line\">\t\t\tif (num &lt; redisSharedInterges &amp;&amp; num &gt; 0) &#123;</span><br><span class=\"line\">\t\t\t\treturn 0;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\treturn 8;</span><br><span class=\"line\">\t\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn Jemalloc.assign(sds(value.length()));</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long DictOverhead(long size) &#123;</span><br><span class=\"line\">\t return Jemalloc.assign(56 + 2*pointerSize + nextPower(size) * 3*8);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static boolean isInteger(String str) &#123;</span><br><span class=\"line\">\tPattern pattern = Pattern.compile(patternString);</span><br><span class=\"line\">\treturn pattern.matcher(str).matches();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">/**</span><br><span class=\"line\"> * 过期时间也是存储为一个 dictEntry，时间戳为 int64；</span><br><span class=\"line\"> * </span><br><span class=\"line\"> * @param kv</span><br><span class=\"line\"> * @return</span><br><span class=\"line\"> */</span><br><span class=\"line\">// KeyExpiryOverhead get memory useage of a key expiry</span><br><span class=\"line\">// Key expiry is stored in a hashtable, so we have to pay for the cost of a</span><br><span class=\"line\">// hashtable entry</span><br><span class=\"line\">// The timestamp itself is stored as an int64, which is a 8 bytes</span><br><span class=\"line\">@SuppressWarnings(&quot;rawtypes&quot;)</span><br><span class=\"line\">public static long KeyExpiryOverhead(KeyValuePair kv) &#123;</span><br><span class=\"line\">\t// If there is no expiry, there isn&apos;t any overhead</span><br><span class=\"line\">\tif (kv.getExpiredType() == ExpiredType.NONE) &#123;</span><br><span class=\"line\">\t\treturn 0;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn HashtableEntryOverhead() + 8;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long HashtableOverhead(long size) &#123;</span><br><span class=\"line\">\treturn 4 + 7 * longSize + 4 * pointerSize + nextPower(size) * pointerSize * 3 / 2;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// HashtableEntryOverhead get memory use of hashtable entry</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/dict.h</span><br><span class=\"line\">// Each dictEntry has 2 pointers + int64</span><br><span class=\"line\">public static long HashtableEntryOverhead() &#123;</span><br><span class=\"line\">\treturn 2 * pointerSize + 8;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long ZiplistOverhead(long size) &#123;</span><br><span class=\"line\">\treturn Jemalloc.assign(12 + 21 * size);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long ZiplistAlignedStringOverhead(String value) &#123;</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\tLong.parseLong(value);</span><br><span class=\"line\">\t\treturn 8;</span><br><span class=\"line\">\t&#125; catch (NumberFormatException e) &#123;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn Jemalloc.assign(value.length());</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// LinkedlistOverhead get memory use of a linked list</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/adlist.h</span><br><span class=\"line\">// A list has 5 pointers + an unsigned long</span><br><span class=\"line\">public static long LinkedlistOverhead() &#123;</span><br><span class=\"line\">\treturn longSize + 5 * pointerSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// LinkedListEntryOverhead get memory use of a linked list entry</span><br><span class=\"line\">// See https://github.com/antirez/redis/blob/unstable/src/adlist.h</span><br><span class=\"line\">// A node has 3 pointers</span><br><span class=\"line\">public static long LinkedListEntryOverhead() &#123;</span><br><span class=\"line\">\treturn 3 * pointerSize;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// SkiplistOverhead get memory use of a skiplist</span><br><span class=\"line\">public static long SkiplistOverhead(long size) &#123;</span><br><span class=\"line\">\treturn 2 * pointerSize + HashtableOverhead(size) + (2 * pointerSize + 16);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// SkiplistEntryOverhead get memory use of a skiplist entry</span><br><span class=\"line\">public static long SkiplistEntryOverhead() &#123;</span><br><span class=\"line\">\treturn HashtableEntryOverhead() + 2 * pointerSize + 8 + (pointerSize + 8) * zsetRandLevel();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long nextPower(long size) &#123;</span><br><span class=\"line\">\tlong power = 1;</span><br><span class=\"line\">\twhile (power &lt;= size) &#123;</span><br><span class=\"line\">\t\tpower = power &lt;&lt; 1;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn power;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long zsetRandLevel() &#123;</span><br><span class=\"line\">\tlong level = 1;</span><br><span class=\"line\">\tint rint = new Random().nextInt(65536);</span><br><span class=\"line\">\tint flag = 65535 / 4;</span><br><span class=\"line\">\twhile (rint &lt; flag) &#123;// skiplistP</span><br><span class=\"line\">\t\tlevel++;</span><br><span class=\"line\">\t\trint = new Random().nextInt(65536);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif (level &lt; skiplistMaxLevel) &#123;</span><br><span class=\"line\">\t\treturn level;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn skiplistMaxLevel;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">public static long  IntsetOverhead(long size) &#123;</span><br><span class=\"line\">    //     typedef struct intset &#123;</span><br><span class=\"line\">    //     uint32_t encoding;</span><br><span class=\"line\">    //     uint32_t length;</span><br><span class=\"line\">    //     int8_t contents[];</span><br><span class=\"line\">    //      &#125; intset;</span><br><span class=\"line\">    return (4 + 4) * size;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"RCT分析redis-rdb\"><a href=\"#RCT分析redis-rdb\" class=\"headerlink\" title=\"RCT分析redis rdb\"></a>RCT分析redis rdb</h2><p>RCT是一个一站式redis内存分析分析平台，分析任务是分布式的，需要将RCT-Analyze部署到rdb所在机器上，RCT-Dashboard部署在任何机器，只要能保持和RCT-Analyze通信即可。不同于以上列举的工具，我们最初定位RCT就是一个可以长期运行，尽可能每天分析redis中数据，为redis运维人员提供运维依据，便于做出更好的规范，高效的使用redis。</p>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><p>部署过程略，详见<a href=\"https://github.com/xaecbd/RCT/blob/master/README_zh.md#%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B\" target=\"_blank\" rel=\"noopener\">官方文档</a>，推荐使用docker方式，这样也是我们目前采用的方式。</p>\n<h3 id=\"配置分析任务\"><a href=\"#配置分析任务\" class=\"headerlink\" title=\"配置分析任务\"></a>配置分析任务</h3><h4 id=\"新增RDB分析配置\"><a href=\"#新增RDB分析配置\" class=\"headerlink\" title=\"新增RDB分析配置\"></a>新增RDB分析配置</h4><p>只有先创建了redis节点之后，才能进入到RCT工具导航页面。</p>\n<ol>\n<li>点击导航RDB Analyze</li>\n<li>如若一直没有添加过RDB信息，则可在页面弹出的框中进行完善信息，或者点击下方的Add按钮进行添加</li>\n<li>点击edit则可以对RDB 信息进行修改</li>\n<li>如若已经完善了RDB信息，则点击打开switch开关，则是对RDB直接进行定时任务的开启和关闭 </li>\n<li>点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态 </li>\n</ol>\n<h4 id=\"RDB-Add页面参数说明\"><a href=\"#RDB-Add页面参数说明\" class=\"headerlink\" title=\"RDB Add页面参数说明\"></a>RDB Add页面参数说明</h4><ol>\n<li><strong>Automatic Analyze</strong>：是否开启定时任务</li>\n<li><strong>Schedule</strong>：cron 表达式（填写完成之后，可以点击右侧的图标进行查看定时表达式执行的时间）</li>\n<li><strong>Analyzer</strong>:分析器 （依次是生成报表，根据filter导key到elasticsearch中，根据preix导key到elasticsearch中）</li>\n<li><strong>Data Path</strong>:rdb文件的目录（eg:/opt/app/redis/9002/dump.rdb,data path应为/opt/app/redis）</li>\n<li><strong>Prefixes</strong>:key的前缀，可为空，但是在选择了分析器中的根据preix导key，则必须填写prefixes。我们强烈建议将已知前缀填入，可以提高分析效率，节省时间。</li>\n<li><strong>Report</strong>：是否生成报表</li>\n<li><strong>Mail</strong>：生成报表之后的收件人，平台将会将报表做为附件发送给收件人，如果收件人有多个，请用;隔开</li>\n</ol>\n<h4 id=\"RDB-主页面参数说明\"><a href=\"#RDB-主页面参数说明\" class=\"headerlink\" title=\"RDB 主页面参数说明\"></a>RDB 主页面参数说明</h4><p>RDB主页面参数与add页面参数基本相同，在此不再做赘述，唯一有区别的是：<br>Status：分析RDB文件的进度状态，分为成功，正在分析，失败三种状态，status为正在分析状态时，可通过点击后面的链接进入到实时分析页面，查看实时状态。</p>\n<h4 id=\"分析器介绍\"><a href=\"#分析器介绍\" class=\"headerlink\" title=\"分析器介绍\"></a>分析器介绍</h4><ol>\n<li>生成报表：对rdb文件的数据进行分析并将结果写入数据库中，如果配置了Report，结果会以excel报表的形式发送邮件给用户</li>\n<li>根据filter导key：在对rdb文件分析的时候，根据过滤器导出相应的key，将数据写入到EleasticSearch中</li>\n<li>根据prefix导key：在对rdb文件分析的时候，根据制定的前缀key导出相应的key，将数据写入到EleasticSearch中</li>\n</ol>\n<h3 id=\"手动分析\"><a href=\"#手动分析\" class=\"headerlink\" title=\"手动分析\"></a>手动分析</h3><p>点击Analyze，则是对RDB信息进行手动分析，分析进行的状态可在status中查看，可以通过status后面的链接进入到实时查看rdb分析任务的进度状态</p>\n<h3 id=\"查看报表\"><a href=\"#查看报表\" class=\"headerlink\" title=\"查看报表\"></a>查看报表</h3><p>目前RCT支持dashboard/email两种方式，这里我仅展示dashboard.<br><img src=\"https://github.com/xaecbd/RCT/raw/master/doc/screenshots/rct.jpg?raw=true\" alt=\"image\"></p>\n<h2 id=\"项目地址\"><a href=\"#项目地址\" class=\"headerlink\" title=\"项目地址\"></a>项目地址</h2><p><a href=\"https://github.com/xaecbd/RCT\" target=\"_blank\" rel=\"noopener\">https://github.com/xaecbd/RCT</a>,欢迎使用，欢迎加入我们，帮我们提高RCT。如果你有问题，可以前往github新建issue。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"http://www.innereye.cn/2017/01/17/redis/Redis_RDB_File_Format_20170117/\" target=\"_blank\" rel=\"noopener\">Redis持久化文件RDB的格式解析</a></li>\n<li><a href=\"https://github.com/sripathikrishnan/redis-rdb-tools\" target=\"_blank\" rel=\"noopener\">redis-rdb-tools</a></li>\n<li><a href=\"https://github.com/xueqiu/rdr\" target=\"_blank\" rel=\"noopener\">rdr</a></li>\n<li><a href=\"https://infoq.cn/article/analysis-redis\" target=\"_blank\" rel=\"noopener\">analysis-redis</a></li>\n<li><a href=\"http://www.cnblogs.com/kismetv/p/8654978.html\" target=\"_blank\" rel=\"noopener\">Redis内存模型</a></li>\n</ol>\n"},{"title":"如何监控kafka消费Lag情况","originContent":"","toc":false,"date":"2019-04-13T11:35:10.000Z","_content":"\n## 前言\n为什么会有这个需求？\n\nkafka consumer 消费会存在延迟情况，我们需要查看消息堆积情况，就是所谓的消息Lag。目前是市面上也有相应的监控工具[KafkaOffsetMonitor](https://github.com/quantifind/KafkaOffsetMonitor)，我们自己也写了一套监控[kmanager](https://github.com/xaecbd/kmanager)。但是随着kafka版本的升级，消费方式也发生了很大的变化，因此，我们需要重构一下kafka offset监控。\n## 正文\n\n## 如何计算Lag\n在计算Lag之前先普及几个基本常识\n\n**LEO(LogEndOffset):** 这里说的和官网说的LEO有点区别，主要是指堆consumer可见的offset.即HW(High Watermark)\n\n**CURRENT-OFFSET**: consumer消费到的具体位移\n\n知道以上信息后，可知`Lag=LEO-CURRENT-OFFSET`。计算出来的值即为消费延迟情况。\n\n\n## 官方查看方式\n这里说的官方查看方式是在官网文档中提到的，使用官方包里提供的`bin/kafka-consumer-groups.sh`\n\n最新版的工具只能获取到通过**broker**消费的情况\n\n```\n$ bin/kafka-consumer-groups.sh --describe --bootstrap-server 192.168.0.101:8092 --group test\nConsumer group 'test' has no active members.\n\nTOPIC              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\ntruman_test_offset 2          1325            2361            1036            -               -               -\ntruman_test_offset 6          1265            2289            1024            -               -               -\ntruman_test_offset 4          1245            2243            998             -               -               -\ntruman_test_offset 9          1310            2307            997             -               -               -\ntruman_test_offset 1          1259            2257            998             -               -               -\ntruman_test_offset 8          1410            2438            1028            -               -               -\ntruman_test_offset 3          1225            2167            942             -               -               -\ntruman_test_offset 0          1218            2192            974             -               -               -\ntruman_test_offset 5          1262            2252            990             -               -               -\ntruman_test_offset 7          1265            2277            1012            -               -               -\n\n```\n\n## 程序查询方式\n使用程序查询方式，有什么好处，可以实现自己的offset监控,可以无缝接入任何平台系统。\n\n既然是用程序实现，那么做个更高级的需求，++**根据topic获取不同消费组的消费情况**++。\n\n先定义两个实体\n\nTopicConsumerGroupState\n```\npublic class TopicConsumerGroupState {\n\tprivate String groupId;\n\t/**\n\t * 消费方式：zk/broker 主要指的offset提交到哪里 新版本 broker 旧版本zk\n\t */\n\tprivate String consumerMethod;\n\tprivate List<PartitionAssignmentState> partitionAssignmentStates;\n\t/**\n\t * Dead：组内已经没有任何成员的最终状态，组的元数据也已经被coordinator移除了。这种状态响应各种请求都是一个response：\n\t * UNKNOWN_MEMBER_ID Empty：组内无成员，但是位移信息还没有过期。这种状态只能响应JoinGroup请求\n\t * PreparingRebalance：组准备开启新的rebalance，等待成员加入 AwaitingSync：正在等待leader\n\t * consumer将分配方案传给各个成员 Stable：rebalance完成！可以开始消费了~\n\t */\n\tprivate ConsumerGroupState consumerGroupState;\n\n    //省略set和get方法..\n\n}\n```\nPartitionAssignmentState\n```\npublic class PartitionAssignmentState {\n\tprivate String group; \n\tprivate String topic;\n\tprivate int partition;\n\tprivate long offset;\n\tprivate long lag;\n\tprivate String consumerId;\n\tprivate String host;\n\tprivate String clientId;\n\tprivate long logEndOffset;\n\t//省略set和get方法..\n}\n\n```\n\n### broker消费方式 offset 获取\n#### 实现思路\n1. 根据topic 获取消费该topic的group\n2. 通过使用KafkaAdminClient的describeConsumerGroups读取broker上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition，host等\n3. 通过consumer获取LogEndOffset（可见offset）\n4. 将2与3处信息合并，计算Lag\n#### 代码设计\n引入最新版依赖,使用KafkaAdminClient获取1,2处信息\n```\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>2.2.0</version>\n</dependency>\n```\n1. 步骤1\n```\n\tpublic Set<String> listConsumerGroups(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tfinal Set<String> filteredGroups = new HashSet<>();\n\n\t\tSet<String> allGroups = this.adminClient.listConsumerGroups().all().get(30, TimeUnit.SECONDS).stream()\n\t\t\t\t.map(ConsumerGroupListing::groupId).collect(Collectors.toSet());\n\t\tallGroups.forEach(groupId -> {\n\t\t\ttry {\n\t\t\t\tadminClient.listConsumerGroupOffsets(groupId).partitionsToOffsetAndMetadata().get().keySet().stream()\n\t\t\t\t\t\t.filter(tp -> tp.topic().equals(topic)).forEach(tp -> filteredGroups.add(groupId));\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t});\n\t\treturn filteredGroups;\n\t}\n```\n2. 步骤2\n\n使用describeConsumerGroup获取的消费情况，其中包含有members和无members情况。正常订阅消费是可以获取到members，但是通过制定patition消费拿不到members.\n```\n/**\n\t * 根据topic 获取offset,该结果是不同group 组不同patition的当前消费offset\n\t * \n\t * @param topic\n\t * @return Map<String, Set<Entry<TopicPartition, OffsetAndMetadata>>>\n\t * @throws InterruptedException\n\t * @throws ExecutionException\n\t * @throws TimeoutException\n\t */\n\tpublic Map<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> listConsumerGroupOffsets(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tSet<String> groupIds = this.listConsumerGroups(topic);\n\t\tMap<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> consumerGroupOffsets = new HashMap<>();\n\t\tgroupIds.forEach(groupId -> {\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets = new HashSet<>();\n\t\t\ttry {\n\t\t\t\tconsumerPatitionOffsets = this.adminClient.listConsumerGroupOffsets(groupId)\n\t\t\t\t\t\t.partitionsToOffsetAndMetadata().get(30, TimeUnit.SECONDS).entrySet().stream()\n\t\t\t\t\t\t.filter(entry -> topic.equalsIgnoreCase(entry.getKey().topic())).collect(Collectors.toSet());\n\t\t\t\t;\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tconsumerGroupOffsets.put(groupId, consumerPatitionOffsets);\n\t\t});\n\t\treturn consumerGroupOffsets;\n\t}\n\n\t/**\n\t * 根据topic 获取topicConsumerGroupStates,其中未包含lag/logEndOffset(consumer可见offset)\n\t * \n\t * @param topic\n\t * @return\n\t * @throws InterruptedException\n\t * @throws ExecutionException\n\t * @throws TimeoutException\n\t */\n\tpublic List<TopicConsumerGroupState> describeConsumerGroups(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tfinal List<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\tSet<String> groupIds = this.listConsumerGroups(topic);\n\t\tMap<String, ConsumerGroupDescription> groupDetails = this.adminClient.describeConsumerGroups(groupIds).all()\n\t\t\t\t.get(30, TimeUnit.SECONDS);\n\n\t\tMap<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> consumerPatitionOffsetMap = this\n\t\t\t\t.listConsumerGroupOffsets(topic);\n\n\t\tgroupDetails.entrySet().forEach(entry -> {\n\t\t\tString groupId = entry.getKey();\n\t\t\tConsumerGroupDescription description = entry.getValue();\n\n\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();\n\t\t\ttopicConsumerGroupState.setGroupId(groupId);\n\t\t\ttopicConsumerGroupState.setConsumerMethod(\"broker\");\n\t\t\ttopicConsumerGroupState.setConsumerGroupState(description.state());\n\t\t\t// 获取group下不同patition消费offset信息\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets = consumerPatitionOffsetMap\n\t\t\t\t\t.get(groupId);\n\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\n\t\t\tif (!description.members().isEmpty()) {\n\t\t\t\t// 获取存在consumer(memeber存在的情况)\n\t\t\t\tpartitionAssignmentStates = this.withMembers(consumerPatitionOffsets, topic, groupId, description);\n\t\t\t} else {\n\t\t\t\t// 获取不存在consumer\n\t\t\t\tpartitionAssignmentStates = this.withNoMembers(consumerPatitionOffsets, topic, groupId);\n\t\t\t}\n\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);\n\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);\n\t\t});\n\n\t\treturn topicConsumerGroupStates;\n\t}\n\n\tprivate List<PartitionAssignmentState> withMembers(\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets, String topic, String groupId,\n\t\t\tConsumerGroupDescription description) {\n\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\t\tMap<Integer, Long> consumerPatitionOffsetMap = new HashMap<>();\n\t\tconsumerPatitionOffsets.forEach(entryInfo -> {\n\t\t\tTopicPartition topicPartition = entryInfo.getKey();\n\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();\n\t\t\tconsumerPatitionOffsetMap.put(topicPartition.partition(), offsetAndMetadata.offset());\n\t\t});\n\t\tdescription.members().forEach(memberDescription -> {\n\t\t\tmemberDescription.assignment().topicPartitions().forEach(topicPation -> {\n\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\t\tpartitionAssignmentState.setPartition(topicPation.partition());\n\t\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\t\tpartitionAssignmentState.setClientId(memberDescription.clientId());\n\t\t\t\tpartitionAssignmentState.setGroup(groupId);\n\t\t\t\tpartitionAssignmentState.setConsumerId(memberDescription.consumerId());\n\t\t\t\tpartitionAssignmentState.setHost(memberDescription.host());\n\t\t\t\tpartitionAssignmentState.setOffset(consumerPatitionOffsetMap.get(topicPation.partition()));\n\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t\t});\n\t\t});\n\t\treturn partitionAssignmentStates;\n\t}\n\n\tprivate List<PartitionAssignmentState> withNoMembers(\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets, String topic, String groupId) {\n\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\t\tconsumerPatitionOffsets.forEach(entryInfo -> {\n\t\t\tTopicPartition topicPartition = entryInfo.getKey();\n\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();\n\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\tpartitionAssignmentState.setPartition(topicPartition.partition());\n\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\tpartitionAssignmentState.setGroup(groupId);\n\t\t\tpartitionAssignmentState.setOffset(offsetAndMetadata.offset());\n\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t});\n\t\treturn partitionAssignmentStates;\n\t}\n```\n3. 步骤3\n```\n\tpublic Map<TopicPartition, Long>  consumerOffset (String gorupName,String topicName) {\n\t\tProperties consumerProps = new Properties();\n\t\tconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"192.168.0.1.101:9092\");\n\t\tconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, gorupName);\n\t\tconsumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\t\tconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n\t\tconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class);\n\t\tconsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\t\t@SuppressWarnings(\"resource\")\n\t\tKafkaConsumers<String, String>  consumer  = new KafkaConsumers<>(consumerProps);\n\t\tKafkaConsumer<String, String> kafkaConsumer = consumer.subscribe(topicName);\n\t\tList<PartitionInfo> patitions = kafkaConsumer.partitionsFor(topicName);\n\t\tList<TopicPartition>topicPatitions = new ArrayList<>();\n\t\tpatitions.forEach(patition->{\n\t\t\tTopicPartition topicPartition = new TopicPartition(topicName,patition.partition());\n\t\t\ttopicPatitions.add(topicPartition);\n\t\t});\n\t\tMap<TopicPartition, Long> result = kafkaConsumer.endOffsets(topicPatitions);\n\t\treturn result;\n\t}\n```\n4. 步骤4\n```\n\tprivate List<TopicConsumerGroupState> getBrokerConsumerOffsets(String clusterID, String topic) {\n\t\tList<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\ttry {\n\t\t\ttopicConsumerGroupStates = this.describeConsumerGroups(topic);\n\t\t\t// 填充lag/logEndOffset\n\t\t\ttopicConsumerGroupStates.forEach(topicConsumerGroupState -> {\n\t\t\t\tString groupId = topicConsumerGroupState.getGroupId();\n\t\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = topicConsumerGroupState\n\t\t\t\t\t\t.getPartitionAssignmentStates();\n\t\t\t\tMap<TopicPartition, Long> offsetsMap = this.consumerOffset(clusterID, groupId, topic);\n\t\t\t\tfor (Entry<TopicPartition, Long> entry : offsetsMap.entrySet()) {\n\t\t\t\t\tlong logEndOffset = entry.getValue();\n\t\t\t\t\tfor (PartitionAssignmentState partitionAssignmentState : partitionAssignmentStates) {\n\t\t\t\t\t\tif (partitionAssignmentState.getPartition() == entry.getKey().partition()) {\n\t\t\t\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);\n\t\t\t\t\t\t\tpartitionAssignmentState.setLag(getLag(partitionAssignmentState.getOffset(), logEndOffset));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (InterruptedException e) {\n\t\t\te.printStackTrace();\n\t\t} catch (ExecutionException e) {\n\t\t\te.printStackTrace();\n\t\t} catch (TimeoutException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn topicConsumerGroupStates;\n\t}\n```\n### zookeeper消费方式 offset 获取\n#### 实现思路\n1. 根据topic 获取消费该topic的group\n2. 读取zookeeper上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition。\n3. 通过consumer获取LogEndOffset（可见offset）\n4. 将2与3处信息合并，计算Lag\n#### 代码设计\n引入两个依赖,主要是为了读取zookeeper节点上的数据\n```\n\t\t<dependency>\n\t\t\t<groupId>com.101tec</groupId>\n\t\t\t<artifactId>zkclient</artifactId>\n\t\t\t<version>0.11</version>\n\t\t</dependency>\n\t\t<dependency>\n\t\t\t<groupId>org.apache.kafka</groupId>\n\t\t\t<artifactId>kafka_2.11</artifactId>\n\t\t</dependency>\n```\n\n1. 步骤1\n```\npublic Set<String> listTopicGroups(String topic) {\n\t\tSet<String> groups = new HashSet<>();\n\t\tList<String> allGroups = zkClient.getChildren(\"/consumers\");\n\t\tallGroups.forEach(group -> {\n\t\t\tif (zkClient.exists(\"/consumers/\" + group + \"/offsets\")) {\n\t\t\t\tSet<String> offsets = new HashSet<>(zkClient.getChildren(\"/consumers/\" + group + \"/offsets\"));\n\t\t\t\tif (offsets.contains(topic)) {\n\t\t\t\t\tgroups.add(group);\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t\treturn groups;\n\t}\n```\n2. 步骤2\n```\npublic Map<String, Map<String, String>> getZKConsumerOffsets(String groupId, String topic) {\n\t\tMap<String, Map<String, String>> result = new HashMap<>();\n\t\tString offsetsPath = \"/consumers/\" + groupId + \"/offsets/\" + topic;\n\t\tif (zkClient.exists(offsetsPath)) {\n\t\t\tList<String> offsets = zkClient.getChildren(offsetsPath);\n\t\t\toffsets.forEach(patition -> {\n\t\t\t\ttry {\n\t\t\t\t\tString offset = zkClient.readData(offsetsPath + \"/\" + patition, true);\n\t\t\t\t\tif (offset != null) {\n\t\t\t\t\t\tMap<String, String> map = new HashMap<>();\n\t\t\t\t\t\tmap.put(\"offset\", offset);\n\t\t\t\t\t\tresult.put(patition, map);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t});\n\t\t}\n\t\tString ownersPath = \"/consumers/\" + groupId + \"/owners/\" + topic;\n\t\tif (zkClient.exists(ownersPath)) {\n\t\t\tList<String> owners = zkClient.getChildren(ownersPath);\n\t\t\towners.forEach(patition -> {\n\t\t\t\ttry {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tString owner = zkClient.readData(ownersPath + \"/\" + patition, true);\n\t\t\t\t\t\tif (owner != null) {\n\t\t\t\t\t\t\tMap<String, String> map = result.get(patition);\n\t\t\t\t\t\t\tmap.put(\"owner\", owner);\n\t\t\t\t\t\t\tresult.put(patition, map);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t});\n\t\t}\n\n\t\treturn result;\n\t}\n```\n3. 步骤3\n\n略\n4. 步骤4\n```\n\tprivate List<TopicConsumerGroupState> getZKConsumerOffsets(String topic) {\n\t\tfinal List<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\tSet<String> zkGroups = this.listTopicGroups(topic);\n\t\tzkGroups.forEach(group -> {\n\t\t\tMap<String, Map<String, String>> zkConsumerOffsets = this.getZKConsumerOffsets(group,\n\t\t\t\t\ttopic);\n\n\t\t\tMap<TopicPartition, Long> offsetsMap = this.consumerOffset(group, topic);\n\n\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();\n\t\t\ttopicConsumerGroupState.setGroupId(group);\n\t\t\ttopicConsumerGroupState.setConsumerMethod(\"zk\");\n\n\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\n\t\t\tMap<String, Long> offsetsTempMap = new HashMap<>(offsetsMap.size());\n\t\t\toffsetsMap.forEach((k, v) -> {\n\t\t\t\toffsetsTempMap.put(k.partition() + \"\", v);\n\t\t\t});\n\n\t\t\tzkConsumerOffsets.forEach((patition, topicDesribe) -> {\n\t\t\t\tLong logEndOffset = offsetsTempMap.get(patition);\n\t\t\t\tString owner = topicDesribe.get(\"owner\");\n\t\t\t\tlong offset = Long.parseLong(topicDesribe.get(\"offset\"));\n\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\t\tpartitionAssignmentState.setClientId(owner);\n\t\t\t\tpartitionAssignmentState.setGroup(group);\n\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);\n\t\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\t\tpartitionAssignmentState.setOffset(offset);\n\t\t\t\tpartitionAssignmentState.setPartition(Integer.parseInt(patition));\n\t\t\t\tpartitionAssignmentState.setLag(getLag(offset, logEndOffset));\n\n\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t\t});\n\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);\n\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);\n\t\t});\n\t\treturn topicConsumerGroupStates;\n\t}\n```\n## 参考\n1. [如何获取Kafka的消费者详情——从Scala到Java的切换](https://blog.csdn.net/u013256816/article/details/79968647)\n2. [Kafka的Lag计算误区及正确实现](https://blog.csdn.net/u013256816/article/details/79955578)\n","source":"_posts/如何监控kafka消费Lag情况.md","raw":"---\ntitle: 如何监控kafka消费Lag情况\ntags:\n  - 技术分享\noriginContent: ''\ncategories:\n  - kafka\ntoc: false\ndate: 2019-04-13 19:35:10\n---\n\n## 前言\n为什么会有这个需求？\n\nkafka consumer 消费会存在延迟情况，我们需要查看消息堆积情况，就是所谓的消息Lag。目前是市面上也有相应的监控工具[KafkaOffsetMonitor](https://github.com/quantifind/KafkaOffsetMonitor)，我们自己也写了一套监控[kmanager](https://github.com/xaecbd/kmanager)。但是随着kafka版本的升级，消费方式也发生了很大的变化，因此，我们需要重构一下kafka offset监控。\n## 正文\n\n## 如何计算Lag\n在计算Lag之前先普及几个基本常识\n\n**LEO(LogEndOffset):** 这里说的和官网说的LEO有点区别，主要是指堆consumer可见的offset.即HW(High Watermark)\n\n**CURRENT-OFFSET**: consumer消费到的具体位移\n\n知道以上信息后，可知`Lag=LEO-CURRENT-OFFSET`。计算出来的值即为消费延迟情况。\n\n\n## 官方查看方式\n这里说的官方查看方式是在官网文档中提到的，使用官方包里提供的`bin/kafka-consumer-groups.sh`\n\n最新版的工具只能获取到通过**broker**消费的情况\n\n```\n$ bin/kafka-consumer-groups.sh --describe --bootstrap-server 192.168.0.101:8092 --group test\nConsumer group 'test' has no active members.\n\nTOPIC              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\ntruman_test_offset 2          1325            2361            1036            -               -               -\ntruman_test_offset 6          1265            2289            1024            -               -               -\ntruman_test_offset 4          1245            2243            998             -               -               -\ntruman_test_offset 9          1310            2307            997             -               -               -\ntruman_test_offset 1          1259            2257            998             -               -               -\ntruman_test_offset 8          1410            2438            1028            -               -               -\ntruman_test_offset 3          1225            2167            942             -               -               -\ntruman_test_offset 0          1218            2192            974             -               -               -\ntruman_test_offset 5          1262            2252            990             -               -               -\ntruman_test_offset 7          1265            2277            1012            -               -               -\n\n```\n\n## 程序查询方式\n使用程序查询方式，有什么好处，可以实现自己的offset监控,可以无缝接入任何平台系统。\n\n既然是用程序实现，那么做个更高级的需求，++**根据topic获取不同消费组的消费情况**++。\n\n先定义两个实体\n\nTopicConsumerGroupState\n```\npublic class TopicConsumerGroupState {\n\tprivate String groupId;\n\t/**\n\t * 消费方式：zk/broker 主要指的offset提交到哪里 新版本 broker 旧版本zk\n\t */\n\tprivate String consumerMethod;\n\tprivate List<PartitionAssignmentState> partitionAssignmentStates;\n\t/**\n\t * Dead：组内已经没有任何成员的最终状态，组的元数据也已经被coordinator移除了。这种状态响应各种请求都是一个response：\n\t * UNKNOWN_MEMBER_ID Empty：组内无成员，但是位移信息还没有过期。这种状态只能响应JoinGroup请求\n\t * PreparingRebalance：组准备开启新的rebalance，等待成员加入 AwaitingSync：正在等待leader\n\t * consumer将分配方案传给各个成员 Stable：rebalance完成！可以开始消费了~\n\t */\n\tprivate ConsumerGroupState consumerGroupState;\n\n    //省略set和get方法..\n\n}\n```\nPartitionAssignmentState\n```\npublic class PartitionAssignmentState {\n\tprivate String group; \n\tprivate String topic;\n\tprivate int partition;\n\tprivate long offset;\n\tprivate long lag;\n\tprivate String consumerId;\n\tprivate String host;\n\tprivate String clientId;\n\tprivate long logEndOffset;\n\t//省略set和get方法..\n}\n\n```\n\n### broker消费方式 offset 获取\n#### 实现思路\n1. 根据topic 获取消费该topic的group\n2. 通过使用KafkaAdminClient的describeConsumerGroups读取broker上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition，host等\n3. 通过consumer获取LogEndOffset（可见offset）\n4. 将2与3处信息合并，计算Lag\n#### 代码设计\n引入最新版依赖,使用KafkaAdminClient获取1,2处信息\n```\n<dependency>\n    <groupId>org.apache.kafka</groupId>\n    <artifactId>kafka-clients</artifactId>\n    <version>2.2.0</version>\n</dependency>\n```\n1. 步骤1\n```\n\tpublic Set<String> listConsumerGroups(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tfinal Set<String> filteredGroups = new HashSet<>();\n\n\t\tSet<String> allGroups = this.adminClient.listConsumerGroups().all().get(30, TimeUnit.SECONDS).stream()\n\t\t\t\t.map(ConsumerGroupListing::groupId).collect(Collectors.toSet());\n\t\tallGroups.forEach(groupId -> {\n\t\t\ttry {\n\t\t\t\tadminClient.listConsumerGroupOffsets(groupId).partitionsToOffsetAndMetadata().get().keySet().stream()\n\t\t\t\t\t\t.filter(tp -> tp.topic().equals(topic)).forEach(tp -> filteredGroups.add(groupId));\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t});\n\t\treturn filteredGroups;\n\t}\n```\n2. 步骤2\n\n使用describeConsumerGroup获取的消费情况，其中包含有members和无members情况。正常订阅消费是可以获取到members，但是通过制定patition消费拿不到members.\n```\n/**\n\t * 根据topic 获取offset,该结果是不同group 组不同patition的当前消费offset\n\t * \n\t * @param topic\n\t * @return Map<String, Set<Entry<TopicPartition, OffsetAndMetadata>>>\n\t * @throws InterruptedException\n\t * @throws ExecutionException\n\t * @throws TimeoutException\n\t */\n\tpublic Map<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> listConsumerGroupOffsets(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tSet<String> groupIds = this.listConsumerGroups(topic);\n\t\tMap<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> consumerGroupOffsets = new HashMap<>();\n\t\tgroupIds.forEach(groupId -> {\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets = new HashSet<>();\n\t\t\ttry {\n\t\t\t\tconsumerPatitionOffsets = this.adminClient.listConsumerGroupOffsets(groupId)\n\t\t\t\t\t\t.partitionsToOffsetAndMetadata().get(30, TimeUnit.SECONDS).entrySet().stream()\n\t\t\t\t\t\t.filter(entry -> topic.equalsIgnoreCase(entry.getKey().topic())).collect(Collectors.toSet());\n\t\t\t\t;\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (ExecutionException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} catch (TimeoutException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t\t\tconsumerGroupOffsets.put(groupId, consumerPatitionOffsets);\n\t\t});\n\t\treturn consumerGroupOffsets;\n\t}\n\n\t/**\n\t * 根据topic 获取topicConsumerGroupStates,其中未包含lag/logEndOffset(consumer可见offset)\n\t * \n\t * @param topic\n\t * @return\n\t * @throws InterruptedException\n\t * @throws ExecutionException\n\t * @throws TimeoutException\n\t */\n\tpublic List<TopicConsumerGroupState> describeConsumerGroups(String topic)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tfinal List<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\tSet<String> groupIds = this.listConsumerGroups(topic);\n\t\tMap<String, ConsumerGroupDescription> groupDetails = this.adminClient.describeConsumerGroups(groupIds).all()\n\t\t\t\t.get(30, TimeUnit.SECONDS);\n\n\t\tMap<String, Set<Entry<TopicPartition, OffsetAndMetadata>>> consumerPatitionOffsetMap = this\n\t\t\t\t.listConsumerGroupOffsets(topic);\n\n\t\tgroupDetails.entrySet().forEach(entry -> {\n\t\t\tString groupId = entry.getKey();\n\t\t\tConsumerGroupDescription description = entry.getValue();\n\n\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();\n\t\t\ttopicConsumerGroupState.setGroupId(groupId);\n\t\t\ttopicConsumerGroupState.setConsumerMethod(\"broker\");\n\t\t\ttopicConsumerGroupState.setConsumerGroupState(description.state());\n\t\t\t// 获取group下不同patition消费offset信息\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets = consumerPatitionOffsetMap\n\t\t\t\t\t.get(groupId);\n\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\n\t\t\tif (!description.members().isEmpty()) {\n\t\t\t\t// 获取存在consumer(memeber存在的情况)\n\t\t\t\tpartitionAssignmentStates = this.withMembers(consumerPatitionOffsets, topic, groupId, description);\n\t\t\t} else {\n\t\t\t\t// 获取不存在consumer\n\t\t\t\tpartitionAssignmentStates = this.withNoMembers(consumerPatitionOffsets, topic, groupId);\n\t\t\t}\n\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);\n\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);\n\t\t});\n\n\t\treturn topicConsumerGroupStates;\n\t}\n\n\tprivate List<PartitionAssignmentState> withMembers(\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets, String topic, String groupId,\n\t\t\tConsumerGroupDescription description) {\n\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\t\tMap<Integer, Long> consumerPatitionOffsetMap = new HashMap<>();\n\t\tconsumerPatitionOffsets.forEach(entryInfo -> {\n\t\t\tTopicPartition topicPartition = entryInfo.getKey();\n\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();\n\t\t\tconsumerPatitionOffsetMap.put(topicPartition.partition(), offsetAndMetadata.offset());\n\t\t});\n\t\tdescription.members().forEach(memberDescription -> {\n\t\t\tmemberDescription.assignment().topicPartitions().forEach(topicPation -> {\n\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\t\tpartitionAssignmentState.setPartition(topicPation.partition());\n\t\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\t\tpartitionAssignmentState.setClientId(memberDescription.clientId());\n\t\t\t\tpartitionAssignmentState.setGroup(groupId);\n\t\t\t\tpartitionAssignmentState.setConsumerId(memberDescription.consumerId());\n\t\t\t\tpartitionAssignmentState.setHost(memberDescription.host());\n\t\t\t\tpartitionAssignmentState.setOffset(consumerPatitionOffsetMap.get(topicPation.partition()));\n\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t\t});\n\t\t});\n\t\treturn partitionAssignmentStates;\n\t}\n\n\tprivate List<PartitionAssignmentState> withNoMembers(\n\t\t\tSet<Entry<TopicPartition, OffsetAndMetadata>> consumerPatitionOffsets, String topic, String groupId) {\n\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\t\tconsumerPatitionOffsets.forEach(entryInfo -> {\n\t\t\tTopicPartition topicPartition = entryInfo.getKey();\n\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();\n\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\tpartitionAssignmentState.setPartition(topicPartition.partition());\n\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\tpartitionAssignmentState.setGroup(groupId);\n\t\t\tpartitionAssignmentState.setOffset(offsetAndMetadata.offset());\n\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t});\n\t\treturn partitionAssignmentStates;\n\t}\n```\n3. 步骤3\n```\n\tpublic Map<TopicPartition, Long>  consumerOffset (String gorupName,String topicName) {\n\t\tProperties consumerProps = new Properties();\n\t\tconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"192.168.0.1.101:9092\");\n\t\tconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, gorupName);\n\t\tconsumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\n\t\tconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n\t\tconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class);\n\t\tconsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\t\t@SuppressWarnings(\"resource\")\n\t\tKafkaConsumers<String, String>  consumer  = new KafkaConsumers<>(consumerProps);\n\t\tKafkaConsumer<String, String> kafkaConsumer = consumer.subscribe(topicName);\n\t\tList<PartitionInfo> patitions = kafkaConsumer.partitionsFor(topicName);\n\t\tList<TopicPartition>topicPatitions = new ArrayList<>();\n\t\tpatitions.forEach(patition->{\n\t\t\tTopicPartition topicPartition = new TopicPartition(topicName,patition.partition());\n\t\t\ttopicPatitions.add(topicPartition);\n\t\t});\n\t\tMap<TopicPartition, Long> result = kafkaConsumer.endOffsets(topicPatitions);\n\t\treturn result;\n\t}\n```\n4. 步骤4\n```\n\tprivate List<TopicConsumerGroupState> getBrokerConsumerOffsets(String clusterID, String topic) {\n\t\tList<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\ttry {\n\t\t\ttopicConsumerGroupStates = this.describeConsumerGroups(topic);\n\t\t\t// 填充lag/logEndOffset\n\t\t\ttopicConsumerGroupStates.forEach(topicConsumerGroupState -> {\n\t\t\t\tString groupId = topicConsumerGroupState.getGroupId();\n\t\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = topicConsumerGroupState\n\t\t\t\t\t\t.getPartitionAssignmentStates();\n\t\t\t\tMap<TopicPartition, Long> offsetsMap = this.consumerOffset(clusterID, groupId, topic);\n\t\t\t\tfor (Entry<TopicPartition, Long> entry : offsetsMap.entrySet()) {\n\t\t\t\t\tlong logEndOffset = entry.getValue();\n\t\t\t\t\tfor (PartitionAssignmentState partitionAssignmentState : partitionAssignmentStates) {\n\t\t\t\t\t\tif (partitionAssignmentState.getPartition() == entry.getKey().partition()) {\n\t\t\t\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);\n\t\t\t\t\t\t\tpartitionAssignmentState.setLag(getLag(partitionAssignmentState.getOffset(), logEndOffset));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (InterruptedException e) {\n\t\t\te.printStackTrace();\n\t\t} catch (ExecutionException e) {\n\t\t\te.printStackTrace();\n\t\t} catch (TimeoutException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn topicConsumerGroupStates;\n\t}\n```\n### zookeeper消费方式 offset 获取\n#### 实现思路\n1. 根据topic 获取消费该topic的group\n2. 读取zookeeper上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition。\n3. 通过consumer获取LogEndOffset（可见offset）\n4. 将2与3处信息合并，计算Lag\n#### 代码设计\n引入两个依赖,主要是为了读取zookeeper节点上的数据\n```\n\t\t<dependency>\n\t\t\t<groupId>com.101tec</groupId>\n\t\t\t<artifactId>zkclient</artifactId>\n\t\t\t<version>0.11</version>\n\t\t</dependency>\n\t\t<dependency>\n\t\t\t<groupId>org.apache.kafka</groupId>\n\t\t\t<artifactId>kafka_2.11</artifactId>\n\t\t</dependency>\n```\n\n1. 步骤1\n```\npublic Set<String> listTopicGroups(String topic) {\n\t\tSet<String> groups = new HashSet<>();\n\t\tList<String> allGroups = zkClient.getChildren(\"/consumers\");\n\t\tallGroups.forEach(group -> {\n\t\t\tif (zkClient.exists(\"/consumers/\" + group + \"/offsets\")) {\n\t\t\t\tSet<String> offsets = new HashSet<>(zkClient.getChildren(\"/consumers/\" + group + \"/offsets\"));\n\t\t\t\tif (offsets.contains(topic)) {\n\t\t\t\t\tgroups.add(group);\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t\treturn groups;\n\t}\n```\n2. 步骤2\n```\npublic Map<String, Map<String, String>> getZKConsumerOffsets(String groupId, String topic) {\n\t\tMap<String, Map<String, String>> result = new HashMap<>();\n\t\tString offsetsPath = \"/consumers/\" + groupId + \"/offsets/\" + topic;\n\t\tif (zkClient.exists(offsetsPath)) {\n\t\t\tList<String> offsets = zkClient.getChildren(offsetsPath);\n\t\t\toffsets.forEach(patition -> {\n\t\t\t\ttry {\n\t\t\t\t\tString offset = zkClient.readData(offsetsPath + \"/\" + patition, true);\n\t\t\t\t\tif (offset != null) {\n\t\t\t\t\t\tMap<String, String> map = new HashMap<>();\n\t\t\t\t\t\tmap.put(\"offset\", offset);\n\t\t\t\t\t\tresult.put(patition, map);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t});\n\t\t}\n\t\tString ownersPath = \"/consumers/\" + groupId + \"/owners/\" + topic;\n\t\tif (zkClient.exists(ownersPath)) {\n\t\t\tList<String> owners = zkClient.getChildren(ownersPath);\n\t\t\towners.forEach(patition -> {\n\t\t\t\ttry {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tString owner = zkClient.readData(ownersPath + \"/\" + patition, true);\n\t\t\t\t\t\tif (owner != null) {\n\t\t\t\t\t\t\tMap<String, String> map = result.get(patition);\n\t\t\t\t\t\t\tmap.put(\"owner\", owner);\n\t\t\t\t\t\t\tresult.put(patition, map);\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\te.printStackTrace();\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n\n\t\t\t});\n\t\t}\n\n\t\treturn result;\n\t}\n```\n3. 步骤3\n\n略\n4. 步骤4\n```\n\tprivate List<TopicConsumerGroupState> getZKConsumerOffsets(String topic) {\n\t\tfinal List<TopicConsumerGroupState> topicConsumerGroupStates = new ArrayList<>();\n\t\tSet<String> zkGroups = this.listTopicGroups(topic);\n\t\tzkGroups.forEach(group -> {\n\t\t\tMap<String, Map<String, String>> zkConsumerOffsets = this.getZKConsumerOffsets(group,\n\t\t\t\t\ttopic);\n\n\t\t\tMap<TopicPartition, Long> offsetsMap = this.consumerOffset(group, topic);\n\n\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();\n\t\t\ttopicConsumerGroupState.setGroupId(group);\n\t\t\ttopicConsumerGroupState.setConsumerMethod(\"zk\");\n\n\t\t\tList<PartitionAssignmentState> partitionAssignmentStates = new ArrayList<>();\n\n\t\t\tMap<String, Long> offsetsTempMap = new HashMap<>(offsetsMap.size());\n\t\t\toffsetsMap.forEach((k, v) -> {\n\t\t\t\toffsetsTempMap.put(k.partition() + \"\", v);\n\t\t\t});\n\n\t\t\tzkConsumerOffsets.forEach((patition, topicDesribe) -> {\n\t\t\t\tLong logEndOffset = offsetsTempMap.get(patition);\n\t\t\t\tString owner = topicDesribe.get(\"owner\");\n\t\t\t\tlong offset = Long.parseLong(topicDesribe.get(\"offset\"));\n\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();\n\t\t\t\tpartitionAssignmentState.setClientId(owner);\n\t\t\t\tpartitionAssignmentState.setGroup(group);\n\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);\n\t\t\t\tpartitionAssignmentState.setTopic(topic);\n\t\t\t\tpartitionAssignmentState.setOffset(offset);\n\t\t\t\tpartitionAssignmentState.setPartition(Integer.parseInt(patition));\n\t\t\t\tpartitionAssignmentState.setLag(getLag(offset, logEndOffset));\n\n\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);\n\t\t\t});\n\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);\n\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);\n\t\t});\n\t\treturn topicConsumerGroupStates;\n\t}\n```\n## 参考\n1. [如何获取Kafka的消费者详情——从Scala到Java的切换](https://blog.csdn.net/u013256816/article/details/79968647)\n2. [Kafka的Lag计算误区及正确实现](https://blog.csdn.net/u013256816/article/details/79955578)\n","slug":"如何监控kafka消费Lag情况","published":1,"updated":"2019-08-22T12:31:46.182Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzmobvhn00b98cee3zf99b36","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>为什么会有这个需求？</p>\n<p>kafka consumer 消费会存在延迟情况，我们需要查看消息堆积情况，就是所谓的消息Lag。目前是市面上也有相应的监控工具<a href=\"https://github.com/quantifind/KafkaOffsetMonitor\" target=\"_blank\" rel=\"noopener\">KafkaOffsetMonitor</a>，我们自己也写了一套监控<a href=\"https://github.com/xaecbd/kmanager\" target=\"_blank\" rel=\"noopener\">kmanager</a>。但是随着kafka版本的升级，消费方式也发生了很大的变化，因此，我们需要重构一下kafka offset监控。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h2 id=\"如何计算Lag\"><a href=\"#如何计算Lag\" class=\"headerlink\" title=\"如何计算Lag\"></a>如何计算Lag</h2><p>在计算Lag之前先普及几个基本常识</p>\n<p><strong>LEO(LogEndOffset):</strong> 这里说的和官网说的LEO有点区别，主要是指堆consumer可见的offset.即HW(High Watermark)</p>\n<p><strong>CURRENT-OFFSET</strong>: consumer消费到的具体位移</p>\n<p>知道以上信息后，可知<code>Lag=LEO-CURRENT-OFFSET</code>。计算出来的值即为消费延迟情况。</p>\n<h2 id=\"官方查看方式\"><a href=\"#官方查看方式\" class=\"headerlink\" title=\"官方查看方式\"></a>官方查看方式</h2><p>这里说的官方查看方式是在官网文档中提到的，使用官方包里提供的<code>bin/kafka-consumer-groups.sh</code></p>\n<p>最新版的工具只能获取到通过<strong>broker</strong>消费的情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/kafka-consumer-groups.sh --describe --bootstrap-server 192.168.0.101:8092 --group test</span><br><span class=\"line\">Consumer group &apos;test&apos; has no active members.</span><br><span class=\"line\"></span><br><span class=\"line\">TOPIC              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID</span><br><span class=\"line\">truman_test_offset 2          1325            2361            1036            -               -               -</span><br><span class=\"line\">truman_test_offset 6          1265            2289            1024            -               -               -</span><br><span class=\"line\">truman_test_offset 4          1245            2243            998             -               -               -</span><br><span class=\"line\">truman_test_offset 9          1310            2307            997             -               -               -</span><br><span class=\"line\">truman_test_offset 1          1259            2257            998             -               -               -</span><br><span class=\"line\">truman_test_offset 8          1410            2438            1028            -               -               -</span><br><span class=\"line\">truman_test_offset 3          1225            2167            942             -               -               -</span><br><span class=\"line\">truman_test_offset 0          1218            2192            974             -               -               -</span><br><span class=\"line\">truman_test_offset 5          1262            2252            990             -               -               -</span><br><span class=\"line\">truman_test_offset 7          1265            2277            1012            -               -               -</span><br></pre></td></tr></table></figure>\n<h2 id=\"程序查询方式\"><a href=\"#程序查询方式\" class=\"headerlink\" title=\"程序查询方式\"></a>程序查询方式</h2><p>使用程序查询方式，有什么好处，可以实现自己的offset监控,可以无缝接入任何平台系统。</p>\n<p>既然是用程序实现，那么做个更高级的需求，++<strong>根据topic获取不同消费组的消费情况</strong>++。</p>\n<p>先定义两个实体</p>\n<p>TopicConsumerGroupState<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class TopicConsumerGroupState &#123;</span><br><span class=\"line\">\tprivate String groupId;</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 消费方式：zk/broker 主要指的offset提交到哪里 新版本 broker 旧版本zk</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tprivate String consumerMethod;</span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; partitionAssignmentStates;</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * Dead：组内已经没有任何成员的最终状态，组的元数据也已经被coordinator移除了。这种状态响应各种请求都是一个response：</span><br><span class=\"line\">\t * UNKNOWN_MEMBER_ID Empty：组内无成员，但是位移信息还没有过期。这种状态只能响应JoinGroup请求</span><br><span class=\"line\">\t * PreparingRebalance：组准备开启新的rebalance，等待成员加入 AwaitingSync：正在等待leader</span><br><span class=\"line\">\t * consumer将分配方案传给各个成员 Stable：rebalance完成！可以开始消费了~</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tprivate ConsumerGroupState consumerGroupState;</span><br><span class=\"line\"></span><br><span class=\"line\">    //省略set和get方法..</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>PartitionAssignmentState<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class PartitionAssignmentState &#123;</span><br><span class=\"line\">\tprivate String group; </span><br><span class=\"line\">\tprivate String topic;</span><br><span class=\"line\">\tprivate int partition;</span><br><span class=\"line\">\tprivate long offset;</span><br><span class=\"line\">\tprivate long lag;</span><br><span class=\"line\">\tprivate String consumerId;</span><br><span class=\"line\">\tprivate String host;</span><br><span class=\"line\">\tprivate String clientId;</span><br><span class=\"line\">\tprivate long logEndOffset;</span><br><span class=\"line\">\t//省略set和get方法..</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"broker消费方式-offset-获取\"><a href=\"#broker消费方式-offset-获取\" class=\"headerlink\" title=\"broker消费方式 offset 获取\"></a>broker消费方式 offset 获取</h3><h4 id=\"实现思路\"><a href=\"#实现思路\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h4><ol>\n<li>根据topic 获取消费该topic的group</li>\n<li>通过使用KafkaAdminClient的describeConsumerGroups读取broker上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition，host等</li>\n<li>通过consumer获取LogEndOffset（可见offset）</li>\n<li><p>将2与3处信息合并，计算Lag</p>\n<h4 id=\"代码设计\"><a href=\"#代码设计\" class=\"headerlink\" title=\"代码设计\"></a>代码设计</h4><p>引入最新版依赖,使用KafkaAdminClient获取1,2处信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.2.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Set&lt;String&gt; listConsumerGroups(String topic)</span><br><span class=\"line\">\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\tfinal Set&lt;String&gt; filteredGroups = new HashSet&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\tSet&lt;String&gt; allGroups = this.adminClient.listConsumerGroups().all().get(30, TimeUnit.SECONDS).stream()</span><br><span class=\"line\">\t\t\t.map(ConsumerGroupListing::groupId).collect(Collectors.toSet());</span><br><span class=\"line\">\tallGroups.forEach(groupId -&gt; &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tadminClient.listConsumerGroupOffsets(groupId).partitionsToOffsetAndMetadata().get().keySet().stream()</span><br><span class=\"line\">\t\t\t\t\t.filter(tp -&gt; tp.topic().equals(topic)).forEach(tp -&gt; filteredGroups.add(groupId));</span><br><span class=\"line\">\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\treturn filteredGroups;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤2</p>\n</li>\n</ol>\n<p>使用describeConsumerGroup获取的消费情况，其中包含有members和无members情况。正常订阅消费是可以获取到members，但是通过制定patition消费拿不到members.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">\t * 根据topic 获取offset,该结果是不同group 组不同patition的当前消费offset</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param topic</span><br><span class=\"line\">\t * @return Map&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt;</span><br><span class=\"line\">\t * @throws InterruptedException</span><br><span class=\"line\">\t * @throws ExecutionException</span><br><span class=\"line\">\t * @throws TimeoutException</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Map&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; listConsumerGroupOffsets(String topic)</span><br><span class=\"line\">\t\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\t\tSet&lt;String&gt; groupIds = this.listConsumerGroups(topic);</span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; consumerGroupOffsets = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tgroupIds.forEach(groupId -&gt; &#123;</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\tconsumerPatitionOffsets = this.adminClient.listConsumerGroupOffsets(groupId)</span><br><span class=\"line\">\t\t\t\t\t\t.partitionsToOffsetAndMetadata().get(30, TimeUnit.SECONDS).entrySet().stream()</span><br><span class=\"line\">\t\t\t\t\t\t.filter(entry -&gt; topic.equalsIgnoreCase(entry.getKey().topic())).collect(Collectors.toSet());</span><br><span class=\"line\">\t\t\t\t;</span><br><span class=\"line\">\t\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125; catch (TimeoutException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tconsumerGroupOffsets.put(groupId, consumerPatitionOffsets);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn consumerGroupOffsets;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据topic 获取topicConsumerGroupStates,其中未包含lag/logEndOffset(consumer可见offset)</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param topic</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t * @throws InterruptedException</span><br><span class=\"line\">\t * @throws ExecutionException</span><br><span class=\"line\">\t * @throws TimeoutException</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic List&lt;TopicConsumerGroupState&gt; describeConsumerGroups(String topic)</span><br><span class=\"line\">\t\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\t\tfinal List&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tSet&lt;String&gt; groupIds = this.listConsumerGroups(topic);</span><br><span class=\"line\">\t\tMap&lt;String, ConsumerGroupDescription&gt; groupDetails = this.adminClient.describeConsumerGroups(groupIds).all()</span><br><span class=\"line\">\t\t\t\t.get(30, TimeUnit.SECONDS);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; consumerPatitionOffsetMap = this</span><br><span class=\"line\">\t\t\t\t.listConsumerGroupOffsets(topic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tgroupDetails.entrySet().forEach(entry -&gt; &#123;</span><br><span class=\"line\">\t\t\tString groupId = entry.getKey();</span><br><span class=\"line\">\t\t\tConsumerGroupDescription description = entry.getValue();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setGroupId(groupId);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setConsumerMethod(&quot;broker&quot;);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setConsumerGroupState(description.state());</span><br><span class=\"line\">\t\t\t// 获取group下不同patition消费offset信息</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets = consumerPatitionOffsetMap</span><br><span class=\"line\">\t\t\t\t\t.get(groupId);</span><br><span class=\"line\">\t\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tif (!description.members().isEmpty()) &#123;</span><br><span class=\"line\">\t\t\t\t// 获取存在consumer(memeber存在的情况)</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates = this.withMembers(consumerPatitionOffsets, topic, groupId, description);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\t// 获取不存在consumer</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates = this.withNoMembers(consumerPatitionOffsets, topic, groupId);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn topicConsumerGroupStates;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; withMembers(</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets, String topic, String groupId,</span><br><span class=\"line\">\t\t\tConsumerGroupDescription description) &#123;</span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tMap&lt;Integer, Long&gt; consumerPatitionOffsetMap = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tconsumerPatitionOffsets.forEach(entryInfo -&gt; &#123;</span><br><span class=\"line\">\t\t\tTopicPartition topicPartition = entryInfo.getKey();</span><br><span class=\"line\">\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();</span><br><span class=\"line\">\t\t\tconsumerPatitionOffsetMap.put(topicPartition.partition(), offsetAndMetadata.offset());</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\tdescription.members().forEach(memberDescription -&gt; &#123;</span><br><span class=\"line\">\t\t\tmemberDescription.assignment().topicPartitions().forEach(topicPation -&gt; &#123;</span><br><span class=\"line\">\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setPartition(topicPation.partition());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setClientId(memberDescription.clientId());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setGroup(groupId);</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setConsumerId(memberDescription.consumerId());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setHost(memberDescription.host());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setOffset(consumerPatitionOffsetMap.get(topicPation.partition()));</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn partitionAssignmentStates;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; withNoMembers(</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets, String topic, String groupId) &#123;</span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tconsumerPatitionOffsets.forEach(entryInfo -&gt; &#123;</span><br><span class=\"line\">\t\t\tTopicPartition topicPartition = entryInfo.getKey();</span><br><span class=\"line\">\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();</span><br><span class=\"line\">\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setPartition(topicPartition.partition());</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setGroup(groupId);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setOffset(offsetAndMetadata.offset());</span><br><span class=\"line\">\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn partitionAssignmentStates;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li><p>步骤3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Map&lt;TopicPartition, Long&gt;  consumerOffset (String gorupName,String topicName) &#123;</span><br><span class=\"line\">\tProperties consumerProps = new Properties();</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.0.1.101:9092&quot;);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, gorupName);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);</span><br><span class=\"line\">\t@SuppressWarnings(&quot;resource&quot;)</span><br><span class=\"line\">\tKafkaConsumers&lt;String, String&gt;  consumer  = new KafkaConsumers&lt;&gt;(consumerProps);</span><br><span class=\"line\">\tKafkaConsumer&lt;String, String&gt; kafkaConsumer = consumer.subscribe(topicName);</span><br><span class=\"line\">\tList&lt;PartitionInfo&gt; patitions = kafkaConsumer.partitionsFor(topicName);</span><br><span class=\"line\">\tList&lt;TopicPartition&gt;topicPatitions = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\tpatitions.forEach(patition-&gt;&#123;</span><br><span class=\"line\">\t\tTopicPartition topicPartition = new TopicPartition(topicName,patition.partition());</span><br><span class=\"line\">\t\ttopicPatitions.add(topicPartition);</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\tMap&lt;TopicPartition, Long&gt; result = kafkaConsumer.endOffsets(topicPatitions);</span><br><span class=\"line\">\treturn result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private List&lt;TopicConsumerGroupState&gt; getBrokerConsumerOffsets(String clusterID, String topic) &#123;</span><br><span class=\"line\">\tList&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttopicConsumerGroupStates = this.describeConsumerGroups(topic);</span><br><span class=\"line\">\t\t// 填充lag/logEndOffset</span><br><span class=\"line\">\t\ttopicConsumerGroupStates.forEach(topicConsumerGroupState -&gt; &#123;</span><br><span class=\"line\">\t\t\tString groupId = topicConsumerGroupState.getGroupId();</span><br><span class=\"line\">\t\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = topicConsumerGroupState</span><br><span class=\"line\">\t\t\t\t\t.getPartitionAssignmentStates();</span><br><span class=\"line\">\t\t\tMap&lt;TopicPartition, Long&gt; offsetsMap = this.consumerOffset(clusterID, groupId, topic);</span><br><span class=\"line\">\t\t\tfor (Entry&lt;TopicPartition, Long&gt; entry : offsetsMap.entrySet()) &#123;</span><br><span class=\"line\">\t\t\t\tlong logEndOffset = entry.getValue();</span><br><span class=\"line\">\t\t\t\tfor (PartitionAssignmentState partitionAssignmentState : partitionAssignmentStates) &#123;</span><br><span class=\"line\">\t\t\t\t\tif (partitionAssignmentState.getPartition() == entry.getKey().partition()) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);</span><br><span class=\"line\">\t\t\t\t\t\tpartitionAssignmentState.setLag(getLag(partitionAssignmentState.getOffset(), logEndOffset));</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; catch (TimeoutException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn topicConsumerGroupStates;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"zookeeper消费方式-offset-获取\"><a href=\"#zookeeper消费方式-offset-获取\" class=\"headerlink\" title=\"zookeeper消费方式 offset 获取\"></a>zookeeper消费方式 offset 获取</h3><h4 id=\"实现思路-1\"><a href=\"#实现思路-1\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h4><ol>\n<li>根据topic 获取消费该topic的group</li>\n<li>读取zookeeper上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition。</li>\n<li>通过consumer获取LogEndOffset（可见offset）</li>\n<li><p>将2与3处信息合并，计算Lag</p>\n<h4 id=\"代码设计-1\"><a href=\"#代码设计-1\" class=\"headerlink\" title=\"代码设计\"></a>代码设计</h4><p>引入两个依赖,主要是为了读取zookeeper节点上的数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;com.101tec&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;zkclient&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.11&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Set&lt;String&gt; listTopicGroups(String topic) &#123;</span><br><span class=\"line\">\t\tSet&lt;String&gt; groups = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\tList&lt;String&gt; allGroups = zkClient.getChildren(&quot;/consumers&quot;);</span><br><span class=\"line\">\t\tallGroups.forEach(group -&gt; &#123;</span><br><span class=\"line\">\t\t\tif (zkClient.exists(&quot;/consumers/&quot; + group + &quot;/offsets&quot;)) &#123;</span><br><span class=\"line\">\t\t\t\tSet&lt;String&gt; offsets = new HashSet&lt;&gt;(zkClient.getChildren(&quot;/consumers/&quot; + group + &quot;/offsets&quot;));</span><br><span class=\"line\">\t\t\t\tif (offsets.contains(topic)) &#123;</span><br><span class=\"line\">\t\t\t\t\tgroups.add(group);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn groups;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤2</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Map&lt;String, Map&lt;String, String&gt;&gt; getZKConsumerOffsets(String groupId, String topic) &#123;</span><br><span class=\"line\">\t\tMap&lt;String, Map&lt;String, String&gt;&gt; result = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tString offsetsPath = &quot;/consumers/&quot; + groupId + &quot;/offsets/&quot; + topic;</span><br><span class=\"line\">\t\tif (zkClient.exists(offsetsPath)) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; offsets = zkClient.getChildren(offsetsPath);</span><br><span class=\"line\">\t\t\toffsets.forEach(patition -&gt; &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tString offset = zkClient.readData(offsetsPath + &quot;/&quot; + patition, true);</span><br><span class=\"line\">\t\t\t\t\tif (offset != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tMap&lt;String, String&gt; map = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t\tmap.put(&quot;offset&quot;, offset);</span><br><span class=\"line\">\t\t\t\t\t\tresult.put(patition, map);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tString ownersPath = &quot;/consumers/&quot; + groupId + &quot;/owners/&quot; + topic;</span><br><span class=\"line\">\t\tif (zkClient.exists(ownersPath)) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; owners = zkClient.getChildren(ownersPath);</span><br><span class=\"line\">\t\t\towners.forEach(patition -&gt; &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\t\tString owner = zkClient.readData(ownersPath + &quot;/&quot; + patition, true);</span><br><span class=\"line\">\t\t\t\t\t\tif (owner != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tMap&lt;String, String&gt; map = result.get(patition);</span><br><span class=\"line\">\t\t\t\t\t\t\tmap.put(&quot;owner&quot;, owner);</span><br><span class=\"line\">\t\t\t\t\t\t\tresult.put(patition, map);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn result;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤3</p>\n</li>\n</ol>\n<p>略</p>\n<ol start=\"4\">\n<li>步骤4<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private List&lt;TopicConsumerGroupState&gt; getZKConsumerOffsets(String topic) &#123;</span><br><span class=\"line\">\tfinal List&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\tSet&lt;String&gt; zkGroups = this.listTopicGroups(topic);</span><br><span class=\"line\">\tzkGroups.forEach(group -&gt; &#123;</span><br><span class=\"line\">\t\tMap&lt;String, Map&lt;String, String&gt;&gt; zkConsumerOffsets = this.getZKConsumerOffsets(group,</span><br><span class=\"line\">\t\t\t\ttopic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;TopicPartition, Long&gt; offsetsMap = this.consumerOffset(group, topic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setGroupId(group);</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setConsumerMethod(&quot;zk&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;String, Long&gt; offsetsTempMap = new HashMap&lt;&gt;(offsetsMap.size());</span><br><span class=\"line\">\t\toffsetsMap.forEach((k, v) -&gt; &#123;</span><br><span class=\"line\">\t\t\toffsetsTempMap.put(k.partition() + &quot;&quot;, v);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tzkConsumerOffsets.forEach((patition, topicDesribe) -&gt; &#123;</span><br><span class=\"line\">\t\t\tLong logEndOffset = offsetsTempMap.get(patition);</span><br><span class=\"line\">\t\t\tString owner = topicDesribe.get(&quot;owner&quot;);</span><br><span class=\"line\">\t\t\tlong offset = Long.parseLong(topicDesribe.get(&quot;offset&quot;));</span><br><span class=\"line\">\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setClientId(owner);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setGroup(group);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setOffset(offset);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setPartition(Integer.parseInt(patition));</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setLag(getLag(offset, logEndOffset));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);</span><br><span class=\"line\">\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\treturn topicConsumerGroupStates;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/79968647\" target=\"_blank\" rel=\"noopener\">如何获取Kafka的消费者详情——从Scala到Java的切换</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/79955578\" target=\"_blank\" rel=\"noopener\">Kafka的Lag计算误区及正确实现</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>为什么会有这个需求？</p>\n<p>kafka consumer 消费会存在延迟情况，我们需要查看消息堆积情况，就是所谓的消息Lag。目前是市面上也有相应的监控工具<a href=\"https://github.com/quantifind/KafkaOffsetMonitor\" target=\"_blank\" rel=\"noopener\">KafkaOffsetMonitor</a>，我们自己也写了一套监控<a href=\"https://github.com/xaecbd/kmanager\" target=\"_blank\" rel=\"noopener\">kmanager</a>。但是随着kafka版本的升级，消费方式也发生了很大的变化，因此，我们需要重构一下kafka offset监控。</p>\n<h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h2 id=\"如何计算Lag\"><a href=\"#如何计算Lag\" class=\"headerlink\" title=\"如何计算Lag\"></a>如何计算Lag</h2><p>在计算Lag之前先普及几个基本常识</p>\n<p><strong>LEO(LogEndOffset):</strong> 这里说的和官网说的LEO有点区别，主要是指堆consumer可见的offset.即HW(High Watermark)</p>\n<p><strong>CURRENT-OFFSET</strong>: consumer消费到的具体位移</p>\n<p>知道以上信息后，可知<code>Lag=LEO-CURRENT-OFFSET</code>。计算出来的值即为消费延迟情况。</p>\n<h2 id=\"官方查看方式\"><a href=\"#官方查看方式\" class=\"headerlink\" title=\"官方查看方式\"></a>官方查看方式</h2><p>这里说的官方查看方式是在官网文档中提到的，使用官方包里提供的<code>bin/kafka-consumer-groups.sh</code></p>\n<p>最新版的工具只能获取到通过<strong>broker</strong>消费的情况</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ bin/kafka-consumer-groups.sh --describe --bootstrap-server 192.168.0.101:8092 --group test</span><br><span class=\"line\">Consumer group &apos;test&apos; has no active members.</span><br><span class=\"line\"></span><br><span class=\"line\">TOPIC              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID</span><br><span class=\"line\">truman_test_offset 2          1325            2361            1036            -               -               -</span><br><span class=\"line\">truman_test_offset 6          1265            2289            1024            -               -               -</span><br><span class=\"line\">truman_test_offset 4          1245            2243            998             -               -               -</span><br><span class=\"line\">truman_test_offset 9          1310            2307            997             -               -               -</span><br><span class=\"line\">truman_test_offset 1          1259            2257            998             -               -               -</span><br><span class=\"line\">truman_test_offset 8          1410            2438            1028            -               -               -</span><br><span class=\"line\">truman_test_offset 3          1225            2167            942             -               -               -</span><br><span class=\"line\">truman_test_offset 0          1218            2192            974             -               -               -</span><br><span class=\"line\">truman_test_offset 5          1262            2252            990             -               -               -</span><br><span class=\"line\">truman_test_offset 7          1265            2277            1012            -               -               -</span><br></pre></td></tr></table></figure>\n<h2 id=\"程序查询方式\"><a href=\"#程序查询方式\" class=\"headerlink\" title=\"程序查询方式\"></a>程序查询方式</h2><p>使用程序查询方式，有什么好处，可以实现自己的offset监控,可以无缝接入任何平台系统。</p>\n<p>既然是用程序实现，那么做个更高级的需求，++<strong>根据topic获取不同消费组的消费情况</strong>++。</p>\n<p>先定义两个实体</p>\n<p>TopicConsumerGroupState<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class TopicConsumerGroupState &#123;</span><br><span class=\"line\">\tprivate String groupId;</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 消费方式：zk/broker 主要指的offset提交到哪里 新版本 broker 旧版本zk</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tprivate String consumerMethod;</span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; partitionAssignmentStates;</span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * Dead：组内已经没有任何成员的最终状态，组的元数据也已经被coordinator移除了。这种状态响应各种请求都是一个response：</span><br><span class=\"line\">\t * UNKNOWN_MEMBER_ID Empty：组内无成员，但是位移信息还没有过期。这种状态只能响应JoinGroup请求</span><br><span class=\"line\">\t * PreparingRebalance：组准备开启新的rebalance，等待成员加入 AwaitingSync：正在等待leader</span><br><span class=\"line\">\t * consumer将分配方案传给各个成员 Stable：rebalance完成！可以开始消费了~</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tprivate ConsumerGroupState consumerGroupState;</span><br><span class=\"line\"></span><br><span class=\"line\">    //省略set和get方法..</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>PartitionAssignmentState<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class PartitionAssignmentState &#123;</span><br><span class=\"line\">\tprivate String group; </span><br><span class=\"line\">\tprivate String topic;</span><br><span class=\"line\">\tprivate int partition;</span><br><span class=\"line\">\tprivate long offset;</span><br><span class=\"line\">\tprivate long lag;</span><br><span class=\"line\">\tprivate String consumerId;</span><br><span class=\"line\">\tprivate String host;</span><br><span class=\"line\">\tprivate String clientId;</span><br><span class=\"line\">\tprivate long logEndOffset;</span><br><span class=\"line\">\t//省略set和get方法..</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"broker消费方式-offset-获取\"><a href=\"#broker消费方式-offset-获取\" class=\"headerlink\" title=\"broker消费方式 offset 获取\"></a>broker消费方式 offset 获取</h3><h4 id=\"实现思路\"><a href=\"#实现思路\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h4><ol>\n<li>根据topic 获取消费该topic的group</li>\n<li>通过使用KafkaAdminClient的describeConsumerGroups读取broker上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition，host等</li>\n<li>通过consumer获取LogEndOffset（可见offset）</li>\n<li><p>将2与3处信息合并，计算Lag</p>\n<h4 id=\"代码设计\"><a href=\"#代码设计\" class=\"headerlink\" title=\"代码设计\"></a>代码设计</h4><p>引入最新版依赖,使用KafkaAdminClient获取1,2处信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.2.0&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Set&lt;String&gt; listConsumerGroups(String topic)</span><br><span class=\"line\">\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\tfinal Set&lt;String&gt; filteredGroups = new HashSet&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\tSet&lt;String&gt; allGroups = this.adminClient.listConsumerGroups().all().get(30, TimeUnit.SECONDS).stream()</span><br><span class=\"line\">\t\t\t.map(ConsumerGroupListing::groupId).collect(Collectors.toSet());</span><br><span class=\"line\">\tallGroups.forEach(groupId -&gt; &#123;</span><br><span class=\"line\">\t\ttry &#123;</span><br><span class=\"line\">\t\t\tadminClient.listConsumerGroupOffsets(groupId).partitionsToOffsetAndMetadata().get().keySet().stream()</span><br><span class=\"line\">\t\t\t\t\t.filter(tp -&gt; tp.topic().equals(topic)).forEach(tp -&gt; filteredGroups.add(groupId));</span><br><span class=\"line\">\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\treturn filteredGroups;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤2</p>\n</li>\n</ol>\n<p>使用describeConsumerGroup获取的消费情况，其中包含有members和无members情况。正常订阅消费是可以获取到members，但是通过制定patition消费拿不到members.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/**</span><br><span class=\"line\">\t * 根据topic 获取offset,该结果是不同group 组不同patition的当前消费offset</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param topic</span><br><span class=\"line\">\t * @return Map&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt;</span><br><span class=\"line\">\t * @throws InterruptedException</span><br><span class=\"line\">\t * @throws ExecutionException</span><br><span class=\"line\">\t * @throws TimeoutException</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic Map&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; listConsumerGroupOffsets(String topic)</span><br><span class=\"line\">\t\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\t\tSet&lt;String&gt; groupIds = this.listConsumerGroups(topic);</span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; consumerGroupOffsets = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tgroupIds.forEach(groupId -&gt; &#123;</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\tconsumerPatitionOffsets = this.adminClient.listConsumerGroupOffsets(groupId)</span><br><span class=\"line\">\t\t\t\t\t\t.partitionsToOffsetAndMetadata().get(30, TimeUnit.SECONDS).entrySet().stream()</span><br><span class=\"line\">\t\t\t\t\t\t.filter(entry -&gt; topic.equalsIgnoreCase(entry.getKey().topic())).collect(Collectors.toSet());</span><br><span class=\"line\">\t\t\t\t;</span><br><span class=\"line\">\t\t\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125; catch (TimeoutException e) &#123;</span><br><span class=\"line\">\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tconsumerGroupOffsets.put(groupId, consumerPatitionOffsets);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn consumerGroupOffsets;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t/**</span><br><span class=\"line\">\t * 根据topic 获取topicConsumerGroupStates,其中未包含lag/logEndOffset(consumer可见offset)</span><br><span class=\"line\">\t * </span><br><span class=\"line\">\t * @param topic</span><br><span class=\"line\">\t * @return</span><br><span class=\"line\">\t * @throws InterruptedException</span><br><span class=\"line\">\t * @throws ExecutionException</span><br><span class=\"line\">\t * @throws TimeoutException</span><br><span class=\"line\">\t */</span><br><span class=\"line\">\tpublic List&lt;TopicConsumerGroupState&gt; describeConsumerGroups(String topic)</span><br><span class=\"line\">\t\t\tthrows InterruptedException, ExecutionException, TimeoutException &#123;</span><br><span class=\"line\">\t\tfinal List&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tSet&lt;String&gt; groupIds = this.listConsumerGroups(topic);</span><br><span class=\"line\">\t\tMap&lt;String, ConsumerGroupDescription&gt; groupDetails = this.adminClient.describeConsumerGroups(groupIds).all()</span><br><span class=\"line\">\t\t\t\t.get(30, TimeUnit.SECONDS);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;String, Set&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt;&gt; consumerPatitionOffsetMap = this</span><br><span class=\"line\">\t\t\t\t.listConsumerGroupOffsets(topic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tgroupDetails.entrySet().forEach(entry -&gt; &#123;</span><br><span class=\"line\">\t\t\tString groupId = entry.getKey();</span><br><span class=\"line\">\t\t\tConsumerGroupDescription description = entry.getValue();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setGroupId(groupId);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setConsumerMethod(&quot;broker&quot;);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setConsumerGroupState(description.state());</span><br><span class=\"line\">\t\t\t// 获取group下不同patition消费offset信息</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets = consumerPatitionOffsetMap</span><br><span class=\"line\">\t\t\t\t\t.get(groupId);</span><br><span class=\"line\">\t\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tif (!description.members().isEmpty()) &#123;</span><br><span class=\"line\">\t\t\t\t// 获取存在consumer(memeber存在的情况)</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates = this.withMembers(consumerPatitionOffsets, topic, groupId, description);</span><br><span class=\"line\">\t\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\t\t// 获取不存在consumer</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates = this.withNoMembers(consumerPatitionOffsets, topic, groupId);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);</span><br><span class=\"line\">\t\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn topicConsumerGroupStates;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; withMembers(</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets, String topic, String groupId,</span><br><span class=\"line\">\t\t\tConsumerGroupDescription description) &#123;</span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tMap&lt;Integer, Long&gt; consumerPatitionOffsetMap = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tconsumerPatitionOffsets.forEach(entryInfo -&gt; &#123;</span><br><span class=\"line\">\t\t\tTopicPartition topicPartition = entryInfo.getKey();</span><br><span class=\"line\">\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();</span><br><span class=\"line\">\t\t\tconsumerPatitionOffsetMap.put(topicPartition.partition(), offsetAndMetadata.offset());</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\tdescription.members().forEach(memberDescription -&gt; &#123;</span><br><span class=\"line\">\t\t\tmemberDescription.assignment().topicPartitions().forEach(topicPation -&gt; &#123;</span><br><span class=\"line\">\t\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setPartition(topicPation.partition());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setClientId(memberDescription.clientId());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setGroup(groupId);</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setConsumerId(memberDescription.consumerId());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setHost(memberDescription.host());</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentState.setOffset(consumerPatitionOffsetMap.get(topicPation.partition()));</span><br><span class=\"line\">\t\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn partitionAssignmentStates;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tprivate List&lt;PartitionAssignmentState&gt; withNoMembers(</span><br><span class=\"line\">\t\t\tSet&lt;Entry&lt;TopicPartition, OffsetAndMetadata&gt;&gt; consumerPatitionOffsets, String topic, String groupId) &#123;</span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\t\tconsumerPatitionOffsets.forEach(entryInfo -&gt; &#123;</span><br><span class=\"line\">\t\t\tTopicPartition topicPartition = entryInfo.getKey();</span><br><span class=\"line\">\t\t\tOffsetAndMetadata offsetAndMetadata = entryInfo.getValue();</span><br><span class=\"line\">\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setPartition(topicPartition.partition());</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setGroup(groupId);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setOffset(offsetAndMetadata.offset());</span><br><span class=\"line\">\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn partitionAssignmentStates;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure></p>\n<ol start=\"3\">\n<li><p>步骤3</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Map&lt;TopicPartition, Long&gt;  consumerOffset (String gorupName,String topicName) &#123;</span><br><span class=\"line\">\tProperties consumerProps = new Properties();</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.0.1.101:9092&quot;);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, gorupName);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class);</span><br><span class=\"line\">\tconsumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;);</span><br><span class=\"line\">\t@SuppressWarnings(&quot;resource&quot;)</span><br><span class=\"line\">\tKafkaConsumers&lt;String, String&gt;  consumer  = new KafkaConsumers&lt;&gt;(consumerProps);</span><br><span class=\"line\">\tKafkaConsumer&lt;String, String&gt; kafkaConsumer = consumer.subscribe(topicName);</span><br><span class=\"line\">\tList&lt;PartitionInfo&gt; patitions = kafkaConsumer.partitionsFor(topicName);</span><br><span class=\"line\">\tList&lt;TopicPartition&gt;topicPatitions = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\tpatitions.forEach(patition-&gt;&#123;</span><br><span class=\"line\">\t\tTopicPartition topicPartition = new TopicPartition(topicName,patition.partition());</span><br><span class=\"line\">\t\ttopicPatitions.add(topicPartition);</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\tMap&lt;TopicPartition, Long&gt; result = kafkaConsumer.endOffsets(topicPatitions);</span><br><span class=\"line\">\treturn result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤4</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private List&lt;TopicConsumerGroupState&gt; getBrokerConsumerOffsets(String clusterID, String topic) &#123;</span><br><span class=\"line\">\tList&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\ttry &#123;</span><br><span class=\"line\">\t\ttopicConsumerGroupStates = this.describeConsumerGroups(topic);</span><br><span class=\"line\">\t\t// 填充lag/logEndOffset</span><br><span class=\"line\">\t\ttopicConsumerGroupStates.forEach(topicConsumerGroupState -&gt; &#123;</span><br><span class=\"line\">\t\t\tString groupId = topicConsumerGroupState.getGroupId();</span><br><span class=\"line\">\t\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = topicConsumerGroupState</span><br><span class=\"line\">\t\t\t\t\t.getPartitionAssignmentStates();</span><br><span class=\"line\">\t\t\tMap&lt;TopicPartition, Long&gt; offsetsMap = this.consumerOffset(clusterID, groupId, topic);</span><br><span class=\"line\">\t\t\tfor (Entry&lt;TopicPartition, Long&gt; entry : offsetsMap.entrySet()) &#123;</span><br><span class=\"line\">\t\t\t\tlong logEndOffset = entry.getValue();</span><br><span class=\"line\">\t\t\t\tfor (PartitionAssignmentState partitionAssignmentState : partitionAssignmentStates) &#123;</span><br><span class=\"line\">\t\t\t\t\tif (partitionAssignmentState.getPartition() == entry.getKey().partition()) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);</span><br><span class=\"line\">\t\t\t\t\t\tpartitionAssignmentState.setLag(getLag(partitionAssignmentState.getOffset(), logEndOffset));</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t&#125; catch (InterruptedException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; catch (ExecutionException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125; catch (TimeoutException e) &#123;</span><br><span class=\"line\">\t\te.printStackTrace();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn topicConsumerGroupStates;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"zookeeper消费方式-offset-获取\"><a href=\"#zookeeper消费方式-offset-获取\" class=\"headerlink\" title=\"zookeeper消费方式 offset 获取\"></a>zookeeper消费方式 offset 获取</h3><h4 id=\"实现思路-1\"><a href=\"#实现思路-1\" class=\"headerlink\" title=\"实现思路\"></a>实现思路</h4><ol>\n<li>根据topic 获取消费该topic的group</li>\n<li>读取zookeeper上指定group和topic的消费情况，可以获取到clientId,CURRENT-OFFSET,patition。</li>\n<li>通过consumer获取LogEndOffset（可见offset）</li>\n<li><p>将2与3处信息合并，计算Lag</p>\n<h4 id=\"代码设计-1\"><a href=\"#代码设计-1\" class=\"headerlink\" title=\"代码设计\"></a>代码设计</h4><p>引入两个依赖,主要是为了读取zookeeper节点上的数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;com.101tec&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;zkclient&lt;/artifactId&gt;</span><br><span class=\"line\">\t&lt;version&gt;0.11&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">\t&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">\t&lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Set&lt;String&gt; listTopicGroups(String topic) &#123;</span><br><span class=\"line\">\t\tSet&lt;String&gt; groups = new HashSet&lt;&gt;();</span><br><span class=\"line\">\t\tList&lt;String&gt; allGroups = zkClient.getChildren(&quot;/consumers&quot;);</span><br><span class=\"line\">\t\tallGroups.forEach(group -&gt; &#123;</span><br><span class=\"line\">\t\t\tif (zkClient.exists(&quot;/consumers/&quot; + group + &quot;/offsets&quot;)) &#123;</span><br><span class=\"line\">\t\t\t\tSet&lt;String&gt; offsets = new HashSet&lt;&gt;(zkClient.getChildren(&quot;/consumers/&quot; + group + &quot;/offsets&quot;));</span><br><span class=\"line\">\t\t\t\tif (offsets.contains(topic)) &#123;</span><br><span class=\"line\">\t\t\t\t\tgroups.add(group);</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\treturn groups;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤2</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public Map&lt;String, Map&lt;String, String&gt;&gt; getZKConsumerOffsets(String groupId, String topic) &#123;</span><br><span class=\"line\">\t\tMap&lt;String, Map&lt;String, String&gt;&gt; result = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\tString offsetsPath = &quot;/consumers/&quot; + groupId + &quot;/offsets/&quot; + topic;</span><br><span class=\"line\">\t\tif (zkClient.exists(offsetsPath)) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; offsets = zkClient.getChildren(offsetsPath);</span><br><span class=\"line\">\t\t\toffsets.forEach(patition -&gt; &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\tString offset = zkClient.readData(offsetsPath + &quot;/&quot; + patition, true);</span><br><span class=\"line\">\t\t\t\t\tif (offset != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\tMap&lt;String, String&gt; map = new HashMap&lt;&gt;();</span><br><span class=\"line\">\t\t\t\t\t\tmap.put(&quot;offset&quot;, offset);</span><br><span class=\"line\">\t\t\t\t\t\tresult.put(patition, map);</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tString ownersPath = &quot;/consumers/&quot; + groupId + &quot;/owners/&quot; + topic;</span><br><span class=\"line\">\t\tif (zkClient.exists(ownersPath)) &#123;</span><br><span class=\"line\">\t\t\tList&lt;String&gt; owners = zkClient.getChildren(ownersPath);</span><br><span class=\"line\">\t\t\towners.forEach(patition -&gt; &#123;</span><br><span class=\"line\">\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\ttry &#123;</span><br><span class=\"line\">\t\t\t\t\t\tString owner = zkClient.readData(ownersPath + &quot;/&quot; + patition, true);</span><br><span class=\"line\">\t\t\t\t\t\tif (owner != null) &#123;</span><br><span class=\"line\">\t\t\t\t\t\t\tMap&lt;String, String&gt; map = result.get(patition);</span><br><span class=\"line\">\t\t\t\t\t\t\tmap.put(&quot;owner&quot;, owner);</span><br><span class=\"line\">\t\t\t\t\t\t\tresult.put(patition, map);</span><br><span class=\"line\">\t\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t&#125; catch (Exception e) &#123;</span><br><span class=\"line\">\t\t\t\t\te.printStackTrace();</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t&#125;);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\treturn result;</span><br><span class=\"line\">\t&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>步骤3</p>\n</li>\n</ol>\n<p>略</p>\n<ol start=\"4\">\n<li>步骤4<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">private List&lt;TopicConsumerGroupState&gt; getZKConsumerOffsets(String topic) &#123;</span><br><span class=\"line\">\tfinal List&lt;TopicConsumerGroupState&gt; topicConsumerGroupStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\">\tSet&lt;String&gt; zkGroups = this.listTopicGroups(topic);</span><br><span class=\"line\">\tzkGroups.forEach(group -&gt; &#123;</span><br><span class=\"line\">\t\tMap&lt;String, Map&lt;String, String&gt;&gt; zkConsumerOffsets = this.getZKConsumerOffsets(group,</span><br><span class=\"line\">\t\t\t\ttopic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;TopicPartition, Long&gt; offsetsMap = this.consumerOffset(group, topic);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tTopicConsumerGroupState topicConsumerGroupState = new TopicConsumerGroupState();</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setGroupId(group);</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setConsumerMethod(&quot;zk&quot;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tList&lt;PartitionAssignmentState&gt; partitionAssignmentStates = new ArrayList&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tMap&lt;String, Long&gt; offsetsTempMap = new HashMap&lt;&gt;(offsetsMap.size());</span><br><span class=\"line\">\t\toffsetsMap.forEach((k, v) -&gt; &#123;</span><br><span class=\"line\">\t\t\toffsetsTempMap.put(k.partition() + &quot;&quot;, v);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tzkConsumerOffsets.forEach((patition, topicDesribe) -&gt; &#123;</span><br><span class=\"line\">\t\t\tLong logEndOffset = offsetsTempMap.get(patition);</span><br><span class=\"line\">\t\t\tString owner = topicDesribe.get(&quot;owner&quot;);</span><br><span class=\"line\">\t\t\tlong offset = Long.parseLong(topicDesribe.get(&quot;offset&quot;));</span><br><span class=\"line\">\t\t\tPartitionAssignmentState partitionAssignmentState = new PartitionAssignmentState();</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setClientId(owner);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setGroup(group);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setLogEndOffset(logEndOffset);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setTopic(topic);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setOffset(offset);</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setPartition(Integer.parseInt(patition));</span><br><span class=\"line\">\t\t\tpartitionAssignmentState.setLag(getLag(offset, logEndOffset));</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tpartitionAssignmentStates.add(partitionAssignmentState);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t\ttopicConsumerGroupState.setPartitionAssignmentStates(partitionAssignmentStates);</span><br><span class=\"line\">\t\ttopicConsumerGroupStates.add(topicConsumerGroupState);</span><br><span class=\"line\">\t&#125;);</span><br><span class=\"line\">\treturn topicConsumerGroupStates;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ol>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/79968647\" target=\"_blank\" rel=\"noopener\">如何获取Kafka的消费者详情——从Scala到Java的切换</a></li>\n<li><a href=\"https://blog.csdn.net/u013256816/article/details/79955578\" target=\"_blank\" rel=\"noopener\">Kafka的Lag计算误区及正确实现</a></li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjzmobv4d00008cee6m6qnwe4","category_id":"cjzmobv4k00028ceekmdai5yg","_id":"cjzmobv4v000c8cee4ipxlqq6"},{"post_id":"cjzmobv4s000a8ceeqej0o8nc","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv50000j8cee720u9jm5"},{"post_id":"cjzmobv4i00018ceeacz9tw2i","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv54000o8ceexuxvo75p"},{"post_id":"cjzmobv4u000b8cee0eiaze1m","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv56000r8ceeechlwup0"},{"post_id":"cjzmobv4x000g8cee3ztn5p5y","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv59000w8ceeo7dj2n6p"},{"post_id":"cjzmobv4n00048ceeqw0lbrcy","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobv5c000z8ceepdv1z6qa"},{"post_id":"cjzmobv4o00058ceeks8s3o4d","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv5e00148cee6eh3a1y2"},{"post_id":"cjzmobv4q00068ceeiv809oxt","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobv5g00178cee204bquqx"},{"post_id":"cjzmobv4z000i8cee8ttfb5ic","category_id":"cjzmobv5c00108ceecpk18hws","_id":"cjzmobv5l001e8ceeyldm3hjw"},{"post_id":"cjzmobv5f00168ceegh2q6auc","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobv5m001g8cee6qd1mx04"},{"post_id":"cjzmobv53000n8ceepk9bffm8","category_id":"cjzmobv5c00108ceecpk18hws","_id":"cjzmobv5m001i8cee9xztmf4y"},{"post_id":"cjzmobv5j001c8ceene9rkw4g","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobv5o001l8ceexhlfqmie"},{"post_id":"cjzmobv55000q8ceem89l20c2","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobv5p001o8cee3psgktcb"},{"post_id":"cjzmobv58000v8ceelyhsla51","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobv5r001s8ceelfetranr"},{"post_id":"cjzmobv5a000y8ceedhi9e0in","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobv5t001w8ceeugus40cb"},{"post_id":"cjzmobv5d00138ceeh1jnkgm2","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobv5u001z8ceei4oparqg"},{"post_id":"cjzmobv5h00198cee8axmefrq","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobv5v00228ceed47o86de"},{"post_id":"cjzmobvaw00268ceedsojr8ys","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobvb6002g8cee79eiloyn"},{"post_id":"cjzmobvb0002a8ceenfphscxu","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobvb8002k8ceelzx4xpsl"},{"post_id":"cjzmobvat00248cee4xzkkqvq","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvba002m8ceebsaammtg"},{"post_id":"cjzmobvb5002f8ceekvv058gw","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvbd002q8cee56rns0vc"},{"post_id":"cjzmobvb7002j8ceeej094w85","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvbe002t8ceezh5b20z7"},{"post_id":"cjzmobvb9002l8ceesb8j0l9y","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvbh002x8ceeu0lf67vr"},{"post_id":"cjzmobvb2002d8cee4cyktd50","category_id":"cjzmobvb6002h8ceem3ffbtq3","_id":"cjzmobvbk00318ceejiu6e3hi"},{"post_id":"cjzmobvbb002o8ceeo5gnbfvd","category_id":"cjzmobvbf002u8ceeal8co9kh","_id":"cjzmobvbq00398ceespy6ewo2"},{"post_id":"cjzmobvbg002w8ceekmfxaorl","category_id":"cjzmobvbm00348ceekbjt2kj4","_id":"cjzmobvbw003g8cee7f0v8cme"},{"post_id":"cjzmobvbl00338ceejc5rt3dc","category_id":"cjzmobvbm00348ceekbjt2kj4","_id":"cjzmobvc5003u8ceeyqm0dhp3"},{"post_id":"cjzmobvc2003p8ceemcvk3i8p","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvca00408cee5b9cvgzr"},{"post_id":"cjzmobvbn00368ceee8ut5p9s","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvcc00448ceegchpcnkn"},{"post_id":"cjzmobvc4003s8ceez5ggzfog","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvcf00478cee7rwydqgd"},{"post_id":"cjzmobvc6003w8ceeknjr3mca","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvch004b8ceevh1kltc5"},{"post_id":"cjzmobvbp00388ceeyuu0zezg","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvcj004e8cee9gd35v6h"},{"post_id":"cjzmobvc9003z8ceejt4nu3mh","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvcl004h8cee7ef5fjsu"},{"post_id":"cjzmobvcb00438cee0n5kglf6","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvco004l8cee2kqkcoch"},{"post_id":"cjzmobvbt003d8ceeqpdayxo3","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvcp004o8ceekgdhtfg1"},{"post_id":"cjzmobvcg004a8ceebn1psif5","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvcs004s8ceex7m6scr0"},{"post_id":"cjzmobvbj00308ceeajiknuky","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvcu004v8ceeye3kk47p"},{"post_id":"cjzmobvbj00308ceeajiknuky","category_id":"cjzmobvcg00498ceevf8vm30b","_id":"cjzmobvcx004z8ceeid3zk7yu"},{"post_id":"cjzmobvcl004g8ceerj8e6sah","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvcy00528ceeqlufa73n"},{"post_id":"cjzmobvcn004k8ceemgtq3ieo","category_id":"cjzmobv4k00028ceekmdai5yg","_id":"cjzmobvd000568ceemxnhzgxx"},{"post_id":"cjzmobvbv003f8cee6co6gr9c","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvd200598cee5vaj7e64"},{"post_id":"cjzmobvcp004n8ceerxfm0crh","category_id":"cjzmobv4k00028ceekmdai5yg","_id":"cjzmobvd4005d8ceehzj3h0yb"},{"post_id":"cjzmobvbx003j8ceecg8n0b3q","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvd6005g8ceelu81ijk8"},{"post_id":"cjzmobvcw004y8cee56q7f8yq","category_id":"cjzmobv5k001d8ceeux24pe1l","_id":"cjzmobvd8005k8ceer2dcvy9z"},{"post_id":"cjzmobvbz003m8ceerwail3vn","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvda005n8cee4zve5ldb"},{"post_id":"cjzmobvcy00518ceeg4ycfxq2","category_id":"cjzmobv5c00108ceecpk18hws","_id":"cjzmobvdc005r8ceexn22kodg"},{"post_id":"cjzmobvd000558ceeojck08yc","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvde005u8ceepxba86k4"},{"post_id":"cjzmobvcd00468ceeanr8fu3p","category_id":"cjzmobvcz00538ceesyazglay","_id":"cjzmobvdg005x8cee8k4umyl5"},{"post_id":"cjzmobvd100588cee8o8h4xyp","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvdi00618ceenp1w8exm"},{"post_id":"cjzmobvd3005c8ceekqlkko6w","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvdj00648ceepaubdkab"},{"post_id":"cjzmobvd5005f8ceevp7rye92","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvdm00688ceef5cr3l3z"},{"post_id":"cjzmobvcr004r8cee83ie48du","category_id":"cjzmobvd6005i8ceebdi2f2ne","_id":"cjzmobvdo006c8ceevqljn4c6"},{"post_id":"cjzmobvd9005m8cee2d5dmuhf","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvdr006g8ceejiazvmmc"},{"post_id":"cjzmobvdb005q8ceea98du10a","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvds006j8cee3k6pvrth"},{"post_id":"cjzmobvdd005t8ceea0ovbc87","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvdv006n8cee862bzdf0"},{"post_id":"cjzmobvdf005w8ceeoj7afcr8","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvdy006q8ceeuru252yt"},{"post_id":"cjzmobvdh00608ceehjkfs34s","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobve0006t8cee0ezah8g0"},{"post_id":"cjzmobvdi00638ceelpv06h1u","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobve2006y8cee8tbl910o"},{"post_id":"cjzmobvdl00678ceexh4cllr7","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobve500718ceecaei17cb"},{"post_id":"cjzmobvd7005j8ceengkhyyp5","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobve600748ceejgexksm7"},{"post_id":"cjzmobvd7005j8ceengkhyyp5","category_id":"cjzmobvdk00658ceet8e9kjqx","_id":"cjzmobve800778ceeh2w8qypm"},{"post_id":"cjzmobvdn006b8cee8npkks9y","category_id":"cjzmobvcz00538ceesyazglay","_id":"cjzmobvea007a8ceeglrlk4c8"},{"post_id":"cjzmobvdq006f8ceefkl0y6tt","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvec007e8cee1207ihlm"},{"post_id":"cjzmobvdr006i8ceezo6hr0fk","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvee007h8ceeksxzht0z"},{"post_id":"cjzmobvdu006m8cee2xctp73z","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobveg007l8cee9fwelvbc"},{"post_id":"cjzmobvdx006p8cee9h5cftd9","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvei007o8ceedltk0hvh"},{"post_id":"cjzmobvci004d8ceencry8x8q","category_id":"cjzmobvd2005a8ceecwj0xwgb","_id":"cjzmobvek007s8cee9xr703dg"},{"post_id":"cjzmobvci004d8ceencry8x8q","category_id":"cjzmobvdg005y8ceelt5mqmgx","_id":"cjzmobvem007v8ceevq22yocl"},{"post_id":"cjzmobvci004d8ceencry8x8q","category_id":"cjzmobvdt006l8ceeeiiz6pcr","_id":"cjzmobven007y8cee5df62bm9"},{"post_id":"cjzmobvdz006s8ceef73wwfiu","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvep00828cee34u2ohi8"},{"post_id":"cjzmobve1006x8ceewv58z16v","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobver00858ceel2l6n9y4"},{"post_id":"cjzmobvct004u8ceen8figdj1","category_id":"cjzmobvdb005p8ceejeces10c","_id":"cjzmobveu00898ceer7q5fvcq"},{"post_id":"cjzmobvct004u8ceen8figdj1","category_id":"cjzmobvdp006e8ceexdcqc98b","_id":"cjzmobvew008d8ceer1fj1kx0"},{"post_id":"cjzmobvct004u8ceen8figdj1","category_id":"cjzmobve0006u8ceexotx8m69","_id":"cjzmobvez008h8ceeefq1hy1q"},{"post_id":"cjzmobve300708ceercnsk2yb","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvf1008l8ceerx5o0bui"},{"post_id":"cjzmobve600738ceeiqkygl73","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvf4008p8cee1dytk4i8"},{"post_id":"cjzmobve700768ceekpbronzj","category_id":"cjzmobveb007c8ceeziikyu7h","_id":"cjzmobvf6008s8ceen5zwddyo"},{"post_id":"cjzmobve900798ceeyvztslbi","category_id":"cjzmobveb007c8ceeziikyu7h","_id":"cjzmobvf8008w8ceeed1rrc8n"},{"post_id":"cjzmobveb007d8ceeor1oornt","category_id":"cjzmobvek007r8ceeyd8q7n04","_id":"cjzmobvfa00908ceezdgxwvff"},{"post_id":"cjzmobved007g8ceeh1jmmnrv","category_id":"cjzmobvek007r8ceeyd8q7n04","_id":"cjzmobvfc00948ceeeqf6w3ag"},{"post_id":"cjzmobvet00888ceeprf0f24l","category_id":"cjzmobv4w000d8cee7ttihvfu","_id":"cjzmobvfe00988cees6062yrl"},{"post_id":"cjzmobvef007j8cees0yrdwcw","category_id":"cjzmobvek007r8ceeyd8q7n04","_id":"cjzmobvfh009b8cee1ptecs7g"},{"post_id":"cjzmobvey008g8ceemul5cl1o","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvfi009d8cee8tshpfb6"},{"post_id":"cjzmobveh007n8ceeqkgp220t","category_id":"cjzmobvex008e8cee0lj4vm4h","_id":"cjzmobvfk009g8ceegq81ltd3"},{"post_id":"cjzmobvf0008j8cee7haumd5e","category_id":"cjzmobvek007r8ceeyd8q7n04","_id":"cjzmobvfl009j8ceekrckswky"},{"post_id":"cjzmobvej007q8cee1p50d8js","category_id":"cjzmobvek007r8ceeyd8q7n04","_id":"cjzmobvfm009l8cee1h7kc69n"},{"post_id":"cjzmobvel007u8cee4knz3i7e","category_id":"cjzmobvf7008u8ceeyf3o0n7p","_id":"cjzmobvfn009o8ceenbhk4z27"},{"post_id":"cjzmobvf9008z8ceemcbcxppt","category_id":"cjzmobvbr003a8cee4fkycz6h","_id":"cjzmobvfo009q8cee6x6hkse3"},{"post_id":"cjzmobvfd00968ceewr1a89ek","category_id":"cjzmobvb6002h8ceem3ffbtq3","_id":"cjzmobvfp009t8ceelmfxioym"},{"post_id":"cjzmobvem007x8cee5a5g6z9b","category_id":"cjzmobvfb00928ceecckaipki","_id":"cjzmobvfq009v8cee5sw3riye"},{"post_id":"cjzmobvep00818cee0hn1y2m3","category_id":"cjzmobvfh009a8ceenakuz4cn","_id":"cjzmobvfr009y8ceebkbbrmmh"},{"post_id":"cjzmobveq00848ceew711iw19","category_id":"cjzmobvfk009h8ceea1qvd1i1","_id":"cjzmobvfr009z8cee24f91a8l"},{"post_id":"cjzmobvew008c8ceeaacmmm2s","category_id":"cjzmobvfm009n8cee9u5lxk94","_id":"cjzmobvfs00a08ceeojee88k5"},{"post_id":"cjzmobvf3008o8cee2i3wawyw","category_id":"cjzmobvfo009s8ceekj9v0t3o","_id":"cjzmobvfs00a28ceevjwatd6q"},{"post_id":"cjzmobvf5008r8cee812j5558","category_id":"cjzmobvfq009x8cee32jj65k3","_id":"cjzmobvft00a38ceeaac72vv2"},{"post_id":"cjzmobvf7008v8ceemh1v768u","category_id":"cjzmobvfs00a18ceeewafz21x","_id":"cjzmobvfu00a58ceey8e9k0gp"},{"post_id":"cjzmobvfc00938cee6te99ur8","category_id":"cjzmobvft00a48ceevm1xso0r","_id":"cjzmobvfu00a68ceenniozvyt"},{"post_id":"cjzmobvga00a88ceepfv0rfj1","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvgg00af8ceeod7u5182"},{"post_id":"cjzmobvgb00ab8ceeyfjbgdtw","category_id":"cjzmobvbf002u8ceeal8co9kh","_id":"cjzmobvgi00aj8ceeodh3rh0r"},{"post_id":"cjzmobvgd00ac8ceelq5czgsv","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvgk00am8ceefvg33cci"},{"post_id":"cjzmobvg800a78ceef1yojrjj","category_id":"cjzmobvgb00a98ceetrbm7d88","_id":"cjzmobvgm00ap8cee9dqnvw58"},{"post_id":"cjzmobvgf00ae8ceem373fd1t","category_id":"cjzmobvdb005p8ceejeces10c","_id":"cjzmobvgo00as8ceezz3rbizy"},{"post_id":"cjzmobvgf00ae8ceem373fd1t","category_id":"cjzmobvdp006e8ceexdcqc98b","_id":"cjzmobvgp00au8ceey1japtcu"},{"post_id":"cjzmobvgf00ae8ceem373fd1t","category_id":"cjzmobve0006u8ceexotx8m69","_id":"cjzmobvgp00aw8ceedx1qzi6x"},{"post_id":"cjzmobvgh00ai8cee2jnl17tw","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvgq00ay8ceeenk8colw"},{"post_id":"cjzmobvgj00al8ceewbq5xooh","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvgr00b08cee8ipo1pvd"},{"post_id":"cjzmobvgl00ao8cee8yh58d0l","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvgs00b28ceepioa58aq"},{"post_id":"cjzmobvgn00ar8ceet23wokk0","category_id":"cjzmobv4r00078ceeifczslqh","_id":"cjzmobvgs00b48ceea2r19rql"},{"post_id":"cjzmobvhm00b88cee7rqvkh0q","category_id":"cjzmobvc1003o8ceekw8pz4y3","_id":"cjzmobvhp00bc8ceeyheu4pxf"},{"post_id":"cjzmobvhn00b98cee3zf99b36","category_id":"cjzmobvaz00288ceegrgrrmou","_id":"cjzmobvhq00bd8cee9c7maes3"}],"PostTag":[{"post_id":"cjzmobv4d00008cee6m6qnwe4","tag_id":"cjzmobv4n00038ceeqshjpl5i","_id":"cjzmobv4s00098cee5tec17je"},{"post_id":"cjzmobv4s000a8ceeqej0o8nc","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobv4w000f8ceeb9lugtee"},{"post_id":"cjzmobv4i00018ceeacz9tw2i","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobv4z000h8ceeqm9661kg"},{"post_id":"cjzmobv4u000b8cee0eiaze1m","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobv52000m8ceewsmybmxf"},{"post_id":"cjzmobv4x000g8cee3ztn5p5y","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobv55000p8cee55mqya78"},{"post_id":"cjzmobv4n00048ceeqw0lbrcy","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobv58000u8ceeo5txy7kp"},{"post_id":"cjzmobv53000n8ceepk9bffm8","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobv5a000x8ceern4sv5ua"},{"post_id":"cjzmobv4o00058ceeks8s3o4d","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobv5d00118ceem4ywt3tj"},{"post_id":"cjzmobv4q00068ceeiv809oxt","tag_id":"cjzmobv57000t8ceew0k8k08e","_id":"cjzmobv5f00158cee2ktlbb6t"},{"post_id":"cjzmobv4z000i8cee8ttfb5ic","tag_id":"cjzmobv5d00128ceer1gxc2n4","_id":"cjzmobv5j001b8cee7uw3fje2"},{"post_id":"cjzmobv55000q8ceem89l20c2","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobv5m001h8ceeadjbjybj"},{"post_id":"cjzmobv58000v8ceelyhsla51","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobv5o001m8cee1oj4tr7f"},{"post_id":"cjzmobv5a000y8ceedhi9e0in","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobv5q001q8cee55idem97"},{"post_id":"cjzmobv5d00138ceeh1jnkgm2","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobv5s001u8cee1c5nnwuv"},{"post_id":"cjzmobv5f00168ceegh2q6auc","tag_id":"cjzmobv5r001t8ceezhkls35a","_id":"cjzmobv5u001y8cee4m9q2gho"},{"post_id":"cjzmobv5h00198cee8axmefrq","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobv5v00218cee932p0e6s"},{"post_id":"cjzmobv5j001c8ceene9rkw4g","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobv5w00238ceel4tdzps0"},{"post_id":"cjzmobvat00248cee4xzkkqvq","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvaz00298cee1nqtp42a"},{"post_id":"cjzmobvaw00268ceedsojr8ys","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvb1002c8ceedwngd7y9"},{"post_id":"cjzmobvb0002a8ceenfphscxu","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvb4002e8cee0xxm3fzf"},{"post_id":"cjzmobvb7002j8ceeej094w85","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvbb002n8ceestxu2rwx"},{"post_id":"cjzmobvb9002l8ceesb8j0l9y","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvbd002r8ceetonjs1ww"},{"post_id":"cjzmobvb2002d8cee4cyktd50","tag_id":"cjzmobvb7002i8cee1pgq06kx","_id":"cjzmobvbf002v8ceesqvd0btt"},{"post_id":"cjzmobvbb002o8ceeo5gnbfvd","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvbi002z8ceesz0puwit"},{"post_id":"cjzmobvbg002w8ceekmfxaorl","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvbl00328ceeha2s6mrq"},{"post_id":"cjzmobvb5002f8ceekvv058gw","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvbp00378cee5y1cbyr2"},{"post_id":"cjzmobvb5002f8ceekvv058gw","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvbr003b8cee4bkrr8sy"},{"post_id":"cjzmobvbl00338ceejc5rt3dc","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvbu003e8ceepnhu37ek"},{"post_id":"cjzmobvbe002s8ceehi743wza","tag_id":"cjzmobvbn00358ceed1oabsq2","_id":"cjzmobvbw003h8ceew488gvcg"},{"post_id":"cjzmobvbp00388ceeyuu0zezg","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvby003l8cee6rdb2wqu"},{"post_id":"cjzmobvbv003f8cee6co6gr9c","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvc1003n8cee83ayxyfk"},{"post_id":"cjzmobvbj00308ceeajiknuky","tag_id":"cjzmobvbs003c8ceewa1564br","_id":"cjzmobvc3003r8ceenairgr81"},{"post_id":"cjzmobvbz003m8ceerwail3vn","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvc5003t8ceeh81n6non"},{"post_id":"cjzmobvbn00368ceee8ut5p9s","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvc8003y8ceegnplrpjj"},{"post_id":"cjzmobvc2003p8ceemcvk3i8p","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvca00418ceeghyh6afh"},{"post_id":"cjzmobvc4003s8ceez5ggzfog","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvcc00458ceei0yir29k"},{"post_id":"cjzmobvbt003d8ceeqpdayxo3","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvcf00488cee1s7jlbtg"},{"post_id":"cjzmobvc6003w8ceeknjr3mca","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvci004c8cee8650gi6f"},{"post_id":"cjzmobvc9003z8ceejt4nu3mh","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvck004f8ceeudhxyhx9"},{"post_id":"cjzmobvbx003j8ceecg8n0b3q","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvcm004j8ceee17fyrzn"},{"post_id":"cjzmobvcb00438cee0n5kglf6","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvco004m8cee53w3uta9"},{"post_id":"cjzmobvcd00468ceeanr8fu3p","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvcq004q8ceefb2xe7pt"},{"post_id":"cjzmobvcg004a8ceebn1psif5","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvcs004t8cee68roem8g"},{"post_id":"cjzmobvci004d8ceencry8x8q","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvcv004x8ceegajunmdd"},{"post_id":"cjzmobvcl004g8ceerj8e6sah","tag_id":"cjzmobvb7002i8cee1pgq06kx","_id":"cjzmobvcx00508ceeflxjhgrb"},{"post_id":"cjzmobvcn004k8ceemgtq3ieo","tag_id":"cjzmobv4n00038ceeqshjpl5i","_id":"cjzmobvcz00548cee64icnant"},{"post_id":"cjzmobvcp004n8ceerxfm0crh","tag_id":"cjzmobv4n00038ceeqshjpl5i","_id":"cjzmobvd100578ceemesp6d5k"},{"post_id":"cjzmobvcr004r8cee83ie48du","tag_id":"cjzmobvbs003c8ceewa1564br","_id":"cjzmobvd3005b8ceemtgrhunr"},{"post_id":"cjzmobvct004u8ceen8figdj1","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvd4005e8cees40fvr0d"},{"post_id":"cjzmobvcw004y8cee56q7f8yq","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvd6005h8ceexeymfzeh"},{"post_id":"cjzmobvcy00518ceeg4ycfxq2","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvd9005l8ceec2lrn955"},{"post_id":"cjzmobvd000558ceeojck08yc","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvdb005o8cee1ph1xk4z"},{"post_id":"cjzmobvd100588cee8o8h4xyp","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvdd005s8ceekj1dr8bh"},{"post_id":"cjzmobvd3005c8ceekqlkko6w","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvde005v8cee89dmm9tn"},{"post_id":"cjzmobvd5005f8ceevp7rye92","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobvdg005z8ceei0iktdke"},{"post_id":"cjzmobvd7005j8ceengkhyyp5","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvdi00628ceemm5k9rst"},{"post_id":"cjzmobvd9005m8cee2d5dmuhf","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvdk00668ceeh3l319rv"},{"post_id":"cjzmobvd9005m8cee2d5dmuhf","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvdn00698cee51rvcia7"},{"post_id":"cjzmobvdb005q8ceea98du10a","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvdp006d8cees5pe2fil"},{"post_id":"cjzmobvdb005q8ceea98du10a","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvdr006h8ceeb9e2eibg"},{"post_id":"cjzmobvdd005t8ceea0ovbc87","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvdt006k8ceeo5gwadlu"},{"post_id":"cjzmobvdd005t8ceea0ovbc87","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvdv006o8ceeutdusvyl"},{"post_id":"cjzmobvdf005w8ceeoj7afcr8","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvdy006r8ceeaqkaia1f"},{"post_id":"cjzmobvdh00608ceehjkfs34s","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobve0006v8cee012utwf4"},{"post_id":"cjzmobvdh00608ceehjkfs34s","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobve3006z8cee7vaotrev"},{"post_id":"cjzmobvdl00678ceexh4cllr7","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobve500728ceehhc4m5wy"},{"post_id":"cjzmobvdn006b8cee8npkks9y","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobve700758ceev6srvpsf"},{"post_id":"cjzmobvdq006f8ceefkl0y6tt","tag_id":"cjzmobv5v00208ceevsuwigvt","_id":"cjzmobve900788cee7ho0upbg"},{"post_id":"cjzmobvdi00638ceelpv06h1u","tag_id":"cjzmobvdn006a8ceefnsbbpq8","_id":"cjzmobveb007b8cee2kk0mjnh"},{"post_id":"cjzmobvdi00638ceelpv06h1u","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobved007f8ceekl91al8a"},{"post_id":"cjzmobvdr006i8ceezo6hr0fk","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvee007i8ceehpfz7f4k"},{"post_id":"cjzmobvdu006m8cee2xctp73z","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobveg007m8ceemvuczs1a"},{"post_id":"cjzmobvdz006s8ceef73wwfiu","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvei007p8ceecbn5m33u"},{"post_id":"cjzmobve1006x8ceewv58z16v","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvek007t8ceev1lzva9m"},{"post_id":"cjzmobve300708ceercnsk2yb","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvem007w8cee93l1c6qv"},{"post_id":"cjzmobvdx006p8cee9h5cftd9","tag_id":"cjzmobve1006w8ceeh34iga35","_id":"cjzmobveo00808ceekdd1j5vq"},{"post_id":"cjzmobve600738ceeiqkygl73","tag_id":"cjzmobvb7002i8cee1pgq06kx","_id":"cjzmobveq00838ceelyjmer5m"},{"post_id":"cjzmobve700768ceekpbronzj","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobves00878ceesdvyqe8m"},{"post_id":"cjzmobve900798ceeyvztslbi","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvev008a8ceedel351tl"},{"post_id":"cjzmobveb007d8ceeor1oornt","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvex008f8ceek2njeriv"},{"post_id":"cjzmobved007g8ceeh1jmmnrv","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvez008i8ceexsck5ect"},{"post_id":"cjzmobvef007j8cees0yrdwcw","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvf2008m8ceem1zsvqvt"},{"post_id":"cjzmobveh007n8ceeqkgp220t","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvf4008q8ceeh28jh7ep"},{"post_id":"cjzmobvej007q8cee1p50d8js","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvf6008t8ceewohbg4z6"},{"post_id":"cjzmobvel007u8cee4knz3i7e","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvf9008x8ceeodbcrt45"},{"post_id":"cjzmobvem007x8cee5a5g6z9b","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvfb00918ceepdqfox6t"},{"post_id":"cjzmobvep00818cee0hn1y2m3","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvfd00958ceeap8tfa4t"},{"post_id":"cjzmobvet00888ceeprf0f24l","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvfg00998cee90xlnh9u"},{"post_id":"cjzmobvey008g8ceemul5cl1o","tag_id":"cjzmobve1006w8ceeh34iga35","_id":"cjzmobvfi009c8ceeluenfag3"},{"post_id":"cjzmobveq00848ceew711iw19","tag_id":"cjzmobvev008b8ceejmwytxs3","_id":"cjzmobvfj009f8ceevybbahpp"},{"post_id":"cjzmobvf0008j8cee7haumd5e","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvfk009i8cee5hb9cuos"},{"post_id":"cjzmobvf3008o8cee2i3wawyw","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvfl009k8ceee0cqprl0"},{"post_id":"cjzmobvew008c8ceeaacmmm2s","tag_id":"cjzmobvf0008k8cee6jv46aq2","_id":"cjzmobvfm009m8cee3qfekfbj"},{"post_id":"cjzmobvf5008r8cee812j5558","tag_id":"cjzmobvf9008y8ceex3pzi8b9","_id":"cjzmobvfn009p8ceep07qjxhg"},{"post_id":"cjzmobvfd00968ceewr1a89ek","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvfo009r8ceets9kzrqt"},{"post_id":"cjzmobvf9008z8ceemcbcxppt","tag_id":"cjzmobvfe00978ceeia6wqj4o","_id":"cjzmobvfp009u8cee4499ti6e"},{"post_id":"cjzmobvfc00938cee6te99ur8","tag_id":"cjzmobvfj009e8ceeh9v8fimh","_id":"cjzmobvfq009w8ceef06g05d9"},{"post_id":"cjzmobvgb00ab8ceeyfjbgdtw","tag_id":"cjzmobv5i001a8ceesx1mmagm","_id":"cjzmobvge00ad8ceey4rj6o2k"},{"post_id":"cjzmobvgd00ac8ceelq5czgsv","tag_id":"cjzmobvby003k8ceewts9hke9","_id":"cjzmobvgh00ah8ceecyuzdokn"},{"post_id":"cjzmobvg800a78ceef1yojrjj","tag_id":"cjzmobvgb00aa8cee643gzylu","_id":"cjzmobvgj00ak8ceedzyn8hep"},{"post_id":"cjzmobvg800a78ceef1yojrjj","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvgk00an8ceecijsc2cz"},{"post_id":"cjzmobvg800a78ceef1yojrjj","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvgn00aq8ceeugcwsdwd"},{"post_id":"cjzmobvgf00ae8ceem373fd1t","tag_id":"cjzmobv4w000e8ceeqgkdqgyp","_id":"cjzmobvgo00at8ceehvscz23q"},{"post_id":"cjzmobvgh00ai8cee2jnl17tw","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvgp00av8ceeweab5g3a"},{"post_id":"cjzmobvgh00ai8cee2jnl17tw","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvgq00ax8ceemor9wa6c"},{"post_id":"cjzmobvga00a88ceepfv0rfj1","tag_id":"cjzmobvgg00ag8cee89nn8rhb","_id":"cjzmobvgq00az8ceev183n3bb"},{"post_id":"cjzmobvga00a88ceepfv0rfj1","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvgr00b18ceevb7j7c89"},{"post_id":"cjzmobvgj00al8ceewbq5xooh","tag_id":"cjzmobvbc002p8ceebqmnhpn7","_id":"cjzmobvgs00b38ceeqc4wi1b7"},{"post_id":"cjzmobvgj00al8ceewbq5xooh","tag_id":"cjzmobvbh002y8ceeobjqqw61","_id":"cjzmobvgt00b58cee66zwfqde"},{"post_id":"cjzmobvgl00ao8cee8yh58d0l","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvgt00b68cee3ec8fte4"},{"post_id":"cjzmobvgn00ar8ceet23wokk0","tag_id":"cjzmobv4r00088ceesuagogbf","_id":"cjzmobvgu00b78ceexhhnig2m"},{"post_id":"cjzmobvhm00b88cee7rqvkh0q","tag_id":"cjzmobvgg00ag8cee89nn8rhb","_id":"cjzmobvho00ba8ceez1ogunkz"},{"post_id":"cjzmobvhn00b98cee3zf99b36","tag_id":"cjzmobvgg00ag8cee89nn8rhb","_id":"cjzmobvhp00bb8cee9pycqf07"}],"Tag":[{"name":"虚拟化","_id":"cjzmobv4n00038ceeqshjpl5i"},{"name":"research","_id":"cjzmobv4r00088ceesuagogbf"},{"name":"开发笔记","_id":"cjzmobv4w000e8ceeqgkdqgyp"},{"name":"经验","_id":"cjzmobv57000t8ceew0k8k08e"},{"name":"命令","_id":"cjzmobv5d00128ceer1gxc2n4"},{"name":"大数据","_id":"cjzmobv5i001a8ceesx1mmagm"},{"name":"编程","_id":"cjzmobv5r001t8ceezhkls35a"},{"name":"笔记","_id":"cjzmobv5v00208ceevsuwigvt"},{"name":"教程","_id":"cjzmobvb7002i8cee1pgq06kx"},{"name":"kafka","_id":"cjzmobvbc002p8ceebqmnhpn7"},{"name":"专栏","_id":"cjzmobvbh002y8ceeobjqqw61"},{"name":"写作技巧","_id":"cjzmobvbn00358ceed1oabsq2"},{"name":"问题答疑","_id":"cjzmobvbs003c8ceewa1564br"},{"name":"nosql","_id":"cjzmobvby003k8ceewts9hke9"},{"name":"专刊","_id":"cjzmobvdn006a8ceefnsbbpq8"},{"name":"运维","_id":"cjzmobve1006w8ceeh34iga35"},{"name":"散心","_id":"cjzmobvev008b8ceejmwytxs3"},{"name":"部署与维护","_id":"cjzmobvf0008k8cee6jv46aq2"},{"name":"爬虫","_id":"cjzmobvf9008y8ceex3pzi8b9"},{"name":"基础理论知识","_id":"cjzmobvfe00978ceeia6wqj4o"},{"name":"算法","_id":"cjzmobvfj009e8ceeh9v8fimh"},{"name":"实时分析","_id":"cjzmobvgb00aa8cee643gzylu"},{"name":"技术分享","_id":"cjzmobvgg00ag8cee89nn8rhb"}]}}